# Descriptive Statistics {#descriptive-stat}
When you have an area of interest that you to research, a problem that you want to solve, a relationship that you want to investigate, theoretical and empirical processes will help you.  

 | Discrete Variable | Continuous Variable
--- | --- | ---
$E(Y)$ | $\sum_{i=1}^k y_i p_i$ | $\int_{-\infty}^{\infty} f(y) dy$
$\overline{y}$ | $\frac{1}{n} \sum_{i = 1}^{n} y_i$ | $\frac{1}{n} \sum_{i = 1}^{n} y_i$



## Numerical Measures

There is a difference between a population and a sample

Measures of | Category | Population | Sample
---|---|---|---
- |What is it? | Reality | A small fraction of reality (inference)
- |Characteristics described by | Parameters | Statistics
Central Tendency | Mean | $\mu = E(Y)$ | $\hat{\mu} = \overline{y}$
Central Tendency | Median | 50-th percentile | $y_{(\frac{n+1}{2})}$
Dispersion | Variance | $\sigma^2=var(Y)$ \ $=E(Y-\mu)^2$ | $s^2=\frac{1}{n-1} \sum_{i = 1}^{n} (y_i-\overline{y})^2$ \ $=\frac{1}{n-1} \sum_{i = 1}^{n} (y_i^2-n\overline{y}^2)$
Dispersion | Coefficient of Variation | $\frac{\sigma}{\mu}$ | $\frac{s}{\overline{y}}$
Dispersion | Interquartile Range | difference between 25th and 75th percentiles. Robust to outliers | 
Shape | Skewness \ Standardized 3rd central moment (unitless) | $g_1=\frac{\mu_3}{\mu_2^{3/2}}$ | $\hat{g_1}=\frac{m_3}{m_2sqrt(m_2)}$
Shape | Central moments | $\mu=E(Y)$ \ $\mu_2 = \sigma^2=E(Y-\mu)^2$ \ $\mu_3 = E(Y-\mu)^3$ \ $\mu_4 = E(Y-\mu)^4$  | $m_2=\sum_{i=1}^{n}(y_1-\overline{y})^2/n$ \  $m_3=\sum_{i=1}^{n}(y_1-\overline{y})^3/n$
Shape | Kurtosis (peakedness and tail thickness) \ Standardized 4th central moment | $g_2^*=\frac{E(Y-\mu)^4}{\sigma^4}$ | $\hat{g_2}=\frac{m_4}{m_2^2}-3$



Note:  

 * Order Statistics: $y_{(1)},y_{(2)},...,y_{(n)}$ where $y_{(1)}<y_{(2)}<...<y_{(n)}$
 * Coefficient of variation: standard deviation over mean. This metric is stable, dimensionless statistic for comparison.
 * Symmetric: mean = median, skewness = 0
 * Skewed right: mean > median, skewness > 0
 * Skewed left: mean < median, skewness < 0
 * Central moments: $\mu=E(Y)$ , $\mu_2 = \sigma^2=E(Y-\mu)^2$ , $\mu_3 = E(Y-\mu)^3$, $\mu_4 = E(Y-\mu)^4$
 * For normal distributions, $\mu_3=0$, so $g_1=0$
 * $\hat{g_1}$ is distributed approximately as N(0,6/n) if sample is from a normal population. (valid when n > 150)  
    + For large samples, inferece on skewness can be based on normal tables with 95% confidence itnerval for $g_1$ as $\hat{g_1}\pm1.96\sqrt{6/n}$
    + For small samples, special tables from Snedecor and Cochran 1989, Table A 19(i) or Monte Carlo test

 | | |
---|---|---
Kurtosis > 0 (leptokurtic) | heavier tail | compared to a normal distribution with the same $\sigma$ (e.g., t-distribution)
Kurtosis < 0 (platykurtic) | lighter tail | compared to a normal distribution with the same $\sigma$

 * For a normal distribution, $g_2^*=3$. Kurtosis is often redefined as: $g_2=\frac{E(Y-\mu)^4}{\sigma^4}-3$ where the 4th central moment is estimated by $m_4=\sum_{i=1}^{n}(y_i-\overline{y})^4/n$
    + the asymptotic sampling distribution for $\hat{g_2}$ is approximately N(0,24/n) (with n > 1000)
    + large sample on kurtosis uses standard normal tables
    + small sample uses tables by Snedecor and Cochran, 1989, Table A 19(ii) or Geary 1936

## Graphical Measures
### Shape 
It's a good habit to label your graph, so others can easily follow.

```{r eval = T}
data = rnorm(100)

# Histogram
hist(data,labels = T,col="grey",breaks = 12) 

# Interactive histogram  
pacman::p_load("highcharter")
hchart(data) 

# Box-and-Whisker plot
boxplot(count ~ spray, data = InsectSprays,col = "lightgray",main="boxplot")

# Notched Boxplot
boxplot(len~supp*dose, data=ToothGrowth, notch=TRUE,
  col=(c("gold","darkgreen")),
  main="Tooth Growth", xlab="Suppliment and Dose")
# If notches differ -> medians differ

# Stem-and-Leaf Plots
stem(data)


# Bagplot - A 2D Boxplot Extension
pacman::p_load(aplpack)
attach(mtcars)
bagplot(wt,mpg, xlab="Car Weight", ylab="Miles Per Gallon",
  main="Bagplot Example")

```


Others more advanced plots
```{r}
# boxplot.matrix()  #library("sfsmisc")
# boxplot.n()       #library("gplots")
# vioplot()         #library("vioplot")
```

## Normality Assessment

Since Normal (Gaussian) distribution has many applications, we typically want/ wish our data or our variable is normal. Hence, we have to assess the normality based on not only [Numerical Measures] but also [Graphical Measures]

### Graphical Assessment
```{r}
pacman::p_load("car")
qqnorm(precip, ylab = "Precipitation [in/yr] for 70 US cities")
qqline(precip)
```
The straight line represents the theoretical line for normally distributed data. The dots represent real empirical data that we are checking.
If all the dots fall on the straight line, we can be confident that our data follow a normal distribution. If our data wiggle and deviate from the line, we should be concerned with the normality assumption. 

### Summary Statistics
Sometimes it's hard to tell whether your data follow the normal distribution by just looking at the graph. Hence, we often have to conduct statistical test to aid our decision.
Common tests are

 * [Methods based on normal probability plot]  
    + [Correlation Coefficient with Normal Probability Plots]  
    + [Shapiro-Wilk Test]  
 * [Methods based on empirical cumulative distribution function]  
    + [Anderson-Darling Test]  
    + [Kolmogorov-Smirnov Test]  
    + [Cramer-von Mises Test]

#### Methods based on normal probability plot
##### Correlation Coefficient with Normal Probability Plots

##### Shapiro-Wilk Test

#### Methods based on empirical cumulative distribution function
##### Anderson-Darling Test

##### Kolmogorov-Smirnov Test


##### Cramer-von Mises Test


