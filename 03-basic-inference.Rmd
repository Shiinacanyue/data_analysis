# Basic Statistical Inference

 * [One Sample Inference]  
 * [Two Sample Inference]  
 * [Categorical Data Analysis]  

Random sample of size n: A collection of n independent random variables taken from the distribution X, each with the same distribution as X.  

**Sample mean**   

$$
\bar{X}= (\sum_{i=1}^{n}X_i)/n
$$

**Sample Median**  

$\tilde{x}$ = the middle observation in a sample of observation order from smallest to largest (or vice versa).   

If n is odd, $\tilde{x}$ is the middle observation,  
If n is even, $\tilde{x}$ is the average of the two middle observations.

**Sample variance**
$$
S^2 = \frac{\sum_{i=1}^{n}(X_i = \bar{X})^2}{n-1}= \frac{n\sum_{i=1}^{n}X_i^2 -(\sum_{i=1}^{n}X_i)^2}{n(n-1)}
$$

**Sample standard deviation**
$$
S = \sqrt{S^2}
$$

**Sample proportions**
$$
\hat{p} = \frac{X}{n} = \frac{\text{number in the sample with trait}}{\text{sample size}}
$$


$$
\widehat{p_1-p_2} = \hat{p_1}-\hat{p_2} = \frac{X_1}{n_1} - \frac{X_2}{n_2} = \frac{n_2X_1 = n_1X_2}{n_1n_2}
$$

**Estimators**  
**Point Estimator**  
$\hat{\theta}$ is a statistic used to approximate a population parameter $\theta$

<br>

**Point estimate**  
The numerical value assumed by $\hat{\theta}$ when evaluated for a given sample

<br>

**Unbiased estimator**  
If $E(\hat{\theta}) = \theta$, then $\hat{\theta}$ is an unbiased estimator for $\theta$   

 1. $\bar{X}$ is an unbiased estimator for $\mu$
 2. $S^2$ is an unbiased estimator for $\sigma^2$
 3. $\hat{p}$ is an unbiased estimator for p
 4. $\widehat{p_1-P_2}$ is an unbiased estimator for $p_1- p_2$
 5. $\bar{X_1} - \bar{X_2}$ is an unbiased estimator for $\mu_1 - \mu_2$

**Note**: $S$ is a biased estimator for $\sigma$

**Distribution of the sample mean**  

If $\bar{X}$ is the sample mean based on a random sample of size n drawn from a normal distribution X with mean $\mu$ and standard deviation $\sigma$, the $\bar{X}$ is normally distributed, with mean $\mu_{\bar{X}} = \mu$ and variance $\sigma_{\bar{X}}^2 = Var(\bar{X}) = \frac{\sigma^2}{n}$. Then the **standard error of the mean** is: $\sigma_{\bar{X}}= \frac{\sigma}{\sqrt{n}}$



## One Sample Inference 

$Y_i \sim  i.i.d. N(\mu, \sigma^2)$

i.i.d. standards for "independent and identically distributed"

Hence, we have the following model:  

$Y_i=\mu +\epsilon_i$ where  

 * $\epsilon_i \sim^{iid} N(0,\sigma^2)$  
 * $E(Y_i)=\mu$  
 * $Var(Y_i)=\sigma^2$  
 * $\bar{y} \sim N(\mu,\sigma^2/n)$  

<br>

### Interval Estimation of the Mean

When $\sigma^2$ is estimated by $s^2$, then  

$$
\frac{\bar{y}-\mu}{s/\sqrt{n}} \sim t_{n-1}
$$

Then, a $100(1-\alpha) \%$ confidence interval for $\mu$ is obtained from:  

$$
1 - \alpha = P(-t_{\alpha/2;n-1} \le \frac{\bar{y}-\mu}{s/\sqrt{n}} \le t_{\alpha/2;n-1}) \\
= P(\bar{y} - (t_{\alpha/2;n-1})s/\sqrt{n} \le \mu \le \bar{y} + (t_{\alpha/2;n-1})s/\sqrt{n})
$$

And the interval is  

$$
\bar{y} \pm (t_{\alpha/2;n-1})s/\sqrt{n}
$$

and $s/\sqrt{n}$ is the standard error of $\bar{y}$  

If the experiment were repeated many times, $100(1-\alpha) \%$ of these intervals would contain $\mu$


### Interval Estimation for the Variance

$$
1 - \alpha = P( \chi_{1-\alpha/2;n-1}^2) \le (n-1)s^2/\sigma^2 \le \chi_{\alpha/2;n-1}^2) \\
= P(\frac{(n-1)s^2}{\chi_{\alpha/2}^2} \le \sigma^2 \le \frac{(n-1)s^2}{\chi_{1-\alpha/2}^2})
$$

and a $100(1-\alpha) \%$ confidence interval for $\sigma^2$ is:  

$$
(\frac{(n-1)s^2}{\chi_{\alpha/2;n-1}^2},\frac{(n-1)s^2}{\chi_{1-\alpha/2;n-1}^2})
$$
Confidence limits for $\sigma^2$ are obtained by computing the positive square roots of these limits









## Two Sample Inference

## Categorical Data Analysis

