# Regression Analysis

**Estimator Properties**

 1. unbiased 
 
 2. Consistency

 * $plim\hat{\beta_n}=\beta$
 * based on the law of large numbers, we can derive consistency
 * More observations means more precise, closer to the true value.

 3. Efficiency 

 * Minimum variance in comparison to another estimator.  
    + OLS is BlUE (best linear unbiased estimator) means that OLS is the most efficient among the class of linear unbiased estimator (Gauss-Markov Theorem)
    + If we have correct distributional assumptions, then the Maximum Likelihood is asymptotically efficient among consistent estimators.

## Linear Regression

### Ordinary Least Squares
The most fundamental model in statistics or econometric is a OLS linear regression. 
OLS = Maximum likelihood when the error term is assumed to be normally distributed. 


#### OLS Assumptions
##### A1 Linearity
$$
\begin{equation}
A1: y=\mathbf{x}\beta + \epsilon
(\#eq:A1)
\end{equation}
$$

Not restrictive

 * x can be nonlinear transformation including interactions, natural log, quadratic

With A3 (Exogeneity of Independent), linearity can be restrictive

###### Log-linear
###### Linear-Log
###### Log-Log
###### Higher Orders
$y=\beta_0 + x_1\beta_1 + x_1^2\beta_2 + \epsilon$
$$
\frac{\partial y}{\partial x_1}=\beta_1 + 2x_1\beta_2
$$

 * The effect of $x_1$ on y depends on the level of $x_1$
 * The partial effect at the average = $\beta_1+2E(x_1)\beta_2$
 * Average Partial Effect = $E(\beta_1 + 2x_1\beta_2)$

###### Interactions
$y=\beta_0 + x_1\beta_1 + x_2\beta_2 + x_1x_2\beta_3 + \epsilon$

 * $\beta_1$ is the average effect on y for a unit change in $x_1$ when $x_2=0$
 * $\beta_1 + x_2\beta_3$ is the partial effect of $x_1$ on y which depends on the level of $x_2$

<br>
<br>

##### A2 Full rank
$$
\begin{equation}
A2: rank(E(x'x))=k
(\#eq:A2)
\end{equation}
$$


also known as **identification condition**

 * columns of $\mathbf{x}$ cannot be written as a linear function of the other columns
 * which ensures that each parameter is unique and exists in the population regression equation

<br>
<br>

##### A3 Exogeneity of Independent Variables
$$
\begin{equation}
A3: E[\epsilon|x_1,x_2,...,x_k]=E[\epsilon|\mathbf{x}]=0
(\#eq:A3)
\end{equation}
$$
**strict exogeneity**

 * also known as **mean independence** check back on [Correlation and Independence]
 * by the [Law of Iterated Expectations] $E(\epsilon)=0$, which can be satisfied by always including an intercept.
 * independent variables do not carry information for prediction of $\epsilon$
 * A3 implies $E(y|x)=x\beta$, which means the conditional mean function must be a linear function of x [A1 Linearity]

###### A3a


<br>
<br>

##### A4 Homoskedasticity
$$
\begin{equation}
A4: Var(\epsilon|x)=Var(\epsilon)=\sigma^2
(\#eq:A4)
\end{equation}
$$

 * Variation in the disturbance to be the same over the independent variables

<br>
<br>

##### A5 Data Generation (random Sampling)
$$
\begin{equation}
A5: {y_i,x_{i1},...,x_{ik-1}: i = 1,..., n}
(\#eq:A5)
\end{equation}
$$
is a random sample

 * random sample mean samples are independent and identically distributed (iid) from a joint distribution of $(y,\mathbf{x})$
 * with [A3][A3 Exogeneity of Independent Variables] and [A4][A4 Homoskedasticity], we have
    + **Strict Exogeneity**: $E(\epsilon_i|x_1,...,x_n)=0$. independent variables do not carry information for prediction of $\epsilon$
    + **Non-autocorrelation**: $E(\epsilon_i\epsilon_j|x_1,...,x_n)=0$ The error term is uncorrelated across the draws conditional on the independent variables $\rightarrow$ $A4: Var(\epsilon|\mathbf{X})=Var(\epsilon)=\sigma^2I_n$
 * In times series and spatial settings, A5 is less likely to hold.
 
<br>

##### A6 Normal Distribution 
$$
\begin{equation}
A6: \epsilon|\mathbf{x}\sim N(0,\sigma^2I_n)
(\#eq:A6)
\end{equation}
$$
The error term is normally distributed

<br>

From A1-A3, we have **identification** (also known as **Orthogonality Condition**) of the population parameter $\beta$

\begin{align}
y &= {x}\beta + \epsilon && \text{A1} \\
x'y &= x'x\beta + x'\epsilon && \text{} \\
E(x'y) &= E(x'x)\beta + E(x'\epsilon)  && \text{} \\
E(x'y) &= E(x'x)\beta && \text{A3} \\
[E(x'x)]^{-1}E(x'y) &= [E(x'x)]^{-1}E(x'x)\beta && \text{A2} \\
[E(x'x)]^{-1}E(x'y) &= \beta
\end{align}


\beta is the row vector of parameters that produces the best predictor of y
we choose the min of  \gamma : 
$$
\underset{\gamma}{\operatorname{argmin}}E((y-x\gamma)^2)
$$
First Order Condition
$$
\begin{split}
\frac{\partial((y-x\gamma)^2)}{\partial\gamma}&=0 \\
-2E(x'(y-x\gamma))&=0 \\
E(x'y)-E(x'x\gamma) &=0 \\
E(x'y) &= E(x'x)\gamma \\
(E(x'x))^{-1}E(x'y) &= \gamma
\end{split}
$$

Second Order Conditon
$$
\begin{split}
\frac{\partial^2E((y-x\gamma)^2)}{}&=0 \\
E(\frac{\partial(y-x\partial)^2)}{\partial\gamma\partial\gamma'}) &= 2E(x'x)
\end{split}
$$
If [A3][A3 Exogeneity of Independent Variables] holds, then $2E(x'x)$ is PSD $\rightarrow$ minimum

<br>
<br>

#### Frisch-Waugh-Lovell Theorem
$$
\mathbf{y=X\beta + \epsilon=X_1\beta_1+X_2\beta_2 +\epsilon}
$$
Equivalently, 
$$
\left(
\begin{array}{c}
X_1'X_1 & X_1'X_2 \\
X_2'X_1 & X_2'X_2
\end{array}
\right)
\left(
\begin{array}{c}
\hat{\beta_1} \\
\hat{\beta_2}
\end{array}
\right)
=
\left(
\begin{array}{c}
X_1'y \\
X_2'y
\end{array}
\right)
$$
Hence, 
$$
\mathbf{\hat{\beta_1}=(X_1'X_1)^{-1}X_1'y - (X_1'X_1)^{-1}X_1'X_2\hat{\beta_2}}
$$

 1. Betas from the multiple regression are not the same as the betas from each of the individual simple regression 
 2. Different set of X will affect all the coefficient estimates. 
 3. If $X_1'X_2 = 0$ or $\hat{\beta_2}=0, then 1 and 2 do not hold.

<br>

**Hierarchy of OLS Assumptions**

|Identification Data Description | Unbiasedness Consistency | Gauss-Markov(BLUE) Asymptotic Inference (z and Chi-squared) | Classical LM (BUE) Small-sample Inference (t and F)|
|--- | --- | --- | ---|
|Variation in X | Variation in X | Variation in X | Variation in X|
|     | Random Sampling | Random Sampling  | Random Sampling |
|     | Linearity in Parameters | Linearity in Parameters | Linearity in Parameters|
|     | Zero Conditional Mean | Zero Conditional Mean | Zero Conditional Mean|
|     |                | Homoskedasticity | Homoskedasticity|
|     |           |        | Normality of Errors|

<br>
<br>
<br>
<br>

### Feasible Generalized Least Squares

Motivation for a more efficient estimator  

 * Gauss Markov Theorem holds under A1-A4
 * A4: $Var(\epsilon| \mathbf{X} )=\sigma^2I_n$
    + Heteroskedasticity: $Var(\epsilon_i|\mathbf{X}) \neq \sigma^2I_n$
    + Serial Correlation: $Cov(\epsilon_i,\epsilon_j|\mathbf{X}) \neq 0$
 * Without A4, how can we know which unbiased estimator is the most efficient?
 
Original (unweighted) model:

$$
\mathbf{y=X\beta+ \epsilon}
$$
Suppose A1-A3 hold, but A4 does not hold, 
$$
\mathbf{Var(\epsilon|X)=\Omega \neq \sigma^2 I_n}
$$

We will try to use OLS to estimate the transformed (weighted) model

$$
\mathbf{wy=wX\beta + w\epsilon}
$$
We need to choose $\mathbf{w}$ so that

$$
\mathbf{w'w = \Omega^{-1}}
$$
then $\mathbf{w}$ (full-rank matrix) is the **Cholesky decomposition** of $\mathbf{\Omega^{-1}}$ (full-rank matrix)

In other words, $\mathbf{w}$ is the squared root of $\Omega$ (squared root version in matrix)

$$
\Omega = var(\epsilon | X) \\
\Omega^{-1} = var(\epsilon | X)^{-1}
$$

Then, the transformed equation (IGLS) will have the following properties.

\begin{equation}
\begin{split}
\mathbf{\hat{\beta}_{IGLS}} &= \mathbf{(X'w'wX)^{-1}X'w'wy} \\
& = \mathbf{(X'\Omega^{-1}X)^{-1}X'\Omega^{-1}y} \\
& = \mathbf{\beta + X'\Omega^{-1}X'\Omega^{-1}\epsilon}
\end{split}
\end{equation}

Since A1-A3 hold for the unweighted model
\begin{equation}
\begin{split}
\mathbf{E(\hat{\beta}_{IGLS}|X)} & = E(\mathbf{\beta + (X'\Omega^{-1}X'\Omega^{-1}\epsilon)}|X)\\
& = \mathbf{\beta + E(X'\Omega^{-1}X'\Omega^{-1}\epsilon)|X)} \\
& = \mathbf{\beta + X'\Omega^{-1}X'\Omega^{-1}E(\epsilon|X)}  && \text{since A3: $E(\epsilon|X)=0$} \\
& = \mathbf{\beta}
\end{split}
\end{equation}

$\rightarrow$ IGLS estimator is unbiased

\begin{equation}
\begin{split}
\mathbf{Var(w\epsilon|X)} &= \mathbf{wVar(\epsilon|X)w'} \\
& = \mathbf{w\Omega w'} \\
& = \mathbf{w(w'w)^{-1}w'} && \text{since w is a full-rank matrix}\\
& = \mathbf{ww^{-1}(w')^{-1}w'} \\
& = \mathbf{I_n}
\end{split}
\end{equation}

$\rightarrow$ A4 holds for the transformed (weighted) equation

Then, the variance for the estimator is

\begin{equation}
\begin{split}
Var(\hat{\beta}_{IGLS}|\mathbf{X}) & = \mathbf{Var(\beta + (X'\Omega ^{-1}X)^{-1}X'\Omega^{-1}\epsilon|X)} \\
&= \mathbf{Var((X'\Omega ^{-1}X)^{-1}X'\Omega^{-1}\epsilon|X)} \\
&= \mathbf{(X'\Omega ^{-1}X)^{-1}X'\Omega^{-1} Var(\epsilon|X)   \Omega^{-1}X(X'\Omega ^{-1}X)^{-1}} && \text{because A4 holds}\\
&= \mathbf{(X'\Omega ^{-1}X)^{-1}X'\Omega^{-1} \Omega \Omega^{-1} \Omega^{-1}X(X'\Omega ^{-1}X)^{-1}} \\
&= \mathbf{(X'\Omega ^{-1}X)^{-1}}
\end{split}
\end{equation}

Let $A = \mathbf{(X'X)^{-1}X'-(X'\Omega ^{-1} X)X' \Omega^{-1}}$ then
$$
Var(\hat{\beta}_{OLS}|X)- Var(\hat{\beta}_{IGLS}|X) = A\Omega A'
$$
And $\Omega$ is Positive Semi Definite, then $A\Omega A'$ also PSD, then IGLS is more efficient 

The name **Infeasible** comes from the fact that it is impossible to compute this estimator. 

\begin{equation}
\mathbf{w} = 
\left(
\begin{array}{c}
w_{11} & 0 & 0 & ... & 0 \\
w_{21} & w_{22} & 0 & ... & 0 \\
w_{31} & w_{32} & w_{33} & ... & ... \\
w_{n1} & w_{n2} & w_{n3} & ... & w_{nn} \\
\end{array}
\right)
\end{equation}

With $n(n+1)/2$ number of elements and n observations $\rightarrow$ infeasible to estimate. (number of equation > data)

Hence, we need to make assumption on $\Omega$ to make it feasible to estimate $\mathbf{w}$:  

 1. [Heteroskedasticity]  : multiplicative exponential model
 2. Serial Correlation: AR(1)
 3. Serial Correlation: Cluster
 
 
#### Heteroskedasticity


\begin{equation}
\begin{split}
Var(\epsilon_i |x_i) & = E(\epsilon^2|x_i) \neq \sigma^2 \\
& = h(x_i) = \sigma_i^2 \text{(variance of the error term is a function of x)}
\end{split}
(\#eq:h-var-error-term)
\end{equation}


For our model,

$$
y_i = x_i\beta + \epsilon_i \\
(1/\sigma_i)y_i = (1/\sigma_i)x_i\beta + (1/\sigma_i)\epsilon_i
$$

then, from \@ref(eq:h-var-error-term)

\begin{equation}
\begin{split}
Var((1/\sigma_i)\epsilon_i|X) &= (1/\sigma_i^2) Var(\epsilon_i|X) \\
&= (1/\sigma_i^2)\sigma_i^2 \\
&= 1
\end{split}
\end{equation}

then the


### Weighted Least Squares

### Generalized Least Squares



### Maximum Likelihood
Premise: find values of the parameters that maximize the probability of observing the data
In other words, we try to maximize the value of theta in the likelihood function 
$$
L(\theta)=\prod_{i=1}^{n}f(y_i|\theta)
$$
$f(y|\theta)$ is the probability density of observing a single value of Y given some value of $\theta$
$f(y|\theta)$ can be specify as various type of distributions. You can review back section [Distributions]. For example 
If y is a dichotomous variable, then 
$$
L(\theta)=\prod_{i=1}^{n}\theta^{y_i}(1-\theta)^{1-y_i}
$$



**Assumption**: 
observations are independent and have the same density function. 
Under multivariate normal assumption, ML yields consistent estimates of the means and the covariance matrix for multivariate distribution with finite fourth moments [@Little_1987]


**Properties** 
[@EJD_1998]

 (1) Consistent: estimates are approximately unbiased in large samples
 (2) Asymptotically efficient: approximately smaller standard errors compared to other estimator
 (3) Asymptotically normal: with repeated sampling, the estimates will have an approximately normal distribution. 





## Non-linear Regression






