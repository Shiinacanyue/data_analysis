# Basic Statistical Inference

 * [One Sample Inference]  
 * [Two Sample Inference]  
 * [Categorical Data Analysis]  

Random sample of size n: A collection of n independent random variables taken from the distribution X, each with the same distribution as X.  

**Sample mean**   

$$
\bar{X}= (\sum_{i=1}^{n}X_i)/n
$$

**Sample Median**  

$\tilde{x}$ = the middle observation in a sample of observation order from smallest to largest (or vice versa).   

If n is odd, $\tilde{x}$ is the middle observation,  
If n is even, $\tilde{x}$ is the average of the two middle observations.

**Sample variance**
$$
S^2 = \frac{\sum_{i=1}^{n}(X_i = \bar{X})^2}{n-1}= \frac{n\sum_{i=1}^{n}X_i^2 -(\sum_{i=1}^{n}X_i)^2}{n(n-1)}
$$

**Sample standard deviation**
$$
S = \sqrt{S^2}
$$

**Sample proportions**
$$
\hat{p} = \frac{X}{n} = \frac{\text{number in the sample with trait}}{\text{sample size}}
$$


$$
\widehat{p_1-p_2} = \hat{p_1}-\hat{p_2} = \frac{X_1}{n_1} - \frac{X_2}{n_2} = \frac{n_2X_1 = n_1X_2}{n_1n_2}
$$

**Estimators**  
**Point Estimator**  
$\hat{\theta}$ is a statistic used to approximate a population parameter $\theta$

<br>

**Point estimate**  
The numerical value assumed by $\hat{\theta}$ when evaluated for a given sample

<br>

**Unbiased estimator**  
If $E(\hat{\theta}) = \theta$, then $\hat{\theta}$ is an unbiased estimator for $\theta$   

 1. $\bar{X}$ is an unbiased estimator for $\mu$
 2. $S^2$ is an unbiased estimator for $\sigma^2$
 3. $\hat{p}$ is an unbiased estimator for p
 4. $\widehat{p_1-P_2}$ is an unbiased estimator for $p_1- p_2$
 5. $\bar{X_1} - \bar{X_2}$ is an unbiased estimator for $\mu_1 - \mu_2$

**Note**: $S$ is a biased estimator for $\sigma$

**Distribution of the sample mean**  

If $\bar{X}$ is the sample mean based on a random sample of size n drawn from a normal distribution X with mean $\mu$ and standard deviation $\sigma$, the $\bar{X}$ is normally distributed, with mean $\mu_{\bar{X}} = \mu$ and variance $\sigma_{\bar{X}}^2 = Var(\bar{X}) = \frac{\sigma^2}{n}$. Then the **standard error of the mean** is: $\sigma_{\bar{X}}= \frac{\sigma}{\sqrt{n}}$



## One Sample Inference 

$Y_i \sim  i.i.d. N(\mu, \sigma^2)$

i.i.d. standards for "independent and identically distributed"

Hence, we have the following model:  

$Y_i=\mu +\epsilon_i$ where  

 * $\epsilon_i \sim^{iid} N(0,\sigma^2)$  
 * $E(Y_i)=\mu$  
 * $Var(Y_i)=\sigma^2$  
 * $\bar{y} \sim N(\mu,\sigma^2/n)$  

<br>

### Interval Estimation of the Mean

When $\sigma^2$ is estimated by $s^2$, then  

$$
\frac{\bar{y}-\mu}{s/\sqrt{n}} \sim t_{n-1}
$$

Then, a $100(1-\alpha) \%$ confidence interval for $\mu$ is obtained from:  

$$
1 - \alpha = P(-t_{\alpha/2;n-1} \le \frac{\bar{y}-\mu}{s/\sqrt{n}} \le t_{\alpha/2;n-1}) \\
= P(\bar{y} - (t_{\alpha/2;n-1})s/\sqrt{n} \le \mu \le \bar{y} + (t_{\alpha/2;n-1})s/\sqrt{n})
$$

And the interval is  

$$
\bar{y} \pm (t_{\alpha/2;n-1})s/\sqrt{n}
$$

and $s/\sqrt{n}$ is the standard error of $\bar{y}$  

If the experiment were repeated many times, $100(1-\alpha) \%$ of these intervals would contain $\mu$


### Interval Estimation for the Variance

$$
1 - \alpha = P( \chi_{1-\alpha/2;n-1}^2) \le (n-1)s^2/\sigma^2 \le \chi_{\alpha/2;n-1}^2) \\
= P(\frac{(n-1)s^2}{\chi_{\alpha/2}^2} \le \sigma^2 \le \frac{(n-1)s^2}{\chi_{1-\alpha/2}^2})
$$

and a $100(1-\alpha) \%$ confidence interval for $\sigma^2$ is:  

$$
(\frac{(n-1)s^2}{\chi_{\alpha/2;n-1}^2},\frac{(n-1)s^2}{\chi_{1-\alpha/2;n-1}^2})
$$
Confidence limits for $\sigma^2$ are obtained by computing the positive square roots of these limits

### Power 

Formally, power (for the test of the mean) is given by:  

$$
\pi(\mu) = 1 - \beta = P(\text{test rejects } H_0|\mu)
$$
To evaluate the power, one needs to know the distribution of the test statistic if the null hypothesis is false.  

For 1-sided z-test where $H_0: \mu \le \mu_0 \\ H_A: \mu >0$  

The power is:  

$$
\begin{align}
\pi(\mu) &= P(\bar{y} > \mu_0 + z_{\alpha} \sigma/\sqrt{n}|\mu) \\
&= P(Z = \frac{\bar{y} - \mu}{\sigma / \sqrt{n}} > z_{\alpha} + \frac{\mu_0 - \mu}{\sigma/ \sqrt{n}}|\mu) \\
&= 1 - \Phi(z_{\alpha} + \frac{(\mu_0 - \mu)\sqrt{n}}{\sigma}) \\
&= \Phi(-z_{\alpha}+\frac{(\mu -\mu_0)\sqrt{n}}{\sigma})
\end{align}
$$

where $1-\Phi(x) = \Phi(-x)$ since the normal pdf is symmetric  

Power is correlated to the difference in $\mu - \mu_0$, sample size n, variance $\sigma^2$, and the $\alpha$-level of the test (through $z_{\alpha}$)  
Equivalently, power can be increased by making $\alpha$ large, $\sigma^2$ smaller, or n larger. 

For 2-sided z-test is:  

$$
\pi(\mu) = \Phi(-z_{\alpha/2} + \frac{(\mu_0 - \mu)\sqrt{n}}{\sigma}) + \Phi(-z_{\alpha/2}+\frac{(\mu - \mu_0)\sqrt{n}}{\sigma})
$$



### Sample Size

#### 1-sided Z-test

Example: to show that the mean response $\mu$ under the treatment is higher than the mean response $\mu_0$ without treatment (show that the treatment effect $\delta = \mu -\mu_0$ is large)

Because power is an increasing function of $\mu - \mu_0$, it is only necessary to find n that makes the power equal to $1- \beta$ at $\mu = \mu_0 + \delta$  

Hence, we have 

$$
\pi(\mu_0 + \delta) = \Phi(-z_{\alpha} + \frac{\delta \sqrt{n}}{\sigma}) = 1 - \beta
$$

Since $\Phi (z_{\beta})= 1-\beta$, we have  

$$
-z_{\alpha} + \frac{\delta \sqrt{n}}{\sigma} = z_{\beta}
$$

Then n is 

$$
n = (\frac{(z_{\alpha}+z_{\beta})\sigma}{\delta})^2
$$

Then, we need larger samples, when  

 * the sample variability is large ($\sigma$ is large)
 * $\alpha$ is small ($z_{\alpha}$ is large)
 * power $1-\beta$ is large ($z_{\beta}$ is large)
 * The magnitude of the effect is smaller ($\delta$ is small) 

Since we don't know $\delta$ and $\sigma$. We can base $\sigma$ on previous studies, pilot studies. Or, obtain an estimate of $\sigma$ by anticipating the range of the observation (without outliers). divide this range by 4 and use the resulting number as an approximate estimate of $\sigma$. For normal (distribution) data, this is reasonable. 


#### 2-sided Z-test

We want to know the min n, required to guarantee $1-\beta$ power when the treatment effect $\delta = |\mu - \mu_0|$ is at least greater than 0. Since the power function for the 2-sided is increasing and symmetric in $|\mu - \mu_0|$, we only need to find n that makes the power equal to $1-\beta$ when $\mu = \mu_0 + \delta$  

$$
n = (\frac{(z_{\alpha/2} + z_{\beta}) \sigma}{\delta})^2
$$

We could also use the confidence interval apporach. If we reuqire that an $\alpha$-level two-cided CI for $\mu$ be 

$$
\bar{y} \pm D
$$
where $D = z_{\alpha/2}\sigma/\sqrt{n}$ gives  

$$
n = (\frac{z_{\alpha/2}\sigma}{D})^2
$$
(round up to the nearest integer)  

### Note

For t-tests, the sample and power are not as easy as z-test. 

$$
\pi(\mu) = P(\frac{\bar{y}-\mu_0}{s/\sqrt{n}}> t_{n-1;\alpha}|\mu)
$$

when $\mu > \mu_0$ (i.e., $\mu - \mu_0 = \delta$), the random variable $(\bar{y} - \mu_0)/(s/\sqrt{n})$ does not have a [Student's t distribution][Student T], but rather is distributed as a non-central t-distribution with non-centrality parameter $\delta \sqrt{n}/\sigma$ and d.f. of $n-1$  

 * The power is an increasing function of this non-centrality parameter (note, when $\delta = 0$ the distribution is usual Student's t-distribution). 
 * To evaluate power, one must consider numerical procedure or use special charts 

Approximate Sample Size Adjustment for t-test. We use an adjustment to the z-test determination for sample size.  

Let $v = n-1$, where n is sample size derived based on the z-test power. Then the 2-sided t-test sample size (apporximate) is given:  

$$
n^* = \frac{(t_{v;\alpha/2}+t_{v;\beta})^2 \sigma^2}{\delta^2}
$$

### Non-parametric Methods
#### Sign Test 

If we want to test $H_0: \mu_{(0.5)} = 0; H_a: \mu_{(0.5)} >0$ where $\mu_{(0.5)}$ is the population median. We can  

 (1) Count the number of observation ($y_i$'s) that exceed 0. Denote this number by $s_+$, called the number of plus signs. Let $s_- = n - s_+$, which is the number of minus signs. 
 (2) Reject $H_0$ if $s_+$ is large or equivalently, if $s_-$ is small.  

To determine how large $s_+$ must be to reject $H_0$ at a given significance level, we need to know the distribution of the corresponding random variable $S_+$ under the null hypothesis, which is a [binomial][Binomial] with p = 1/2,w hen the null is true.  

To work out the null distribution using the binomial formula, we have $\alpha$-level test rejects $H_0$ if $s_+ \ge b_{n,\alpha}$, where $b_{n,\alpha}$ is the upper $\alpha$ critical point of the $Bin(n,1/2)$ distribution. Both $S_+$ and $S_-$ have this same distribution ($S = S_+ = S_-$).  

$$
\text{p-value} = P(S \ge s_+) = \sum_{i = s_+}^{n} {{n}\choose{i}} (\frac{1}{2})^n
$$
equivalently, 

$$
P(S \le s_-) = \sum_{i=0}^{s_-}{{n}\choose{i}} (\frac{1}{2})^2
$$
For large sample sizes, we could use the normal approximation for the binomial, in which case reject $H_0$ if 

$$
s_+ \ge n/2 + 1/2 + z_{\alpha}\sqrt{n/4}
$$

For the 2-sided test, we use the tests statistic $s_{max} = max(s_+,s_-)$ or $s_{min} = min(s_+, s_-)$. An $\alpha$-level test rejects $H_0$ if the p-value is $\le \alpha$, where the p-value is computed from:  

$$
p-value = 2 \sum_{i=s_{max}}^{n} {{n}\choose{i}} (\frac{1}{2})^n = s \sum_{i=0}^{s_{min}} {{n}\choose{i}} (\frac{1}{2})^n
$$
Equivalently, rejecting $H_0$ if $s_{max} \ge b_{n,\alpha/2}$ 

A large sample normal approximation can be used, where  

$$
z = \frac{s_{max}- n/2 -1/2}{\sqrt{n/4}}
$$
and reject $H_0$ at $\alpha$ if $z \ge z_{\alpha/2}$  

However, treatment of 0 is problematic for this test.  

 * Solution 1: randomly assign 0 to the positive or negative (2 researchers might get different results).  
 * Solution 2: count each 0 as a contribution 1/2 toward $s_+$ and $s_-$ (but then could not apply the [binomial][Binomial] distribution)  
 * Solution 3: ignore 0 (reduces the power of test due to decreased sample size). 


#### Wilcoxon Signed Rank Test

Since the [Sign Test] could not consider the magnitude of each observation from 0, the [Wilcoxon Signed Rank Test] improves by  taking account the ordered magnitudes of the observation, but it will impose the requirement of symmetric to this test (while [Sign Test] does not)

$$
H_0: \mu_{0.5} = 0 \\
H_a: \mu_{0.5} > 0
$$
(assume no ties or same observations)  

The signed rank test procedure:  

 1. rank order the observation $y_i$ in terms of their absolute values. Let $r_i$ be the rank of $y_i$ in this ordering. Since we assume no ties, the ranks $r_i$ are uniquely determined and are a permutation of the integers 1,2,...,n.  
 2. Calculate $w_+$, which is the sum of the ranks of the positive values, and $w_-$, which is the sum of the ranks of the negative values. Note that $w_+ + w_- = r_1 + r_2 + ... = 1 + 2 + ... + n = n(n+1)/2$  
 3. Reject $H_0$ if $w_+$ is large (or if $w_-$ is small)  
 
To know what is large or small with regard to $w_+$ and $w_-$, we need the distribution of $W_+$ and $W_-$ when the null is true.  

Since these null distributions are identical and symmetric, the p-value is $P(W \ge w_+) = P(W \le w_-)$  

An $\alpha$-level test rejects the null if the p-value is $\le \alpha$, or if $w_+ \ge w_{n,\alpha}$, where $w_{n,\alpha}$ is the upper $\alpha$ critical point of the null distribution of W.  

This distribution of W has a special table. For large n, the distribution of W is approximately normal.  

$$
z = \frac{w_+ - n(n+1) /4 -1/2}{\sqrt{n(n+1)(2n+1)/24}}
$$

The test rejcets $H_0$ at level $\alpha$ if  

$$
w_+ \ge n(n+1)/4 +1/2 + z_{\alpha}\sqrt{n(n+1)(2n+1)/24} \approx w_{n,\alpha}
$$

For the 2-sided test, we use $w_{max}=max(w_+,w_-)$ or $w_{min}=min(w_+,w_-)$, with p-value given by: 

$$
p-value = 2P(W \ge w_{max}) = 2P(W \le w_{min})
$$
Same as [Sign Test],we ignore 0. In some cases where some of the $|y_i|$'s may be tied for the same rank, we simply assign each of the tied ranks the average rank (or "midrank").  

Example, if $y_1 = -1$, $y_3 = 3$ and $y_3 = -3$, and $y_4 =5$, then $r_1 = 1$, $r_2 = r_3=(2+3)/2 = 2.5$, $r_4 = 4$


## Two Sample Inference
### Means

Suppose we have 2 sets of observations,  

 * $y_1,..., y_{n_y}$  
 * $x_1,...,x_{n_x}$

that are random samples from two independent populations with means $\mu_y$ and $\mu_x$ and variances $\sigma^2_y$,$\sigma^2_x$. 
Our goal is to compare $\mu_x$ and $\mu_y$ or $\sigma^2_y = \sigma^2_x$

#### Large Sample Tests

Assume that $n_y$ and $n_x$ are large ($\ge 30$). Then,  

$$
E(\bar{y} - \bar{x}) = \mu_y - \mu_x \\
Var(\bar{y} - \bar{x}) = \sigma^2_y /n_y + \sigma^2_x/n_x
$$

Then,  

$$
Z = \frac{\bar{y}-\bar{x} - (\mu_y - \mu_x)}{\sqrt{\sigma^2_y /n_y + \sigma^2_x/n_x}} \sim N(0,1)
$$
(according to [Central Limit Theorem]). For large samples, we can replace variances by their unbiased estimators ($s^2_y,s^2_x$), and get the same large sample distribution.   

An approximate $100(1-\alpha) \%$ CI for $\mu_y - \mu_x$ is given by:  

$$
\bar{y} - \bar{x} \pm z_{\alpha/2}\sqrt{s^2_y/n_y + s^2_x/n_x}
$$

$$
H_0: \mu_y - \mu_x = \delta_0 \\
H_A: \mu_y - \mu_x \neq \delta_0
$$

at the $\alpha$-level with the statistic:  

$$
z = \frac{\bar{y}-\bar{x} - \delta_0}{\sqrt{s^2_y /n_y + s^2_x/n_x}}
$$

and reject $H_0$ if $|z| > z_{\alpha/2}$  

If $\delta = )$, it means that we are testing whether two means are equal.

#### Small Sample Tests

If the two samples are from normal distribution, iid $N(\mu_y,\sigma^2_y)$ and iid $N(\mu_x,\sigma^2_x)$ and the two samples are independent, we can do inference based on the [t-distribution][Student T]  

Then we have 2 cases  

 * [Equal Variance]
 * [Unequal Variance]

##### Equal variance

**Assumptions**  

 * iid: so that $var(\bar{y}) = \sigma^2_y / n_y ; var(\bar{x}) = \sigma^2_x / n_x$  
 * Independence between samples: No observation from one sample can influence any observation from the other sample, to have  

$$
\begin{align}
var(\bar{y} - \bar{x}) &= var(\bar{y}) + var{\bar{x}} - 2cov(\bar{y},\bar{x}) \\
&= var(\bar{y}) + var{\bar{x}} \\
&= \sigma^2_y / n_y + \sigma^2_x / n_x 
\end{align}
$$

 * Normality: Justifies the use of the [t-distribution][Student T]

Let $\sigma^2 = \sigma^2_y = \sigma^2_x$. Then, $s^2_y$ and $s^2_x$ are both unbiased estimators of $\sigma^2$. We then can pool them.  

Then the pooled variance estimate is 
$$
s^2 = \frac{(n_y - 1)s^2_y + (n_x - 1)s^2_x}{(n_y-1)+(n_x-1)}
$$
has $n_y + n_x -2$ df.  

Then the test statistic  

$$
T = \frac{\bar{y}- \bar{x} -(\mu_y - \mu_x)}{s\sqrt{1/n_y + 1/n_x}} \sim t_{n_y + n_x -2}
$$

$100(1 - \alpha) \%$ CI for $\mu_y - \mu_x$ is

$$
\bar{y} - \bar{x} \pm (t_{n_y + n_x -2})s\sqrt{1/n_y + 1/n_x}
$$

Hypothesis testing:  
$$
H_0: \mu_y - \mu_x = \delta_0 \\
H_1: \mu_y - \mu_x \neq \delta_0
$$

we reject $H_0$ if $|t| > t_{n_y + n_x -2;\alpha/2}$

##### Unequal Variance

**Assumptions**  

 1. Two samples are independent  
        1. Scatter plots  
        2. Correlation coefficient (if normal)
 2. Independence of observation in each sample  
        1. Test for serial correlation  
 3. For each sample, homogeneity of variance  
        1. Scatter plots
        2. Formal tests 
 4. [Normality][Normality Assessment]  
 5. Equality of variances (homogeneity of variance between samples)  
        1. F-test  
        2. Barlett test  
        3. Modified Levene Test  


To compare 2 normal $\sigma^2_y \neq \sigma^2_x$, we use the test statistic:  

$$
T = \frac{\bar{y}- \bar{x} -(\mu_y - \mu_x)}{\sqrt{s^2_y/n_y + s^2_x/n_x}} 
$$
In this case, T does not follow the [t-distribution][Student T] (its distribution depends on the ratio of the unknown variances $\sigma^2_y,\sigma^2_x$). In the case of small sizes, we can can approximate tests by using the Welch-Satterthwaite method [@Satterthwaite_1946]. We assume T can be approximated by a [t-distribution][Student T], and adjust the degrees of freedom.  

Let $w_y = s^2_y /n_y$ and $w_x = s^2_x /n_x$ (the w's are the square of the respective standard errors)  
Then, the degrees of freedom are  

$$
v = \frac{(w_y + w_x)^2}{w^2_y / (n_y-1) + w^2_x / (n_x-1)}
$$

Since v is usually fractional, we truncate down to the nearest integer.  

$100 (1-\alpha) \%$ CI for $\mu_y - \mu_x$ is 

$$
\bar{y} - \bar{x} \pm t_{v,\alpha/2} \sqrt{s^2_y/n_y + s^2_x /n_x}
$$

Reject $H_0$ if $|t| > t_{v,\alpha/2}$, where 

$$
t = \frac{\bar{y} - \bar{x}-\delta_0}{\sqrt{s^2_y/n_y + s^2_x /n_x}}
$$


### Variances
#### F-test

Test 

$$
H_0: \sigma^2_y = \sigma^2_x \\
H_a: \sigma^2_y \neq \sigma^2_x
$$

Consider the test statistic,  

$$
F= \frac{s^2_y}{s^2_x}
$$

Reject $H_0$ if  

 * $F>f_{n_y -1,n_x -1,\alpha/2}$ or  
 * $F<f_{n_y -1,n_x -1,1-\alpha/2}$  

Where $F>f_{n_y -1,n_x -1,\alpha/2}$ and $F<f_{n_y -1,n_x -1,1-\alpha/2}$ are the upper and lower $\alpha/2$ critical points of an [F-distribution][F-Distribution], with a $n_y-1$ and $n_x-1$ degrees of freedom.  

**Note**  

 * This test depends heavily on the assumption Normality.  
 * In particular, it could give to many significant results when observations come from long-tailed distributions (i.e., positive kurtosis).  
 * If we cannot find support for [normality][Normality Assessment], then we can use nonparametric tests such as the [Modified Levene Test] 
 

#### Modified Levene Test

 * considers averages of absolute deviations rather than squared deviations. Hence, less sensitive to long-tailed distributions.  
 * This test is still good for normal data 

For each sample, we consider the absolute deviation of each observation form the median:  

$$
d_{y,i} = |y_i - y_{.5}| \\
d_{x,i} = |x_i - x_{.5}|
$$
Then,  

$$
t_L^* = \frac{\bar{d}_y-\bar{d}_x}{s \sqrt{1/n_y + 1/n_x}}
$$

The pooled variance $s^2$ is given by: 

$$
s^2 = \frac{\sum_i^{n_y}(d_{y,i}-\bar{d}_y)^2 + \sum_j^{n_x}(d_{x,i}-\bar{d}_x)^2}{n_y + n_x -2}
$$

 * If the error terms have constant variance and $n_y$ and $n_x$ are not extremely small, then $t_L^* \sim t_{n_x + n_y -2}$  
 * We reject the null hypothesis when $|t_L^*| > t_{n_y + n_x -2;\alpha/2}$  
 * This is just the two-sample t-test applied to the absolute deviations.   

### Sample Size

### Power

## Categorical Data Analysis

