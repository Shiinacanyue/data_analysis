# Regression Analysis

**Estimator Desirable Properties**

 1. Unbiased 
 
 2. Consistency

 * $plim\hat{\beta_n}=\beta$
 * based on the law of large numbers, we can derive consistency
 * More observations means more precise, closer to the true value.

 3. Efficiency 

 * Minimum variance in comparison to another estimator.  
    + OLS is BlUE (best linear unbiased estimator) means that OLS is the most efficient among the class of linear unbiased estimator [Gauss-Markov Theorem] 
    + If we have correct distributional assumptions, then the Maximum Likelihood is asymptotically efficient among consistent estimators.

## Linear Regression

### Ordinary Least Squares
The most fundamental model in statistics or econometric is a OLS linear regression. 
OLS = Maximum likelihood when the error term is assumed to be normally distributed. 


#### Simple Regression (Basic Model)

$$
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
$$

 * $Y_i$: response (dependent) variable at i-th observation  
 * $\beta_0,\beta_1$: regression parameters for intercept and slope.  
 * $X_i$: known constant (independent or predicotr variable) for i-th observation  
 * $\epsilon_i$: random error term  

$$
E(\epsilon_i) = 0 \\
var(\epsilon_i) = \sigma^2 \\
cov(\epsilon_i,\epsilon_j) = 0 \text{ for all $i \neq j$}
$$

$Y_i$ is random since $\epsilon_i$ is:  

$$
\begin{align}
E(Y_i) &= E(\beta_0 + \beta_1 X_i + \epsilon_i) \\
&= E(\beta_0) + E(\beta_1 X_i) + E(\epsilon) \\
&= \beta_0 + \beta_1 X_i
\end{align}
$$

$$
\begin{align}
var(Y_i) &= var(\beta_0 + \beta_1 X_i + \epsilon_i) \\
&= var(\epsilon_i) \\
&= \sigma^2
\end{align}
$$

Since $cov(\epsilon_i, \epsilon_j) = 0$ (uncorrelated), the outcome in any one trail has no effect on the outcome of any other. Hence, $Y_i, Y_j$ are uncorrelated as well (conditioned on the X's) 

**Note**  
[Least Squares][Ordinary Least Squares] does not require a distributional assumption  

<br>

##### Estimation

Deviation of $Y_i$ from its expected value:  

$$
Y_i - E(Y_i) = Y_i - (\beta_0 + \beta_1 X_i)
$$

Consider the sum of the square of such deviations:  

$$
Q = \sum_{i=1}^{n} (Y_i - \beta_0 -\beta_1 X_i)^2
$$

$$
b_1 = \frac{\sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^{n}(X_i - \bar{X})^2} \\
b_0 = \frac{1}{n}(\sum_{i=1}^{n}Y_i - b_1\sum_{i=1}^{n}X_i) = \bar{Y} - b_1 \bar{X}
$$

##### Properties of Least Least Estimators

$$
E(b_1) = \beta_1 \\
E(b_0) = E(\bar{Y}) - \bar{X}\beta_1 \\
E(\bar{Y}) = \beta_0 + \beta_1 \bar{X} \\
E(b_0) = \beta_0 \\
var(b_1) = \frac{\sigma^2}{\sum_{i=1}^{n}(X_i - \bar{X})^2} \\
var(b_0) = \sigma^2 (\frac{1}{n} + \frac{\bar{X}^2}{\sum (X_i - \bar{X})^2})
$$

$var(b_1)$ approaches 0 as more measurements are taken at more $X_i$ values (unless $X_i$ is at its mean value)  
$var(b_0)$ approaches 0 as n increases when the $X_i$ values are judiciously selected.  


**Mean Square Error** 

$$
MSE = \frac{SSE}{n-2} = \frac{\sum_{i=1}^{n}e_i^2}{n-2} = \frac{\sum(Y_i - \hat{Y_i})^2}{n-2}
$$

Unbiased estimator of MSE:  

$$
E(MSE) = \sigma^2
$$

$$
s^2(b_1) = \widehat{var(b_1)} = \frac{MSE}{\sum_{i=1}^{n}(X_i - \bar{X})^2} \\
s^2(b_0) = \widehat{var(b_0)} = MSE(\frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^{n}(X_i - \bar{X})^2})
$$

$$
E(s^2(b_1)) = var(b_1) \\
E(s^2(b_0))=var(b_0)
$$


##### Residuals

$$
e_i = Y_i - \hat{Y} = Y_i - (b_0 + b_1 X_i)
$$

 * $e_i$ is an estimate of $\epsilon_i = Y_i - E(Y_i)$  
 * $\epsilon_i$ is always unknown since we don't know the true $\beta_0, \beta_1$


$$
\sum_{i=1}^{n} e_i = 0 \\
\sum_{i=1}^{n} X_i e_i = 0
$$


<br>


##### Inference

**Normality Assumption**  

 * Least Squares estimation does not require assumptions of normality.  
 * However, to do inference on the parameters, we need distributional assumptions.  
 * Inference on $\beta_0,\beta_1$ and $Y_h$ are not extremely sensitive to moderate departures from normality, especially if the sample size is large  
 * Inference on $Y_{pred}$ is very sensitive to the normality assumptions.  


**Normal Error Regression Model**  

$$
Y_i \sim N(\beta_0+\beta_1X_i, \sigma^2) \\
$$

<br>


###### $\beta_1$

Under the normal error model, 

$$
b_1 \sim N(\beta_1,\frac{\sigma^2}{\sum_{i=1}^{n}(X_i - \bar{X})^2})
$$

A linear combination of independent normal random variable is normally distributed  

Hence,  

$$
\frac{b_1 - \beta_1}{s(b_1)} \sim t_{n-2}
$$

A $(1-\alpha) 100 \%$ confidence interval for $\beta_1$ is  

$$
b_1 \pm t_{t-\alpha/2 ; n-2}s(b_1)
$$

<br>


###### $\beta_0$

Under the normal error model, the sampling distribution for $b_0$ is  

$$
b_0 \sim N(\beta_0,\sigma^2(\frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^{n}(X_i - \bar{X})^2}))
$$

Hence,  

$$
\frac{b_0 - \beta_0}{s(b_0)} \sim t_{n-2}
$$
A $(1-\alpha)100 \%$ confidence interval for $\beta_0$ is  

$$
b_0 \pm t_{1-\alpha/2;n-2}s(b_0)
$$

<br>


######  Mean Response

Let $X_h$ denote the level of X for which we wish to estimate the mean response  

 * We denote the mean response when $X = X_h$ by $E(Y_h)$  
 * A point estimator of $E(Y_h)$ is $\hat{Y}_h$:  

$$
\hat{Y}_h = b_0 + b_1 X_h
$$
**Note**  

$$
E(\bar{Y}_h)= E(b_0 + b_1X_h) \\
= \beta_0 + \beta_1 X_h \\
= E(Y_h)
$$
(unbiased estimator) 


$$
\begin{align}
var(\hat{Y}_h) &= var(b_0 + b_1 X_h) \\
&= var(\hat{Y} + b_1 (X_h - \bar{X})) \\
&= var(\bar{Y}) + (X_h - \bar{X})^2var(b_1) + 2(X_h - \bar{X})cov(\bar{Y},b_1) \\
&= \frac{\sigma^2}{n} + (X_h - \bar{X})^2 \frac{\sigma^2}{\sum(X_i - \bar{X})^2} \\
&= \sigma^2(\frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2})
\end{align}
$$

Since $cov(\bar{Y},b_1) = 0$ due to the iid assumption on $\epsilon_i$  

An estimate of this variance is  

$$
s^2(\hat{Y}_h) = MSE (\frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n}(X_i - \bar{X})^2})
$$

the sampling distribution for the mean response is  

$$
\hat{Y}_h \sim N(E(Y_h),var(\hat{Y_h})) \\
\frac{\hat{Y}_h - E(Y_h)}{s(\hat{Y}_h)} \sim t_{n-2}
$$

A $100(1-\alpha) \%$ CI for $E(Y_h)$ is 

$$
\hat{Y}_h \pm t_{1-\alpha/2;n-2}s(\hat{Y}_h)
$$

<br>


###### Prediction of a new observation


Regarding the [Mean Response], we are interested in estimating **mean** of the distribution of Y given a certain X.  

Now, we want to **predict** an individual outcome for the distribution of Y at a given X. We call $Y_{pred}$  

Estimation of mean response versus prediction of a new observation:  

 * the point estimates are the same in both cases: $\hat{Y}_{pred} = \hat{Y}_h$  
 * It is the variance of the prediction that is different; hence, prediction intervals are different than confidence intervals. The prediction variance must consider:  
    + Variation in the mean of the distribution of Y  
    + variation within the distribution of Y  

We want to predict:  mean response + error 

$$
\beta_0 + \beta_1 X_h + \epsilon
$$

Since $E(\epsilon) = 0$, use the least squares predictor: 

$$
\hat{Y}_h = b_0 + b_1 X_h
$$

The variance of the predictor is  

$$
\begin{align}
var(b_0 + b_1 X_h + \epsilon) &= var(b_0 + b_1 X_h) + var(\epsilon) \\
&= \sigma^2(\frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n}(X_i - \bar{X})^2}) + \sigma^2 \\
&= \sigma^2(1+\frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n}(X_i - \bar{X})^2})
\end{align}
$$

An estimate of the variance is given by  

$$
s^2(pred)= MSE (1+ \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum_{i=1}^{n}(X_i - \bar{X})^2}) \\
\frac{Y_{pred}-\hat{Y}_h}{s(pred)} \sim t_{n-2}
$$

$100(1-\alpha) \%$ prediction interval is  

$$
\bar{Y}_h \pm t_{1-\alpha/2; n-2}s(pred)
$$

The prediction interval is very sensitive to the distributional assumption on the errors, $\epsilon$  


<br>

###### Confidence Band

We want to know the confidence interval for the entire regression line, so we can draw conclusions about any and all mean response fo the entire regression line $E(Y) = \beta_0 + \beta_1 X$ rather than for a given response Y  

**Working-Hotelling Confidence Band**  

For a given $X_h$, this band is  

$$
\hat{Y}_h \pm W s(\hat{Y}_h)
$$
where $W^2 = 2F_{1-\alpha;2,n-2}$, which is just 2 times the F-stat with 2 and n-2 degrees of freedom  

 * the interval width will change with each $X_h$ (since $s(\hat{Y}_h)$ changes)  
 * the boundary values for this confidence band will always define a hyperbole containing the regression line  
 * will be smallest at $X = \bar{X}$  


###### ANOVA

Partitioning the Total Sum of Squares: Consider the corrected Total sum of squres:  

$$
SSTO = \sum_{i=1}^{n} (Y_i -\bar{Y})^2
$$
Measures the overall dispersion in the response variable  
We use the term corrected because we correct for mean, the uncorrected total sum of squares is given by $\sum Y_i^2$  

use $\hat{Y}_i = b_0 + b_1 X_i$ to estimate the conditional mean for Y at $X_i$  

$$
\begin{align}
\sum_{i=1}^n (Y_i - \bar{Y})^2 &= \sum_{i=1}^n (Y_i - \hat{Y}_i + \hat{Y}_i - \bar{Y})^2 \\
&= \sum_{i=1}^n(Y_i - \hat{Y}_i)^2 + \sum_{i=1}^n(\hat{Y}_i - \bar{Y})^2 + 2\sum_{i=1}^n(Y_i - \hat{Y}_i)(\hat{Y}_i-\bar{Y}) \\
&= \sum_{i=1}^n(Y_i - \hat{Y}_i)^2 + \sum_{i=1}^n(\bar{Y}_i -\bar{Y})^2 \\
STTO &= SSE + SSR \\
\end{align}
$$

where SSR is the regression sum of squares, which measures how the conditional mean varies about a central value.  

The cross-product term in the decomposition is 0:  

$$
\begin{align}
\sum_{i=1}^n (Y_i - \hat{Y}_i)(\hat{Y}_i - \bar{Y}) &= \sum_{i=1}^{n}(Y_i - \bar{Y} -b_1 (X_i - \bar{X}))(\bar{Y} + b_1 (X_i - \bar{X})-\bar{Y}) \\
&= b_1 \sum_{i=1}^{n} (Y_i - \bar{Y})(X_i - \bar{X}) - b_1^2\sum_{i=1}^{n}(X_i - \bar{X})^2 \\
&= b_1 \frac{\sum_{i=1}^{n}(Y_i -\bar{Y})(X_i - \bar{X})}{\sum_{i=1}^{n}(X_i - \bar{X})^2} \sum_{i=1}^{n}(X_i - \bar{X})^2 - b_1^2\sum_{i=1}^{n}(X_i - \bar{X})^2 \\
&= b_1^2 \sum_{i=1}^{n}(X_i - \bar{X})^2 - b_1^2 \sum_{i=1}^{n}(X_i - \bar{X})^2 \\
&= 0
\end{align}
$$

$$
\begin{align}
SSTO &= SSR + SSE \\
(n-1 d.f) &= (1 d.f.) + (n-2 d.f.)
\end{align}
$$



| Source of Variation | Sum of Squares | df | Mean Square | F | 
|---|---|---|---|---|
|Regression (model) | SSR | 1 | MSR = SSR/df | MSR/MSE | 
| Error | SSE | n-2 | MSE = SSE/df | | 
|Total (Corrected) | SSTO | n-1| | |


$$
E(MSE) = \sigma^2 \\
E(MSR) = \sigma^2 + \beta_1^2 \sum_{i=1}^{n} (X_i - \bar{X})^2 
$$

 * If $\beta_1 = 0$, then these two expected values are the same  
 * if $\beta_1 \neq 0$ then E(MSR) will be larger than E(MSE)  

which means the ratio of these two quantities, we can infer something about $\beta_1$  

Distribution theory tells us that if $\epsilon_i \sim iid N(0,\sigma^2)$ and assuming $H_0: \beta_1 = 0$ is true,  

$$
\frac{MSE}{\sigma^2} \sim \chi_{n-2}^2 \\
\frac{MSR}{\sigma^2} \sim \chi_{1}^2 \text{ if $\beta_1=0$}
$$

where these two chi-square random variables are independent.  

Since the ratio of 2 independent chi-square random variable follows an F distribution, we consider:  

$$
F = \frac{MSR}{MSE} \sim F_{1,n-2}
$$
when $\beta_1 =0$. Thus, we reject $H_0: \beta_1 = 0$ (or $E(Y_i)$ = constant) at $\alpha$ if 

$$
F > F_{1 - \alpha;1,n-2}
$$
this is the only null hypothesis that can be tested with this approach.  


**Coefficient of Determination**  

$$
R^2 = \frac{SSR}{SSTO} = 1- \frac{SSE}{SSTO}
$$

where $0 \le R^2 \le 1$  

**Interpretation**: The proportionate reduction of the total variation in Y after fitting a linear model in X.  
It is not really correct to say that $R^2$ is the "variation in Y explained by X".  

$R^2$ is related to the correlation coefficient between Y and X:  

$$
R^2 = (r)^2
$$
where $r= corr(x,y)$ is an estimate of the Pearson correlation coefficient. Also, note  

$$
b_1 = (\frac{\sum_{i=1}^{n}(Y_i - \bar{Y})^2}{\sum_{i=1}^{n} (X_i - \bar{X})^2})^{1/2} \\
r = \frac{s_y}{s_x} r
$$

**Lack of Fit**

$Y_{11},Y_{21},...,Y_{n_1,1}$: $n_1$  repeat obs at $X_1$  

$Y_{1c},Y_{2c},...,Y_{n_c,c}$: $n_c$  repeat obs at $X_c$  

So, there are c distinct X values.  


Let $\bar{Y}_j$ be the mean over replicates for $X_j$  

Partition the Error Sum of Squares:  

$$
\begin{align}
\sum_{i} \sum_{j} (Y_{ij} - \hat{Y}_{ij})^2 &= \sum_{i} \sum_{j} (Y_{ij} - \bar{Y}_j + \bar{Y}_j + \hat{Y}_{ij})^2 \\
&=  \sum_{i} \sum_{j} (Y_{ij} - \bar{Y}_j)^2 + \sum_{i} \sum_{j} (\bar{Y}_j - \hat{Y}_{ij})^2 + \text{cross product term} \\
&= \sum_{i} \sum_{j}(Y_{ij} - \bar{Y}_j)^2 + \sum_j n_j (\bar{Y}_j- \hat{Y}_{ij})^2 \\
SSE &= SSPE + SSLF \\
\end{align}
$$


 * SSPE: "pure error sum of squares" has n-c degrees of freedom since we need to estimate c means  
 * SSLF: "lack of fit sum of squares" has c - 2 degrees of freedom (the number of unique X values - number of parameters used to specify the conditional mean regression model)  

$$
MSPE = \frac{SSPE}{df_{pe}} = \frac{SSPE}{n-c} \\
MSLF = \frac{SSLF}{df_{lf}} = \frac{SSLF}{c-2}
$$
The **F-test for Lack-of-Fit** tests  

$$
H_0: Y_{ij} = \beta_0 + \beta_1 X_i + \epsilon_{ij}, \epsilon_{ij} \sim iid N(0,\sigma^2) \\
H_a: Y_{ij} = \alpha_0 + \alpha_1 X_i + f(X_i, Z_1,...) + \epsilon_{ij}^*,\epsilon_{ij}^* \sim iid N(0, \sigma^2)
$$

$E(MSPE) = \sigma^2$ under either $H_0$, $H_a$  

$E(MSLF) = \sigma^2 + \frac{\sum n_j(f(X_i,...))^2}{n-2}$ in general and  

$E(MSLF) = \sigma^2$ when $H_0$ is true  

We reject $H_0$ (i.e., the model $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$ is not adequate) if  

$$
F = \frac{MSLF}{MSPE} > F_{1-\alpha;c-2,n-c}
$$

Failing to reject $H_0$ does not imply that $H_0: Y_{ij} = \beta_0 + \beta_1 X_i + \epsilon_{ij}$ is exactly true, but it suggests that this model may provide a reasonable approximation to the true model.  

| Source of Variation | Sum of Squares | df | Mean Square | F |
|---|---|---|---|---|
|Regression| SSR | 1 | MSR | MSR / MSE |
|Error | SSE | n-2 | MSE | |
|Lack of fit | SSLF | c-2 | MSLF | MSLF / MSPE|
|Pure Error | SSPE | n-c | MSPE | |
|Total(Corrected) | SSTO | n-1 | | |


Repeat observations have an effect on $R^2$:  

 * It is impossible for $R^2$ to attain 1 when repeat obs. exist (SSE can't be 0)  
 * The maximum $R^2$ attainable in this situation:  

$$
R^2_{max} = \frac{SSTo - SSPE}{SSTO}
$$

 * Not all levels of X need have repeat observations.  
 * Typically, when $H_0$ is appropriate, one still uses MSE as the estimate for $\sigma^2$ rather than MSPE, Since MSE has more degrees of freedom, sometimes people will pool these estimates. 

<br>

**Joint Inference**  
The confidence coefficient for both $\beta_0$ and $\beta_1$ considered simultaneously is $\le \alpha$  

Let  

 * $\bar{A}_1$ be the event that the first interval covers $\beta_0$  
 * $\bar{A}_2$ be the event that teh second interval covers $\beta_1$  

$$
P(\bar{A}_1) = 1 - \alpha \\
P(\bar{A}_2) = 1 - \alpha
$$

The probability that both $\bar{A}_1$ and $\bar{A}_2$  

$$
\begin{align}
P(\bar{A}_1 \cap \bar{A}_2) &= 1 - P(\bar{A}_1 \cup \bar{A}_2) \\
&= 1 - P(A_1) - P(A_2) + P(A_1 \cap A_2) \\
&\ge 1 - P(A_1) - P(A_2) \\
&= 1 - 2\alpha
\end{align}
$$

If $\beta_0$ and $\beta_1$ have separate 95% confidence intervals, the joint (family) confidence coefficient is at least $1 - 2(0.05) = 0.9$. This is called a **Bonferroni Inequality**  
We could use a procedure in which we obtained $1-\alpha/2$ confidence intervals for the two regression parameters separately, then the joint (Bonferroni) family confidence coefficient would be at least $1- \alpha$  

The $1-\alpha$ joint Bonferroni confidence interval for $\beta_0$ and $\beta_1$ is given by calculating:  

$$
b_0 \pm B s(b_0) \\
b_1 \pm B s(b_1) 
$$

where $B= t_{1-\alpha/4;n-2}$  

Interpretation:  If repeated samples were taken and the joint $(1-\alpha)$ intervals for $\beta_0$ and $\beta_1$ were obtained, $(1-\alpha)100$% of the joint intervals would contain the true pair $(\beta_0, \beta_1)$. That is, in $\alpha \times 100$% of the samples, one or both intervals would not contain the true value.  

 * The Bonferroni interval is **conservative**. It is a lower bound and the joint intervals will tend to be correct more than $(1-\alpha)100$% of the time (lower power). People usually consider a larger $\alpha$ for the Bonferroni joint tests (e.g, $\alpha=0.1$)  
 * The Bonferroni procedure extends to testing more than 2 parameters. Say we are interested in testing $\beta_0,\beta_1,..., \beta_{g-1}$ (g parameters to test). Then, the joint Bonferroni interval is obtained by calculating the $(1-\alpha/g)$ 100% level interval for each separately.  
 * For example, if $\alpha = 0.05$ and $g=10$, each individual test is done at the $1- \frac{.05}{10}$ level. For 2-sided intervals, this corresponds to using $t_{1-\frac{0.05}{2(10)};n-p}$ in the CI formula. This procedure works best if g is relatively small, otherwise the intervals for each individual parameter are very wide and teh test is way too conservative.  
 * $b_0,b_1$ are usually correlated (negatively if $\bar{X} >0$ and positively if $\bar{X}<0$)  
 * Other multiple comparison procedures are available. 





#### OLS Assumptions

 * [A1 Linearity]
 * [A2 Full rank]
 * [A3 Exogeneity of Independent Variables]
 * [A4 Homoskedasticity]
 * [A5 Data Generation (random Sampling)]
 * [A6 Normal Distribution ]

##### A1 Linearity
$$
\begin{equation}
A1: y=\mathbf{x}\beta + \epsilon
(\#eq:A1)
\end{equation}
$$

Not restrictive

 * x can be nonlinear transformation including interactions, natural log, quadratic

With A3 (Exogeneity of Independent), linearity can be restrictive

###### Log Model

Model | Form | Interpretation of $\beta$ | In words
---|---|---|---
Level-Level | $y =\beta_0+\beta_1x+\epsilon$ | $\Delta y = \beta_1 \Delta x$| A unit change in x will result in $\beta_1$ unit change in y
Log-Level | $ln(y)= \beta_0 + \beta_1x +epsilon$ | $\% \Delta y=100 \beta_1 \Delta x$ | A unit change in x result in 100 $\beta_1$ % change in y
Level-Log | $y = beta_0 + \beta_1 ln(x) + \epsilon$ | $\Delta y = (\beta_1/100)\%\Delta x$ | One percent change in x result in $\beta_1/100$ units change in y
Log-Log | $ln(y) = \beta_0 + \beta_1 ln(x) +\epsilon$ | $\% \Delta y= \beta_1 \% \Delta x$ | One percent change in x result in $\beta_1$ percent change in y

###### Higher Orders
$y=\beta_0 + x_1\beta_1 + x_1^2\beta_2 + \epsilon$
$$
\frac{\partial y}{\partial x_1}=\beta_1 + 2x_1\beta_2
$$

 * The effect of $x_1$ on y depends on the level of $x_1$
 * The partial effect at the average = $\beta_1+2E(x_1)\beta_2$
 * Average Partial Effect = $E(\beta_1 + 2x_1\beta_2)$

###### Interactions
$y=\beta_0 + x_1\beta_1 + x_2\beta_2 + x_1x_2\beta_3 + \epsilon$

 * $\beta_1$ is the average effect on y for a unit change in $x_1$ when $x_2=0$
 * $\beta_1 + x_2\beta_3$ is the partial effect of $x_1$ on y which depends on the level of $x_2$

<br>
<br>

##### A2 Full rank
$$
\begin{equation}
A2: rank(E(x'x))=k
(\#eq:A2)
\end{equation}
$$


also known as **identification condition**

 * columns of $\mathbf{x}$ cannot be written as a linear function of the other columns
 * which ensures that each parameter is unique and exists in the population regression equation

<br>
<br>

##### A3 Exogeneity of Independent Variables


\begin{equation}
A3: E[\epsilon|x_1,x_2,...,x_k]=E[\epsilon|\mathbf{x}]=0
(\#eq:A3)
\end{equation}


**strict exogeneity**

 * also known as **mean independence** check back on [Correlation and Independence]
 * by the [Law of Iterated Expectations] $E(\epsilon)=0$, which can be satisfied by always including an intercept.
 * independent variables do not carry information for prediction of $\epsilon$
 * A3 implies $E(y|x)=x\beta$, which means the conditional mean function must be a linear function of x [A1 Linearity]

###### A3a
Weaker Exogeneity Assumption 

**Exogeneity of Independent variables**



A3a: $E(\mathbf{x_i'}\epsilon_i)=0$


 * $x_i$ is **uncorrelated** with $\epsilon_i$ [Correlation and Independence]
 * Weaker than **mean independence** A3
    + A3 implies A3a, not the reverse
    + No causality interpretations
    + Cannot test the difference


<br>
<br>

##### A4 Homoskedasticity
$$
\begin{equation}
A4: Var(\epsilon|x)=Var(\epsilon)=\sigma^2
(\#eq:A4)
\end{equation}
$$

 * Variation in the disturbance to be the same over the independent variables

<br>
<br>

##### A5 Data Generation (random Sampling)
$$
\begin{equation}
A5: {y_i,x_{i1},...,x_{ik-1}: i = 1,..., n}
(\#eq:A5)
\end{equation}
$$
is a random sample

 * random sample mean samples are independent and identically distributed (iid) from a joint distribution of $(y,\mathbf{x})$
 * with [A3][A3 Exogeneity of Independent Variables] and [A4][A4 Homoskedasticity], we have
    + **Strict Exogeneity**: $E(\epsilon_i|x_1,...,x_n)=0$. independent variables do not carry information for prediction of $\epsilon$
    + **Non-autocorrelation**: $E(\epsilon_i\epsilon_j|x_1,...,x_n)=0$ The error term is uncorrelated across the draws conditional on the independent variables $\rightarrow$ $A4: Var(\epsilon|\mathbf{X})=Var(\epsilon)=\sigma^2I_n$
 * In times series and spatial settings, A5 is less likely to hold.


###### A5a

A stochastic process $\{x_t\}_{t=1}^T$ is **stationary** if for every collection fo time indices $\{t_1,t_2,...,t_m\}$, the joint distribution of 

$$
x_{t_1},x_{t_2},...,x_{t_m}
$$
is the same as the joint distribution of 

$$
x_{t_1+h},x_{t_2+h},...,x_{t_m+h}
$$
for any $h \ge 1$  

 * The joint distribution for the first ten observation is the same for the next ten, etc. 
 * Independent draws automatically satisfies this


<br>



A stochastic process $\{x_t\}_{t=1}^T$ is **weakly stationary** if $x_t$ and $x_{t+h}$ are "almost independent" as h increases without bounds.  
 * two observation that are very far apart should be "almost independent"

Common Weakly Dependent Processes  

 1. Moving Average process of order 1 (MA(1))

MA(1) means that there is only one period lag.

$$
y_t = u_t + \alpha_1 u_{t-1} \\
E(y_t) = E(u_t) + \alpha_1E(u_{t-1}) = 0 \\
Var(y_t) = var(u_t) + \alpha_1 var(u_{t-1}) = \sigma^2 + \alpha_1^2 \sigma^2 = \sigma^2(1+\alpha_1^2)
$$
where $u_t$ is drawn iid over t with variance $\sigma^2$

An increase in the absolute value of $\alpha_1$ increases the variance

When the MA(1) process can be **inverted** ($|\alpha|<1$ then

$$
u_t = y_t - \alpha_1u_{t-1}
$$
called the autoregressive representation (express current observation in term of past observation).


We can expand it to more than 1 lag, then we have MA(q) process

$$
y_t = u_t + \alpha_1 u_{t-1} + ... + \alpha_q u_{t-q}
$$

where $u_t \sim WN(0,\sigma^2)$

 * Covariance stationary: irrespective of the value of the parameters. 
 * Invertibility when $\alpha < 1$
 * The conditional mean of MA(q) depends on the q lags (long-term memory).
 * In MA(q), all autorcorrealtions beyond q are 0.

$$
\begin{align}
Cov(y_t,y_{t-1}) &= Cov(u_t + \alpha_1 u_{t-1},u_{t-1}+\alpha_1u_{t-2}) \\
&= \alpha_1var(u_{t-1}) \\
&= \alpha_1\sigma^2
\end{align}
$$


$$
\begin{align}
Cov(y_t,y_{t-2}) &= Cov(u_t + \alpha_1 u_{t-1},u_{t-2}+\alpha_{1}u_{t-3}) \\
&= 0
\end{align}
$$

An MA models a linear relationship between teh dependent variable and teh current and past values of a stochastic term. 


 2. Auto regressive process of order 1 (AR(1))

$$
y_t = \rho y_{t-1}+ u_t, |\rho|<1
$$

where $u_t$ is drawn iid over t with variance $\sigma^2$


$$
\begin{align}
Cov(y_t,y_{t-1}) &= Cov(\rho y_{t-1} + u-t,y_{t-1}) \\
&= \rho Var(y_{t-1}) \\
&= \rho \frac{\sigma^2}{1-\rho^2}
\end{align}
$$

$$
\begin{align}
Cov(y_t,y_{t-h}) &= \rho^h \frac{\sigma^2}{1-\rho^2}
\end{align}
$$

Stationarity: 
in the continuum of t, the distribution of each t is the same 

$$
E(y_t) = E(y_{t-1}) = ...= E(y_0) \\
y_1 = \rho y_0 + u_1
$$
where the initial observation $y_0=0$

Assume $E(y_t)=0$


$$
y_t = \rho^t y_{t-t} + \rho^{t-1}u_1 + \rho^{t-2}u_2 +...+ \rho u_{t-1} + u_t \\
= \rho^t y_0 + \rho^{t-1}u_1 + \rho^{t-2}u_2 +...+ \rho u_{t-1} + u_t
$$

Hence, $y_t$ is the weighted of all of the $u_t$ time observations before. 
y will be correlated with all the previous observations as well as future observations. 

$$
Var(y_t) = Var(\rho y_{t-1} + u_t) \\
= \rho^2 Var(y_{t-1}) + Var(u_t) + 2\rho Cov(y_{t-1}u_t) \\
= \rho^2 Var(y_{t-1}) + \sigma^2
$$
Hence, 

$$
Var(y_t) = \frac{\sigma^2}{1-\rho^2}
$$
to have Variance constantly over time, then $\rho \neq 1$ or $-1$.

**Then** stationarity requires $\rho \neq 1$ or -1. 
weakly dependent process $|\rho|<1$


To estimate the AR(1) process, we use **Yule-Walker Equation**

$$
y_t = \epsilon_t + \phi y_{t-1} \\
y_t y_{t-\tau} = \epsilon_t y_{t-\tau} + \phi y_{t-1}y_{t-\tau} \\
$$
For $\tau \ge 1$, we have

$$
\gamma \tau = \phi \gamma (\tau -1) \\
\rho_t = \phi^t
$$
when you generalize to pth order autoregressive process, AR(p):

$$
y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + ... + \phi_p y_{t-p} + \epsilon_t
$$

AR(p) process is **covariance stationary**, and decay in autocorrelations. 

When we combine MA(q) and AR(p), we have ARMA(p,q) process, where you can see seasonality. For example, ARMA(1,1)

$$
y_t = \phi y_{t-1} + \epsilon_t + \alpha \epsilon_{t-1}
$$



Random Walk process  


$$
y_t = y_0 + \sum_{s=1}^{t}u_t
$$

 * not stationary : when $y_0 = 0$ then $E(y_t)= 0$, but $Var(y_t)=t\sigma^2$. Further along in the spectrum, the variance will be larger
 * not weakly dependent: $Cov(\sum_{s=1}^{t}u_s,\sum_{s=1}^{t-h}u_s) = (t-h)\sigma^2$. So the covariance (fixed) is not diminishing as h increases


$$
Assumption A5a: \{y_t,x_{t1},..,x_{tk-1} \}
$$
where $t=1,...,T$ are **stationary and weakly dependent processes**.

Alternative [Weak Law], [Central Limit Theorem]  
If $z_t$ is a weakly dependent stationary process with a finite first absolute moment and $E(z_t) = \mu$, then 

$$
T^{-1}\sum_{t=1}^{T}z_t \to^p \mu
$$
If additional regulatory conditions hold [@Greene_1990], then 

$$
\sqrt{T}(\bar{z}-\mu) \to^d N(0,B)
$$
where $B= Var(z_t) + 2\sum_{h=1}^{\infty}Cov(z_t,z_{t-h})$



<br>

##### A6 Normal Distribution
$$
\begin{equation}
A6: \epsilon|\mathbf{x}\sim N(0,\sigma^2I_n)
(\#eq:A6)
\end{equation}
$$
The error term is normally distributed

<br>

From A1-A3, we have **identification** (also known as **Orthogonality Condition**) of the population parameter $\beta$

\begin{align}
y &= {x}\beta + \epsilon && \text{A1} \\
x'y &= x'x\beta + x'\epsilon && \text{} \\
E(x'y) &= E(x'x)\beta + E(x'\epsilon)  && \text{} \\
E(x'y) &= E(x'x)\beta && \text{A3} \\
[E(x'x)]^{-1}E(x'y) &= [E(x'x)]^{-1}E(x'x)\beta && \text{A2} \\
[E(x'x)]^{-1}E(x'y) &= \beta
\end{align}


\beta is the row vector of parameters that produces the best predictor of y
we choose the min of  \gamma : 
$$
\underset{\gamma}{\operatorname{argmin}}E((y-x\gamma)^2)
$$
First Order Condition
$$
\begin{split}
\frac{\partial((y-x\gamma)^2)}{\partial\gamma}&=0 \\
-2E(x'(y-x\gamma))&=0 \\
E(x'y)-E(x'x\gamma) &=0 \\
E(x'y) &= E(x'x)\gamma \\
(E(x'x))^{-1}E(x'y) &= \gamma
\end{split}
$$

Second Order Conditon
$$
\begin{split}
\frac{\partial^2E((y-x\gamma)^2)}{}&=0 \\
E(\frac{\partial(y-x\partial)^2)}{\partial\gamma\partial\gamma'}) &= 2E(x'x)
\end{split}
$$
If [A3][A3 Exogeneity of Independent Variables] holds, then $2E(x'x)$ is PSD $\rightarrow$ minimum

<br>
<br>

#### Theorems
##### Frisch-Waugh-Lovell Theorem

$$
\mathbf{y=X\beta + \epsilon=X_1\beta_1+X_2\beta_2 +\epsilon}
$$
Equivalently, 
$$
\left(
\begin{array}{c}
X_1'X_1 & X_1'X_2 \\
X_2'X_1 & X_2'X_2
\end{array}
\right)
\left(
\begin{array}{c}
\hat{\beta_1} \\
\hat{\beta_2}
\end{array}
\right)
=
\left(
\begin{array}{c}
X_1'y \\
X_2'y
\end{array}
\right)
$$
Hence, 
$$
\mathbf{\hat{\beta_1}=(X_1'X_1)^{-1}X_1'y - (X_1'X_1)^{-1}X_1'X_2\hat{\beta_2}}
$$

 1. Betas from the multiple regression are not the same as the betas from each of the individual simple regression 
 2. Different set of X will affect all the coefficient estimates. 
 3. If $X_1'X_2 = 0$ or $\hat{\beta_2}=0, then 1 and 2 do not hold.

<br>

##### Gauss-Markov Theorem

For a linear regression model

$$
\mathbf{y=X\beta + \epsilon}
$$

Under [A1][A1 Linearity], [A2][A2 Full rank], [A3][A3 Exogeneity of Independent Variables], [A4][A4 Homoskedasticity], OLS estimator defined as 

$$
\hat{\beta} = \mathbf{(X'X)^{-1}X'y}
$$
is the minimum variance linear (in y) unbiased estimator of $\beta$

Let $\tilde{\beta}=\mathbf{Cy}$, be another linear estimator where $\mathbf{C}$ is k x n and only function of X), then for it be unbiased, 

$$
\begin{split}
E(\tilde{\beta}|\mathbf{X}) &= E(\mathbf{Cy|X}) \\
&= E(\mathbf{CX\beta + C\epsilon|X}) \\
&= \mathbf{CX\beta}
\end{split}
$$
which equals the true parameter $\beta$ only if $\mathbf{CX=I}$  
Equivalently, 
$\tilde{\beta} = \beta + \mathbf{C}\epsilon$
and the variance of the estimator is $Var(\tilde{\beta}|\mathbf{X}) = \sigma^2\mathbf{CC'}$

To show minimum variance, 
$$
\begin{split}
&=\sigma^2\mathbf{(C-(X'X)^{-1}X')(C-(X'X)^{-1}X')'} \\
&= \sigma^2\mathbf{(CC' - CX(X'X)^{-1})-(X'X)^{-1}X'C + (X'X)^{-1}X'X(X'X)^{-1})} \\
&= \sigma^2 (\mathbf{CC' - (X'X)^{-1}-(X'X)^{-1} + (X'X)^{-1}}) \\
&= \sigma^2\mathbf{CC'} - \sigma^2(\mathbf{X'X})^{-1} \\
&= Var(\tilde{\beta}|\mathbf{X}) - Var(\hat{\beta}|\mathbf{X})
\end{split}
$$

**Hierarchy of OLS Assumptions**

|Identification Data Description | Unbiasedness Consistency | [Gauss-Markov][Gauss-Markov Theorem] (BLUE) Asymptotic Inference (z and Chi-squared) | Classical LM (BUE) Small-sample Inference (t and F)|
|--- | --- | --- | ---|
|Variation in X | Variation in X | Variation in X | Variation in X|
|     | Random Sampling | Random Sampling  | Random Sampling |
|     | Linearity in Parameters | Linearity in Parameters | Linearity in Parameters|
|     | Zero Conditional Mean | Zero Conditional Mean | Zero Conditional Mean|
|     |                | Homoskedasticity | Homoskedasticity|
|     |           |        | Normality of Errors|

<br>
<br>
<br>
<br>

#### Finite Sample Properties

 * n is fixed 
 * **Bias** On average, how close is our estimate to the true value
    + $Bias = E(\hat{\beta}) -\beta$ where $\beta$ is the true parameter value and $\hat{\beta}$ is the estimator for $\beta$
    + An estimator is **unbiased** when  
        - $Bias = E(\hat{\beta}) -\beta = 0$ or $E(\hat{\beta})=\beta$
        - means that the estimator will produce estimates that are, on average, equal to the value it it trying to estimate
 * **Distribution of an estimator**: An estimator is a function of random variables (data) 
 * **Standard Deviation**: the spread of the estimator.

**OLS**

Under [A1][A1 Linearity] [A2][A2 Full rank] [A3][A3 Exogeneity of Independent Variables], OLS is unbiased

$$
\begin{align}
E(\hat{\beta}) &= E(\mathbf{(X'X)^{-1}X'y}) && \text{A2}\\
     &= E(\mathbf{(X'X)^{-1}X'(X\beta + \epsilon)}) && \text{A1}\\
     &= E(\mathbf{(X'X)^{-1}X'X\beta + (X'X)^{-1}X'\epsilon})  && \text{} \\
     &= E(\beta + \mathbf{(X'X)^{-1}X'\epsilon}) \\
     &= \beta + E(\mathbf{(X'X^{-1}\epsilon)}) \\
     &= \beta + E(E((\mathbf{X'X)^{-1}X'\epsilon|X})) &&\text{LIE} \\
     &= \beta + E((\mathbf{X'X)^{-1}X'}E\mathbf{(\epsilon|X})) \\
     &= \beta + E((\mathbf{X'X)^{-1}X'}0)) && \text{A3} \\
     &= \beta
\end{align}
$$

where LIE stands for [Law of Iterated Expectation]

If [A3][A3 Exogeneity of Independent Variables] does not hold, then OLS will be **biased**

From **Frisch-Waugh-Lovell Theorem**, if we have the omitted variable $\hat{\beta}_2 \neq 0$ and $\mathbf{X_1'X_2} \neq 0$, then the omitted variable will cause OLS estimator to be biased.


Under [A1][A1 Linearity] [A2][A2 Full rank] [A3][A3 Exogeneity of Independent Variables] [A4][A4 Homoskedasticity],  we have the conditional variance of the OLS estimator as follows]

$$
\begin{align}
Var(\hat{\beta}|\mathbf{X}) &= Var(\beta + \mathbf{(X'X)^{-1}X'\epsilon|X}) && \text{A1-A2}\\
    &= Var((\mathbf{X'X)^{-1}X'\epsilon|X)} \\
    &= \mathbf{X'X^{-1}X'} Var(\epsilon|\mathbf{X})\mathbf{X(X'X)^{-1}} \\
    &= \mathbf{X'X^{-1}X'} \sigma^2I \mathbf{X(X'X)^{-1}} && \text{A4} \\
    &= \sigma^2\mathbf{X'X^{-1}X'} I \mathbf{X(X'X)^{-1}} \\
    &= \sigma^2\mathbf{(X'X)^{-1}}
\end{align}
$$

Sources of variation 

 1. $\sigma^2=Var(\epsilon_i|\mathbf{X})$  
    * The amount of unexplained variation $\epsilon_i$ is large relative to the explained $\mathbf{x_i \beta}$ variation

 2. "Small" $Var(x_{i1}), Var(x_{i1}),..$  
    * Not a lot of variation in $\mathbf{X}$ (no information)
    * small sample size

 3. "Strong" correlation between the explanatory variables
    * $x_{i1}$ is highly correlated with a linear combination of 1, $x_{i2}$, $x_{i3}$, ... 
    * include many irrelevant variables will contribute to this.
    * If $x_1$ is perfectly determined in the regression $\rightarrow$ **Perfect Collinearity** $\rightarrow$ [A2][A2 Full rank] is violated. 
    * If $x_1$ is highly correlated with a linear combination of other variables, then we have **Multicollinearity**

##### Check for Multicollinearity

**Variance Inflation Factor** (VIF)
Rule of thumb $VIF \ge 10$ is large

$$
VIF = \frac{1}{1-R_1^2} 
$$


##### Standard Errors

 * $Var(\hat{\beta}|\mathbf{X})=\sigma^2\mathbf{(X'X)^{-1}}$ is the variance of the estimate $\hat{\beta}$
 * **Standard Errors**  are estimators/estimates of the standard deviation (square root of the variance) of the estimator $\hat{\beta}$
 * Under A1-A5, then we can estimate $\sigma^2=Var(\epsilon^2|\mathbf{X})$ the standard errors as 

$$
s^2 = \frac{1}{n-k}\sum_{i=1}^{n}e_i^2 \\
= \frac{1}{n-k}SSR
$$
 
 * degrees of freedom adjustment: because $e_i \neq \epsilon_i$ and are estimated using k estimates for $\beta$, we lose degrees of freedom in our variance estimate.
 * $s=\sqrt{s^2}$ is a biased estimator for the standard deviation ([Jensen’s Inequality])

**Standard Errors for $\hat{\beta}$**

$$
SE(\hat{\beta}_{j-1})=s\sqrt{[(\mathbf{X'X})^{-1}]_{jj}} \\
= \frac{s}{\sqrt{SST_{j-1}(1-R_{j-1}^2)}}
$$
where $SST_{j-1}$ and $R_{j-1}^2$ from the following regression 

$x_{j-1}$ on 1, $x_1$,... $x_{j-2}$,$x_j$,$x_{j+1}$, ..., $x_{k-1}$

**Summary of Finite Sample Properties**

 * Under A1-A3: OLS is unbiased 
 * Under A1-A4: The variance of the OLS estimator is $Var(\hat{\beta}|\mathbf{X})=\sigma^2\mathbf{(X'X)^{-1}}$
 * Under A1-A4, A6: OLS estimator $\hat{\beta} \sim N(\beta,\sigma^2\mathbf{(X'X)^{-1}})$
 * Under A1-A4, Gauss-Markov Theorem holds $\rightarrow$ OLS is BLUE
 * Under A1-A5, the above standard errors are unbiased estimator of standard deviation for $\hat{\beta}$


#### Large Sample Properties

 * let $n \rightarrow \infty$
 * A perspective that allows us to evaluate the "quality" of estimators when finite sample properties are not informative, or impossible to compute
 * consistency, asymptotic distribution, asymptotic variance

**Motivation**

 * [Finite Sample Properties] need strong assumption [A1][A1 Linearity] [A3][A3 Exogeneity of Independent Variables] [A4][A4 Homoskedasticity] [A6][A6 Normal Distribution]
 * Other estimation such as GLS, MLE need to be analyzed using [Large Sample Properties]


Let $\mu(\mathbf{X})=E(y|\mathbf{X})$ be the **Conditional Expectation Function** 

 * $\mu(\mathbf{X})$ is the minimum mean squared predictor (over all possible functions) 

$$
minE((y-f(\mathbf{X}))^2)
$$

under A1 and A3, 

$$
\mu(\mathbf{X})=\mathbf{X}\beta
$$

Then the **linear projection**

$$
L(y|1,\mathbf{X})=\gamma_0 + \mathbf{X}Var(X)^{-1}Cov(X,Y)
$$

where $\mathbf{X}Var(X)^{-1}Cov(X,Y)=\gamma$

is the minimum mean squared linear approximation to be conditional mean function 

$$
(\gamma_0,\gamma) = arg min E((E(y|\mathbf{X})-(a+\mathbf{Xb})^2)
$$

 * OLS is always **consistent** for the linear projection, but not necessarily unbiased.
 * Linear projection has no causal interpretation 
 * Linear projection does not depend on assumption A1 and A3


Evaluating an estimator using large sample properties:

 * Consistency: measure of centrality 
 * Limiting Distribution: the shape of the scaled estimator as the sample size increases 
 * Asymptotic variance: spread of the estimator with regards to its limiting distribution.

An estimator $\hat{\theta}$ is consistent for $\theta$ if $\hat{\theta}_n \to^p \theta$

 * As n increases, the estimator converges to the population parameter value. 
 * Unbiased does not imply consistency and consistency does not imply unbiased. 

Based on [Weak Law] of Large Numbers

$$
\begin{split}
\hat{\beta} &= \mathbf{(X'X)^{-1}X'y} \\
&= \mathbf{(\sum_{i=1}^{n}x_i'x_i)^{-1} \sum_{i=1}^{n}x_i'y_i} \\
&= (n^{-1}\mathbf{\sum_{i=1}^{n}x_i'x_i)^{-1}} n^{-1}\mathbf{\sum_{i=1}^{n}x_i'y_i} \\
plim(\hat{\beta}) &= plim((n^{-1}\mathbf{\sum_{i=1}^{n}x_i'x_i)^{-1}} n^{-1}\mathbf{\sum_{i=1}^{n}x_i'y_i}) \\
&= plim((n^{-1}\mathbf{\sum_{i=1}^{n}x_i'x_i)^{-1}})plim(n^{-1}\mathbf{\sum_{i=1}^{n}x_i'y_i}) \\
&= (plim(n^{-1}\mathbf{\sum_{i=1}^{n}x_i'x_i)^{-1}})plim(n^{-1}\mathbf{\sum_{i=1}^{n}x_i'y_i}) && \text{ due to A2, A5} \\
&= E(\mathbf{x_i'x_i})^{-1}E(\mathbf{x_i'y_i})
\end{split}
$$

$$
E(\mathbf{x_i'x_i})^{-1}E(\mathbf{x_i'y_i}) = \beta + E(\mathbf{x_i'x_i})^{-1}E(\mathbf{x_i'\epsilon_i})
$$

Under [A1][A1 Linearity], [A2][A2 Full rank], [A3a], [A5][A5 Data Generation (random Sampling)] OLS is consistent, but not guarantee unbiased.


Under [A1][A1 Linearity], [A2][A2 Full rank], [A3a], [A5][A5 Data Generation (random Sampling)], and $\mathbf{x_i'x_i}$ has finite first and second moments ([CLT][Central Limit Theorem]), $Var(\mathbf{x_i'}\epsilon_i)=\mathbf{B}$  

 * $(n^{-1}\sum_{i=1}^{n}\mathbf{x_i'x_i})^{-1} \to^p (E(\mathbf{x'_ix_i}))^{-1}$
 * $\sqrt{n}(n^{-1}\sum_{i=1}^{n}\mathbf{x_i'}\epsilon_i) \to^d N(0,\mathbf{B})$

$$
\sqrt{n}(\hat{\beta}-\beta) = (n^{-1}\sum_{i=1}^{n}\mathbf{x_i'x_i})^{-1}\sqrt{n}(n^{-1}\sum_{i=1}^{n}\mathbf{x_i'x_i}) \to^{d} N(0,\Sigma)
$$
where $\Sigma=(E(\mathbf{x_i'x_i}))^{-1}\mathbf{B}(E(\mathbf{x_i'x_i}))^{-1}$  

 * holds under [A3a]
 * Do not need [A4][A4 Homoskedasticity] and [A6][A6 Normal Distribution] to apply CLT
    + If [A4][A4 Homoskedasticity] does not hold, then $\mathbf{B}=Var(\mathbf{x_i'}\epsilon_i)=\sigma^2E(x_i'x_i)$ which means $\Sigma=\sigma^2(E(\mathbf{x_i'x_i}))^{-1}$, use standard errors


Heteroskedasticity can be from

 * Limited dependent variable
 * Dependent variables with large/skewed ranges



Solving Asymptotic Variance

$$
\begin{split}
\Sigma &= (E(\mathbf{x_i'x_i}))^{-1}\mathbf{B}(E(\mathbf{x_i'x_i}))^{-1} \\
&= (E(\mathbf{x_i'x_i}))^{-1}Var(\mathbf{x_i'}\epsilon_i)(E(\mathbf{x_i'x_i}))^{-1} \\
&= (E(\mathbf{x_i'x_i}))^{-1}E[(\mathbf{x_i'}\epsilon_i-0)(\mathbf{x_i'}\epsilon_i-0)](E(\mathbf{x_i'x_i}))^{-1} && \text{A3a} \\
&= (E(\mathbf{x_i'x_i}))^{-1}E[E(\mathbf{\epsilon_i^2|x_i)x_i'x_i]}(E(\mathbf{x_i'x_i}))^{-1} && \text{LIE} \\
&= (E(\mathbf{x_i'x_i}))^{-1}\sigma^2E(\mathbf{x_i'x_i})(E(\mathbf{x_i'x_i}))^{-1} && \text{A4} \\
&= \sigma^2(E(\mathbf{x_i'x_i}))
\end{split}
$$

Under [A1][A1 Linearity], [A2][A2 Full rank], [A3a], [A4][A4 Homoskedasticity], [A5][A5 Data Generation (random Sampling)]:

$$
\sqrt{n}(\hat{\beta}-\beta) \to^d N(0,\sigma^2(E(\mathbf{x_i'x_i}))^{-1})
$$


 * The Asymptotic variance is approximation for the variance in the scaled random variable for $\sqrt{n}(\hat{\beta}-\beta)$ when n is large.
 * use $Avar(\sqrt{n}(\hat{\beta}-\beta))/n$ as an approximation for finite sample variance for large n:

$$
Avar(\sqrt{n}(\hat{\beta}-\beta)) \approx Var(\sqrt{n}(\hat{\beta}-\beta)) \\
Avar(\sqrt{n}(\hat{\beta}-\beta))/n \approx Var(\sqrt{n}(\hat{\beta}-\beta))/n = Var(\hat{\beta})
$$

 * Avar(.) does not behave the same way as Var(.)

$$
Avar(\sqrt{n}(\hat{\beta}-\beta))/n \neq Avar(\sqrt{n}(\hat{\beta}-\beta)/\sqrt{n}) \\
\neq Avar(\hat{\beta})
$$

In [Finite Sample Properties], we calculate standard errors as an estimate for the conditional standard deviation:

$$
SE_{fs}(\hat{\beta}_{j-1})=\sqrt{\hat{Var}}(\hat{\beta}_{j-1}|\mathbf{X}) = \sqrt{s^2[\mathbf{(X'X)}^{-1}]_{jj}}
$$

In [Large Sample Properties], we calculate standard errors as an estimate for the square root of asymptotic variance

$$
SE_{ls}(\hat{\beta}_{j-1})=\sqrt{\hat{Avar}(\sqrt{n}\hat{\beta}_{j-1})/n} = \sqrt{s^2[\mathbf{(X'X)}^{-1}]_{jj}}
$$

Hence, the standard error estimator is the same for finite sample and large sample.
 * Same estimator, but conceptually estimating two different things.
 * Valid under weaker assumptions: the assumptions needed to produce a consistent estimator for the finite sample conditional variance (A1-A5) are stronger than those needed to produce a consistent estimator for the asymptotic variance (A1,A2,A3a,A4,A5)





#### Application

```{r}

```






### Feasible Generalized Least Squares

Motivation for a more efficient estimator  

 * [Gauss-Markov Theorem] holds under A1-A4
 * A4: $Var(\epsilon| \mathbf{X} )=\sigma^2I_n$
    + Heteroskedasticity: $Var(\epsilon_i|\mathbf{X}) \neq \sigma^2I_n$
    + Serial Correlation: $Cov(\epsilon_i,\epsilon_j|\mathbf{X}) \neq 0$
 * Without A4, how can we know which unbiased estimator is the most efficient?
 
Original (unweighted) model:

$$
\mathbf{y=X\beta+ \epsilon}
$$
Suppose A1-A3 hold, but A4 does not hold, 
$$
\mathbf{Var(\epsilon|X)=\Omega \neq \sigma^2 I_n}
$$

We will try to use OLS to estimate the transformed (weighted) model

$$
\mathbf{wy=wX\beta + w\epsilon}
$$
We need to choose $\mathbf{w}$ so that

$$
\mathbf{w'w = \Omega^{-1}}
$$
then $\mathbf{w}$ (full-rank matrix) is the **Cholesky decomposition** of $\mathbf{\Omega^{-1}}$ (full-rank matrix)

In other words, $\mathbf{w}$ is the squared root of $\Omega$ (squared root version in matrix)

$$
\Omega = var(\epsilon | X) \\
\Omega^{-1} = var(\epsilon | X)^{-1}
$$

Then, the transformed equation (IGLS) will have the following properties.

\begin{equation}
\begin{split}
\mathbf{\hat{\beta}_{IGLS}} &= \mathbf{(X'w'wX)^{-1}X'w'wy} \\
& = \mathbf{(X'\Omega^{-1}X)^{-1}X'\Omega^{-1}y} \\
& = \mathbf{\beta + X'\Omega^{-1}X'\Omega^{-1}\epsilon}
\end{split}
\end{equation}

Since A1-A3 hold for the unweighted model
\begin{equation}
\begin{split}
\mathbf{E(\hat{\beta}_{IGLS}|X)} & = E(\mathbf{\beta + (X'\Omega^{-1}X'\Omega^{-1}\epsilon)}|X)\\
& = \mathbf{\beta + E(X'\Omega^{-1}X'\Omega^{-1}\epsilon)|X)} \\
& = \mathbf{\beta + X'\Omega^{-1}X'\Omega^{-1}E(\epsilon|X)}  && \text{since A3: $E(\epsilon|X)=0$} \\
& = \mathbf{\beta}
\end{split}
\end{equation}

$\rightarrow$ IGLS estimator is unbiased

\begin{equation}
\begin{split}
\mathbf{Var(w\epsilon|X)} &= \mathbf{wVar(\epsilon|X)w'} \\
& = \mathbf{w\Omega w'} \\
& = \mathbf{w(w'w)^{-1}w'} && \text{since w is a full-rank matrix}\\
& = \mathbf{ww^{-1}(w')^{-1}w'} \\
& = \mathbf{I_n}
\end{split}
\end{equation}

$\rightarrow$ A4 holds for the transformed (weighted) equation

Then, the variance for the estimator is

\begin{equation}
\begin{split}
Var(\hat{\beta}_{IGLS}|\mathbf{X}) & = \mathbf{Var(\beta + (X'\Omega ^{-1}X)^{-1}X'\Omega^{-1}\epsilon|X)} \\
&= \mathbf{Var((X'\Omega ^{-1}X)^{-1}X'\Omega^{-1}\epsilon|X)} \\
&= \mathbf{(X'\Omega ^{-1}X)^{-1}X'\Omega^{-1} Var(\epsilon|X)   \Omega^{-1}X(X'\Omega ^{-1}X)^{-1}} && \text{because A4 holds}\\
&= \mathbf{(X'\Omega ^{-1}X)^{-1}X'\Omega^{-1} \Omega \Omega^{-1} \Omega^{-1}X(X'\Omega ^{-1}X)^{-1}} \\
&= \mathbf{(X'\Omega ^{-1}X)^{-1}}
\end{split}
\end{equation}

Let $A = \mathbf{(X'X)^{-1}X'-(X'\Omega ^{-1} X)X' \Omega^{-1}}$ then
$$
Var(\hat{\beta}_{OLS}|X)- Var(\hat{\beta}_{IGLS}|X) = A\Omega A'
$$
And $\Omega$ is Positive Semi Definite, then $A\Omega A'$ also PSD, then IGLS is more efficient 

The name **Infeasible** comes from the fact that it is impossible to compute this estimator. 

\begin{equation}
\mathbf{w} = 
\left(
\begin{array}{c}
w_{11} & 0 & 0 & ... & 0 \\
w_{21} & w_{22} & 0 & ... & 0 \\
w_{31} & w_{32} & w_{33} & ... & ... \\
w_{n1} & w_{n2} & w_{n3} & ... & w_{nn} \\
\end{array}
\right)
\end{equation}

With $n(n+1)/2$ number of elements and n observations $\rightarrow$ infeasible to estimate. (number of equation > data)

Hence, we need to make assumption on $\Omega$ to make it feasible to estimate $\mathbf{w}$:  

 1. [Heteroskedasticity]  : multiplicative exponential model
 2. [AR(1)]
 3. [Cluster]
 
 
#### Heteroskedasticity


\begin{equation}
\begin{split}
Var(\epsilon_i |x_i) & = E(\epsilon^2|x_i) \neq \sigma^2 \\
& = h(x_i) = \sigma_i^2 \text{(variance of the error term is a function of x)}
\end{split}
(\#eq:h-var-error-term)
\end{equation}


For our model,

$$
y_i = x_i\beta + \epsilon_i \\
(1/\sigma_i)y_i = (1/\sigma_i)x_i\beta + (1/\sigma_i)\epsilon_i
$$

then, from \@ref(eq:h-var-error-term)

$$
\begin{equation}
\begin{split}
Var((1/\sigma_i)\epsilon_i|X) &= (1/\sigma_i^2) Var(\epsilon_i|X) \\
&= (1/\sigma_i^2)\sigma_i^2 \\
&= 1
\end{split}
\end{equation}
$$


then the weight matrix $\mathbf{w}$ in the matrix equation

$$
\mathbf{wy=wX\beta + w\epsilon}
$$

$$
\mathbf{w}= 
\left(
\begin{array}{c}
1/\sigma_1 & 0 & 0 & ... & 0 \\
0 & 1/\sigma_2 & 0 & ... & 0 \\
0 & 0 & 1/\sigma_3 & ... & . \\
. & . & . & . & 0 \\
0 & 0 & . & . & 1/\sigma_n
\end{array}
\right)
$$

**Infeasible Weighted Least Squares**

 1. Assume we know $\sigma_i^2$ (Infeasible)
 2. The IWLS estimator is obtained as the least squared estimated for the following weighted equation

$$
(1/\sigma_i)y_i = (1/\sigma_i)\mathbf{x}_i\beta + (1/\sigma_i)\epsilon_i 
$$

 * Usual standard errors for the weighted equation are valid if $Var(\epsilon | \mathbf{X}) = \sigma_i^2$
 * If $Var(\epsilon | \mathbf{X}) \neq \sigma_i^2$ then heteroskedastic robust standard errors are valid.

**Problem**: We do not know $\sigma_i^2=Var(\epsilon_i|\mathbf{x_i})=E(\epsilon_i^2|\mathbf{x}_i)$  

 * One observation $\epsilon_i$ cannot estimate a sample variance estimate $\sigma_i^2$
    + Model $\epsilon_i^2$ as reasonable (strictly positive) function of $x_i$ and independent error $v_i$ (strictly positive)

$$
\epsilon_i^2=v_i exp(\mathbf{x_i\gamma})
$$
Then we can apply a log transformation to recover a linear in parameters model,

$$
ln(\epsilon_i^2) = \mathbf{x_i\gamma} + ln(v_i)
$$
where $ln(v_i)$ is independent $\mathbf{x}_i$

We do not observe $\epsilon_i$
 * OLS residual ($e_i$) as an approximate



#### Serial Correlation

$$
Cov(\epsilon_i, \epsilon_j | \mathbf{X}) \neq 0
$$

Under covariance stationary, 

$$
Cov(\epsilon_i,\epsilon_j|\mathbf{X}) = Cov(\epsilon_i, \epsilon_{i+h}|\mathbf{x_i,x_{i+h}})=\gamma_h
$$

And the variance covariance matrix is

$$
Var(\epsilon|\mathbf{X}) = \Omega = 
\left(
\begin{array}{c}
\sigma^2 & \gamma_1 & \gamma_2 & ... & \gamma_{n-1} \\
\gamma_1 & \sigma^2 & \gamma_1 & ... & \gamma_{n-2} \\
\gamma_2 & \gamma_1 & \sigma^2 & ... & ... \\
. & . & . & . & \gamma_1 \\
\gamma_{n-1} & \gamma_{n-2} & . & \gamma_1 & \sigma^2
\end{array}
\right)
$$

There n parameters to estimate - need some sort fo structure to reduce number of parameters to estimate. 

 * [Time Series][AR(1)]
    + Effect of inflation and deficit on Treasury BIll interest rates
 * [Cross-sectional][Cluster]
    + Clustering

##### AR(1)

$$
y_t= \beta_0 + x_t\beta_1 + \epsilon_t \\
\epsilon_t = \rho \epsilon_{t-1} + u_t
$$

and the variance covariance matrix is 
$$
Var(\epsilon | \mathbf{X})= \frac{\sigma^2_u}{1-\rho}
\left(
\begin{array}{c}
1 & \rho & \rho^2 & ... & \rho^{n-1} \\
\rho & 1 & \rho & ... & \rho^{n-2} \\
\rho^2 & \rho & 1 & . & . \\
. & . & . & . & \rho \\
\rho^{n-1} & \rho^{n-2} & . & \rho & 1 \\
\end{array}
\right)
$$

Hence, there is only 1 parameter to estimate: $\rho$


 * Under A1, A2, A3a, A5a, OLS is consistent and asymptotically normal
 * Use [Newey West Standard Errors] for valid inference.
 * Apply [Infeasible Cochrane Orcutt] (as if we knew $\rho$)
 * Because

$$
u_t = \epsilon_t - \rho \epsilon_{t-1}
$$
satisfies  A3, A4, A5 we'd like to to transform the above equation to one that has $u_t$ as the error.

$$
\begin{align}
y_t - \rho y_{t-1} &= (\beta_0 + x\beta_1 + \epsilon_t) - \rho (\beta_0 + x_{t-1}\beta_1 + \epsilon_{t-1}) \\
& = (1-\rho)\beta_0 + (x_t - \rho x_{t-1})\beta_1 + u_t
\end{align}
$$

###### Infeasible Cochrane Orcutt
 1. Assume that we know $\rho$ (Infeasible)
 2. The ICO estimator is obtained as the least squared estimated for the following weighted first difference equation

$$
y_t -\rho y_{t-1} = (1-\rho)\beta_0 + (x_t - \rho x_{t-1})\beta_1 + u_t
$$

 * Usual standard errors for the weighted first difference equation are valid if the errors truly follow an AR(1) process
 * If the serial correlation is generated from a more complex dynamic process then [Newey-West HAC standard errors][Newey West Standard Errors] are valid

**Problem** 
We do not know $\rho$

 * $\rho$ is the correlation between $\epsilon_t$ and $\epsilon_{t-1}$: estimate using OLS residuals ($e_i$) as proxy

$$
\hat{\rho} = \frac{\sum_{t=1}^{T}e_te_{t-1}}{\sum_{t=1}^{T}e_t^2}
$$

which can be obtained from the OLS regression of 

$$
e_t = \rho e_{t-1} + u_t
$$
where we suppress the intercept.

 * We are losing an observation
    + By taking the first difference we are dropping the first observation

$$
y_1 = \beta_0 + x_1 \beta_1 + \epsilon_1
$$
    + [Feasiable Prais Winsten] Transformation applies the [Infeasible Cochrane Orcutt] but includes a weighted version of the first observation

$$
(\sqrt{1-\rho^2})y_1 = \beta_0 + (\sqrt{1-\rho^2})x_1 \beta_1 + (\sqrt{1-\rho^2}) \epsilon_1
$$

##### Cluster

$$
y_{gi} = \mathbf{x}_{gi}\beta + \epsilon_{gi}
$$


$$
Cov(\epsilon_{gi}, \epsilon_{hj})
\begin{cases}
= 0 & \text{for $g \neq h$ and any pair (i,j)} \\
\neq 0 & \text{for any (i,j) pair}\\
\end{cases}
$$

**Intra-group Correlation**  
Each individual in a single group may be correlated but independent across groups.   

 * [A4][A4 Homoskedasticity] is violated. usual standard errors for OLS are valid. 
 * Use **cluster robust standard errors** for OLS.

Suppose there are 3 groups with different n

$$
Var(\epsilon| \mathbf{X})= \Omega =
\left(
\begin{array}{c}
\sigma^2 & \delta_{12}^1 & \delta_{13}^1 & 0 & 0 & 0 \\
\delta_{12}^1 & \sigma^2 & \delta_{23}^1 & 0 & 0 & 0 \\
\delta_{13}^1 & \delta_{23}^1 & \sigma^2 & 0 & 0 & 0 \\
0 & 0 & 0 & \sigma^2 & \delta_{12}^2 & 0 \\
0 & 0 & 0 & \delta_{12}^2 & \sigma^2 & 0 \\
0 & 0 & 0 & 0 & 0 & \sigma^2
\end{array}
\right)
$$

where $Cov(\epsilon_{gi}, \epsilon_{gj}) = \delta_{ij}^g$ and $Cov(\epsilon_{gi}, \epsilon_{hj}) = 0$ for any i and j 

**Infeasible Generalized Least Squares (Cluster)**

 1. Assume that $\sigma^2$ and $\delta_{ij}^g$ are known, plug into $\Omega$ and solve for the inverse $\Omega^{-1}$ (infeasible)
 2. The Infeasible Generalized Least Squares Estimator is 

$$
\hat{\beta}_{IGLS} = \mathbf{(X'\Omega^{-1}X)^{-1}X'\Omega^{-1}y}
$$

**Problem** 
 * We do not know $\sigma^2$ and $\delta_{ij}^g$
    + Can make assumptions about data generating process that is causing the clustering behavior.
        - Will give structure to $Cov(\epsilon_{gi},\epsilon_{gj})= \delta_{ij}^g$ which makes it feasible to estimate
        - if the assumptions are wrong then we should use cluster robust standard errors.

**Solution**
Assume **group level random effects** specification in the error

$$
y_{gi} = \mathbf{g}_i \beta + c_g + u_{gi} \\
Var(c_g|\mathbf{x}_i) = \sigma^2_c \\
Var(u_{gi}|\mathbf{x}_i) = \sigma^2_u
$$

where $c_g$ and $u_{gi}$ are independent of each other, and mean independent of $\mathbf{x}_i$  

 * $c_g$ captures the common group shocks (independent across groups)
 * $u_{gi}$ captures the individual shocks (independent across individuals and groups)


Then the error variance is 

$$
Var(\epsilon| \mathbf{X})= \Omega =
\left(
\begin{array}{c}
\sigma^2_c + \sigma^2_u & \sigma^2_c & \sigma^2_c & 0 & 0 & 0 \\
\sigma^2_c & \sigma^2 + \sigma^2_u & \sigma^2_c & 0 & 0 & 0 \\
\sigma^2_c & \sigma^2_c  & \sigma^2+ \sigma^2_u & 0 & 0 & 0 \\
0 & 0 & 0 & \sigma^2+ \sigma^2_u & \sigma^2_c & 0 \\
0 & 0 & 0 & \sigma^2_c & \sigma^2+ \sigma^2_u & 0 \\
0 & 0 & 0 & 0 & 0 & \sigma^2+ \sigma^2_u
\end{array}
\right)
$$

Use [Feasible group level Random Effects]



### Weighted Least Squares

 1. Estimate the following equation using OLS

$$
y_i = \mathbf{x}_i \beta + \epsilon_i
$$
and obtain the residuals $e_i=y_i -\mathbf{x}_i \hat{\beta}$

 2. Transform the residual and estimate the following by OLS, 

$$
ln(e_i^2)= \mathbf{x}_i\gamma + ln(v_i)
$$

and obtain the predicted values $g_i=\mathbf{x}_i \hat{\gamma}$

 3. The weights will be the untransformed predicted outcome, 

$$
\hat{\sigma}_i =\sqrt{exp(g_i)}
$$

 4. The FWLS (Feasible WLS) estimator is obtained as the least squared estimated for the following weighted equation

$$
(1/\hat{\sigma}_i)y_i = (1/\hat{\sigma}_i) \mathbf{x}_i\beta + (1/\hat{\sigma}_i)\epsilon_i
$$

**Properties of the FWLS**  

 * The infeasible WLS estimator is unbiased under A1-A3 for the unweighted equation.
 * The FWLS estimator is NOT an unbiased estimator. 
 * The FWLS estimator is consistent under [A1][A1 Linearity], [A2][A2 Full rank], (for the unweighted equation), [A5][A5 Data Generation (random Sampling)], and $E(\mathbf{x}_i'\epsilon_i/\sigma^2_i)=0$
    + [A3a] is not sufficient for the above equation 
    + [A3][A3 Exogeneity of Independent Variables] is sufficient for the above equation.
 * The FWLS estimator is asymptotically more efficient than OLS if the errors have multiplicative exponential heteroskedasticity.
    + If the errors are truly multiplicative exponential heteroskedasticity, then usual standard errors are valid
    + If we believe that there may be some mis-specification with the **multiplicative exponential model**, then we should report heteroskedastic robust standard errors.


### Feasiable Prais Winsten

Weighting Matrix

$$
\mathbf{w} = 
\left(
\begin{array}{c}
\sqrt{1- \hat{\rho}^2} & 0 & 0 &... & 0 \\
-\hat{\rho} & 1 & 0 & ... & 0 \\
0 &  -\hat{\rho} & 1 & & . \\
. & . & . & . & 0 \\
0 & . & 0 & -\hat{\rho} & 1
\end{array}
\right)
$$

 1. Estimate the following equation using OLS

$$
y_t = \mathbf{x}_t \beta + \epsilon_t
$$
and obtain the residuals $e_t =  y_t - \mathbf{x}_t \hat{\beta}$

 2. Estimate the correlation coefficient for the [AR(1)] process by estimating the following by OLS (without no intercept)

$$
e_t = \rho e_{t-1} + u_t
$$

 3. Transform the outcome and independent variables $\mathbf{wy}$ and $\mathbf{wX}$ respectively (weight matrix as stated). 
 4. The [FPW][Feasiable Prais Winsten] estimator is obtained as the least squared estimated for the following weighted equation 

$$
\mathbf{wy = wX\beta + w\epsilon}
$$


**Properties of [Feasiable Prais Winsten] Estimator**

 * The Infeasible PW estimator is under A1-A3 for the unweighted equation
 * The [FPW][Feasiable Prais Winsten] estimator is biased
 * The [FPW][Feasiable Prais Winsten] is consistent under [A1][A1 Linearity] [A2][A2 Full rank] [A5][A5 Data Generation (random Sampling)] and 

$$
E((\mathbf{x_t - \rho x_{t-1}})')(\epsilon_t - \rho \epsilon_{t-1})=0
$$
    + [A3a] is not sufficient for the above equation 
    + [A3][A3 Exogeneity of Independent Variables] is sufficient for the above equation

 * The [FPW][Feasiable Prais Winsten] estimator is asymptotically more efficient than OLS if the errors are truly generated as AR(1) process
    + If the errors are truly generated as AR(1) process then usual standard errors are valid 
    + If we are concerned that there may be a more complex dependence structure of heteroskedasticity, then we use [Newey West Standard Errors]

### Feasible group level Random Effects

 1. Estimate the following equation using OLS

$$
y_{gi} = \mathbf{x}_{gi}\beta + \epsilon_{gi}
$$
and obtain the residuals $e_{gi} = y_{gi} - \mathbf{x}_{gi}\hat{\beta}$
 2. Estimate the variance using the usual $s^2 estimator 

$$
s^2 = \frac{1}{n-k}\sum_{i=1}^{n}e_i^2
$$
as an estimator for $\sigma^2_c + \sigma^2_u$ and estimate the within group correlation,

$$
\hat{\sigma}^2_c = \frac{1}{G} \sum_{g=1}^{G} (\frac{1}{\sum_{i=1}^{n_g-1}i}\sum_{i\neq j}\sum_{j}^{n_g}e_{gi}e_{gj})
$$

and plug in the estimates to obtain $\hat{\Omega}$

 3. The feasible group level RE estimator is obtained as 

$$
\hat{\beta}= \mathbf{(X'\hat{\Omega}^{-1}X)^{-1}X'\hat{\Omega}^{-1}y}
$$

**Properties of the [Feasible group level Random Effects] Estimator**

 * The infeasible group RE estimator is a linear estimator and is unbiased under A1-A3 for the unweighted equation 
    + A3 requires $E(\epsilon_{gi}|\mathbf{x}_i) = E(c_{g}|\mathbf{x}_i)+ (u_{gi}|\mathbf{x}_i)=0$ so we generally assume $E(c_{g}|\mathbf{x}_i)+ (u_{gi}|\mathbf{x}_i)=0$. The assumption $E(c_{g}|\mathbf{x}_i)=0$ is generally called **random effects assumption**
 * The [Feasible group level Random Effects] is biased
 * The [Feasible group level Random Effects] is consistent under A1-A3a, and A5a for the unweighted equation.
    + [A3a] requires $E(\mathbf{x}_i'\epsilon_{gi}) = E(\mathbf{x}_i'c_{g})+ (\mathbf{x}_i'u_{gi})=0$
 * The [Feasible group level Random Effects] estimator is asymptotically more efficient than OLS if the errors follow the random effects specification
    + If the errors do follow the random effects specification than the usual standard errors are consistent
    + If there might be a more complex dependence structure or heteroskedasticity, then we need cluster robust standard errors.


### Generalized Least Squares



### Maximum Likelihood
Premise: find values of the parameters that maximize the probability of observing the data
In other words, we try to maximize the value of theta in the likelihood function 
$$
L(\theta)=\prod_{i=1}^{n}f(y_i|\theta)
$$
$f(y|\theta)$ is the probability density of observing a single value of Y given some value of $\theta$
$f(y|\theta)$ can be specify as various type of distributions. You can review back section [Distributions]. For example 
If y is a dichotomous variable, then 
$$
L(\theta)=\prod_{i=1}^{n}\theta^{y_i}(1-\theta)^{1-y_i}
$$

$\hat{\theta}$ is the Maximum Likelihood estimate if $L(\hat{\theta}) > L(\theta_0)$ for all values of $\theta_0$ in the parameter space.

#### Motivation for MLE

Suppose we know the conditional distribution of y given x: 

$$
f_{Y|X}(y,x;\theta)
$$
where $\theta$ is the unknown parameter of distribution. Sometimes we are only concerned with the unconditional distribution $f_{Y}(y;\theta)$  

Then given a sample of iid data, we can calculate the joint distribution of the entire sample, 

$$
f_{Y_1,...,Y_n|X_1,...,X_n(y_1,...y_n,x_1,...,x_n;\theta)}= \prod_{i=1}^{n}f_{Y|X}(y_i,x_i;\theta)
$$
The joint distribution evaluated at the sample is the likelihood (probability) that we observed this particular sample (depends on $\theta$)

Idea for MLE: Given a sample, we choose our estimates of the parameters that gives the highest likelihood (probability) of observing our particular sample 

$$
max_{\theta} \prod_{i=1}^{n}f_{Y|X}(y_i,x_i; \theta)
$$

Equivalently,

$$
max_{\theta} \prod_{i=1}^{n} ln(f_{Y|X}(y_i,x_i; \theta))
$$
Solving for the Maximum Likelihood Estimator  

 1. Solve First Order Condition

$$
\frac{\partial}{\partial \theta}\sum_{i=1}^{n} ln(f_{Y|X}(y_i,x_i;\hat{\theta}_{MLE})) = 0
$$
where $\hat{\theta}_{MLE}$ is defined.

 2. Evaluate Second Order Condition

$$
\frac{\partial^2}{\partial \theta^2} \sum_{i=1}^{n} ln(f_{Y|X}(y_i,x_i;\hat{\theta}_{MLE})) < 0
$$

where the above condition ensures we can solve for a maximum




Examples:  
Unconditional Poisson Distribution: Number of products ordered on Amazon within an hour, number of website visits a day for a political campaign.  

Exponential Distribution: Length of time until an earthquake occurs, length of time a car battery lasts.  

$$
f_{Y|X}(y,x;\theta) = exp(-y/x\theta)/x\theta \\
f_{Y_1,..Y_n|X_1,...,X_n(y_1,...,y_n,x_1,...,x_n;\theta)} = \prod_{i=1}^{n}exp(-y_i/x_i \theta)/x_i \theta
$$



#### Assumption

 * **High Level Regulatory Assumptions** is the sufficient condition used to show large sample properties 
    + Hence, for each MLE, we will need to either assume or verify if the regulatory assumptiosn holds.
 * observations are independent and have the same density function. 
 * Under multivariate normal assumption, ML yields consistent estimates of the means and the covariance matrix for multivariate distribution with finite fourth moments [@Little_1987]

To find the MLE, we usually differentiate the **log-likelihood** function and set it equal to 0.

$$
\frac{d}{d\theta}l(\theta) = 0 
$$
This is the **score** equation

Our confidence in the MLE is quantified by the "pointedness" of the log-likelihood
$$
I_O(\theta)= \frac{d^2}{d\theta^2}l(\theta) = 0 
$$
called the **observed information**

while 
$$
I(\theta)=E[I_O(\theta;Y)]
$$
is the expected information. (also known as Fisher Information). which we base our variance of the estimator. 

$$
V(\hat{\Theta}) \approx I(\theta)^{-1}
$$


**Consistency** of MLE   
Suppose that $y_i$ and $x_i$ are iid drawn from the true conditional pdf $f_{Y|X}(y_i,x_i;\theta_0)$. If the following regulatory assumptions hold,  

 R1: If $\theta \neq \theta_0$ then $f_{Y|X}(y_i,x_i;\theta) \neq f_{Y|X}(y_i,x_i;\theta_0)$  
 R2: The set $\Theta$ that contains the true parameters $\theta_0$ is compact   
 R3: The log-likelihood $ln(f_{Y|X}(y_i,x_i;\theta_0))$ is continuous at each $\theta$ with probability 1  
 R4: $E(sup_{\theta \in \Theta}|ln(f_{Y|X}(y_i,x_i;\theta_0))|)$  
 
then the MLE estimator is consistent, 

$$
\hat{\theta}_{MLE} \to^p \theta_0
$$


**Asymptotic Normality** of MLE  

Suppose that $y_1$ and $x_i$ are iid drawn from the true conditional pdf $f_{Y|X}(y_i,x_i;\theta)$. If R1-R4 and the following hold  

 R5: $\theta_0$ is in the interior of the set $\Theta$  
 R6: $f_{Y|X}(y_i,x_i;\theta)$ is twice continuously differentiable in $\theta$ and $f_{Y|X}(y_i,x_i;\theta) >0$ for a neighborhood $N \in \Theta$ around $\theta_0$  
 R7: $\int sup_{\theta \in N}||\partial f_{Y|X}(y_i,x_i;\theta)\partial\theta||d(y,x) <\infty$, $\int sup_{\theta \in N} || \partial^2 f_{Y|X}(y_i,x_i;\theta)/\partial \theta \partial \theta' || d(y,x) < \infty$ and $E(sup_{\theta \in N} || \partial^2ln(f_{Y|X}(y_i,x_i;\theta)) / \partial \theta \partial \theta' ||) < \infty$  
 R8: The information matrix $I(\theta_0) = Var(\partial f_{Y|X}(y,x_i; \theta_0)/\partial \theta)$ exists and is non-singular  
 
then the MLE estimator is asymptotically normal, 

$$
\sqrt{n}(\hat{\theta}_{MLE} - \theta_0) \to^d N(0,I(\theta_0)^{-1})
$$

#### Properties
[@EJD_1998]

 (1) Consistent: estimates are approximately unbiased in large samples
 (2) Asymptotically efficient: approximately smaller standard errors compared to other estimator
 (3) Asymptotically normal: with repeated sampling, the estimates will have an approximately normal distribution. 
 (4) Invariance: MLE for $g(\theta) = g(\hat{\theta})$

$$
\hat{\Theta} \approx^d (\theta,I(\hat{\theta)^{-1}}))
$$

Explicit vs Implicit MLE

 * If we solve the score equation to get an expression of MLE, then it's called **explicit**
 * If there is no closed form for MLE, and we need some algorithms to derive its expression, it's called **implicit**


**Large Sample Property** of MLE  

Implicit in these theorems is the assumption that we know what the conditional distribution,  

$$
f_{Y|X}(y_i,x_i;\theta_0)
$$
but just do now know the exact parameter value.  

 * Any Distributional mis-specification will result in inconsistent parameter estimates.  
 * Quasi-MLE: Particular settings/ assumption that allow for certain types of distributional mis-specification (Ex: as long as the distribution is part of particular class or satisfies a particular assumption, then estimating with a wrong distribution will not lead to inconsistent parameter estimates).  
 * non-parametric/ Semi-parametric estimation: no or very little distributional assumption are made. (hard to implement, derive properties, and interpret)  

<br>

The asymptotic variance of the MLE achieves the **Cramer-Rao Lower Bound**  

 * The **Cramer-Rao Lower Bound** is a lower brand for the asymptotic variance of a consistent and asymptotically normally distributed estimator.  
 * If an estimator achieves the lower bound then it is the most efficient estimator.  

The maximum Likelihood estimator (assuming the distribution is correctly specified and R1-R8 hold) is the most efficient consistent and asymptotically normal estimator.  
 * most efficient among ALL consistent estimators (not limited to unbiased or linear estimators).  

**Note**  

 * ML is better choice for binary, strictly positive, count, or inherent heteroskedasticity than linear model. 
 * ML will assume that we know the conditional distribution of the outcome, and derive an estimator using that information. 
    + Adds an assumption that we know the distribution (which is similar to [A6 Normal Distribution] in linear model)
    + will produce a more efficient estimator.

#### Application 

Other applications of MLE   

 * Corner Solution  
    + Ex: hours worked, donations to charity  
    + Estimate with Tobit  
 * Non-negative count  
    + Ex: Numbers of arrest, Number of cigarettes smoked a day  
    + Estimate with Poisson regression  
 * Multinomial Choice  
    + Ex: Demand for cars, votes for primary election
    + Estimate with mutinomial probit or logit  
 * Ordinal Choice  
    + Ex: Levels of Happiness, Levels of Income  
    + Ordered Probit  

Model for binary Response  
A binary variable will have a [Bernoulli] distribution:  

$$
f_Y(y_i;p) = p^{y_i}(1-p)^{(1-y_i)}
$$
where p is the probability of success. The conditional distribution is:  

$$
f_{Y|X}(y_i,x_i;p(.)) = p(x_i)^{y_i} (1-p(x_i))^{(1-y_i)}
$$

So choose $p(x_i)$ to be a reasonable function of $x_i$ and unknown parameters $\theta$  

We can use **latent variable model** as probability functions 

$$
y_i = 1\{y_i^* > 0 \}  \\
y_i^* = x_i \beta-\epsilon_i
$$

 * $y_i^*$ is a latent variable (unobserved) that is not well-defined in terms of units/magnitudes  
 * $\epsilon_i$ is a mean 0 unobserved random variable.  

We can rewrite the mdoel without the latent variable,  

$$
y_i = 1\{x_i beta > \epsilon_i \}
$$

Then the probability function, 

$$
p(x_i) = P(y_i = 1|x_i) \\
= P(x_i \beta > \epsilon_i | x_i) \\
= F_{\epsilon|X}(x_i \beta | x_i)
$$

then we need to choose a conditional distribution for $\epsilon_i$. Hence, we can make additional strong independence assumption   

$\epsilon_i$ is independent of $x_i$  

Then the probability function is simply,  

$$
p(x_i) = F_\epsilon(x_i \beta)
$$
The probability function is also the conditional expectation function,  

$$
E(y_i | x_i) = P(y_i = 1|x_i) = F_\epsilon (x_i \beta)
$$

so we allow the conditional expectation function to be non-linear.   

Common distributional assumption  

 1. **Probit**: Assume $\epsilon_i$ is standard normally distributed, then $F_\epsilon(.) = \Phi(.)$ is the standard normal CDF.  
 2. **Logit**: Assume $\epsilon_i$ is standard logistically distributed, then $F_\epsilon(.) = \Lambda(.)$ is the standard normal CDF.  

Step to derive  

 1. Choose a distribution (normal or logistic) and plug into the following log likelihood,  

$$
ln(f_{Y|X} (y_i , x_i; \beta)) = y_i ln(F_\epsilon(x_i \beta)) + (1-y_i)ln(1-F_\epsilon(x_i \beta))
$$

 2. Solve the MLE by finding the Maximum of 

$$
\hat{\beta}_{MLE} = argmax \sum_{i=1}^{n}ln(f_{Y|X}(y_i,x_i; \beta))
$$

<br>

**Properties** of the Probit and Logit Estimators  

 * Probit or Logit is consistent and asymptotically normal if  
    + [A2][] holds: $E(x_i' x_i)$ exists and is non-singular  
    + [A5][] (or [A5a]) holds: {y_i,x_i} are iid (or stationary and weakly dependent). 
    + Distributional assumptions on $\epsilon_i$ hold: Normal/Logistic and independent of $x_i$ 
 * Under the same assumptions, Probit or Logit is also asymptotically efficient with asymptotic variance,  

$$
I(\beta_0)^{-1} = [E(\frac{(f_\epsilon(x_i \beta_0))^2}{F_\epsilon(x_i\beta_0)(1-F_\epsilon(x_i\beta_0))}x_i' x_i)]^{-1}
$$

where $F_\epsilon(x_i\beta_0)$ is the probability density function (derivative of the CDF)  

<br>


##### Interpretation

$\beta$ is the average response in the latent variable associated with a change in $x_i$  

 * Magnitudes do not have meaning 
 * Direction does have meaning 

The **partial effect** for a Non-linear binary response model  

$$
E(y_i |x_i) = F_\epsilon (x_i \beta) \\
PE(x_{ij}) = \frac{\partial E(y_i |x_i)}{\partial x_{ij}} = f_\epsilon (x_i \beta)\beta_j
$$

 * The partial effect is the coefficient parameter $\beta_j$ multiplied by a scaling factor $f_\epsilon (x_i \beta)$  
 * The scaling factor depends on $x_i$ so the partial effect changes depending on what $x_i$ is

Single value for the partial effect  

 * **Partial Effect at the Average (PEA)** is the partial effect for an average individual 

$$
f_{\epsilon}(\bar{x}\hat{\beta})\hat{\beta}_j
$$

 * **Average Partial Effect (APE)** is the average of all partial effect for each individual. 

$$
\frac{1}{n}\sum_{i=1}^{n}f_\epsilon(x_i \hat{\beta})\hat{\beta}_j
$$

In the linear model, APE = PEA.  
In a non-linear model (e.g., binary response), APE $\neq$ PEA


## Quantile Regression

For academic review on quantile regression, check [@Yu_2003]

[Linear Regression] is based on the conditional mean function $E(y|x)$  

In Quantile regression, we can view each points in the conditional distribution of y. Quantile regression estimates the conditional median or any other quantile of Y.

In the case that we're interested in the 50th percentile, quantile regression is median regression, also known as least-absolute-deviations (LAD) regression, minimizes $\sum_{i}|e_i|$


Properties of estimators $\beta$

 * Asymptotically normally distributed


Advantages  

 * More robust to outliers compared to [OLS][Ordinary Least Squares]
 * In the case the dependent variable has a bimodal or multimodal (multiple humps with multiple modes) distribution, quantile regression can be extremely useful. 
 * Avoids parametric distribution assumption of the error process. In another word, no assumptions regarding the distribution of the error term.
 * Better characterization of the data (not just its conditional mean)
 * is invariant to monotonic transformations (such as log) while OLS is not. In another word, $E(g(y))=g(E(y))$
 
Disadvantages

 * The dependent variable needs to be continuous with no zeroes or too many repeated values. 
 
 
$$
y_i = x_i'\beta_q + e_i
$$
 
Let $e(x) = y -\hat{y}(x)$, then $L(e(x)) = L(y -\hat{y}(x))$ is the loss function of the error term.  

If $L(e) = |e|$ (called absolute-error loss function) then $\hat{\beta}$ can be estimated by minimizing $\sum_{i}|y_i-x_i'\beta|$  

More specifically, the objective function is 
$$
Q(\beta_q)=\sum_{i:y_i \ge x_i'\beta}^{N} q|y_i - x_i'\beta_q| + \sum_{i:y_i < x_i'\beta}^{N} (1-q)|y_i-x_i'\beta_q
$$
where $0<q<1$

The sum penalizes $q|e_i|$ for under-prediction and $(1-q)|e_i|$ for over-prediction


We use simplex method to minimize this function (cannot use analytical solution since it's non-differentiable). Standard errors can be estimated by bootstrap.


The absolute-error loss function is symmetric. 

**Interpretation**
For the jth regressor ($x_j$), the marginal effect is the coefficient for the qth quantile

$$
\frac{\partial Q_q(y|x)}{\partial x_j} = \beta_{qj}
$$
At the quantile q of the dependent variable y, $\beta_q$ represents a one unit change in the independent variable $x_j$ on the dependent variable y.  

In other words, at the qth percentile, a one unit change in x results in $\beta_q$ unit change in y.




### Application

```{r}
# generate data with non-constant variance

x <- seq(0,100,length.out = 100)        # independent variable
sig <- 0.1 + 0.05*x                     # non-constant variance
b_0 <- 3                                # true intercept
b_1 <- 0.05                             # true slope
set.seed(1)                             # reproducibility
e <- rnorm(100,mean = 0, sd = sig)      # normal random error with non-constant variance
y <- b_0 + b_1*x + e                    # dependent variable
dat <- data.frame(x,y)
hist(y)
library(ggplot2)
ggplot(dat, aes(x,y)) + geom_point()
ggplot(dat, aes(x,y)) + geom_point() + geom_smooth(method="lm")
```

We follow [@Roger_1996] to estimate quantile regression 
```{r}
library(quantreg)
qr <- rq(y ~ x, data=dat, tau = 0.5) # tau: quantile of interest. Here we have it at 50th percentile.
summary(qr)
```
adding the regression line

```{r}
ggplot(dat, aes(x,y)) + geom_point() + 
  geom_abline(intercept=coef(qr)[1], slope=coef(qr)[2])
```


To have R estimate multiple quantile at once

```{r}
qs <- 1:9/10
qr1 <- rq(y ~ x, data=dat, tau = qs)
#check for its coefficients
coef(qr1)

# plot
ggplot(dat, aes(x,y)) + geom_point() + geom_quantile(quantiles = qs)
```

To examine if the quantile regression is appropriate, we can see its plot compared to least squares regression 
```{r}
plot(summary(qr1), parm="x")
```

where red line is the least squares estimates, and its confidence interval. 
x-axis is the quantile 
y-axis is the value of the quantile regression coefficients at different quantile

If the error term is normally distributed, the quantile regression line will fall inside the coefficient interval of least squares regression. 
```{r}
# generate data with constant variance

x <- seq(0, 100, length.out = 100)    # independent variable
b_0 <- 3                              # true intercept
b_1 <- 0.05                           # true slope
set.seed(1)                           # reproducibility
e <- rnorm(100, mean = 0, sd = 1)     # normal random error with constant variance
y <- b_0 + b_1 * x + e                # dependent variable
dat2 <- data.frame(x, y)
qr2 = rq(y ~ x, data = dat2, tau = qs)
plot(summary(qr2), parm = "x")
```






