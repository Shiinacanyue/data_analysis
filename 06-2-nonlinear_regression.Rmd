# Non-linear Regression

**Definition**: models in which the derivatives of the mean function with respect to the parameters depend on one or more of the parameters.  


To approximate data, we can approximate the function  

 * by a high-order polynomial  
 * by a linear model (e.g., a Taylor expansion around X's)  
 * a collection of locally linear models or basis function  

but it would not easy to interpret, or not enough data, or can't interpret them globally.  

**intrinsically nonlinear** models:  

$$
Y_i = f(\mathbf{x_i;\theta}) + \epsilon_i
$$
where $f(\mathbf{x_i;\theta})$ is a nonlinear function relating $E(Y_i)$ to the independent variables $x_i$  

 * $\mathbf{x}_i$ is a k x 1 vector of independent variables (fixed).  
 * $\mathbf{\theta}$ is a p x 1 vector of parameters.  
 * $\epsilon_i$s are iid variables mean 0 and variance $\sigma^2$. (sometimes it's normal).  

## Inference

Since $Y_i = f(\mathbf{x}_i,\theta) + \epsilon_i$, where $\epsilon_i \sim iid(0,\sigma^2)$. We can obtain $\hat{\theta}$ by minimizing $\sum_{i=1}^{n}(Y_i - f(x_i,\theta))^2$ and estimate  $s^2 = \hat{\sigma}^2_{\epsilon}=\frac{\sum_{i=1}^{n}(Y_i - f(x_i,\theta))^2}{n-p}$

### Linear Function of the Parameters

If we assume $\epsilon_i \sim N(0,\sigma^2)$, then 

$$
\hat{\theta} \sim AN(\mathbf{\theta},\sigma^2[\mathbf{F}(\theta)'\mathbf{F}(\theta)]^{-1})
$$
where An = asymptotic normality

Asymptotic means we have enough data to make inference (As your sample size increases, this becomes more and more accurate (to the true value)).


Since we want to do inference on linear combinations of parameters or contrasts.  

If we have $\mathbf{\theta} = (\theta_0,\theta_1,\theta_2)'$ and we want to look at $\theta_1 - \theta_2$; we can define vector $\mathbf{a} = (0,1,-1)'$, consider inference for $\mathbf{a'\theta}$  


Rules for expectation and variance of a fixed vector $\mathbf{a}$ and random vector $\mathbf{Z}$;  

$$
E(\mathbf{a'Z}) = \mathbf{a'}E(\mathbf{Z}) \\
var(\mathbf{a'Z}) = \mathbf{a'}var(\mathbf{Z})\mathbf{a}
$$

Then,  

$$
\mathbf{a'\hat{\theta}} \sim AN(\mathbf{a'\theta},\sigma^2\mathbf{a'[F(\theta)'F(\theta)]^{-1}a})
$$

and $\mathbf{a'\hat{\theta}}$ is asymptotically independent of $s^2$ (to order 1/n) then  

$$
\frac{\mathbf{a'\hat{\theta}-a'\theta}}{s(\mathbf{a'[F(\theta)'F(\theta)]^{-1}a})^{1/2}} \sim t_{n-p}
$$
to construct $100(1-\alpha)\%$ confidence interval for $\mathbf{a'\theta}$  

$$
\mathbf{a'\theta} \pm t_{(1-\alpha/2,n-p)}s(\mathbf{a'[F(\theta)'F(\theta)]^{-1}a})^{1/2}
$$

Suppose $\mathbf{a'} = (0,...,j,...,0)$. Then, a confidence interval for the jth element of $\mathbf{\theta}$ is 

$$
\hat{\theta}_j \pm t_{(1-\alpha/2,n-p)}s\sqrt{\hat{c}^{j}}
$$
where $\hat{c}^{j}$ is the jth diagonal element of $[\mathbf{F(\hat{\theta})'F(\hat{\theta})}]^{-1}$  


### Nonlinear
Suppose that $h(\theta)$ is a nonlinear function of the parameters. We can use Taylor series about $\theta$  

$$
h(\hat{\theta}) \approx h(\theta) + \mathbf{h}'[\hat{\theta}-\theta]
$$
where $\mathbf{h} = (\frac{\partial h}{\partial \theta_1},...,\frac{\partial h}{\partial \theta_p})'$  

with 

$$
E(\hat{\theta}) \approx \theta \\
var(\hat{\theta}) \approx  \sigma^2[\mathbf{F(\theta)'F(\theta)}]^{-1} \\
E(h(\hat{\theta})) \approx h(\theta) \\
var(h(\hat{\theta})) \approx \sigma^2 \mathbf{h'[F(\theta)'F(\theta)]^{-1}h}
$$

Thus,  

$$
h(\hat{\theta}) \sim AN(h(\theta),\sigma^2\mathbf{h'[F(\theta)'F(\theta)]^{-1}h})
$$
and an approximate $100(1-\alpha)\%$ confidence interval for $h(\theta)$ is 

$$
h(\hat{\theta}) \pm t_{(1-\alpha/2;n-p)}s(\mathbf{h'[F(\theta)'F(\theta)]^{-1}h})^{1/2}
$$

where $\mathbf{h}$ and $\mathbf{F}(\theta)$ are evaluated at $\hat{\theta}$  

Regarding **prediction interval** for Y at $x=x_0$  

$$
Y_0 = f(x_0;\theta) + \epsilon_0, \epsilon_0 \sim N(0,\sigma^2) \\
\hat{Y}_0 = f(x_0,\hat{\theta})
$$

As $n \to \infty$, $\hat{\theta} \to \theta$, so we  

$$
f(x_0, \hat{\theta}) \approx f(x_0,\theta) + \mathbf{f}_0(\mathbf{\theta})'[\hat{\theta}-\theta]
$$
where $f_0(\theta)= (\frac{\partial f(x_0,\theta)}{\partial \theta_1},..,\frac{\partial f(x_0,\theta)}{\partial \theta_p})'$ (note: this $f_0(\theta)$ is different from $f(\theta)$).  

$$
Y_0 - \hat{Y}_0 \approx Y_0  - f(x_0,\theta) - f_0(\theta)'[\hat{\theta}-\theta]  \\
= \epsilon_0 - f_0(\theta)'[\hat{\theta}-\theta]
$$

$$
E(Y_0 - \hat{Y}_0) \approx E(\epsilon_0)E(\hat{\theta}-\theta) = 0 \\
var(Y_0 - \hat{Y}_0) \approx var(\epsilon_0 - \mathbf{(f_0(\theta)'[\hat{\theta}-\theta])}) \\
= \sigma^2 + \sigma^2 \mathbf{f_0 (\theta)'[F(\theta)'F(\theta)]^{-1}f_0(\theta)} \\
= \sigma^2 (1 + \mathbf{f_0 (\theta)'[F(\theta)'F(\theta)]^{-1}f_0(\theta)})
$$

Hence, combining  

$$
Y_0 - \hat{Y}_0 \sim AN (0,\sigma^2 (1 + \mathbf{f_0 (\theta)'[F(\theta)'F(\theta)]^{-1}f_0(\theta)}))
$$






## Non-linear Least Squares

 * The LS estimate of $\theta$, $\hat{\theta}$ is the set of parameters that minimizes the residual sum of squares:  
$$
S(\hat{\theta}) = SSE(\hat{\theta}) = \sum_{i=1}^{n}\{Y_i - f(\mathbf{x_i};\hat{\theta})\}^2
$$
 * to obtain the solution, we can consider the partial derivatives of $S(\theta)$ with respect to each $\theta_j$ and set them to 0, which gives a system of p equations. Each normal equation is 
$$
\frac{\partial S(\theta)}{\partial \theta_j} = -2\sum_{i=1}^{n}\{Y_i -f(\mathbf{x}_i;\theta)\}[\frac{\partial(\mathbf{x}_i;\theta)}{\partial \theta_j}] = 0
$$
 * but we can't obtain a solution directly/analytically for this equation. 

**Numerical Solutions**  

 * Grid search  
    + A "grid" of possible parameter values and see which one minimize the residual sum of squares.  
    + finer grid = greater accuracy  
    + could be inefficient, and hard when p is large.  
 * Gauss-Newton Algorithm  
    + we have an initial estimate of $\theta$ denoted as $\hat{\theta}^{(0)}$  
    + use a Taylor expansions of $f(\mathbf{x}_i;\theta)$ as a function of $\theta$ about the point $\hat{\theta}^{(0)}$  

$$
\begin{align} 
Y_i &= f(x_i;\theta) + \epsilon_i \\
&= f(x_i;\theta) + \sum_{j=1}^{p}\{\frac{\partial f(x_i;\theta)}{\partial \theta_j}\}_{\theta = \hat{\theta}^{(0)}} (\theta_j - \hat{\theta}^{(0)}) + \text{remainder} + \epsilon_i
\end{align}
$$

Equivalently,  

In matrix notation,  

$$
\mathbf{Y} = 
\left[ \begin{array}
{ccc}
Y_1 \\
. \\
Y_n
\end{array} \right]
$$

$$
\mathbf{f}(\hat{\theta}^{(0)}) =
\left[ \begin{array}
{ccc}
f(\mathbf{x_1,\hat{\theta}}^{(0)}) \\
. \\
f(\mathbf{x_n,\hat{\theta}}^{(0)})
\end{array} \right]
$$


$$
\mathbf{\epsilon} = 
\left[ \begin{array}
{ccc}
\epsilon_1 \\
. \\
\epsilon_n
\end{array} \right]
$$

$$
\mathbf{F}(\hat{\theta}^{(0)}) = 
\left[ \begin{array}
{ccc}
\frac{\partial f(x_1,\mathbf{\theta})}{\partial \theta_1} && ... && \frac{\partial f(x_1,\mathbf{\theta})}{\partial \theta_p}\\
. && . && . \\
\frac{\partial f(x_n,\mathbf{\theta})}{\partial \theta_1} && ... && \frac{\partial f(x_n,\mathbf{\theta})}{\partial \theta_p}
\end{array} \right]_{\theta = \hat{\theta}^{(0)}}
$$


Hence,  


$$
\mathbf{Y} = \mathbf{f}(\hat{\theta}^{(0)}) + \mathbf{F}(\hat{\theta}^{(0)})(\theta - \hat{\theta}^{(0)}) + \epsilon + \text{remainder}
$$
where we assume that the remainder is small and the error term is only assumed to be iid with mean 0 and variance $\sigma^2$.  

We can rewrite the above equation as  

$$
\mathbf{Y} - \mathbf{f}(\hat{\theta}^{(0)}) \approx \mathbf{F}(\hat{\theta}^{(0)})(\theta - \hat{\theta}^{(0)}) + \epsilon
$$
where it is in the form of linear model. After we solve for $(\theta - \hat{\theta}^{(0)})$ and let it equal to $\hat{\delta}^{(1)}$  
 Then we new estimate is given by adding the Gauss increment adjustment to the initial estimate $\hat{\theta}^{(1)} = \hat{\theta}^{(0)} + \hat{\delta}^{(1)}$  
 We can repeat this process. 


Gauss-Newton Algorithm Steps:  

 1. initial estimate $\hat{\theta}^{(0)}$, set j = 0  
 2. Taylor series expansion and calculate  $\mathbf{f}(\hat{\theta}^{(j)})$ and $\mathbf{F}(\hat{\theta}^{(j)})$ 
 3. Use OLS to get $\hat{\delta}^{(j+1)}$  
 4. get the new estimate $\hat{\theta}^{(j+1)}$, return to step 2  
 5. continue until "convergence"  
 6. With the final parameter estimate $\hat{\theta}$, we can estimate $\sigma^2$ if $\epsilon \sim (\mathbf{0}, \sigma^2 \mathbf{I})$ by  

$$
\hat{\sigma}^2= \frac{1}{n-p}(\mathbf{Y}-\mathbf{f}(x;\hat{\theta}))'(\mathbf{Y}-\mathbf{f}(x;\hat{\theta}))
$$

<br>

**Criteria for convergence**  

 1. Minor change in the objective function (SSE = residual sum of squares)  
$$
\frac{|SSE(\hat{\theta}^{(j+1)})-SSE(\hat{\theta}^{(j)})|}{SSE(\hat{\theta}^{(j)})} < \gamma_1
$$
 2. Minor change in the parameter estimates  
$$
|\hat{\theta}^{(j+1)}-\hat{\theta}^{(j)}| < \gamma_2
$$
 3. "residual projection" criterion of [@Bates_1981]  

### Alternative of Gauss-Newton Algorithm

#### Gauss-Newton Algorithm

Normal equations:  

$$
\frac{\partial SSE(\theta)}{\partial \theta} = 2\mathbf{F}(\theta)'[\mathbf{Y}-\mathbf{f}(\theta)]
$$

$$
\begin{align}
\hat{\theta}^{(j+1)} &= \hat{\theta}^{(j)} + \hat{\delta}^{(j+1)} \\
&= \hat{\theta}^{(j)} + [\mathbf{F}((\hat{\theta})^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})]^{-1}\mathbf{F}(\hat{\theta})^{(j)} \\
&= \hat{\theta}^{(j)} - \frac{1}{2}[\mathbf{F}(\hat{\theta}^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})]^{-1}\frac{\partial SSE(\hat{\theta}^{(j)})}{\partial \theta}
\end{align}
$$
where  

 * $\frac{\partial SSE(\hat{\theta}^{(j)})}{\partial \theta}$ is a gradient vecotr  (points in the direction in which the SSE increases most rapidly). This path is known as steepest ascent.  
 * $[\mathbf{F}(\hat{\theta}^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})]^{-1}$ indicates how far to move  
 * $-1/2$: indicator of the direction of steepest descent.  


#### Modified Gauss-Newton Algorithm

To avoid overstepping (the local min), we can use the modified Gauss-Newton Algorithm. We define a new proposal for $\theta$  

$$
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} + \alpha_j \hat{\delta}^{(j+1)}, 0 < \alpha_j < 1
$$
where  

 * $\alpha_j$ (called the "learning rate"): is used to modify the step length.  

We could also have $\alpha *1/2$, but typically it is assumed to be absorbed into the learning rate. 

A way to choose $\alpha_j$, we can use **step halving**  

$$
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} + \frac{1}{2^k}\hat{\delta}^{(j+1)}
$$
where  

 * k is the smallest non-negative integer such that  
$$
SSE(\hat{\theta}^{(j)}+\frac{1}{2^k}\hat{\delta}^{(j+1)}) < SSE(\hat{\theta}^{(j)})
$$
which means we try $\hat{\delta}^{(j+1)}$, then $\hat{\delta}^{(j+1)}/2$, $\hat{\delta}^{(j+1)}/4$, etc.  


The most general form of the convergence algorithm is  

$$
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j \mathbf{A}_j \frac{\partial Q(\hat{\theta}^{(j)})}{\partial \theta} 
$$
where  

 * $\mathbf{A}_j$ is a positive definite matrix  
 * $\alpha_j$ is the learning rate  
 * $\frac{\partial Q(\hat{\theta}^{(j)})}{\partial \theta}$is the gradient based on some objective function Q (a function of $\theta$), which is typically the SSE in nonlinear regression applications (e.g., cross-entropy for classification).  

Refer back to the **Modified Gauss-Newton Algorithm**, we can see it is in this form  

$$
\hat{\theta}^{(j+1)} =\hat{\theta}^{(j)} - \alpha_j[\mathbf{F}(\hat{\theta}^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})]^{-1}\frac{\partial SSE(\hat{\theta}^{(j)})}{\partial \theta}
$$
where Q = SSE, $[\mathbf{F}(\hat{\theta}^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})]^{-1} = \mathbf{A}$


#### Steepest Descent

(also known just "gradient descent")  

$$
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j \mathbf{I}_{p \times p}\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}
$$

 * slow to converge, moves rapidly initially.  
 * could be use for starting values  

#### Levenberg -Marquardt

$$
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j [\mathbf{F}(\hat{\theta}^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})+ \tau \mathbf{I}_{p \times p}]\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}
$$

which is a compromise between the [Gauss-Newton Algorithm] and the [Steepest Descent].  

 * best when $\mathbf{F}(\hat{\theta}^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})$ is nearly singular ($\mathbf{F}(\hat{\theta}^{(j)})$ isn't of full rank)  
 * similar to ridge regression  
 * If $SSE(\hat{\theta}^{(j+1)}) < SSE(\hat{\theta}^{(j)})$, then $\tau= \tau/10$ for the next iteration. Otherwise, $\tau = 10 \tau$


#### Newton-Raphson

$$
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j [\frac{\partial^2Q(\hat{\theta}^{(j)})}{\partial \theta \partial \theta'}]^{-1}\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}
$$

The **Hessian matrix** can be rewritten as:  

$$
\frac{\partial^2Q(\hat{\theta}^{(j)})}{\partial \theta \partial \theta'} = 2 \mathbf{F}((\hat{\theta})^{(j)})'\mathbf{F}(\hat{\theta}^{(j)}) - 2\sum_{i=1}^{n}[Y_i - f(x_i;\theta)]\frac{\partial^2f(x_i;\theta)}{\partial \theta \partial \theta'}
$$
which contains the same term that [Gauss-Newton Algorithm
], combined with one containing the second partial derivatives of f(). (methods that require the second derivatives of the objective function are known as "second-order methods".)  
However, the last term \frac{\partial^2f(x_i;\theta)}{\partial \theta \partial \theta'} can sometimes be nonsingular.  



#### Quasi-Newton

update $\theta$ according to  

$$
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j \mathbf{H}_j^{-1}\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}
$$
where $H_j$ is a symmetric positive definite approximation to the Hessian, which gets closer as $j \to \infty$.  

 * $\mathbf{H}_j$ is computed iteratively  
 * AMong first-order methods(where only first derivatives are required), this method performs best.  

#### Derivative Free Methods

 * **secant Method**: like [Gauss-Newton Algorithm], but calculates the derivatives numerically from past iterations.  
 * **Simplex Methods**  
 * **Genetic Algorithm**  
 * **Differential Evolution Algorithms**  
 * **Particle Swarm Optimization**  
 * **Ant Colony Optimization**  

### Practical Considerations

To converge, algorithm need good initial estimates.  

 * Starting values:  
    + Prior or theoretical info  
    + A grid search or a graph of $SSE(\theta)$  
    + could also use OLS to get starting values.  
    + Model interpretation: if you have some idea regarding the form of the objective function, then you can try to guess the initial value.  
    + Expected Value Parameterization  
 * Constrained Parameters: (constraints on parameters like $\theta_i>a,a< \theta_i <b$)  
    + fit the model first to see if the converged parameter estimates satisfy the constraints. 
    + if they dont' satisfy, then try re-parameterizing  


#### Failure to converge

 * $SSE(\theta)$ may be "flat" in a neighborhood of the minimum.  
 * You can try different or "better" starting values.  
 * Might suggest the model is too complex for the data, might consider simpler model.  


#### Convergence to a Local Minimum

 * Linear least squares has the property that $SSE(\theta) = \mathbf{(Y-X\beta)'(Y-X\beta)}$, which is quadratic and has a unique minimum (or maximum).  
 * Nonlinear east squares need  not have a unique minimum  
 * Using different starting values can help  
 * If the dimension of $\theta$ is low, graph $SSE(\theta)$ as a function of $\theta_i$  
 * Different algorithm can help (e.g., genetic algorithm, particle swarm)  



## Genelized Method of Moments

## Minimum Distance

## Spline Regression

This chapter is based on [CMU stat](https://www.stat.cmu.edu/~ryantibs/advmethods/notes/smoothspline.pdf)  

Definition: a k-th order spline is a piecewise polynomial function of degree k, that is continuous and has continuous derivatives of orders 1,..., k -1, at its knot points   

Equivalently, a function f is a k-th order spline with knot points at $t_1 < ...< t_m$ if  

 * f is a polynomial of degree k on each of the intervals $(-\infty, t_1], [t_1,t_2],...,[t_m, \infty)$  
 * $f^{(j)}$, the j-th derivative of f, is continuous at $t_1,...,t_m$ for each j = 0,1,...,k-1

A common case is when k = 3, called cubic splines. (piecewise cubic functions are continuous, and also continuous at its first and second derivatives)  

To parameterize the set of splines, we could use **truncated power basis**, defined as  

$$
g_1(x) = 1 \\
g_2(x) = x \\
... \\
g_{k+1}(x) = x^k \\
g_{k+1+j}(x) = (x-t_j)^k_+
$$

where j = 1,...,m and $x_+$ = max{x,0}  

However, now software typically use B-spline basis.  

### Regression Splines

To estimate the regression function $r(X) = E(Y|X =x)$, we can fit a k-th order spline with knots at some prespecified locations $t_1,...,t_m$  

Regression splines are functions of 

$$
\sum_{j=1}^{m+k+1} \beta_jg_j
$$

where  

 *$\beta_1,..\beta_{m+k+1}$ are coefficients
 * $g_1,...,g_{m+k+1}$ are the truncated power basis functions for k-th order splines over the knots $t_1,...,t_m$


To estimate the coefficients  

$$
\sum_{i=1}^{n} (y_i - \sum_{j=1}^{m} \beta_j g_j (x_i))^2
$$

then regression spline is  

$$
\hat{r}(x) = \sum_{j=1}^{m+k+1} \hat{\beta}_j g_j (x)
$$


If we define the basis matrix $G \in R^{n \times (m+k+1)}$ by 
$$
G_{ij} = g_j(x_i) 
$$
where $i = 1,..,n$ , $j = 1,..,m+k+1$

Then,  

$$
\sum_{i=1}^{n} (y_i - \sum_{j=1}^{m} \beta_j g_j (x_i))^2 = ||y - G \beta||_2^2
$$

and the regression spline estimate at x is  

$$
\hat{r} (x) = g(x)^T \hat{\beta}= g(x)^T(G^TG)^{-1}G^Ty
$$

### Natural splines

A natural spline of order k, with knots at $t_1 <...< t_m$, is a piecewise polynomial function f such that  

 * f is polynomial of degree k on each of $[t_1,t_2],...,[t_{m-1},t_m]$
 * f is a polynomial of degree $(k-1)/2$ on $(-\infty,t_1]$ and $[t_m,\infty)$
 * f is continuous and has continuous derivatives of orders 1,.,,, k -1 at its knots $t_1,..,t_m$

**Note**  

natural splines are only defined for odd orders k.

### Smoothing spliness

These estimators use a regularized regression over the natural spline basis: placing knots at all points $x_1,...x_n$  

For the case of cubic splines, the coefficients are the minimization of 

$$
||y - G\beta||^2_2 + \lambda \beta^T \Omega \beta
$$

where  $\Omega \in R^{n \times n}$ is the penalty matrix

$$
\Omega_{ij} = \int g''_i(t) g''_j(t) dt,
$$

and i,j = 1,..,n

and $\lambda \beta^T \Omega \beta$ is the **regularization term** used to shrink the components of $\hat{\beta}$ towards 0. 
$\lambda > 0$ is the tuning parameter (or smoothing parameter). Higher value of $\lambda$, faster shrinkage  (shrinking away basis functions)  

**Note**  
smoothing splines have similar fits as kernel regression. 

| | Smoothing splines | kernel regression |
|---|---|---|
|tuning parameter | smoothing parameter $\lambda$ | bandwidth h |


### Application 
```{r}

library(tidyverse)
library(caret)
theme_set(theme_classic())

# Load the data
data("Boston", package = "MASS")
# Split the data into training and test set
set.seed(123)
training.samples <- Boston$medv %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]

knots <- quantile(train.data$lstat, p = c(0.25, 0.5, 0.75)) # we use 3 knots at 25,50,and 75 quantile.

library(splines)
# Build the model
knots <- quantile(train.data$lstat, p = c(0.25, 0.5, 0.75))
model <- lm (medv ~ bs(lstat, knots = knots), data = train.data)
# Make predictions
predictions <- model %>% predict(test.data)
# Model performance
data.frame(
  RMSE = RMSE(predictions, test.data$medv),
  R2 = R2(predictions, test.data$medv)
)

ggplot(train.data, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth(method = lm, formula = y ~ splines::bs(x, df = 3))


attach(train.data)
#fitting smoothing splines using smooth.spline(X,Y,df=...)

fit1<-smooth.spline(train.data$lstat,train.data$medv,df=3 ) # 3 degrees of freedom
#Plotting both cubic and Smoothing Splines 
plot(train.data$lstat,train.data$medv,col="grey")
lstat.grid = seq(from = range(lstat)[1], to = range(lstat)[2])
points(lstat.grid,predict(model,newdata = list(lstat=lstat.grid)),col="darkgreen",lwd=2,type="l")
#adding cutpoints
abline(v=c(10,20,30),lty=2,col="darkgreen")
lines(fit1,col="red",lwd=2)

legend("topright",c("Smoothing Spline with 3 df","Cubic Spline"),col=c("red","darkgreen"),lwd=2)

```

## Generalized Additive Models
To overcome [Spline Regression]'s requirements for specifying the knots, we can use [Generalized Additive Models] or GAM. 

```{r}
library(mgcv)
# Build the model
model <- gam(medv ~ s(lstat), data = train.data)
plot(model)
# Make predictions
# predictions <- model %>% predict(test.data)
# Model performance
data.frame(
  RMSE = RMSE(predictions, test.data$medv),
  R2 = R2(predictions, test.data$medv)
)

ggplot(train.data, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth(method = gam, formula = y ~ s(x))
detach(train.data)
```



## Quantile Regression

For academic review on quantile regression, check [@Yu_2003]

[Linear Regression] is based on the conditional mean function $E(y|x)$  

In Quantile regression, we can view each points in the conditional distribution of y. Quantile regression estimates the conditional median or any other quantile of Y.

In the case that we're interested in the 50th percentile, quantile regression is median regression, also known as least-absolute-deviations (LAD) regression, minimizes $\sum_{i}|e_i|$


Properties of estimators $\beta$

 * Asymptotically normally distributed


Advantages  

 * More robust to outliers compared to [OLS][Ordinary Least Squares]
 * In the case the dependent variable has a bimodal or multimodal (multiple humps with multiple modes) distribution, quantile regression can be extremely useful. 
 * Avoids parametric distribution assumption of the error process. In another word, no assumptions regarding the distribution of the error term.
 * Better characterization of the data (not just its conditional mean)
 * is invariant to monotonic transformations (such as log) while OLS is not. In another word, $E(g(y))=g(E(y))$
 
Disadvantages

 * The dependent variable needs to be continuous with no zeroes or too many repeated values. 
 
 
$$
y_i = x_i'\beta_q + e_i
$$
 
Let $e(x) = y -\hat{y}(x)$, then $L(e(x)) = L(y -\hat{y}(x))$ is the loss function of the error term.  

If $L(e) = |e|$ (called absolute-error loss function) then $\hat{\beta}$ can be estimated by minimizing $\sum_{i}|y_i-x_i'\beta|$  

More specifically, the objective function is 
$$
Q(\beta_q)=\sum_{i:y_i \ge x_i'\beta}^{N} q|y_i - x_i'\beta_q| + \sum_{i:y_i < x_i'\beta}^{N} (1-q)|y_i-x_i'\beta_q
$$
where $0<q<1$

The sum penalizes $q|e_i|$ for under-prediction and $(1-q)|e_i|$ for over-prediction


We use simplex method to minimize this function (cannot use analytical solution since it's non-differentiable). Standard errors can be estimated by bootstrap.


The absolute-error loss function is symmetric. 

**Interpretation**
For the jth regressor ($x_j$), the marginal effect is the coefficient for the qth quantile

$$
\frac{\partial Q_q(y|x)}{\partial x_j} = \beta_{qj}
$$
At the quantile q of the dependent variable y, $\beta_q$ represents a one unit change in the independent variable $x_j$ on the dependent variable y.  

In other words, at the qth percentile, a one unit change in x results in $\beta_q$ unit change in y.




### Application

```{r}
# generate data with non-constant variance

x <- seq(0,100,length.out = 100)        # independent variable
sig <- 0.1 + 0.05*x                     # non-constant variance
b_0 <- 3                                # true intercept
b_1 <- 0.05                             # true slope
set.seed(1)                             # reproducibility
e <- rnorm(100,mean = 0, sd = sig)      # normal random error with non-constant variance
y <- b_0 + b_1*x + e                    # dependent variable
dat <- data.frame(x,y)
hist(y)
library(ggplot2)
ggplot(dat, aes(x,y)) + geom_point()
ggplot(dat, aes(x,y)) + geom_point() + geom_smooth(method="lm")
```

We follow [@Roger_1996] to estimate quantile regression 
```{r}
library(quantreg)
qr <- rq(y ~ x, data=dat, tau = 0.5) # tau: quantile of interest. Here we have it at 50th percentile.
summary(qr)
```
adding the regression line

```{r}
ggplot(dat, aes(x,y)) + geom_point() + 
  geom_abline(intercept=coef(qr)[1], slope=coef(qr)[2])
```


To have R estimate multiple quantile at once

```{r}
qs <- 1:9/10
qr1 <- rq(y ~ x, data=dat, tau = qs)
#check for its coefficients
coef(qr1)

# plot
ggplot(dat, aes(x,y)) + geom_point() + geom_quantile(quantiles = qs)
```

To examine if the quantile regression is appropriate, we can see its plot compared to least squares regression 
```{r}
plot(summary(qr1), parm="x")
```

where red line is the least squares estimates, and its confidence interval. 
x-axis is the quantile 
y-axis is the value of the quantile regression coefficients at different quantile

If the error term is normally distributed, the quantile regression line will fall inside the coefficient interval of least squares regression. 
```{r}
# generate data with constant variance

x <- seq(0, 100, length.out = 100)    # independent variable
b_0 <- 3                              # true intercept
b_1 <- 0.05                           # true slope
set.seed(1)                           # reproducibility
e <- rnorm(100, mean = 0, sd = 1)     # normal random error with constant variance
y <- b_0 + b_1 * x + e                # dependent variable
dat2 <- data.frame(x, y)
qr2 = rq(y ~ x, data = dat2, tau = qs)
plot(summary(qr2), parm = "x")
```







