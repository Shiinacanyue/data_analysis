# Non-linear Regression

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(kableExtra)
library(knitr)
library(nlstools)
library(investr)
library(MASS)
```

**Definition**: models in which the derivatives of the mean function with respect to the parameters depend on one or more of the parameters.  


To approximate data, we can approximate the function  

 * by a high-order polynomial  
 * by a linear model (e.g., a Taylor expansion around X's)  
 * a collection of locally linear models or basis function  

but it would not easy to interpret, or not enough data, or can't interpret them globally.  

**intrinsically nonlinear** models:  

$$
Y_i = f(\mathbf{x_i;\theta}) + \epsilon_i
$$
where $f(\mathbf{x_i;\theta})$ is a nonlinear function relating $E(Y_i)$ to the independent variables $x_i$  

 * $\mathbf{x}_i$ is a k x 1 vector of independent variables (fixed).  
 * $\mathbf{\theta}$ is a p x 1 vector of parameters.  
 * $\epsilon_i$s are iid variables mean 0 and variance $\sigma^2$. (sometimes it's normal).  

## Inference

Since $Y_i = f(\mathbf{x}_i,\theta) + \epsilon_i$, where $\epsilon_i \sim iid(0,\sigma^2)$. We can obtain $\hat{\theta}$ by minimizing $\sum_{i=1}^{n}(Y_i - f(x_i,\theta))^2$ and estimate  $s^2 = \hat{\sigma}^2_{\epsilon}=\frac{\sum_{i=1}^{n}(Y_i - f(x_i,\theta))^2}{n-p}$

### Linear Function of the Parameters

If we assume $\epsilon_i \sim N(0,\sigma^2)$, then 

$$
\hat{\theta} \sim AN(\mathbf{\theta},\sigma^2[\mathbf{F}(\theta)'\mathbf{F}(\theta)]^{-1})
$$
where An = asymptotic normality

Asymptotic means we have enough data to make inference (As your sample size increases, this becomes more and more accurate (to the true value)).


Since we want to do inference on linear combinations of parameters or contrasts.  

If we have $\mathbf{\theta} = (\theta_0,\theta_1,\theta_2)'$ and we want to look at $\theta_1 - \theta_2$; we can define vector $\mathbf{a} = (0,1,-1)'$, consider inference for $\mathbf{a'\theta}$  


Rules for expectation and variance of a fixed vector $\mathbf{a}$ and random vector $\mathbf{Z}$;  

$$
E(\mathbf{a'Z}) = \mathbf{a'}E(\mathbf{Z}) \\
var(\mathbf{a'Z}) = \mathbf{a'}var(\mathbf{Z})\mathbf{a}
$$

Then,  

$$
\mathbf{a'\hat{\theta}} \sim AN(\mathbf{a'\theta},\sigma^2\mathbf{a'[F(\theta)'F(\theta)]^{-1}a})
$$

and $\mathbf{a'\hat{\theta}}$ is asymptotically independent of $s^2$ (to order 1/n) then  

$$
\frac{\mathbf{a'\hat{\theta}-a'\theta}}{s(\mathbf{a'[F(\theta)'F(\theta)]^{-1}a})^{1/2}} \sim t_{n-p}
$$
to construct $100(1-\alpha)\%$ confidence interval for $\mathbf{a'\theta}$  

$$
\mathbf{a'\theta} \pm t_{(1-\alpha/2,n-p)}s(\mathbf{a'[F(\theta)'F(\theta)]^{-1}a})^{1/2}
$$

Suppose $\mathbf{a'} = (0,...,j,...,0)$. Then, a confidence interval for the jth element of $\mathbf{\theta}$ is 

$$
\hat{\theta}_j \pm t_{(1-\alpha/2,n-p)}s\sqrt{\hat{c}^{j}}
$$
where $\hat{c}^{j}$ is the jth diagonal element of $[\mathbf{F(\hat{\theta})'F(\hat{\theta})}]^{-1}$  



```{r}
#set a seed value 
set.seed(23)

#Generate x as 100 integers using seq function
x<-seq(0,100,1)

#Generate y as a*e^(bx)+c
y<-runif(1,0,20)*exp(runif(1,0.005,0.075)*x)+runif(101,0,5)

# visulize
plot(x,y)

#define our data frame
datf = data.frame(x,y)

#define our model function
mod =function(a,b,x) a*exp(b*x)
```

In this example, we can get the starting values by using linearized version of the function $\log y = \log a + b x$.  Then, we can fit a linear regression to this and use our estimates as starting values

```{r}
#get starting values by linearizing
lin_mod=lm(log(y)~x,data=datf)

#convert the a parameter back from the log scale; b is ok 
astrt = exp(as.numeric(lin_mod$coef[1]))
bstrt = as.numeric(lin_mod$coef[2])
print(c(astrt,bstrt))
```


with `nls`, we can fit the nonlinear model via least squares

```{r}
nlin_mod=nls(y~mod(a,b,x),start=list(a=astrt,b=bstrt),data=datf) 

#look at model fit summary
summary(nlin_mod)

#add prediction to plot
plot(x,y)
lines(x,predict(nlin_mod),col="red")


```



### Nonlinear
Suppose that $h(\theta)$ is a nonlinear function of the parameters. We can use Taylor series about $\theta$  

$$
h(\hat{\theta}) \approx h(\theta) + \mathbf{h}'[\hat{\theta}-\theta]
$$
where $\mathbf{h} = (\frac{\partial h}{\partial \theta_1},...,\frac{\partial h}{\partial \theta_p})'$  

with 

$$
E(\hat{\theta}) \approx \theta \\
var(\hat{\theta}) \approx  \sigma^2[\mathbf{F(\theta)'F(\theta)}]^{-1} \\
E(h(\hat{\theta})) \approx h(\theta) \\
var(h(\hat{\theta})) \approx \sigma^2 \mathbf{h'[F(\theta)'F(\theta)]^{-1}h}
$$

Thus,  

$$
h(\hat{\theta}) \sim AN(h(\theta),\sigma^2\mathbf{h'[F(\theta)'F(\theta)]^{-1}h})
$$
and an approximate $100(1-\alpha)\%$ confidence interval for $h(\theta)$ is 

$$
h(\hat{\theta}) \pm t_{(1-\alpha/2;n-p)}s(\mathbf{h'[F(\theta)'F(\theta)]^{-1}h})^{1/2}
$$

where $\mathbf{h}$ and $\mathbf{F}(\theta)$ are evaluated at $\hat{\theta}$  

Regarding **prediction interval** for Y at $x=x_0$  

$$
Y_0 = f(x_0;\theta) + \epsilon_0, \epsilon_0 \sim N(0,\sigma^2) \\
\hat{Y}_0 = f(x_0,\hat{\theta})
$$

As $n \to \infty$, $\hat{\theta} \to \theta$, so we  

$$
f(x_0, \hat{\theta}) \approx f(x_0,\theta) + \mathbf{f}_0(\mathbf{\theta})'[\hat{\theta}-\theta]
$$
where $f_0(\theta)= (\frac{\partial f(x_0,\theta)}{\partial \theta_1},..,\frac{\partial f(x_0,\theta)}{\partial \theta_p})'$ (note: this $f_0(\theta)$ is different from $f(\theta)$).  

$$
Y_0 - \hat{Y}_0 \approx Y_0  - f(x_0,\theta) - f_0(\theta)'[\hat{\theta}-\theta]  \\
= \epsilon_0 - f_0(\theta)'[\hat{\theta}-\theta]
$$

$$
E(Y_0 - \hat{Y}_0) \approx E(\epsilon_0)E(\hat{\theta}-\theta) = 0 \\
var(Y_0 - \hat{Y}_0) \approx var(\epsilon_0 - \mathbf{(f_0(\theta)'[\hat{\theta}-\theta])}) \\
= \sigma^2 + \sigma^2 \mathbf{f_0 (\theta)'[F(\theta)'F(\theta)]^{-1}f_0(\theta)} \\
= \sigma^2 (1 + \mathbf{f_0 (\theta)'[F(\theta)'F(\theta)]^{-1}f_0(\theta)})
$$

Hence, combining  

$$
Y_0 - \hat{Y}_0 \sim AN (0,\sigma^2 (1 + \mathbf{f_0 (\theta)'[F(\theta)'F(\theta)]^{-1}f_0(\theta)}))
$$


Note:  

Confidence intervals for the mean response $Y_i$ (which is different from prediction intervals) can be obtained similarly. 





## Non-linear Least Squares

 * The LS estimate of $\theta$, $\hat{\theta}$ is the set of parameters that minimizes the residual sum of squares:  
$$
S(\hat{\theta}) = SSE(\hat{\theta}) = \sum_{i=1}^{n}\{Y_i - f(\mathbf{x_i};\hat{\theta})\}^2
$$
 * to obtain the solution, we can consider the partial derivatives of $S(\theta)$ with respect to each $\theta_j$ and set them to 0, which gives a system of p equations. Each normal equation is 
$$
\frac{\partial S(\theta)}{\partial \theta_j} = -2\sum_{i=1}^{n}\{Y_i -f(\mathbf{x}_i;\theta)\}[\frac{\partial(\mathbf{x}_i;\theta)}{\partial \theta_j}] = 0
$$
 * but we can't obtain a solution directly/analytically for this equation. 

**Numerical Solutions**  

 * Grid search  
    + A "grid" of possible parameter values and see which one minimize the residual sum of squares.  
    + finer grid = greater accuracy  
    + could be inefficient, and hard when p is large.  
 * Gauss-Newton Algorithm  
    + we have an initial estimate of $\theta$ denoted as $\hat{\theta}^{(0)}$  
    + use a Taylor expansions of $f(\mathbf{x}_i;\theta)$ as a function of $\theta$ about the point $\hat{\theta}^{(0)}$  

$$
\begin{align} 
Y_i &= f(x_i;\theta) + \epsilon_i \\
&= f(x_i;\theta) + \sum_{j=1}^{p}\{\frac{\partial f(x_i;\theta)}{\partial \theta_j}\}_{\theta = \hat{\theta}^{(0)}} (\theta_j - \hat{\theta}^{(0)}) + \text{remainder} + \epsilon_i
\end{align}
$$

Equivalently,  

In matrix notation,  

$$
\mathbf{Y} = 
\left[ \begin{array}
{ccc}
Y_1 \\
. \\
Y_n
\end{array} \right]
$$

$$
\mathbf{f}(\hat{\theta}^{(0)}) =
\left[ \begin{array}
{ccc}
f(\mathbf{x_1,\hat{\theta}}^{(0)}) \\
. \\
f(\mathbf{x_n,\hat{\theta}}^{(0)})
\end{array} \right]
$$


$$
\mathbf{\epsilon} = 
\left[ \begin{array}
{ccc}
\epsilon_1 \\
. \\
\epsilon_n
\end{array} \right]
$$

$$
\mathbf{F}(\hat{\theta}^{(0)}) = 
\left[ \begin{array}
{ccc}
\frac{\partial f(x_1,\mathbf{\theta})}{\partial \theta_1} && ... && \frac{\partial f(x_1,\mathbf{\theta})}{\partial \theta_p}\\
. && . && . \\
\frac{\partial f(x_n,\mathbf{\theta})}{\partial \theta_1} && ... && \frac{\partial f(x_n,\mathbf{\theta})}{\partial \theta_p}
\end{array} \right]_{\theta = \hat{\theta}^{(0)}}
$$


Hence,  


$$
\mathbf{Y} = \mathbf{f}(\hat{\theta}^{(0)}) + \mathbf{F}(\hat{\theta}^{(0)})(\theta - \hat{\theta}^{(0)}) + \epsilon + \text{remainder}
$$
where we assume that the remainder is small and the error term is only assumed to be iid with mean 0 and variance $\sigma^2$.  

We can rewrite the above equation as  

$$
\mathbf{Y} - \mathbf{f}(\hat{\theta}^{(0)}) \approx \mathbf{F}(\hat{\theta}^{(0)})(\theta - \hat{\theta}^{(0)}) + \epsilon
$$
where it is in the form of linear model. After we solve for $(\theta - \hat{\theta}^{(0)})$ and let it equal to $\hat{\delta}^{(1)}$  
 Then we new estimate is given by adding the Gauss increment adjustment to the initial estimate $\hat{\theta}^{(1)} = \hat{\theta}^{(0)} + \hat{\delta}^{(1)}$  
 We can repeat this process. 


Gauss-Newton Algorithm Steps:  

 1. initial estimate $\hat{\theta}^{(0)}$, set j = 0  
 2. Taylor series expansion and calculate  $\mathbf{f}(\hat{\theta}^{(j)})$ and $\mathbf{F}(\hat{\theta}^{(j)})$ 
 3. Use OLS to get $\hat{\delta}^{(j+1)}$  
 4. get the new estimate $\hat{\theta}^{(j+1)}$, return to step 2  
 5. continue until "convergence"  
 6. With the final parameter estimate $\hat{\theta}$, we can estimate $\sigma^2$ if $\epsilon \sim (\mathbf{0}, \sigma^2 \mathbf{I})$ by  

$$
\hat{\sigma}^2= \frac{1}{n-p}(\mathbf{Y}-\mathbf{f}(x;\hat{\theta}))'(\mathbf{Y}-\mathbf{f}(x;\hat{\theta}))
$$

<br>

**Criteria for convergence**  

 1. Minor change in the objective function (SSE = residual sum of squares)  
$$
\frac{|SSE(\hat{\theta}^{(j+1)})-SSE(\hat{\theta}^{(j)})|}{SSE(\hat{\theta}^{(j)})} < \gamma_1
$$
 2. Minor change in the parameter estimates  
$$
|\hat{\theta}^{(j+1)}-\hat{\theta}^{(j)}| < \gamma_2
$$
 3. "residual projection" criterion of [@Bates_1981]  

### Alternative of Gauss-Newton Algorithm

#### Gauss-Newton Algorithm

Normal equations:  

$$
\frac{\partial SSE(\theta)}{\partial \theta} = 2\mathbf{F}(\theta)'[\mathbf{Y}-\mathbf{f}(\theta)]
$$

$$
\begin{align}
\hat{\theta}^{(j+1)} &= \hat{\theta}^{(j)} + \hat{\delta}^{(j+1)} \\
&= \hat{\theta}^{(j)} + [\mathbf{F}((\hat{\theta})^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})]^{-1}\mathbf{F}(\hat{\theta})^{(j)} \\
&= \hat{\theta}^{(j)} - \frac{1}{2}[\mathbf{F}(\hat{\theta}^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})]^{-1}\frac{\partial SSE(\hat{\theta}^{(j)})}{\partial \theta}
\end{align}
$$
where  

 * $\frac{\partial SSE(\hat{\theta}^{(j)})}{\partial \theta}$ is a gradient vecotr  (points in the direction in which the SSE increases most rapidly). This path is known as steepest ascent.  
 * $[\mathbf{F}(\hat{\theta}^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})]^{-1}$ indicates how far to move  
 * $-1/2$: indicator of the direction of steepest descent.  


#### Modified Gauss-Newton Algorithm

To avoid overstepping (the local min), we can use the modified Gauss-Newton Algorithm. We define a new proposal for $\theta$  

$$
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} + \alpha_j \hat{\delta}^{(j+1)}, 0 < \alpha_j < 1
$$
where  

 * $\alpha_j$ (called the "learning rate"): is used to modify the step length.  

We could also have $\alpha *1/2$, but typically it is assumed to be absorbed into the learning rate. 

A way to choose $\alpha_j$, we can use **step halving**  

$$
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} + \frac{1}{2^k}\hat{\delta}^{(j+1)}
$$
where  

 * k is the smallest non-negative integer such that  
$$
SSE(\hat{\theta}^{(j)}+\frac{1}{2^k}\hat{\delta}^{(j+1)}) < SSE(\hat{\theta}^{(j)})
$$
which means we try $\hat{\delta}^{(j+1)}$, then $\hat{\delta}^{(j+1)}/2$, $\hat{\delta}^{(j+1)}/4$, etc.  


The most general form of the convergence algorithm is  

$$
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j \mathbf{A}_j \frac{\partial Q(\hat{\theta}^{(j)})}{\partial \theta} 
$$
where  

 * $\mathbf{A}_j$ is a positive definite matrix  
 * $\alpha_j$ is the learning rate  
 * $\frac{\partial Q(\hat{\theta}^{(j)})}{\partial \theta}$is the gradient based on some objective function Q (a function of $\theta$), which is typically the SSE in nonlinear regression applications (e.g., cross-entropy for classification).  

Refer back to the **Modified Gauss-Newton Algorithm**, we can see it is in this form  

$$
\hat{\theta}^{(j+1)} =\hat{\theta}^{(j)} - \alpha_j[\mathbf{F}(\hat{\theta}^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})]^{-1}\frac{\partial SSE(\hat{\theta}^{(j)})}{\partial \theta}
$$
where Q = SSE, $[\mathbf{F}(\hat{\theta}^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})]^{-1} = \mathbf{A}$


#### Steepest Descent

(also known just "gradient descent")  

$$
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j \mathbf{I}_{p \times p}\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}
$$

 * slow to converge, moves rapidly initially.  
 * could be use for starting values  

#### Levenberg -Marquardt

$$
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j [\mathbf{F}(\hat{\theta}^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})+ \tau \mathbf{I}_{p \times p}]\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}
$$

which is a compromise between the [Gauss-Newton Algorithm] and the [Steepest Descent].  

 * best when $\mathbf{F}(\hat{\theta}^{(j)})'\mathbf{F}(\hat{\theta}^{(j)})$ is nearly singular ($\mathbf{F}(\hat{\theta}^{(j)})$ isn't of full rank)  
 * similar to ridge regression  
 * If $SSE(\hat{\theta}^{(j+1)}) < SSE(\hat{\theta}^{(j)})$, then $\tau= \tau/10$ for the next iteration. Otherwise, $\tau = 10 \tau$


#### Newton-Raphson

$$
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j [\frac{\partial^2Q(\hat{\theta}^{(j)})}{\partial \theta \partial \theta'}]^{-1}\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}
$$

The **Hessian matrix** can be rewritten as:  

$$
\frac{\partial^2Q(\hat{\theta}^{(j)})}{\partial \theta \partial \theta'} = 2 \mathbf{F}((\hat{\theta})^{(j)})'\mathbf{F}(\hat{\theta}^{(j)}) - 2\sum_{i=1}^{n}[Y_i - f(x_i;\theta)]\frac{\partial^2f(x_i;\theta)}{\partial \theta \partial \theta'}
$$
which contains the same term that [Gauss-Newton Algorithm
], combined with one containing the second partial derivatives of f(). (methods that require the second derivatives of the objective function are known as "second-order methods".)  
However, the last term \frac{\partial^2f(x_i;\theta)}{\partial \theta \partial \theta'} can sometimes be nonsingular.  



#### Quasi-Newton

update $\theta$ according to  

$$
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j \mathbf{H}_j^{-1}\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}
$$
where $H_j$ is a symmetric positive definite approximation to the Hessian, which gets closer as $j \to \infty$.  

 * $\mathbf{H}_j$ is computed iteratively  
 * AMong first-order methods(where only first derivatives are required), this method performs best.  

#### Derivative Free Methods

 * **secant Method**: like [Gauss-Newton Algorithm], but calculates the derivatives numerically from past iterations.  
 * **Simplex Methods**  
 * **Genetic Algorithm**  
 * **Differential Evolution Algorithms**  
 * **Particle Swarm Optimization**  
 * **Ant Colony Optimization**  

### Practical Considerations

To converge, algorithm need good initial estimates.  

 * Starting values:  
    + Prior or theoretical info  
    + A grid search or a graph of $SSE(\theta)$  
    + could also use OLS to get starting values.  
    + Model interpretation: if you have some idea regarding the form of the objective function, then you can try to guess the initial value.  
    + Expected Value Parameterization  
 * Constrained Parameters: (constraints on parameters like $\theta_i>a,a< \theta_i <b$)  
    + fit the model first to see if the converged parameter estimates satisfy the constraints. 
    + if they dont' satisfy, then try re-parameterizing  


#### Failure to converge

 * $SSE(\theta)$ may be "flat" in a neighborhood of the minimum.  
 * You can try different or "better" starting values.  
 * Might suggest the model is too complex for the data, might consider simpler model.  


#### Convergence to a Local Minimum

 * Linear least squares has the property that $SSE(\theta) = \mathbf{(Y-X\beta)'(Y-X\beta)}$, which is quadratic and has a unique minimum (or maximum).  
 * Nonlinear east squares need  not have a unique minimum  
 * Using different starting values can help  
 * If the dimension of $\theta$ is low, graph $SSE(\theta)$ as a function of $\theta_i$  
 * Different algorithm can help (e.g., genetic algorithm, particle swarm)  


To converge, algorithms need good initial estimates.  

 * Starting values:  
    + prior or theoretical info  
    + A grid search or a graph 
    + OLS estimates as starting values  
    + Model interpretation  
    + Expected Value Parameterization  
 * Constrained Parameters:  
    + try the model without the constraints first.  
    + If the resulted parameter estimates does not satisfy the constraint, try re-parameterizing


```{r}
# Grid search
#choose grid of a and b values
aseq = seq(10,18,.2)
bseq = seq(.001,.075,.001)

na = length(aseq)
nb = length(bseq)
SSout = matrix(0,na*nb,3) #matrix to save output
cnt = 0
for (k in 1:na){
   for (j in 1:nb){
      cnt = cnt+1
      ypred = mod(aseq[k],bseq[j],x) #evaluate model w/ these parms
      ss = sum((y-ypred)^2)  #this is our SSE objective function
      #save values of a, b, and SSE
      SSout[cnt,1]=aseq[k]
      SSout[cnt,2]=bseq[j]
      SSout[cnt,3]=ss
   }
}
#find minimum SSE and associated a,b values
mn_indx = which.min(SSout[,3])
astrt = SSout[mn_indx,1]
bstrt = SSout[mn_indx,2]
#now, run nls function with these starting values
nlin_modG=nls(y~mod(a,b,x),start=list(a=astrt,b=bstrt)) 

nlin_modG
# Note, the package `nls_multstart` will allow you to do a grid search without programming your own loop


```

For prediction interval 

```{r}
plotFit(nlin_modG, interval = "both", pch = 19, shade = TRUE, 
        col.conf = "skyblue4", col.pred = "lightskyblue2",data=datf)  
```


Based on the forms of your function, you can also have programmed starting values from `nls` function (e.e.g, logistic growth, asymptotic regression, etc).  

```{r}
apropos("^SS")
```

For example, a logistic growth model:  

$$
P = \frac{K}{1+ exp(P_0+ rt)} + \epsilon
$$
where 

 * P = population at time t  
 * K = carrying capacity   
 * r = population growth rate  

but in `R` you have slight different parameterization:  

$$
P = \frac{asym}{1 + exp(\frac{xmid - t}{scal})}
$$
where  

 * asym = carrying capacity  
 * xmid = the x value at the inflection point of the curve  
 * scal = scaling parameter.  

Hence, you have  

 * K = asym  
 * r = -1/scal  
 * $P_0 = -rxmid$

```{r}
# simulated data
time <- c(1,2,3,5,10,15,20,25,30,35)
population <- c(2.8,4.2,3.5,6.3,15.7,21.3,23.7,25.1,25.8,25.9)
plot(time, population, las=1, pch=16)

# model fitting
logisticModelSS <- nls(population~SSlogis(time, Asym, xmid, scal))
summary(logisticModelSS)
coef(logisticModelSS)
```


Other parameterization

```{r}
#convert to other parameterization
Ks = as.numeric(coef(logisticModelSS)[1])
rs = -1/as.numeric(coef(logisticModelSS)[3])
Pos = - rs * as.numeric(coef(logisticModelSS)[2])
#let's refit with these parameters
logisticModel <- nls(population ~ K / (1 + exp(Po + r * time)),start=list(Po=Pos,r=rs,K=Ks))
summary(logisticModel)
```

```{r}
#note: initial values =  solution (highly unusual, but ok)
plot(time, population, las=1, pch=16)
lines(time,predict(logisticModel),col="red")

```


If can also define your own self-starting fucntion if your models are uncommon (built in `nls`)

Example is based on [@Schabenberger_2001]

```{r}
#Load data
dat <- read.table("images/dat.txt", header = T)
# plot
dat.plot <- ggplot(dat)+geom_point(aes(x=no3,y=ryp, color=as.factor(depth))) +
labs(color='Depth (cm)') + xlab('Soil NO3') + ylab('relative yield percent')
dat.plot
```

The suggested model (known as plateau model) is  

$$
E(Y_{ij}) = (\beta_{0j} + \beta_{1j}N_{ij})I_{N_{ij}\le \alpha_j} + (\beta_{0j} + \beta_{1j}\alpha_j)I_{N_{ij} > \alpha_j}
$$
where  

 * N is an observation   
 * i is a particular observation  
 * j = 1,2 corresponding to depths (30,60)  


```{r}
#First define model as a function
nonlinModel <- function(predictor,b0,b1,alpha){
  ifelse(predictor<=alpha, 
         b0+b1*predictor, #if observation less than cutoff simple linear model
         b0+b1*alpha) #otherwise flat line
}

```

define `selfStart` function. Because we defined our model to be linear in the first part and then plateau (remain constant) we can use the first half of our predictors (sorted by increasing value) to get an initial estimate for the slope and intercept of the model, and the last predictor value (alpha) can be the starting value for the plateau parameter.

```{r}
nonlinModelInit <- function(mCall,LHS,data){
  #sort data by increasing predictor value - 
  #done so we can just use the low level no3 conc to fit a simple model
  xy <- sortedXyData(mCall[['predictor']],LHS,data)
  n <- nrow(xy)
  #For the first half of the data a simple linear model is fit
  lmFit <- lm(xy[1:(n/2),'y']~xy[1:(n/2),'x'])
  b0 <- coef(lmFit)[1]
  b1 <- coef(lmFit)[2]
  #for the cut off to the flat part select the last x value used in creating linear model
  alpha <- xy[(n/2),'x']
  value <- c(b0,b1,alpha)
  names(value) <- mCall[c('b0','b1','alpha')]
  value
}
```

combine model and custom function to calculate starting values.

```{r}
SS_nonlinModel <- selfStart(nonlinModel,nonlinModelInit,c('b0','b1','alpha'))
```

```{r}
#Above code defined model and selfStart now just need to call it for each of the depths
sep30_nls <- nls(ryp~SS_nonlinModel(predictor=no3,b0,b1,alpha),data=dat[dat$depth==30,])

sep60_nls <- nls(ryp~SS_nonlinModel(predictor=no3,b0,b1,alpha),data=dat[dat$depth==60,])

par(mfrow=c(1,2))
plotFit(sep30_nls, interval = "both", pch = 19, shade = TRUE,
col.conf = "skyblue4", col.pred = "lightskyblue2",
data=dat[dat$depth==30,],main='Results 30 cm depth',
ylab = 'relative yield percent',xlab = 'Soil NO3 concentration',
xlim = c(0,120))
plotFit(sep60_nls, interval = "both", pch = 19, shade = TRUE,
col.conf = "lightpink4", col.pred = "lightpink2",
data=dat[dat$depth==60,],main='Results 60 cm depth',
ylab = 'relative yield percent',xlab = 'Soil NO3 concentration',
xlim = c(0,120))
```

```{r}
summary(sep30_nls)
summary(sep60_nls)
```
Instead of modeling the depths model separately we model them together - so there is a common slope, intercept, and plateau.

```{r reduce-model}

red_nls <- nls(ryp~SS_nonlinModel(predictor=no3,b0,b1,alpha),data=dat)

summary(red_nls)

par(mfrow=c(1,1))
plotFit(red_nls, interval = "both", pch = 19, shade = TRUE,
col.conf = "lightblue4", col.pred = "lightblue2",
data=dat,main='Results combined',
ylab = 'relative yield percent',xlab = 'Soil NO3 concentration')

```

Examine residual values for the combined model.

```{r reduce-model-resid}
library(nlstools)
#using nlstools nlsResiduals function to get some quick residual plots 
#can also use test.nlsResiduals(resid)
# https://www.rdocumentation.org/packages/nlstools/versions/1.0-2
resid <- nlsResiduals(red_nls)
plot(resid)
```

can we test whether the parameters for the two soil depth fits are significantly different?  To know if the combined model is appropriate, we consider a parameterization where we let the parameters for the 60cm model be equal to the parameters from the 30cm model plus some increment:

$$
\beta_{02} = \beta_{01} + d_0 \\
\beta_{12} = \beta_{11} + d_1 \\
\alpha_{2} = \alpha_{1} + d_a
$$

We can implement this in the following function:

```{r full-model, eval=TRUE}

nonlinModelF <- function(predictor,soildep,b01,b11,a1,d0,d1,da){
   b02 = b01 + d0 #make 60cm parms = 30cm parms + increment
   b12 = b11 + d1
   a2 = a1 + da
   
   y1 = ifelse(predictor<=a1, 
         b01+b11*predictor, #if observation less than cutoff simple linear model
         b01+b11*a1) #otherwise flat line
   y2 = ifelse(predictor<=a2, 
               b02+b12*predictor, 
               b02+b12*a2) 
   y =  y1*(soildep == 30) + y2*(soildep == 60)  #combine models
   return(y)
}

```

Starting values are easy now because we fit each model individually.
```{r full-model-fit, eval=TRUE}

Soil_full=nls(ryp~nonlinModelF(predictor=no3,soildep=depth,b01,b11,a1,d0,d1,da),
              data=dat,
              start=list(b01=15.2,b11=3.58,a1=23.13,d0=-9.74,d1=2.11,da=-6.85)) 

summary(Soil_full)
```
So, the increment parameters, $d_1$,$d_2$,$d_a$ are all significantly different from 0, suggesting that we should have two models here. 





### Model/Estiamtion Adequcy

[@Bates_1980] assess nonlinearity in terms of 2 components of curvature:  

 * **Intrinsic nonlinearity**: the degree of bending and twisting in $f(\theta)$; our estimation approach assumes that hte true function is relatively flat (planar) in the neighborhood fo $\hat{\theta}$, which would not be true if $f()$ has a lot of "bending" int he neighborhood of $\hat{\theta}$ (independent of parameterizaiton)  
    + If bad, the distribution of residuals will be seriously distorted  
    + slow to converge  
    + difficult to identify ( could use this function `rms.curve`)  
    + Solution:   
        - could use higher order Taylor expansions estimation   
        - Bayesian method
 * **Parameter effects nonlinearity**: degree to which curvature (nonlinearity) is affected by choice of $\theta$ (data dependent; dependent on parameterization)  
    + leads to problems with inferecne on $\hat{\theta}$  
    + `rms.curve` in `MASS` can identify  
    + bootstrap-based inference can also be used  
    + Solution: try to reparaemterize. 

```{r}
#check parameter effects and intrinsic curvature

modD = deriv3(~ a*exp(b*x), c("a","b"),function(a,b,x) NULL)

nlin_modD=nls(y~modD(a,b,x),start=list(a=astrt,b=bstrt),data=datf)

rms.curv(nlin_modD)
```



In linear model, we have [Linear Regression], we have goodness of fit measure as $R^2$:  

$$
R^2 = \frac{SSR}{SSTO} = 1- \frac{SSE}{SSTO} \\
= \frac{\sum_{i=1}^n (\hat{Y}_i- \bar{Y})^2}{\sum_{i=1}^n (Y_i- \bar{Y})^2} = 1- \frac{\sum_{i=1}^n ({Y}_i- \hat{Y})^2}{\sum_{i=1}^n (Y_i- \bar{Y})^2}
$$
but not valid in the nonlinear case because the error sum of squares and model sum of squares do not add to the total corrected sum of squares  

$$
SSR + SSE \neq SST
$$

but we can use pseudo-$R^2$:  

$$
R^2_{pseudo} = 1 - \frac{\sum_{i=1}^n ({Y}_i- \hat{Y})^2}{\sum_{i=1}^n (Y_i- \bar{Y})^2}
$$
But we can't interpret this as the proportion of variability explained by the model. We should use as a relative comparison of different models.  



**Residual Plots**:  standardize, similar to OLS. useful when the intrinsic curvature is small:  

The studentized residuals  

$$
r_i = \frac{e_i}{s\sqrt{1-\hat{c}_i}}
$$

where $\hat{c}_i$is the i-th diagonal of $\mathbf{\hat{H}= F(\hat{\theta})[F(\hat{\theta})'F(\hat{\theta})]^{-1}F(\hat{\theta})'}$

We could have problems of  

 * Collinearity: the condition number of $\mathbf{[F(\hat{\theta})'F(\hat{\theta})]^{-1}}$ should be less than 30. Follow [@Magel_1987]; reparameterize if possible  
 * Leverage: Like [OLS][Ordinary Least Squares], but consider $\mathbf{\hat{H}= F(\hat{\theta})[F(\hat{\theta})'F(\hat{\theta})]^{-1}F(\hat{\theta})'}$ (also known as "tangent plant hat matrix") [@Laurent_1992]  
 * Heterogeneous Errors: weighted Non-linear Least Squares  
 * Correlated Errors:  
    + Generalized Nonlinear Least Squares  
    + Nonlinear Mixed Models  
    + Bayesian methods














## Generalized Linear Models

Even though we call it generalized linear model, it is still under the paradigm of non-linear regression, because the form of the regression model is non-linear. The name generalized linear model derived from the fact that we have $\mathbf{x'_i \beta}$ (which is linear form) in the model.  

### Logistic Regression

$$
p_i = f(\mathbf{x}_i ; \beta) = \frac{exp(\mathbf{x_i'\beta})}{1 + exp(\mathbf{x_i'\beta})}
$$
Equivalently,  

$$
logit(p_i) = log(\frac{p_i}{1+p_i}) = \mathbf{x_i'\beta}
$$
where $\frac{p_i}{1+p_i}$is the **odds**.

In this form, the model is specified such that **a function of the mean response is linear**. Hence, **Generalized Linear Models**  

The likelihood function  

$$
L(p_i) = \prod_{i=1}^{n} p_i^{Y_i}(1-p_i)^{1-Y_i}
$$
where $p_i = \frac{\mathbf{x'_i \beta}}{1+\mathbf{x'_i \beta}}$ and $1-p_i = (1+ exp(\mathbf{x'_i \beta}))^{-1}$  

Hence, our objective function is  

$$
Q(\beta) = log(L(\beta)) = \sum_{i=1}^n Y_i \mathbf{x'_i \beta} - \sum_{i=1}^n  log(1+ exp(\mathbf{x'_i \beta}))
$$

we could maximize this function numerically using the optimization method above, which allows us to find numerical MLE for $\hat{\beta}$. Then we can use the standard asymptotic properties of MLEs to make inference.  

Property of MLEs is that parameters are asymptotically unbiased with sample variance-covariance matrix given by the **inverse Fisher information matrix**


$$
\hat{\beta} \dot{\sim} AN(\beta,[\mathbf{I}(\beta)]^{-1})
$$
where the **Fisher Information matrix**, $\mathbf{I}(\beta)$ is 

$$
\begin{align}
\mathbf{I}(\beta) &= E[\frac{\partial \log(L(\beta))}{\partial (\beta)}\frac{\partial \log(L(\beta))}{\partial \beta'}] \\
&= E[(\frac{\partial \log(L(\beta))}{\partial \beta_i} \frac{\partial \log(L(\beta))}{\partial \beta_j})_{ij}]
\end{align}
$$
Under **regularity conditions**, this is equivalent to the negative of the expected value of the Hessian Matrix   

$$
\begin{align}
\mathbf{I}(\beta) &= -E[\frac{\partial^2 \log(L(\beta))}{\partial \beta \partial \beta'}] \\
&= -E[(\frac{\partial^2 \log(L(\beta))}{\partial \beta_i \partial \beta_j})_{ij}]
\end{align}
$$

Example:  

$$
x_i' \beta = \beta_0 + \beta_1 x_i
$$


$$
- \frac{\partial^2 \ln(L(\beta))}{\partial \beta^2_0} = \sum_{i=1}^n \frac{\exp(x'_i \beta)}{1 + \exp(x'_i \beta)} - [\frac{\exp(x_i' \beta)}{1+ \exp(x'_i \beta)}]^2 = \sum_{i=1}^n p_i (1-p_i) \\
- \frac{\partial^2 \ln(L(\beta))}{\partial \beta^2_1} = \sum_{i=1}^n \frac{x_i^2\exp(x'_i \beta)}{1 + \exp(x'_i \beta)} - [\frac{x_i\exp(x_i' \beta)}{1+ \exp(x'_i \beta)}]^2 = \sum_{i=1}^n x_i^2p_i (1-p_i) \\
- \frac{\partial^2 \ln(L(\beta))}{\partial \beta_0 \partial \beta_1} = \sum_{i=1}^n \frac{x_i\exp(x'_i \beta)}{1 + \exp(x'_i \beta)} - x_i[\frac{\exp(x_i' \beta)}{1+ \exp(x'_i \beta)}]^2 = \sum_{i=1}^n x_ip_i (1-p_i) \\
$$

Hence,  

$$
\mathbf{I} (\beta) = 
\left[
\begin{array}
{cc}
\sum_i p_i(1-p_i) && \sum_i x_i p_i(1-p_i) \\
\sum_i x_i p_i(1-p_i) && \sum_i x_i^2 p_i(1-p_i)
\end{array}
\right]
$$


**Inference**  

**Likelihood Ratio Tests**  

To formulate the test, let $\beta = [\beta_1', \beta_2']'$. If you are interested in testing a hypothesis about $\beta_1$, then we leave $\beta_2$ unspecified (called **nuisance parameters**). $\beta_1$ and $\beta_2$ can either a **vector** or **scalar**, or $\beta_2$ can be null.  

Example: $H_0: \beta_1 = \beta_{1,0}$ (where $\beta_{1,0}$ is specified) and $\hat{\beta}_{2,0}$ be the MLE of $\beta_2$ under the restriction that $\beta_1 = \beta_{1,0}$. The likelihood ratio test statistic is  

$$
-2\log\Lambda = -2[\log(L(\beta_{1,0},\hat{\beta}_{2,0})) - \log(L(\hat{\beta}_1,\hat{\beta}_2))]
$$
where  

 * the first term is the value fo the likelihood for the fitted restricted model  
 * the second term is the likelihood value of the fitted unrestricted model  


Under the null, 

$$
-2 \log \Lambda \sim \chi^2_{\upsilon}
$$
where $\upsilon$ is the dimension of $\beta_1$  

We reject the null when $-2\log \Lambda > \chi_{\upsilon,1-\alpha}^2$  

**Wald Statistics**  

Based on 

$$
\hat{\beta} \sim AN (\beta, [\mathbf{I}(\beta)^{-1}])
$$
$$
H_0: \mathbf{L}\hat{\beta} = 0 
$$
where $\mathbf{L}$ is a q x p matrix with q linearly independent rows. Then  

$$
W = (\mathbf{L\hat{\beta}})'(\mathbf{L[I(\hat{\beta})]^{-1}L'})^{-1}(\mathbf{L\hat{\beta}})
$$
under the null hypothesis  

Confidence interval  

$$
\hat{\beta}_i \pm 1.96 \hat{s}_{ii}^2
$$
where $\hat{s}_{ii}^2$ is the i-th diagonal of $\mathbf{[I(\hat{\beta})]}^{-1}$  

If you have   

 * large sample size, the likelihood ratio and Wald tests have similar results.  
 * small sample size, the likelihood ratio test is better.  

**Logistic Regression: Interpretation of $\beta$**  

For single regressor, the model is  

$$
logit\{\hat{p}_{x_i}\} \equiv logit (\hat{p}_i) = \log(\frac{\hat{p}_i}{1 - \hat{p}_i}) = \hat{\beta}_0 + \hat{\beta}_1 x_i
$$

When $x= x_i + 1$  

$$
logit\{\hat{p}_{x_i +1}\} = \hat{\beta}_0 + \hat{\beta}(x_i + 1) = logit\{\hat{p}_{x_i}\} + \hat{\beta}_1
$$

Then,  

$$
logit\{\hat{p}_{x_i +1}\} - logit\{\hat{p}_{x_i}\} = log\{odds[\hat{p}_{x_i +1}]\} - log\{odds[\hat{p}_{x_i}]\} \\
= log(\frac{odds[\hat{p}_{x_i + 1}]}{odds[\hat{p}_{x_i}]}) = \hat{\beta}_1
$$

and  

$$
exp(\hat{\beta}_1) = \frac{odds[\hat{p}_{x_i + 1}]}{odds[\hat{p}_{x_i}]}
$$
the estimated **odds ratio**  


the estimated odds ratio, when there is a difference of c units in the regressor x, is $exp(c\hat{\beta}_1)$. When there are multiple covariates, $exp(\hat{\beta}_k)$ is the estimated odds ratio for the variable $x_k$, assuming that all of the other variables are held constant.  


**Inference on the Mean Response**  

Let $x_h = (1, x_{h1}, ...,x_{h,p-1})'$. Then 

$$
\hat{p}_h = \frac{exp(\mathbf{x'_h \hat{\beta}})}{1 + exp(\mathbf{x'_h \hat{\beta}})}
$$

and $s^2(\hat{p}_h) = \mathbf{x'_h[I(\hat{\beta})]^{-1}x_h}$  

For new observation, we can have a cutoff point to decide whether y = 0 or 1.  


### Probit Regression

$$
E(Y_i) = p_i = \Phi(\mathbf{x_i'\theta})
$$
where $\Phi()$ is the CDF of a N(0,1) random variable.  

Other models (e..g, t--distribution; log-log; I complimentary log-log)  

We let $Y_i = 1$ success, $Y_i =0$ no success. We assume $Y \sim Ber$ and $p_i = P(Y_i =1)$, the success probability. We cosnider a logistic regression with the response function $logit(p_i) = x'_i \beta$


**Confusion matrix**  

| | Predicted |  |
|---|---|---|
| Truth | 1 | 0 |
|1|True Positive (TP) | False Negative (FN) |
|0 | False Positive (FP) | True Negative (TN) |


Sensitivity: ability to identify positive results  

$$
\text{Sensitivity} = \frac{TP}{TP + FN}
$$

Specificity: ability to identify negative results  

$$
\text{Specificity} = \frac{TN}{TN + FP}
$$

False positive rate: Type I error (1- specificity)

$$
\text{ False Positive Rate} = \frac{FP}{TN+ FP}
$$

False Negative Rate: Type II error (1-sensitivity)

$$
\text{False Negative Rate} = \frac{FN}{TP + FN}
$$

| | Predicted |  |
|---|---|---|
| Truth | 1 | 0 |
|1|Sensitivity | False Negative Rate |
|0 | False Positive Rate | Specificity |


### Poisson Regression

From the Poisson distribution  

$$
f(Y_i) = \frac{\mu_i^{Y_i}exp(-\mu_i)}{Y_i!}, Y_i = 0,1,.. \\
E(Y_i) = \mu_i  \\
var(Y_i) = \mu_i
$$
which is a natural distribution for counts. We can see that the variance is a function of the mean. If we let $\mu_i = f(\mathbf{x_i; \theta})$, it would be similar to [Logistic Regression] since we can choose $f()$ as $\mu_i = \mathbf{x_i'\theta}, \mu_i = \exp(\mathbf{x_i'\theta}), \mu_i = \log(\mathbf{x_i'\theta})$  

### Generalization

We can see that Poisson regression looks similar to logistic regression. Hence, we can generalize to a class of modeling. Thanks to [@Nelder_1972], we have the **generalized linear models** (GLMs). Estimation is generalize in these models.  


**Exponential Family**  
The theory of GLMs is developed for data with distribution given y the **exponential family**.  
The form of the data distribution that is useful for GLMs  is  

$$
f(y;\theta, \phi) = \exp(\frac{\theta y - b(\theta)}{a(\phi)} + c(y, \phi))
$$
where  

 * $\theta$ is called the natural parameter  
 * $\phi$ is called the dispersion parameter 

**Note**:   

This family includes the [Gamma], [Normal], [Poisson], and other.  

**Example**  

if we have $Y \sim N(\mu, \sigma^2)$

$$
\begin{align}
f(y; \mu, \sigma^2) &= \frac{1}{(2\pi \sigma^2)^{1/2}}\exp(-\frac{1}{2\sigma^2}(y- \mu)^2) \\
&= \exp(-\frac{1}{2\sigma^2}(y^2 - 2y \mu +\mu^2)- \frac{1}{2}\log(2\pi \sigma^2)) \\
&= \exp(\frac{y \mu - \mu^2/2}{\sigma^2} - \frac{y^2}{2\sigma^2} - \frac{1}{2}\log(2\pi \sigma^2)) \\
&= \exp(\frac{\theta y - b(\theta)}{a(\phi)} + c(y , \phi))
\end{align}
$$
where   

 * $\theta = \mu$  
 * $b(\theta) = \frac{\mu^2}{2}$  
 * $a(\phi) = \sigma^2 = \phi$  
 * $c(y , \phi) = - \frac{1}{2}(\frac{y^2}{\phi}+\log(2\pi \sigma^2))$


**Properties of GLM exponential families**  

 1. $E(Y) = b' (\theta)$ where $b'(\theta) = \frac{\partial b(\theta)}{\partial \theta}$ (here `'` is "prime", not transpose)  
 2. $var(Y) = a(\phi)b''(\theta)= a(\phi)V(\mu)$.  
    + $V(\mu)$ is the *variance function*; however, it is only the variance in the case that $a(\phi) =1$  
 3. If $a(), b(), c()$ are identifiable, we will derive expected value and variance of Y.   

Example  

Normal distribution

$$
b'(\theta) = \frac{\partial b(\mu^2/2)}{\partial \mu} = \mu \\
V(\mu) = \frac{\partial^2 (\mu^2/2)}{\partial \mu^2} = 1 \\
\to var(Y) = a(\phi) = \sigma^2
$$

Poisson distribution   

$$
\begin{align}
f(y, \theta, \phi) &= \frac{\mu^y \exp(-\mu)}{y!} \\
&= \exp(y\log(\mu) - \mu - \log(y!)) \\
&= \exp(y\theta - \exp(\theta) - \log(y!))
\end{align}
$$

where  

 * $\theta = \log(\mu)$  
 * $a(\phi) = 1$  
 * $b(\theta) = \exp(\theta)$  
 * $c(y, \phi) = \log(y!)$  


Hence,  

$$
E(Y) = \frac{\partial b(\theta)}{\partial \theta} = \exp(\theta) = \mu \\
var(Y) = \frac{\partial^2 b(\theta)}{\partial \theta^2} = \mu
$$


Since $\mu = E(Y) = b'(\theta)$  

In GLM, we take some monotone function (typically nonlinear) of $\mu$ to be linear in the set of covariates  

$$
g(\mu) = g(b'(\theta)) = \mathbf{x'\beta}
$$
Equivalently,  

$$
\mu = g^{-1}(\mathbf{x'\beta})
$$
where $g(.)$ is the **link function** since it links mean response ($\mu = E(Y)$) and a linear expression of the covariates  

Some people use $\eta = \mathbf{x'\beta}$ where $\eta$ = the "linear predictor"  


**GLM is composed of 2 components**  

The **random component**:  

 * is the distribution chosen to model the response variables $Y_1,...,Y_n$ 
 * is specified by the choice fo $a(), b(), c()$ in the exponential form  
 * Notation:  
    + Assume that there are n **independent** response variables $Y_1,...,Y_n$ with densities   
$$
f(y_i ; \theta_i, \phi) = \exp(\frac{\theta_i y_i - b(\theta_i)}{a(\phi)}+ c(y_i, \phi))
$$
notice each observation might have different densities 
    + Assume that $\phi$ is constant for all $i = 1,...,n$, but $\theta_i$ will vary. $\mu_i = E(Y_i)$ for all i.  

The **systematic component**  

 * is the portion of the model that gives the relation between $\mu$ and the covariates $\mathbf{x}$  
 * consists of 2 parts:  
    + the *link* function, $g(.)$  
    + the *linear predictor*, $\eta = \mathbf{x'\beta}$  
 * Notation:  
    + assume $g(\mu_i) = \mathbf{x'\beta} = \eta_i$ where $\mathbf{\beta} = (\beta_1,..., \beta_p)'$  
    + The parameters to be estimated are $\beta_1,...\beta_p , \phi$

**The Canonical Link**  

To choose $g(.)$, we can use **canonical link function**  

If the link function $g(.)$ is such $g(\mu_i) = \theta_i$, the natural parameter, then $g(.)$ is the canonical link.  

Example  

Distribution | Mean Response | Canonical link | name 
---|---|---|---
Normal random component | $\mu_i = \theta_i$ | $g(\mu_i) = \mu_i$ | the identity link  
Binomial random component | $\mu_i = \frac{n_i \exp(\theta)}{1+\exp(\theta_i)} \\ \theta(\mu_i) = \log(\frac{p_i}{1-p_i}) = \log (\frac{\mu_i}{n_i - \mu_i})$ | $g(\mu_i) = \log(\frac{\mu_i}{n_i - \mu_i})$ | the logit link  
Poisson random component | $\mu_i = \exp(\theta_i)$ | $g(\mu_i) = \log(\mu_i)$ | 
Gamma random component | $\mu_i = -\frac{1}{\theta_i} \\ \theta(\mu_i) = - \mu_i^{-1}$ | $g(\mu_i) = - \frac{1}{\mu_i}$ | 
Inverse Gaussian random | | $g(\mu_i) = \frac{1}{\mu_i^2}$ |

#### Estimation

 * MLE for parameters of the **systematic component ($\beta$)**  
 * Unification of derivation and computation (thanks to the exponential forms)  
 * No unification for estimation of the dispersion parameter ($\phi$)   

##### Estimation of $\beta$ 

We have  

$$
f(y_i ; \theta_i, \phi) = \exp(\frac{\theta_i y_i - b(\theta_i)}{a(\phi)}+ c(y_i, \phi)) \\
E(Y_i) = \mu_i = b'(\theta) \\
var(Y_i) = b''(\theta)a(\phi) = V(\mu_i)a(\phi) \\
g(\mu_i) = \mathbf{x}_i'\beta = \eta_i
$$

If the log-likelihood for a single observation is $l_i (\beta,\phi)$. The log-likelihood for all n observations is  

$$
\begin{align}
l(\beta,\phi) &= \sum_{i=1}^n l_i (\beta,\phi) \\
&= \sum_{i=1}^n (\frac{\theta_i y_i - b(\theta_i)}{a(\phi)}+ c(y_i, \phi))
\end{align}
$$

Using MLE to find $\beta$, we use the chain rule to get the derivatives    

$$
\begin{align}
\frac{\partial l_i (\beta,\phi)}{\partial \beta_j} &=  \frac{\partial l_i (\beta, \phi)}{\partial \theta_i} \times \frac{\partial \theta_i}{\partial \mu_i} \times \frac{\partial \mu_i}{\partial \eta_i}\times \frac{\partial \eta_i}{\partial \beta_j} \\
&= \sum_{i=1}^{n}(\frac{ y_i - \mu_i}{a(\phi)} \times \frac{1}{V(\mu_i)} \times \frac{\partial \mu_i}{\partial \eta_i} \times x_{ij})
\end{align}
$$

If we let 

$$
w_i \equiv ((\frac{\partial \eta_i}{\partial \mu_i})^2 V(\mu_i))^{-1}
$$

Then,  

$$
\frac{\partial l_i (\beta,\phi)}{\partial \beta_j} = \sum_{i=1}^n (\frac{y_i \mu_i}{a(\phi)} \times w_i \times \frac{\partial \eta_i}{\partial \mu_i} \times x_{ij})
$$

We can also get the second derivatives using the chain rule.  

Example:  

For the [Newton-Raphson] algorithm, we need  

$$
- E(\frac{\partial^2 l(\beta,\phi)}{\partial \beta_j \partial \beta_k})
$$
where $(j,k)$th element of the **Fisher information matrix** $\mathbf{I}(\beta)$  

Hence, 

$$
- E(\frac{\partial^2 l(\beta,\phi)}{\partial \beta_j \partial \beta_k}) = \sum_{i=1}^n \frac{w_i}{a(\phi)}x_{ij}x_{ik}
$$
for the (j,k)th element  

If Bernoulli model with logit link function (which is the canonical link)  

$$
b(\theta) = \log(1 + \exp(\theta)) = \log(1 + \exp(\mathbf{x'\beta})) \\
a(\phi) = 1  \\
c(y_i, \phi) = 0 \\
E(Y) = b'(\theta) = \frac{\exp(\theta)}{1 + \exp(\theta)} = \mu = p \\
\eta = g(\mu) = \log(\frac{\mu}{1-\mu}) = \theta = \log(\frac{p}{1-p}) = \mathbf{x'\beta} 
$$

For $Y_i$, i = 1,.., the log-likelihood is  

$$
l_i (\beta, \phi) = \frac{y_i \theta_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi) = y_i \mathbf{x}'_i \beta - \log(1+ \exp(\mathbf{x'\beta}))
$$
Additionally,

$$
V(\mu_i) = \mu_i(1-\mu_i)= p_i (1-p_i) \\
\frac{\partial \mu_i}{\partial \eta_i} = p_i(1-p_i)
$$

Hence,  

$$
\begin{align}
\frac{\partial l(\beta, \phi)}{\partial \beta_j} &= \sum_{i=1}^n[\frac{y_i - \mu_i}{a(\phi)} \times \frac{1}{V(\mu_i)}\times \frac{\partial \mu_i}{\partial \eta_i} \times x_{ij}] \\
&= \sum_{i=1}^n (y_i - p_i) \times \frac{1}{p_i(1-p_i)} \times p_i(1-p_i) \times x_{ij} \\
&= \sum_{i=1}^n (y_i - p_i) x_{ij} \\
&= \sum_{i=1}^n (y_i - \frac{\exp(\mathbf{x'_i\beta})}{1+ \exp(\mathbf{x'_i\beta})})x_{ij}
\end{align}
$$
then  

$$
w_i = ((\frac{\partial \eta_i}{\partial \mu_i})^2 V(\mu_i))^{-1} = p_i (1-p_i)
$$

$$
\mathbf{I}_{jk}(\mathbf{\beta}) = \sum_{i=1}^n \frac{w_i}{a(\phi)} x_{ij}x_{ik} = \sum_{i=1}^n p_i (1-p_i)x_{ij}x_{ik}
$$

The **Fisher-scoring** algorithm for the MLE of $\mathbf{\beta}$ is 

$$
\left(
\begin{array}
{c}
\beta_1 \\
\beta_2 \\
. \\
. \\
. \\
\beta_p \\
\end{array}
\right)^{(m+1)}
=
\left(
\begin{array}
{c}
\beta_1 \\
\beta_2 \\
. \\
. \\
. \\
\beta_p \\
\end{array}
\right)^{(m)} +
\mathbf{I}^{-1}(\mathbf{\beta})
\left(
\begin{array}
{c}
\frac{\partial l (\beta, \phi)}{\partial \beta_1} \\
\frac{\partial l (\beta, \phi)}{\partial \beta_2} \\
. \\
. \\
. \\
\frac{\partial l (\beta, \phi)}{\partial \beta_p} \\
\end{array}
\right)|_{\beta = \beta^{(m)}}
$$
Similar to [Newton-Raphson] expect the matrix of second derivatives by the expected value of the second derivative matrix.  

In matrix notation,  

$$
\begin{align}
\frac{\partial l }{\partial \beta} &= \frac{1}{a(\phi)}\mathbf{X'W\Delta(y - \mu)} \\
&= \frac{1}{a(\phi)}\mathbf{F'V^{-1}(y - \mu)} \\
\end{align}
$$

$$
\mathbf{I}(\beta) = \frac{1}{a(\phi)}\mathbf{X'WX} = \frac{1}{a(\phi)}\mathbf{F'V^{-1}F} \\
$$
where  

 * $\mathbf{X}$ is an n x p matrix of covariates  
 * $\mathbf{W}$ is an n x n diagonal matrix with (i,i)th element given by $w_i$  
 * $\mathbf{\Delta}$ an n x n diagonal matrix with (i,i)th element given by $\frac{\partial \eta_i}{\partial \mu_i}$  
 * $\mathbf{F} = \mathbf{\frac{\partial \mu}{\partial \beta}}$ an n x p matrix with ith row $\frac{\partial \mu_i}{\partial \beta} = (\frac{\partial \mu_i}{\partial \eta_i})\mathbf{x}'_i$  
 * $\mathbf{V}$ an n x n diagonal matrix with (i,i)th element given by $V(\mu_i)$   

Setting the derivative of the log-likelihood equal to 0, ML estimating equations are  

$$
\mathbf{F'V^{-1}y= F'V^{-1}\mu}
$$
where all components of this equation expect y depends on the parameters $\beta$  

**Special Cases**  

If one has a canonical link, the estimating equations reduce to  

$$
\mathbf{X'y= X'\mu}
$$
If one has an identity link, then  

$$
\mathbf{X'V^{-1}y = X'V^{-1}X\hat{\beta}}
$$
which gives the generalized least squares estimator  

Generally, we can rewrite the Fisher-scoring algorithm as  

$$
\beta^{(m+1)} = \beta^{(m)} + \mathbf{(\hat{F}'\hat{V}^{-1}\hat{F})^{-1}\hat{F}'\hat{V}^{-1}(y- \hat{\mu})}
$$

Since $\hat{F},\hat{V}, \hat{\mu}$ depend on $\beta$, we evaluate at $\beta^{(m)}$  

From starting values $\beta^{(0)}$, we can iterate until convergence.  

Notes:  

 * if $a(\phi)$ is a constant or of the form $m_i \phi$ with known $m_i$, then $\phi$ cancels.  


##### Estimation of $\phi$

2 approaches:  

 1. MLE  

$$
\frac{\partial l_i}{\partial \phi} = \frac{(\theta_i y_i - b(\theta_i)a'(\phi))}{a^2(\phi)} + \frac{\partial c(y_i,\phi)}{\partial \phi}
$$

the MLE of $\phi$ solves  

$$
\frac{a^2(\phi)}{a'(\phi)}\sum_{i=1}^n \frac{\partial c(y_i, \phi)}{\partial \phi} = \sum_{i=1}^n(\theta_i y_i - b(\theta_i))
$$
 * Situation others than normal error case, expression for $\frac{\partial c(y,\phi)}{\partial \phi}$ are not simple  
 * Even for the canonical link and $a(\phi)$ constant, there is no nice general expression for $-E(\frac{\partial^2 l}{\partial \phi^2})$, so the unification GLMs provide for estimation of $\beta$ breaks down for $\phi$  

 2. Moment Estimation ("Bias Corrected $\chi^2$")  

    * The MLE is not conventional approach to estimation of $\phi$ in GLMS.  
    * For the exponential family $var(Y) =V(\mu)a(\phi)$. This implies  
$$
a(\phi) = \frac{var(Y)}{V(\mu)} = \frac{E(Y- \mu)^2}{V(\mu)} \\
a(\hat{\phi})  = \frac{1}{n-p} \sum_{i=1}^n \frac{(y_i -\hat{\mu}_i)^2}{V(\hat{\mu})}
$$
where p is the dimension of $\beta$  
    * GLM with canonical link function $g(.)= (b'(.))^{-1}$  
$$
g(\mu) = \theta = \eta = \mathbf{x'\beta} \\
\mu = g^{-1}(\eta)= b'(\eta)
$$
    * so the method estimator for $a(\phi)=\phi$ is  

$$
\hat{\phi} = \frac{1}{n-p} \sum_{i=1}^n \frac{(y_i - g^{-1}(\hat{\eta}_i))^2}{V(g^{-1}(\hat{\eta}_i))}
$$

#### Inference

We have 

$$
\hat{var}(\beta) = a(\phi)(\mathbf{\hat{F}'\hat{V}\hat{F}})^{-1}
$$
where  

 * $\mathbf{V}$ is an n x n diagonal matrix with diagonal elements given by $V(\mu_i)$  
 * $\mathbf{F}$ is an n x p matrix given by $\mathbf{F} = \frac{\partial \mu}{\partial \beta}$  
 * Both $\mathbf{V,F}$ are dependent on the mean $\mu$, and thus $\beta$. Hence, their estimates ($\mathbf{\hat{V},\hat{F}}$) depend on $\hat{\beta}$.  

$$
H_0: \mathbf{L\beta = d}
$$
where $\mathbf{L}$ is a q x p matrix with a **Wald** test  

$$
W = \mathbf{(L \hat{\beta}-d)'(a(\phi)L(\hat{F}'\hat{V}^{-1}\hat{F})L')^{-1}(L \hat{\beta}-d)}
$$
which follows $\chi_q^2$ distribution (asymptotically), where q is the rank of $\mathbf{L}$  

In the simple case  $H_0: \beta_j = 0$ gives $W = \frac{\hat{\beta}^2_j}{\hat{var}(\hat{\beta}_j)} \sim \chi^2_1$ asymptotically  

Likelihood ratio test  


$$
\Lambda = 2 (l(\hat{\beta}_f)-l(\hat{\beta}_r)) \sim \chi^2_q
$$
where  

 * q is the number of constraints used to fit the reduced model $\hat{\beta}_r$, and $\hat{\beta}_r$ is the fit under the full model.  
 
Wald test is easier to implement, but likelihood ratio test is better (especially for small samples).  



#### Deviance

[Deviance] is necessary for goodness of fit, inference and for alternative estimation of the dispersion parameter. We define and consider [Deviance] from a likelihood ratio perspective.  

 * Assume that $\phi$ is known. Let $\tilde{\theta}$ denote the full and $\hat{\theta}$ denote the reduced model MLEs. Then, the likelihood ratio (2 times the difference in log-likelihoods) is

$$
2\sum_{i=1}^{n} \frac{y_i (\tilde{\theta}_i- \hat{\theta}_i)-b(\tilde{\theta}_i) + b(\hat{\theta}_i)}{a_i(\phi)}
$$





## Genelized Method of Moments

## Minimum Distance

## Spline Regression

This chapter is based on [CMU stat](https://www.stat.cmu.edu/~ryantibs/advmethods/notes/smoothspline.pdf)  

Definition: a k-th order spline is a piecewise polynomial function of degree k, that is continuous and has continuous derivatives of orders 1,..., k -1, at its knot points   

Equivalently, a function f is a k-th order spline with knot points at $t_1 < ...< t_m$ if  

 * f is a polynomial of degree k on each of the intervals $(-\infty, t_1], [t_1,t_2],...,[t_m, \infty)$  
 * $f^{(j)}$, the j-th derivative of f, is continuous at $t_1,...,t_m$ for each j = 0,1,...,k-1

A common case is when k = 3, called cubic splines. (piecewise cubic functions are continuous, and also continuous at its first and second derivatives)  

To parameterize the set of splines, we could use **truncated power basis**, defined as  

$$
g_1(x) = 1 \\
g_2(x) = x \\
... \\
g_{k+1}(x) = x^k \\
g_{k+1+j}(x) = (x-t_j)^k_+
$$

where j = 1,...,m and $x_+$ = max{x,0}  

However, now software typically use B-spline basis.  

### Regression Splines

To estimate the regression function $r(X) = E(Y|X =x)$, we can fit a k-th order spline with knots at some prespecified locations $t_1,...,t_m$  

Regression splines are functions of 

$$
\sum_{j=1}^{m+k+1} \beta_jg_j
$$

where  

 *$\beta_1,..\beta_{m+k+1}$ are coefficients
 * $g_1,...,g_{m+k+1}$ are the truncated power basis functions for k-th order splines over the knots $t_1,...,t_m$


To estimate the coefficients  

$$
\sum_{i=1}^{n} (y_i - \sum_{j=1}^{m} \beta_j g_j (x_i))^2
$$

then regression spline is  

$$
\hat{r}(x) = \sum_{j=1}^{m+k+1} \hat{\beta}_j g_j (x)
$$


If we define the basis matrix $G \in R^{n \times (m+k+1)}$ by 
$$
G_{ij} = g_j(x_i) 
$$
where $i = 1,..,n$ , $j = 1,..,m+k+1$

Then,  

$$
\sum_{i=1}^{n} (y_i - \sum_{j=1}^{m} \beta_j g_j (x_i))^2 = ||y - G \beta||_2^2
$$

and the regression spline estimate at x is  

$$
\hat{r} (x) = g(x)^T \hat{\beta}= g(x)^T(G^TG)^{-1}G^Ty
$$

### Natural splines

A natural spline of order k, with knots at $t_1 <...< t_m$, is a piecewise polynomial function f such that  

 * f is polynomial of degree k on each of $[t_1,t_2],...,[t_{m-1},t_m]$
 * f is a polynomial of degree $(k-1)/2$ on $(-\infty,t_1]$ and $[t_m,\infty)$
 * f is continuous and has continuous derivatives of orders 1,.,,, k -1 at its knots $t_1,..,t_m$

**Note**  

natural splines are only defined for odd orders k.

### Smoothing spliness

These estimators use a regularized regression over the natural spline basis: placing knots at all points $x_1,...x_n$  

For the case of cubic splines, the coefficients are the minimization of 

$$
||y - G\beta||^2_2 + \lambda \beta^T \Omega \beta
$$

where  $\Omega \in R^{n \times n}$ is the penalty matrix

$$
\Omega_{ij} = \int g''_i(t) g''_j(t) dt,
$$

and i,j = 1,..,n

and $\lambda \beta^T \Omega \beta$ is the **regularization term** used to shrink the components of $\hat{\beta}$ towards 0. 
$\lambda > 0$ is the tuning parameter (or smoothing parameter). Higher value of $\lambda$, faster shrinkage  (shrinking away basis functions)  

**Note**  
smoothing splines have similar fits as kernel regression. 

| | Smoothing splines | kernel regression |
|---|---|---|
|tuning parameter | smoothing parameter $\lambda$ | bandwidth h |


### Application 
```{r}

library(tidyverse)
library(caret)
theme_set(theme_classic())

# Load the data
data("Boston", package = "MASS")
# Split the data into training and test set
set.seed(123)
training.samples <- Boston$medv %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- Boston[training.samples, ]
test.data <- Boston[-training.samples, ]

knots <- quantile(train.data$lstat, p = c(0.25, 0.5, 0.75)) # we use 3 knots at 25,50,and 75 quantile.

library(splines)
# Build the model
knots <- quantile(train.data$lstat, p = c(0.25, 0.5, 0.75))
model <- lm (medv ~ bs(lstat, knots = knots), data = train.data)
# Make predictions
predictions <- model %>% predict(test.data)
# Model performance
data.frame(
  RMSE = RMSE(predictions, test.data$medv),
  R2 = R2(predictions, test.data$medv)
)

ggplot(train.data, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth(method = lm, formula = y ~ splines::bs(x, df = 3))


attach(train.data)
#fitting smoothing splines using smooth.spline(X,Y,df=...)

fit1<-smooth.spline(train.data$lstat,train.data$medv,df=3 ) # 3 degrees of freedom
#Plotting both cubic and Smoothing Splines 
plot(train.data$lstat,train.data$medv,col="grey")
lstat.grid = seq(from = range(lstat)[1], to = range(lstat)[2])
points(lstat.grid,predict(model,newdata = list(lstat=lstat.grid)),col="darkgreen",lwd=2,type="l")
#adding cutpoints
abline(v=c(10,20,30),lty=2,col="darkgreen")
lines(fit1,col="red",lwd=2)

legend("topright",c("Smoothing Spline with 3 df","Cubic Spline"),col=c("red","darkgreen"),lwd=2)

```

## Generalized Additive Models
To overcome [Spline Regression]'s requirements for specifying the knots, we can use [Generalized Additive Models] or GAM. 

```{r}
library(mgcv)
# Build the model
model <- gam(medv ~ s(lstat), data = train.data)
plot(model)
# Make predictions
# predictions <- model %>% predict(test.data)
# Model performance
data.frame(
  RMSE = RMSE(predictions, test.data$medv),
  R2 = R2(predictions, test.data$medv)
)

ggplot(train.data, aes(lstat, medv) ) +
  geom_point() +
  stat_smooth(method = gam, formula = y ~ s(x))
detach(train.data)
```



## Quantile Regression

For academic review on quantile regression, check [@Yu_2003]

[Linear Regression] is based on the conditional mean function $E(y|x)$  

In Quantile regression, we can view each points in the conditional distribution of y. Quantile regression estimates the conditional median or any other quantile of Y.

In the case that we're interested in the 50th percentile, quantile regression is median regression, also known as least-absolute-deviations (LAD) regression, minimizes $\sum_{i}|e_i|$


Properties of estimators $\beta$

 * Asymptotically normally distributed


Advantages  

 * More robust to outliers compared to [OLS][Ordinary Least Squares]
 * In the case the dependent variable has a bimodal or multimodal (multiple humps with multiple modes) distribution, quantile regression can be extremely useful. 
 * Avoids parametric distribution assumption of the error process. In another word, no assumptions regarding the distribution of the error term.
 * Better characterization of the data (not just its conditional mean)
 * is invariant to monotonic transformations (such as log) while OLS is not. In another word, $E(g(y))=g(E(y))$
 
Disadvantages

 * The dependent variable needs to be continuous with no zeroes or too many repeated values. 
 
 
$$
y_i = x_i'\beta_q + e_i
$$
 
Let $e(x) = y -\hat{y}(x)$, then $L(e(x)) = L(y -\hat{y}(x))$ is the loss function of the error term.  

If $L(e) = |e|$ (called absolute-error loss function) then $\hat{\beta}$ can be estimated by minimizing $\sum_{i}|y_i-x_i'\beta|$  

More specifically, the objective function is 
$$
Q(\beta_q)=\sum_{i:y_i \ge x_i'\beta}^{N} q|y_i - x_i'\beta_q| + \sum_{i:y_i < x_i'\beta}^{N} (1-q)|y_i-x_i'\beta_q
$$
where $0<q<1$

The sum penalizes $q|e_i|$ for under-prediction and $(1-q)|e_i|$ for over-prediction


We use simplex method to minimize this function (cannot use analytical solution since it's non-differentiable). Standard errors can be estimated by bootstrap.


The absolute-error loss function is symmetric. 

**Interpretation**
For the jth regressor ($x_j$), the marginal effect is the coefficient for the qth quantile

$$
\frac{\partial Q_q(y|x)}{\partial x_j} = \beta_{qj}
$$
At the quantile q of the dependent variable y, $\beta_q$ represents a one unit change in the independent variable $x_j$ on the dependent variable y.  

In other words, at the qth percentile, a one unit change in x results in $\beta_q$ unit change in y.




### Application

```{r}
# generate data with non-constant variance

x <- seq(0,100,length.out = 100)        # independent variable
sig <- 0.1 + 0.05*x                     # non-constant variance
b_0 <- 3                                # true intercept
b_1 <- 0.05                             # true slope
set.seed(1)                             # reproducibility
e <- rnorm(100,mean = 0, sd = sig)      # normal random error with non-constant variance
y <- b_0 + b_1*x + e                    # dependent variable
dat <- data.frame(x,y)
hist(y)
library(ggplot2)
ggplot(dat, aes(x,y)) + geom_point()
ggplot(dat, aes(x,y)) + geom_point() + geom_smooth(method="lm")
```

We follow [@Roger_1996] to estimate quantile regression 
```{r}
library(quantreg)
qr <- rq(y ~ x, data=dat, tau = 0.5) # tau: quantile of interest. Here we have it at 50th percentile.
summary(qr)
```
adding the regression line

```{r}
ggplot(dat, aes(x,y)) + geom_point() + 
  geom_abline(intercept=coef(qr)[1], slope=coef(qr)[2])
```


To have R estimate multiple quantile at once

```{r}
qs <- 1:9/10
qr1 <- rq(y ~ x, data=dat, tau = qs)
#check for its coefficients
coef(qr1)

# plot
ggplot(dat, aes(x,y)) + geom_point() + geom_quantile(quantiles = qs)
```

To examine if the quantile regression is appropriate, we can see its plot compared to least squares regression 
```{r}
plot(summary(qr1), parm="x")
```

where red line is the least squares estimates, and its confidence interval. 
x-axis is the quantile 
y-axis is the value of the quantile regression coefficients at different quantile

If the error term is normally distributed, the quantile regression line will fall inside the coefficient interval of least squares regression. 
```{r}
# generate data with constant variance

x <- seq(0, 100, length.out = 100)    # independent variable
b_0 <- 3                              # true intercept
b_1 <- 0.05                           # true slope
set.seed(1)                           # reproducibility
e <- rnorm(100, mean = 0, sd = 1)     # normal random error with constant variance
y <- b_0 + b_1 * x + e                # dependent variable
dat2 <- data.frame(x, y)
qr2 = rq(y ~ x, data = dat2, tau = qs)
plot(summary(qr2), parm = "x")
```







