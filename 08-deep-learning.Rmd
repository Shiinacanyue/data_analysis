# Deep Learning
## Overview

This section is based on [@TensorFlow]
 
 * What is deep learning?
Input to output via layers of representation

 * What are layers?
A layer is a geometric transformation function on the data that goes through it Weights determine the data transformation behavior of a layer  

<br>
Learning Representation  

 * Transforming input data into useful representation  
 
<br>

The "deep" in deep learning  

 * multiple layers 
 * Other possibly more appropriate names for the field:
    + Layered representations learning 
    + Hierarchical representations learning 
    + Chained geometric transformation learning  
    
<br>
New problem domains for R:  

 * Computer vision  
 * Computer speech recognition  
 * Reinforcement learning applications  
 
<br>
How do we train deep learning models?  

 * Basic of machine learning algorithms  
 * Machine learning vs. statistical modeling  
 * MNIST example  
    + Model definition in R  
    + Layers of representation  
 * The training loop  
 
<br>
Machine learning algorithms  
Learning model parameters via exposure to many example data points  

<br>
Statistics: Often focused on inferring the process by which data is generated 
Machine Learning: Principally focused on predicting future data  

<br>
Deep learning frontiers  

 * Computer vision 
 * Natural language processing 
 * Time series 
 * Biomedical  


\pagebreak


## Tensor Flow and R
TensorFlow APIs  

 * Keras API  
 * Estimator API  
 * Core API  

<br>

### R packages 
TensorFlow APIs 
 * keras: Interface for neural networks, with a focus on enabling fast experimentation. 
 * tfestimators: Implementations of common model types such as regressors and classifiers.  
 * tensorflow: Low *level interface to the TensorFlow computational graph. 
 * tfdatasets: Scalable input pipelines for TensorFlow models.

Supporting Tools 
 * tfruns : Track , visualize, and manage TensorFlow training runs and experiments 
 * tfdeploy : Tools designed to make exporting and serving TensorFlow models straightforward. 
 * cloudml : R interface to Google Cloud Machine Learning Engine.  

R interface to Keras  

 * Step by step example  
 * [Keras layers]  
 * [Compiling models]  
 * [Losses, Optimizers, and Metrics] 
 * More examples  

<br>

### Keras layers  
65 layers available  

 * Dense layers: classic "fully connected" neural network layers  
 * Convolutional layers: "Filters for learning local patterns in data  
 * Recurrent layers: Layers that maintain state based on on previously seen   data 
 * Embedding layers: Vectorization of text that reflects semantic relationships between words  


## Compiling models
Model compilation prepares the model for training by:  
1. Converting the layers into a TensorFlow graph  
2. Applying the specified loss function and optimizer  
3. Arranging for the collection of metrics during training  

## Losses, Optimizers, and Metrics
All available at Keras for R [cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/keras.pdf)


Example of [Image classificaiton](https://blogs.rstudio.com/ai/posts/2017 *12 *14 *image *classification *on *small *datasets/)

## NYU
Materials in this chapter is based on [NYU Deep Learning](https://atcold.github.io/pytorch-Deep-Learning/)

Deep in Deep Learning means multi-layers. "A deep network has several layers and uses them to build a hierarchy of features of increasing complexity"

Supervised Learning (= Function Optimization): With inputs and outputs, you train the machine to tweak its parameter to have the correct outputs from inputs. 
In "traditional machine learning", you have to engineer **feature extractor**, but in deep learning you can multi-layers feature extractor can be trained. And all layers are non-linear, because if they are linear, then all layers would be collapse into one layer. 

Natural Data is compositional. Hence, it is efficiently representable hierarchically. 

Revolutions in Deep Learning:  

 * Speech Recognition: 2010
 * Image Recognition: 2013
 * Natural Language Processing (NLP): 2015


Deep Learning = Learning Representations/ Features
Representation means you turn raw data into something useful   


Image Recognition:  
Pixel, edge, texton, motif, part, object 

Text  
Character, group, word group, clause, sentence, story  

Speech  
Sample, spectral band, sound, phone, phoneme, word. 

Difference between Deep Learning Support-Vector Machines (SVM)  

SVM is a 2-layer neural net, where  

 * layer 1 = templates
 * layer 2 = linear classifier. 

where Deep Learning is "A deep network has several layers and uses them to build a hierarchy of features of increasing complexity"

Deep Machines are more efficient than SVM in representing certain classes of functions. 

The Manifold Hypothesis "states that real-world high-dimensional data lie on low-dimensional manifolds embedded within the high-dimensional space"  

Difference between Deep Learning and PCA (Principal Components Analysis) is that Deep learning uses non-linear function, while PCA uses linear function. 
