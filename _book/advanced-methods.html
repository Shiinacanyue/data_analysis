<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Advanced Methods | A Guide on Data Analysis</title>
  <meta name="description" content="This is a guide on how to conduct a data analysis routine" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Advanced Methods | A Guide on Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a guide on how to conduct a data analysis routine" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Advanced Methods | A Guide on Data Analysis" />
  
  <meta name="twitter:description" content="This is a guide on how to conduct a data analysis routine" />
  

<meta name="author" content="Mike Nguyen" />


<meta name="date" content="2020-11-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="experimental-design.html"/>
<link rel="next" href="appendix.html"/>
<script src="libs/jquery-3.5.0/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.2/htmlwidgets.js"></script>
<script src="libs/proj4js-2.3.15/proj4.js"></script>
<link href="libs/highcharts-8.1.2/css/motion.css" rel="stylesheet" />
<script src="libs/highcharts-8.1.2/highcharts.js"></script>
<script src="libs/highcharts-8.1.2/highcharts-3d.js"></script>
<script src="libs/highcharts-8.1.2/highcharts-more.js"></script>
<script src="libs/highcharts-8.1.2/modules/stock.js"></script>
<script src="libs/highcharts-8.1.2/modules/map.js"></script>
<script src="libs/highcharts-8.1.2/modules/annotations.js"></script>
<script src="libs/highcharts-8.1.2/modules/data.js"></script>
<script src="libs/highcharts-8.1.2/modules/drilldown.js"></script>
<script src="libs/highcharts-8.1.2/modules/item-series.js"></script>
<script src="libs/highcharts-8.1.2/modules/offline-exporting.js"></script>
<script src="libs/highcharts-8.1.2/modules/overlapping-datalabels.js"></script>
<script src="libs/highcharts-8.1.2/modules/exporting.js"></script>
<script src="libs/highcharts-8.1.2/modules/export-data.js"></script>
<script src="libs/highcharts-8.1.2/modules/funnel.js"></script>
<script src="libs/highcharts-8.1.2/modules/heatmap.js"></script>
<script src="libs/highcharts-8.1.2/modules/treemap.js"></script>
<script src="libs/highcharts-8.1.2/modules/sankey.js"></script>
<script src="libs/highcharts-8.1.2/modules/dependency-wheel.js"></script>
<script src="libs/highcharts-8.1.2/modules/organization.js"></script>
<script src="libs/highcharts-8.1.2/modules/solid-gauge.js"></script>
<script src="libs/highcharts-8.1.2/modules/streamgraph.js"></script>
<script src="libs/highcharts-8.1.2/modules/sunburst.js"></script>
<script src="libs/highcharts-8.1.2/modules/vector.js"></script>
<script src="libs/highcharts-8.1.2/modules/wordcloud.js"></script>
<script src="libs/highcharts-8.1.2/modules/xrange.js"></script>
<script src="libs/highcharts-8.1.2/modules/tilemap.js"></script>
<script src="libs/highcharts-8.1.2/modules/venn.js"></script>
<script src="libs/highcharts-8.1.2/modules/gantt.js"></script>
<script src="libs/highcharts-8.1.2/modules/timeline.js"></script>
<script src="libs/highcharts-8.1.2/modules/parallel-coordinates.js"></script>
<script src="libs/highcharts-8.1.2/modules/bullet.js"></script>
<script src="libs/highcharts-8.1.2/modules/coloraxis.js"></script>
<script src="libs/highcharts-8.1.2/modules/dumbbell.js"></script>
<script src="libs/highcharts-8.1.2/modules/lollipop.js"></script>
<script src="libs/highcharts-8.1.2/modules/series-label.js"></script>
<script src="libs/highcharts-8.1.2/plugins/motion.js"></script>
<script src="libs/highcharts-8.1.2/custom/reset.js"></script>
<script src="libs/highcharts-8.1.2/modules/boost.js"></script>
<script src="libs/highchart-binding-0.8.2/highchart.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Guide on Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#tools-of-statistics"><i class="fa fa-check"></i><b>1.1</b> Tools of statistics</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#setup-working-environment"><i class="fa fa-check"></i><b>1.2</b> Setup Working Environment</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="descriptive-stat.html"><a href="descriptive-stat.html"><i class="fa fa-check"></i><b>2</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="descriptive-stat.html"><a href="descriptive-stat.html#numerical-measures"><i class="fa fa-check"></i><b>2.1</b> Numerical Measures</a></li>
<li class="chapter" data-level="2.2" data-path="descriptive-stat.html"><a href="descriptive-stat.html#graphical-measures"><i class="fa fa-check"></i><b>2.2</b> Graphical Measures</a><ul>
<li class="chapter" data-level="2.2.1" data-path="descriptive-stat.html"><a href="descriptive-stat.html#shape"><i class="fa fa-check"></i><b>2.2.1</b> Shape</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="descriptive-stat.html"><a href="descriptive-stat.html#normality-assessment"><i class="fa fa-check"></i><b>2.3</b> Normality Assessment</a><ul>
<li class="chapter" data-level="2.3.1" data-path="descriptive-stat.html"><a href="descriptive-stat.html#graphical-assessment"><i class="fa fa-check"></i><b>2.3.1</b> Graphical Assessment</a></li>
<li class="chapter" data-level="2.3.2" data-path="descriptive-stat.html"><a href="descriptive-stat.html#summary-statistics"><i class="fa fa-check"></i><b>2.3.2</b> Summary Statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="basic-statistical-inference.html"><a href="basic-statistical-inference.html"><i class="fa fa-check"></i><b>3</b> Basic Statistical Inference</a><ul>
<li class="chapter" data-level="3.1" data-path="basic-statistical-inference.html"><a href="basic-statistical-inference.html#one-sample-inference"><i class="fa fa-check"></i><b>3.1</b> One Sample Inference</a></li>
<li class="chapter" data-level="3.2" data-path="basic-statistical-inference.html"><a href="basic-statistical-inference.html#two-sample-inference"><i class="fa fa-check"></i><b>3.2</b> Two Sample Inference</a></li>
<li class="chapter" data-level="3.3" data-path="basic-statistical-inference.html"><a href="basic-statistical-inference.html#categorical-data-analysis"><i class="fa fa-check"></i><b>3.3</b> Categorical Data Analysis</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="regression-analysis.html"><a href="regression-analysis.html"><i class="fa fa-check"></i><b>4</b> Regression Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="regression-analysis.html"><a href="regression-analysis.html#linear-regression"><i class="fa fa-check"></i><b>4.1</b> Linear Regression</a><ul>
<li class="chapter" data-level="4.1.1" data-path="regression-analysis.html"><a href="regression-analysis.html#ordinary-least-squares"><i class="fa fa-check"></i><b>4.1.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="4.1.2" data-path="regression-analysis.html"><a href="regression-analysis.html#weighted-least-squares"><i class="fa fa-check"></i><b>4.1.2</b> Weighted Least Squares</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="regression-analysis.html"><a href="regression-analysis.html#generalized-least-squares"><i class="fa fa-check"></i><b>4.2</b> Generalized Least Squares</a></li>
<li class="chapter" data-level="4.3" data-path="regression-analysis.html"><a href="regression-analysis.html#non-linear-regression"><i class="fa fa-check"></i><b>4.3</b> Non-linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="experimental-design.html"><a href="experimental-design.html"><i class="fa fa-check"></i><b>5</b> Experimental Design</a><ul>
<li class="chapter" data-level="5.1" data-path="experimental-design.html"><a href="experimental-design.html#analysis-of-variance-anova"><i class="fa fa-check"></i><b>5.1</b> Analysis of Variance (ANOVA)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="advanced-methods.html"><a href="advanced-methods.html"><i class="fa fa-check"></i><b>6</b> Advanced Methods</a><ul>
<li class="chapter" data-level="6.1" data-path="advanced-methods.html"><a href="advanced-methods.html#imputation-missing-data"><i class="fa fa-check"></i><b>6.1</b> Imputation (Missing Data)</a><ul>
<li class="chapter" data-level="6.1.1" data-path="advanced-methods.html"><a href="advanced-methods.html#types-of-imputation"><i class="fa fa-check"></i><b>6.1.1</b> Types of Imputation</a></li>
<li class="chapter" data-level="6.1.2" data-path="advanced-methods.html"><a href="advanced-methods.html#another-perspective"><i class="fa fa-check"></i><b>6.1.2</b> Another Perspective</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="advanced-methods.html"><a href="advanced-methods.html#deep-learning"><i class="fa fa-check"></i><b>6.2</b> Deep Learning</a><ul>
<li class="chapter" data-level="6.2.1" data-path="advanced-methods.html"><a href="advanced-methods.html#overview"><i class="fa fa-check"></i><b>6.2.1</b> Overview</a></li>
<li class="chapter" data-level="6.2.2" data-path="advanced-methods.html"><a href="advanced-methods.html#tensor-flow-and-r"><i class="fa fa-check"></i><b>6.2.2</b> Tensor Flow and R</a></li>
<li class="chapter" data-level="6.2.3" data-path="advanced-methods.html"><a href="advanced-methods.html#compiling-models"><i class="fa fa-check"></i><b>6.2.3</b> Compiling models</a></li>
<li class="chapter" data-level="6.2.4" data-path="advanced-methods.html"><a href="advanced-methods.html#losses-optimizers-and-metrics"><i class="fa fa-check"></i><b>6.2.4</b> Losses, Optimizers, and Metrics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>7</b> Appendix</a><ul>
<li class="chapter" data-level="7.1" data-path="appendix.html"><a href="appendix.html#short-cut"><i class="fa fa-check"></i><b>7.1</b> Short-cut</a></li>
<li class="chapter" data-level="7.2" data-path="appendix.html"><a href="appendix.html#function-short-cut"><i class="fa fa-check"></i><b>7.2</b> Function short-cut</a></li>
<li class="chapter" data-level="7.3" data-path="appendix.html"><a href="appendix.html#citation"><i class="fa fa-check"></i><b>7.3</b> Citation</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bookdown-cheat-sheet.html"><a href="bookdown-cheat-sheet.html"><i class="fa fa-check"></i><b>8</b> Bookdown cheat sheet</a><ul>
<li class="chapter" data-level="8.1" data-path="bookdown-cheat-sheet.html"><a href="bookdown-cheat-sheet.html#math-expresssion-syntax"><i class="fa fa-check"></i><b>8.1</b> Math Expresssion/ Syntax</a></li>
<li class="chapter" data-level="8.2" data-path="bookdown-cheat-sheet.html"><a href="bookdown-cheat-sheet.html#heading-blah-blah"><i class="fa fa-check"></i><b>8.2</b> Heading blah blah</a></li>
<li class="chapter" data-level="8.3" data-path="bookdown-cheat-sheet.html"><a href="bookdown-cheat-sheet.html#id-example"><i class="fa fa-check"></i><b>8.3</b> About labelling things</a></li>
<li class="chapter" data-level="8.4" data-path="bookdown-cheat-sheet.html"><a href="bookdown-cheat-sheet.html#cross-references"><i class="fa fa-check"></i><b>8.4</b> Cross-references</a></li>
<li class="chapter" data-level="8.5" data-path="bookdown-cheat-sheet.html"><a href="bookdown-cheat-sheet.html#figures-tables-citations"><i class="fa fa-check"></i><b>8.5</b> Figures, tables, citations</a></li>
<li class="chapter" data-level="8.6" data-path="bookdown-cheat-sheet.html"><a href="bookdown-cheat-sheet.html#how-the-square-bracket-links-work"><i class="fa fa-check"></i><b>8.6</b> How the square bracket links work</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Guide on Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="advanced-methods" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Advanced Methods</h1>
<div id="imputation-missing-data" class="section level2">
<h2><span class="header-section-number">6.1</span> Imputation (Missing Data)</h2>
<div id="types-of-imputation" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Types of Imputation</h3>
<div id="mean-mode-median-imputation" class="section level4">
<h4><span class="header-section-number">6.1.1.1</span> Mean, Mode, Median Imputation</h4>
<ul>
<li><p>Bad:</p>
<ul>
<li>Mean imputation does not preserve the relationships among variables<br />
</li>
<li>Mean imputation leads to An Underestimate of Standard Errors → you’re making Type I errors without realizing it.</li>
</ul></li>
</ul>
</div>
<div id="hot-deck-imputation" class="section level4">
<h4><span class="header-section-number">6.1.1.2</span> Hot Deck Imputation</h4>
<p>Randomly choose value from other observations that share similar values on other variables.</p>
<ul>
<li>Good:
<ul>
<li>Constrained to only possible values.<br />
</li>
<li>Since the value is picked at random, it adds some variability, which might come in handy when calculating standard errors.</li>
</ul></li>
</ul>
</div>
<div id="cold-deck-imputation" class="section level4">
<h4><span class="header-section-number">6.1.1.3</span> Cold Deck Imputation</h4>
<p>Contrary to Hot Deck, Cold Deck choose value systematically from an observation that has similar values on other variables, which remove the random variation that we want.</p>
</div>
<div id="regression-imputation" class="section level4">
<h4><span class="header-section-number">6.1.1.4</span> Regression Imputation</h4>
<p>Missing value is based (regress) on other variables.</p>
<ul>
<li>Good:
<ul>
<li>Maintain the relationship with other variables</li>
</ul></li>
<li>Bad:
<ul>
<li>No variability left.</li>
</ul></li>
</ul>
</div>
<div id="stochatistc-imputation" class="section level4">
<h4><span class="header-section-number">6.1.1.5</span> Stochatistc Imputation</h4>
<p><code>Regression imputation + random residual = Stochastic Imputation</code></p>
<p>Most multiple imputation is based off of some form of stochastic regression imputation.</p>
</div>
<div id="interpolation-and-extrapolation" class="section level4">
<h4><span class="header-section-number">6.1.1.6</span> Interpolation and Extrapolation</h4>
<p>An estimated value from other observations from the same individual. It usually only works in longitudinal data.</p>
</div>
<div id="e-m-expectation-maximization-algorithm" class="section level4">
<h4><span class="header-section-number">6.1.1.7</span> E-M (Expectation-Maximization) Algorithm</h4>
<p>An iterative process:</p>
<ol style="list-style-type: decimal">
<li>Other variables are used to impute a value (Expectation).<br />
</li>
<li>Check whether the value is most likely (Maximization).<br />
</li>
<li>If not, it re-imputes a more likely value.</li>
</ol>
<p>E-M also preserves the relationship with other variables. However, it still underestimates standard error.</p>
</div>
<div id="k-nearest-neighbour-knn-imputation" class="section level4">
<h4><span class="header-section-number">6.1.1.8</span> K-nearest neighbour (KNN) imputation</h4>
<p>The above methods are model-based imputation (regression).<br />
This is an example of neighbor-based imputation (K-nearest neighbor).</p>
<p>For a discrete variable, it uses the most frequent value among the k nearest neighbors.</p>
<ul>
<li>Distance metrics: Hamming distance.</li>
</ul>
<p>For a continuous variable, it uses the mean or mode.</p>
<ul>
<li>Distance metrics:
<ul>
<li>Euclidean<br />
</li>
<li>Mahalanobis<br />
</li>
<li>Manhattan</li>
</ul></li>
</ul>
</div>
<div id="bayesian-ridge-regression-implementation" class="section level4">
<h4><span class="header-section-number">6.1.1.9</span> Bayesian Ridge regression implementation</h4>
</div>
</div>
<div id="another-perspective" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Another Perspective</h3>
<p>In the imputation world, you can go for either single or multiple imputation.<br />
Most of the imputation methods listed above are single imputation. A downfall of single imputation is its underestimation towards standard error.</p>
<p><br>
Model bias can arisen from various factors including:</p>
<ul>
<li>Imputation method<br />
</li>
<li>Missing data mechanism (MCAR vs. MAR)<br />
</li>
<li>Proportion of the missing data<br />
</li>
<li>Information available in the data set</li>
</ul>
<p>Since the imputed observations are themselves estimates, their values have corresponding random error. But when you put in that estimate as a data point, your software doesn’t know that. So it overlooks the extra source of error, resulting in too-small standard errors and too-small p-values.
So multiple imputation comes up with multiple estimates. Two of the methods listed above work as the imputation method in multiple imputation–hot deck and stochastic regression.
Because these two methods have a random component, the multiple estimates are slightly different. This re-introduces some variation that your software can incorporate in order to give your model accurate estimates of standard error.
Multiple imputation was a huge breakthrough in statistics about 20 years ago. It solves a lot of problems with missing data (though, unfortunately not all) and if done well, leads to unbiased parameter estimates and accurate standard errors.
If your rate of missing data is very, very small, it honestly doesn’t matter what technique you use. I’m talking very, very, very small (2-3%).</p>
<p>Missing Data Mechanisms
(1) Missing Completely at Random (MCAR)
- the propensity for a data point to be missing is completely random.
- There’s no relationship between whether a data point is missing and any values in the data set, missing or observed.
- The missing data are just a random subset of the data.
(2) Missing at Random. (MAR)
- the propensity for a data point to be missing is not related to the missing data, but it is related to some of the observed data
- MAR requires that the cause of the missing data is unrelated to the missing values but may be related to the observed values of other variables.
- MAR means that the missing values are related to observed values on other variables. As an example of CD missing data, missing income data may be unrelated to the actual income values but are related to education. Perhaps people with more education are less likely to reveal their income than those with less education
(3) Non-Ignorable (NI)
- the missing data mechanism is related to the missing values
- It commonly occurs when people do not want to reveal something very personal or unpopular about themselves
- Complete case analysis can give highly biased results for NI missing data. If proportionally more low and moderate income individuals are left in the sample because high income people are missing, an estimate of the mean income will be lower than the actual population mean.</p>
<p>Remember that there are three goals of multiple imputation, or any missing data technique: Unbiased parameter estimates in the final analysis (regression coefficients, group means, odds ratios, etc.); accurate standard errors of those parameter estimates, and therefore, accurate p-values in the analysis; and adequate power to find meaningful parameter values significant.
1. Don’t round off imputations for dummy variables. Many common imputation techniques, like MCMC, require normally distributed variables. Suggestions for imputing categorical variables were to dummy code them, impute them, then round off imputed values to 0 or 1. Recent research, however, has found that rounding off imputed values actually leads to biased parameter estimates in the analysis model. You actually get better results by leaving the imputed values at impossible values, even though it’s counter-intuitive.</p>
<ol start="2" style="list-style-type: decimal">
<li><p>Don’t transform skewed variables. Likewise, when you transform a variable to meet normality assumptions before imputing, you not only are changing the distribution of that variable but the relationship between that variable and the others you use to impute. Doing so can lead to imputing outliers, creating more bias than just imputing the skewed variable.</p></li>
<li><p>Use more imputations. The advice for years has been that 5-10 imputations are adequate. And while this is true for unbiasedness, you can get inconsistent results if you run the multiple imputation more than once. Bodner (2008) recommends having as many imputations as the percentage of missing data. Since running more imputations isn’t any more work for the data analyst, there’s no reason not to.
Bodner, T. E. 2008. “What Improves with Increased Missing Data Imputations?”
Structural Equation Modeling 15(4):651–75.</p></li>
<li><p>Create multiplicative terms before imputing. When the analysis model contains a multiplicative term, like an interaction term or a quadratic, create the multiplicative terms first, then impute. Imputing first, and then creating the multiplicative terms actually biases the regression parameters of the multiplicative term (von Hippel, 2009).
von Hippel, P.T. (2009). “How To Impute Squares, Interactions, and Other Transformed Variables.” Sociological Methodology 39.</p></li>
<li><p>Alternatives to multiple imputation aren’t usually better. Multiple imputation assumes the data are missing at random. In most tests, if an assumption is not met, there are better alternatives—a nonparametric test or an alternative type of model. This is often not true with missing data. Alternatives like listwise deletion (a.k.a. ignoring it) have more stringent assumptions. So do nonignorable missing data techniques like Heckman’s selection models.</p></li>
</ol>
<p>Do not use these ad-hoc:
Pairwise Deletion: use the available data for each part of an analysis. This has been shown to result in correlations beyond the 0,1 range and other fun statistical impossibilities.
Mean Imputation: substitute the mean of the observed values for all missing data. There are so many problems, it’s difficult to list them all, but suffice it to say, this technique never meets the above 3 criteria.
Dummy Variable: create a dummy variable that indicates whether a data point is missing, then substitute any arbitrary value for the missing data in the original variable. Use both variables in the analysis. While it does help the loss of power, it usually leads to biased results.</p>
<p>Multiple Imputation of categorical variables is bad
Paul Allison, one of my favorite authors of statistical information for researchers, did a study that showed that the most common method actually gives worse results that listwise deletion. (Did I mention I’ve used it myself?)
What is the bad method?
1. Dummy code the variable
2. Impute a continuous value. This will generally be between 0 and 1.
3. Round off to either 0 or 1, based on whether the imputed value is below or above .5.
As Allison discovered, this method generally leads to biased results, and incorrect standard errors
What to do instead?
Allison compared this approach to four others, each of which generally gave more accurate results, at least under some conditions.
1. Listwise deletion
2. Imputation of the continuous variable without rounding (just leave off step 3).
3. Logistic Regression imputation
4. Discriminant Analysis imputation
These last two generally performed best, but only work in limited situations.</p>
</div>
</div>
<div id="deep-learning" class="section level2">
<h2><span class="header-section-number">6.2</span> Deep Learning</h2>
<div id="overview" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Overview</h3>
<p>This section is based on <span class="citation">(Allaire <a href="#ref-TensorFlow" role="doc-biblioref">2018</a>)</span>
<img src="https://www.youtube.com/watch?v=atiYXm7JZv0&amp;ab_channel=RStudio" /></p>
<ul>
<li><p>What is deep learning?
Input to output via layers of representation</p></li>
<li><p>What are layers?
A layer is a geometric transformation function on the data that goes through it Weights determine the data transformation behavior of a layer</p></li>
</ul>
<p><br>
Learning Representation</p>
<ul>
<li>Transforming input data into useful representation</li>
</ul>
<p><br></p>
<p>The “deep” in deep learning</p>
<ul>
<li>multiple layers</li>
<li>Other possibly more appropriate names for the field:
<ul>
<li>Layered representations learning</li>
<li>Hierarchical representations learning</li>
<li>Chained geometric transformation learning</li>
</ul></li>
</ul>
<p><br>
New problem domains for R:</p>
<ul>
<li>Computer vision<br />
</li>
<li>Computer speech recognition<br />
</li>
<li>Reinforcement learning applications</li>
</ul>
<p><br>
How do we train deep learning models?</p>
<ul>
<li>Basic of machine learning algorithms<br />
</li>
<li>Machine learning vs. statistical modeling<br />
</li>
<li>MNIST example
<ul>
<li>Model definition in R<br />
</li>
<li>Layers of representation<br />
</li>
</ul></li>
<li>The training loop</li>
</ul>
<p><br>
Machine learning algorithms<br />
Learning model parameters via exposure to many example data points</p>
<p><br>
Statistics: Often focused on inferring the process by which data is generated
Machine Learning: Principally focused on predicting future data</p>
<p><br>
Deep learning frontiers</p>
<ul>
<li>Computer vision</li>
<li>Natural language processing</li>
<li>Time series</li>
<li>Biomedical</li>
</ul>
<div style="page-break-after: always;"></div>
</div>
<div id="tensor-flow-and-r" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Tensor Flow and R</h3>
<p>TensorFlow APIs</p>
<ul>
<li>Keras API<br />
</li>
<li>Estimator API<br />
</li>
<li>Core API</li>
</ul>
<p><br></p>
<div id="r-packages" class="section level4">
<h4><span class="header-section-number">6.2.2.1</span> R packages</h4>
<p>TensorFlow APIs
* keras: Interface for neural networks, with a focus on enabling fast experimentation.
* tfestimators: Implementations of common model types such as regressors and classifiers.<br />
* tensorflow: Low <em>level interface to the TensorFlow computational graph.
</em> tfdatasets: Scalable input pipelines for TensorFlow models.</p>
<p>Supporting Tools
* tfruns : Track , visualize, and manage TensorFlow training runs and experiments
* tfdeploy : Tools designed to make exporting and serving TensorFlow models straightforward.
* cloudml : R interface to Google Cloud Machine Learning Engine.</p>
<p>R interface to Keras</p>
<ul>
<li>Step by step example<br />
</li>
<li><a href="advanced-methods.html#keras-layers">Keras layers</a><br />
</li>
<li><a href="advanced-methods.html#compiling-models">Compiling models</a><br />
</li>
<li><a href="advanced-methods.html#losses-optimizers-and-metrics">Losses, Optimizers, and Metrics</a></li>
<li>More examples</li>
</ul>
<p><br></p>
</div>
<div id="keras-layers" class="section level4">
<h4><span class="header-section-number">6.2.2.2</span> Keras layers</h4>
<p>65 layers available</p>
<ul>
<li>Dense layers: classic “fully connected” neural network layers<br />
</li>
<li>Convolutional layers: "Filters for learning local patterns in data<br />
</li>
<li>Recurrent layers: Layers that maintain state based on on previously seen data</li>
<li>Embedding layers: Vectorization of text that reflects semantic relationships between words</li>
</ul>
</div>
</div>
<div id="compiling-models" class="section level3">
<h3><span class="header-section-number">6.2.3</span> Compiling models</h3>
<p>Model compilation prepares the model for training by:<br />
1. Converting the layers into a TensorFlow graph<br />
2. Applying the specified loss function and optimizer<br />
3. Arranging for the collection of metrics during training</p>
</div>
<div id="losses-optimizers-and-metrics" class="section level3">
<h3><span class="header-section-number">6.2.4</span> Losses, Optimizers, and Metrics</h3>
<p>All available at Keras for R <a href="https://github.com/rstudio/cheatsheets/raw/master/keras.pdf">cheatsheet</a></p>
<p>Example of <a href="https://blogs.rstudio.com/ai/posts/2017%20*12%20*14%20*image%20*classification%20*on%20*small%20*datasets/">Image classificaiton</a></p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-TensorFlow">
<p>Allaire, J. J. 2018. <em>Machine Learning with R and Tensorflow</em>. <a href="https://www.youtube.com/watch?v=atiYXm7JZv0&amp;ab_channel=RStudio">https://www.youtube.com/watch?v=atiYXm7JZv0&amp;ab_channel=RStudio</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="experimental-design.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendix.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Data Analysis.pdf", "Data Analysis.epub"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "static"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
