<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>21.4 Discriminant Analysis | A Guide on Data Analysis</title>
  <meta name="description" content="This is a guide on how to conduct data analysis in the field of data science, statistics, or machine learning." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="21.4 Discriminant Analysis | A Guide on Data Analysis" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://bookdown.org/mike/data_analysis/" />
  <meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg" />
  <meta property="og:description" content="This is a guide on how to conduct data analysis in the field of data science, statistics, or machine learning." />
  <meta name="github-repo" content="mikenguyen13/data_analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="21.4 Discriminant Analysis | A Guide on Data Analysis" />
  
  <meta name="twitter:description" content="This is a guide on how to conduct data analysis in the field of data science, statistics, or machine learning." />
  <meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg" />

<meta name="author" content="Mike Nguyen" />


<meta name="date" content="2022-09-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="logo.png" />
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="factor-analysis.html"/>
<link rel="next" href="quasi-experimental.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/proj4js-2.3.15/proj4.js"></script>
<link href="libs/highcharts-9.3.1/css/motion.css" rel="stylesheet" />
<script src="libs/highcharts-9.3.1/highcharts.js"></script>
<script src="libs/highcharts-9.3.1/highcharts-3d.js"></script>
<script src="libs/highcharts-9.3.1/highcharts-more.js"></script>
<script src="libs/highcharts-9.3.1/modules/stock.js"></script>
<script src="libs/highcharts-9.3.1/modules/map.js"></script>
<script src="libs/highcharts-9.3.1/modules/data.js"></script>
<script src="libs/highcharts-9.3.1/modules/exporting.js"></script>
<script src="libs/highcharts-9.3.1/modules/offline-exporting.js"></script>
<script src="libs/highcharts-9.3.1/modules/drilldown.js"></script>
<script src="libs/highcharts-9.3.1/modules/item-series.js"></script>
<script src="libs/highcharts-9.3.1/modules/overlapping-datalabels.js"></script>
<script src="libs/highcharts-9.3.1/modules/annotations.js"></script>
<script src="libs/highcharts-9.3.1/modules/export-data.js"></script>
<script src="libs/highcharts-9.3.1/modules/funnel.js"></script>
<script src="libs/highcharts-9.3.1/modules/heatmap.js"></script>
<script src="libs/highcharts-9.3.1/modules/treemap.js"></script>
<script src="libs/highcharts-9.3.1/modules/sankey.js"></script>
<script src="libs/highcharts-9.3.1/modules/dependency-wheel.js"></script>
<script src="libs/highcharts-9.3.1/modules/organization.js"></script>
<script src="libs/highcharts-9.3.1/modules/solid-gauge.js"></script>
<script src="libs/highcharts-9.3.1/modules/streamgraph.js"></script>
<script src="libs/highcharts-9.3.1/modules/sunburst.js"></script>
<script src="libs/highcharts-9.3.1/modules/vector.js"></script>
<script src="libs/highcharts-9.3.1/modules/wordcloud.js"></script>
<script src="libs/highcharts-9.3.1/modules/xrange.js"></script>
<script src="libs/highcharts-9.3.1/modules/tilemap.js"></script>
<script src="libs/highcharts-9.3.1/modules/venn.js"></script>
<script src="libs/highcharts-9.3.1/modules/gantt.js"></script>
<script src="libs/highcharts-9.3.1/modules/timeline.js"></script>
<script src="libs/highcharts-9.3.1/modules/parallel-coordinates.js"></script>
<script src="libs/highcharts-9.3.1/modules/bullet.js"></script>
<script src="libs/highcharts-9.3.1/modules/coloraxis.js"></script>
<script src="libs/highcharts-9.3.1/modules/dumbbell.js"></script>
<script src="libs/highcharts-9.3.1/modules/lollipop.js"></script>
<script src="libs/highcharts-9.3.1/modules/series-label.js"></script>
<script src="libs/highcharts-9.3.1/plugins/motion.js"></script>
<script src="libs/highcharts-9.3.1/custom/reset.js"></script>
<script src="libs/highcharts-9.3.1/modules/boost.js"></script>
<script src="libs/highchart-binding-0.9.4/highchart.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){window.dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-DMNX2X65HQ');
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Guide on Data Analysis</a></li>

<li class="divider"></li>
<li><a href="index.html#preface" id="toc-preface">Preface</a></li>
<li><a href="introduction.html#introduction" id="toc-introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="prerequisites.html#prerequisites" id="toc-prerequisites"><span class="toc-section-number">2</span> Prerequisites</a>
<ul>
<li><a href="matrix-theory.html#matrix-theory" id="toc-matrix-theory"><span class="toc-section-number">2.1</span> Matrix Theory</a>
<ul>
<li><a href="matrix-theory.html#rank" id="toc-rank"><span class="toc-section-number">2.1.1</span> Rank</a></li>
<li><a href="matrix-theory.html#inverse" id="toc-inverse"><span class="toc-section-number">2.1.2</span> Inverse</a></li>
<li><a href="matrix-theory.html#definiteness" id="toc-definiteness"><span class="toc-section-number">2.1.3</span> Definiteness</a></li>
<li><a href="matrix-theory.html#matrix-calculus" id="toc-matrix-calculus"><span class="toc-section-number">2.1.4</span> Matrix Calculus</a></li>
<li><a href="matrix-theory.html#optimization" id="toc-optimization"><span class="toc-section-number">2.1.5</span> Optimization</a></li>
</ul></li>
<li><a href="probability-theory.html#probability-theory" id="toc-probability-theory"><span class="toc-section-number">2.2</span> Probability Theory</a>
<ul>
<li><a href="probability-theory.html#axiom-and-theorems-of-probability" id="toc-axiom-and-theorems-of-probability"><span class="toc-section-number">2.2.1</span> Axiom and Theorems of Probability</a></li>
<li><a href="probability-theory.html#central-limit-theorem" id="toc-central-limit-theorem"><span class="toc-section-number">2.2.2</span> Central Limit Theorem</a></li>
<li><a href="probability-theory.html#random-variable" id="toc-random-variable"><span class="toc-section-number">2.2.3</span> Random variable</a></li>
<li><a href="probability-theory.html#moment-generating-function" id="toc-moment-generating-function"><span class="toc-section-number">2.2.4</span> Moment generating function</a></li>
<li><a href="probability-theory.html#moment" id="toc-moment"><span class="toc-section-number">2.2.5</span> Moment</a></li>
<li><a href="probability-theory.html#distributions" id="toc-distributions"><span class="toc-section-number">2.2.6</span> Distributions</a></li>
</ul></li>
<li><a href="general-math.html#general-math" id="toc-general-math"><span class="toc-section-number">2.3</span> General Math</a>
<ul>
<li><a href="general-math.html#law-of-large-numbers" id="toc-law-of-large-numbers"><span class="toc-section-number">2.3.1</span> Law of large numbers</a></li>
<li><a href="general-math.html#law-of-iterated-expectation" id="toc-law-of-iterated-expectation"><span class="toc-section-number">2.3.2</span> Law of Iterated Expectation</a></li>
<li><a href="general-math.html#convergence" id="toc-convergence"><span class="toc-section-number">2.3.3</span> Convergence</a></li>
<li><a href="general-math.html#sufficient-statistics" id="toc-sufficient-statistics"><span class="toc-section-number">2.3.4</span> Sufficient Statistics</a></li>
<li><a href="general-math.html#parameter-transformations" id="toc-parameter-transformations"><span class="toc-section-number">2.3.5</span> Parameter transformations</a></li>
</ul></li>
<li><a href="data-importexport.html#data-importexport" id="toc-data-importexport"><span class="toc-section-number">2.4</span> Data Import/Export</a>
<ul>
<li><a href="data-importexport.html#medium-size" id="toc-medium-size"><span class="toc-section-number">2.4.1</span> Medium size</a></li>
<li><a href="data-importexport.html#large-size" id="toc-large-size"><span class="toc-section-number">2.4.2</span> Large size</a></li>
</ul></li>
<li><a href="data-manipulation.html#data-manipulation" id="toc-data-manipulation"><span class="toc-section-number">2.5</span> Data Manipulation</a></li>
</ul></li>
<li><a href="#part-i.-basic" id="toc-part-i.-basic">(PART*) I. BASIC</a></li>
<li><a href="descriptive-stat.html#descriptive-stat" id="toc-descriptive-stat"><span class="toc-section-number">3</span> Descriptive Statistics</a>
<ul>
<li><a href="numerical-measures.html#numerical-measures" id="toc-numerical-measures"><span class="toc-section-number">3.1</span> Numerical Measures</a></li>
<li><a href="graphical-measures.html#graphical-measures" id="toc-graphical-measures"><span class="toc-section-number">3.2</span> Graphical Measures</a>
<ul>
<li><a href="graphical-measures.html#shape" id="toc-shape"><span class="toc-section-number">3.2.1</span> Shape</a></li>
<li><a href="graphical-measures.html#scatterplot" id="toc-scatterplot"><span class="toc-section-number">3.2.2</span> Scatterplot</a></li>
</ul></li>
<li><a href="normality-assessment.html#normality-assessment" id="toc-normality-assessment"><span class="toc-section-number">3.3</span> Normality Assessment</a>
<ul>
<li><a href="normality-assessment.html#graphical-assessment" id="toc-graphical-assessment"><span class="toc-section-number">3.3.1</span> Graphical Assessment</a></li>
<li><a href="normality-assessment.html#summary-statistics" id="toc-summary-statistics"><span class="toc-section-number">3.3.2</span> Summary Statistics</a></li>
</ul></li>
<li><a href="bivariate-statistics.html#bivariate-statistics" id="toc-bivariate-statistics"><span class="toc-section-number">3.4</span> Bivariate Statistics</a></li>
<li><a href="two-continuous.html#two-continuous" id="toc-two-continuous"><span class="toc-section-number">3.5</span> Two Continuous</a>
<ul>
<li><a href="two-continuous.html#pearson-correlation" id="toc-pearson-correlation"><span class="toc-section-number">3.5.1</span> Pearson Correlation</a></li>
<li><a href="two-continuous.html#spearman-correlation" id="toc-spearman-correlation"><span class="toc-section-number">3.5.2</span> Spearman Correlation</a></li>
</ul></li>
<li><a href="categorical-and-continuous.html#categorical-and-continuous" id="toc-categorical-and-continuous"><span class="toc-section-number">3.6</span> Categorical and Continuous</a>
<ul>
<li><a href="categorical-and-continuous.html#point-biserial-correlation" id="toc-point-biserial-correlation"><span class="toc-section-number">3.6.1</span> Point-Biserial Correlation</a></li>
<li><a href="categorical-and-continuous.html#logistic-regression" id="toc-logistic-regression"><span class="toc-section-number">3.6.2</span> Logistic Regression</a></li>
</ul></li>
<li><a href="two-discrete.html#two-discrete" id="toc-two-discrete"><span class="toc-section-number">3.7</span> Two Discrete</a>
<ul>
<li><a href="two-discrete.html#distance-metrics" id="toc-distance-metrics"><span class="toc-section-number">3.7.1</span> Distance Metrics</a></li>
<li><a href="two-discrete.html#statistical-metrics" id="toc-statistical-metrics"><span class="toc-section-number">3.7.2</span> Statistical Metrics</a></li>
<li><a href="two-discrete.html#ordinal-association-rank-correlation" id="toc-ordinal-association-rank-correlation"><span class="toc-section-number">3.7.3</span> Ordinal Association (Rank correlation)</a></li>
<li><a href="two-discrete.html#summary-1" id="toc-summary-1"><span class="toc-section-number">3.7.4</span> Summary</a></li>
<li><a href="two-discrete.html#visualization" id="toc-visualization"><span class="toc-section-number">3.7.5</span> Visualization</a></li>
</ul></li>
</ul></li>
<li><a href="basic-statistical-inference.html#basic-statistical-inference" id="toc-basic-statistical-inference"><span class="toc-section-number">4</span> Basic Statistical Inference</a>
<ul>
<li><a href="one-sample-inference.html#one-sample-inference" id="toc-one-sample-inference"><span class="toc-section-number">4.1</span> One Sample Inference</a>
<ul>
<li><a href="one-sample-inference.html#the-mean" id="toc-the-mean"><span class="toc-section-number">4.1.1</span> The Mean</a></li>
<li><a href="one-sample-inference.html#single-variance" id="toc-single-variance"><span class="toc-section-number">4.1.2</span> Single Variance</a></li>
<li><a href="one-sample-inference.html#single-proportion-p" id="toc-single-proportion-p"><span class="toc-section-number">4.1.3</span> Single Proportion (p)</a></li>
<li><a href="one-sample-inference.html#power" id="toc-power"><span class="toc-section-number">4.1.4</span> Power</a></li>
<li><a href="one-sample-inference.html#sample-size" id="toc-sample-size"><span class="toc-section-number">4.1.5</span> Sample Size</a></li>
<li><a href="one-sample-inference.html#note" id="toc-note"><span class="toc-section-number">4.1.6</span> Note</a></li>
<li><a href="one-sample-inference.html#one-sample-non-parametric-methods" id="toc-one-sample-non-parametric-methods"><span class="toc-section-number">4.1.7</span> One-sample Non-parametric Methods</a></li>
</ul></li>
<li><a href="two-sample-inference.html#two-sample-inference" id="toc-two-sample-inference"><span class="toc-section-number">4.2</span> Two Sample Inference</a>
<ul>
<li><a href="two-sample-inference.html#means" id="toc-means"><span class="toc-section-number">4.2.1</span> Means</a></li>
<li><a href="two-sample-inference.html#variances" id="toc-variances"><span class="toc-section-number">4.2.2</span> Variances</a></li>
<li><a href="two-sample-inference.html#power-1" id="toc-power-1"><span class="toc-section-number">4.2.3</span> Power</a></li>
<li><a href="two-sample-inference.html#sample-size-1" id="toc-sample-size-1"><span class="toc-section-number">4.2.4</span> Sample Size</a></li>
<li><a href="two-sample-inference.html#matched-pair-designs" id="toc-matched-pair-designs"><span class="toc-section-number">4.2.5</span> Matched Pair Designs</a></li>
<li><a href="two-sample-inference.html#nonparametric-tests-for-two-samples" id="toc-nonparametric-tests-for-two-samples"><span class="toc-section-number">4.2.6</span> Nonparametric Tests for Two Samples</a></li>
</ul></li>
<li><a href="categorical-data-analysis.html#categorical-data-analysis" id="toc-categorical-data-analysis"><span class="toc-section-number">4.3</span> Categorical Data Analysis</a>
<ul>
<li><a href="categorical-data-analysis.html#inferences-for-small-samples" id="toc-inferences-for-small-samples"><span class="toc-section-number">4.3.1</span> Inferences for Small Samples</a></li>
<li><a href="categorical-data-analysis.html#test-of-association" id="toc-test-of-association"><span class="toc-section-number">4.3.2</span> Test of Association</a></li>
<li><a href="categorical-data-analysis.html#ordinal-association" id="toc-ordinal-association"><span class="toc-section-number">4.3.3</span> Ordinal Association</a></li>
</ul></li>
<li><a href="divergence-metrics-and-test-for-comparing-distributions.html#divergence-metrics-and-test-for-comparing-distributions" id="toc-divergence-metrics-and-test-for-comparing-distributions"><span class="toc-section-number">4.4</span> Divergence Metrics and Test for Comparing Distributions</a>
<ul>
<li><a href="divergence-metrics-and-test-for-comparing-distributions.html#kullback-leibler-divergence" id="toc-kullback-leibler-divergence"><span class="toc-section-number">4.4.1</span> Kullback-Leibler Divergence</a></li>
<li><a href="divergence-metrics-and-test-for-comparing-distributions.html#jensen-shannon-divergence" id="toc-jensen-shannon-divergence"><span class="toc-section-number">4.4.2</span> Jensen-Shannon Divergence</a></li>
<li><a href="divergence-metrics-and-test-for-comparing-distributions.html#wasserstein-distance" id="toc-wasserstein-distance"><span class="toc-section-number">4.4.3</span> Wasserstein Distance</a></li>
<li><a href="divergence-metrics-and-test-for-comparing-distributions.html#kolmogorov-smirnov-test-1" id="toc-kolmogorov-smirnov-test-1"><span class="toc-section-number">4.4.4</span> Kolmogorov-Smirnov Test</a></li>
</ul></li>
</ul></li>
<li><a href="#part-ii.-regression" id="toc-part-ii.-regression">(PART*) II. REGRESSION</a></li>
<li><a href="linear-regression.html#linear-regression" id="toc-linear-regression"><span class="toc-section-number">5</span> Linear Regression</a>
<ul>
<li><a href="ordinary-least-squares.html#ordinary-least-squares" id="toc-ordinary-least-squares"><span class="toc-section-number">5.1</span> Ordinary Least Squares</a>
<ul>
<li><a href="ordinary-least-squares.html#simple-regression-basic-model" id="toc-simple-regression-basic-model"><span class="toc-section-number">5.1.1</span> Simple Regression (Basic Model)</a></li>
<li><a href="ordinary-least-squares.html#multiple-linear-regression" id="toc-multiple-linear-regression"><span class="toc-section-number">5.1.2</span> Multiple Linear Regression</a></li>
<li><a href="ordinary-least-squares.html#ols-assumptions" id="toc-ols-assumptions"><span class="toc-section-number">5.1.3</span> OLS Assumptions</a></li>
<li><a href="ordinary-least-squares.html#theorems" id="toc-theorems"><span class="toc-section-number">5.1.4</span> Theorems</a></li>
<li><a href="ordinary-least-squares.html#variable-selection" id="toc-variable-selection"><span class="toc-section-number">5.1.5</span> Variable Selection</a></li>
<li><a href="ordinary-least-squares.html#diagnostics-1" id="toc-diagnostics-1"><span class="toc-section-number">5.1.6</span> Diagnostics</a></li>
<li><a href="ordinary-least-squares.html#model-validation" id="toc-model-validation"><span class="toc-section-number">5.1.7</span> Model Validation</a></li>
<li><a href="ordinary-least-squares.html#finite-sample-properties" id="toc-finite-sample-properties"><span class="toc-section-number">5.1.8</span> Finite Sample Properties</a></li>
<li><a href="ordinary-least-squares.html#large-sample-properties" id="toc-large-sample-properties"><span class="toc-section-number">5.1.9</span> Large Sample Properties</a></li>
</ul></li>
<li><a href="feasible-generalized-least-squares.html#feasible-generalized-least-squares" id="toc-feasible-generalized-least-squares"><span class="toc-section-number">5.2</span> Feasible Generalized Least Squares</a>
<ul>
<li><a href="feasible-generalized-least-squares.html#heteroskedasticity" id="toc-heteroskedasticity"><span class="toc-section-number">5.2.1</span> Heteroskedasticity</a></li>
<li><a href="feasible-generalized-least-squares.html#serial-correlation" id="toc-serial-correlation"><span class="toc-section-number">5.2.2</span> Serial Correlation</a></li>
</ul></li>
<li><a href="weighted-least-squares.html#weighted-least-squares" id="toc-weighted-least-squares"><span class="toc-section-number">5.3</span> Weighted Least Squares</a></li>
<li><a href="generalized-least-squares.html#generalized-least-squares" id="toc-generalized-least-squares"><span class="toc-section-number">5.4</span> Generalized Least Squares</a></li>
<li><a href="feasiable-prais-winsten.html#feasiable-prais-winsten" id="toc-feasiable-prais-winsten"><span class="toc-section-number">5.5</span> Feasiable Prais Winsten</a></li>
<li><a href="feasible-group-level-random-effects.html#feasible-group-level-random-effects" id="toc-feasible-group-level-random-effects"><span class="toc-section-number">5.6</span> Feasible group level Random Effects</a></li>
<li><a href="ridge-regression.html#ridge-regression" id="toc-ridge-regression"><span class="toc-section-number">5.7</span> Ridge Regression</a></li>
<li><a href="principal-component-regression.html#principal-component-regression" id="toc-principal-component-regression"><span class="toc-section-number">5.8</span> Principal Component Regression</a></li>
<li><a href="robust-regression.html#robust-regression" id="toc-robust-regression"><span class="toc-section-number">5.9</span> Robust Regression</a>
<ul>
<li><a href="robust-regression.html#least-absolute-residuals-lar-regression" id="toc-least-absolute-residuals-lar-regression"><span class="toc-section-number">5.9.1</span> Least Absolute Residuals (LAR) Regression</a></li>
<li><a href="robust-regression.html#least-median-of-squares-lms-regression" id="toc-least-median-of-squares-lms-regression"><span class="toc-section-number">5.9.2</span> Least Median of Squares (LMS) Regression</a></li>
<li><a href="robust-regression.html#iteratively-reweighted-least-squares-irls-robust-regression" id="toc-iteratively-reweighted-least-squares-irls-robust-regression"><span class="toc-section-number">5.9.3</span> Iteratively Reweighted Least Squares (IRLS) Robust Regression</a></li>
</ul></li>
<li><a href="maximum-likelihood-regression.html#maximum-likelihood-regression" id="toc-maximum-likelihood-regression"><span class="toc-section-number">5.10</span> Maximum Likelihood</a>
<ul>
<li><a href="maximum-likelihood-regression.html#motivation-for-mle" id="toc-motivation-for-mle"><span class="toc-section-number">5.10.1</span> Motivation for MLE</a></li>
<li><a href="maximum-likelihood-regression.html#assumption" id="toc-assumption"><span class="toc-section-number">5.10.2</span> Assumption</a></li>
<li><a href="maximum-likelihood-regression.html#properties" id="toc-properties"><span class="toc-section-number">5.10.3</span> Properties</a></li>
<li><a href="maximum-likelihood-regression.html#compare-to-ols" id="toc-compare-to-ols"><span class="toc-section-number">5.10.4</span> Compare to OLS</a></li>
<li><a href="maximum-likelihood-regression.html#application" id="toc-application"><span class="toc-section-number">5.10.5</span> Application</a></li>
</ul></li>
</ul></li>
<li><a href="non-linear-regression.html#non-linear-regression" id="toc-non-linear-regression"><span class="toc-section-number">6</span> Non-linear Regression</a>
<ul>
<li><a href="inference-1.html#inference-1" id="toc-inference-1"><span class="toc-section-number">6.1</span> Inference</a>
<ul>
<li><a href="inference-1.html#linear-function-of-the-parameters" id="toc-linear-function-of-the-parameters"><span class="toc-section-number">6.1.1</span> Linear Function of the Parameters</a></li>
<li><a href="inference-1.html#nonlinear" id="toc-nonlinear"><span class="toc-section-number">6.1.2</span> Nonlinear</a></li>
</ul></li>
<li><a href="non-linear-least-squares.html#non-linear-least-squares" id="toc-non-linear-least-squares"><span class="toc-section-number">6.2</span> Non-linear Least Squares</a>
<ul>
<li><a href="non-linear-least-squares.html#alternative-of-gauss-newton-algorithm" id="toc-alternative-of-gauss-newton-algorithm"><span class="toc-section-number">6.2.1</span> Alternative of Gauss-Newton Algorithm</a></li>
<li><a href="non-linear-least-squares.html#practical-considerations" id="toc-practical-considerations"><span class="toc-section-number">6.2.2</span> Practical Considerations</a></li>
<li><a href="non-linear-least-squares.html#modelestiamtion-adequcy" id="toc-modelestiamtion-adequcy"><span class="toc-section-number">6.2.3</span> Model/Estiamtion Adequcy</a></li>
<li><a href="non-linear-least-squares.html#application-1" id="toc-application-1"><span class="toc-section-number">6.2.4</span> Application</a></li>
</ul></li>
</ul></li>
<li><a href="generalized-linear-models.html#generalized-linear-models" id="toc-generalized-linear-models"><span class="toc-section-number">7</span> Generalized Linear Models</a>
<ul>
<li><a href="logistic-regression-1.html#logistic-regression-1" id="toc-logistic-regression-1"><span class="toc-section-number">7.1</span> Logistic Regression</a>
<ul>
<li><a href="logistic-regression-1.html#application-2" id="toc-application-2"><span class="toc-section-number">7.1.1</span> Application</a></li>
</ul></li>
<li><a href="probit-regression.html#probit-regression" id="toc-probit-regression"><span class="toc-section-number">7.2</span> Probit Regression</a></li>
<li><a href="binomial-regression.html#binomial-regression" id="toc-binomial-regression"><span class="toc-section-number">7.3</span> Binomial Regression</a></li>
<li><a href="poisson-regression.html#poisson-regression" id="toc-poisson-regression"><span class="toc-section-number">7.4</span> Poisson Regression</a>
<ul>
<li><a href="poisson-regression.html#application-3" id="toc-application-3"><span class="toc-section-number">7.4.1</span> Application</a></li>
</ul></li>
<li><a href="negative-binomial-regression.html#negative-binomial-regression" id="toc-negative-binomial-regression"><span class="toc-section-number">7.5</span> Negative Binomial Regression</a></li>
<li><a href="multinomial.html#multinomial" id="toc-multinomial"><span class="toc-section-number">7.6</span> Multinomial</a></li>
<li><a href="generalization.html#generalization" id="toc-generalization"><span class="toc-section-number">7.7</span> Generalization</a>
<ul>
<li><a href="generalization.html#estimation-1" id="toc-estimation-1"><span class="toc-section-number">7.7.1</span> Estimation</a></li>
<li><a href="generalization.html#inference-2" id="toc-inference-2"><span class="toc-section-number">7.7.2</span> Inference</a></li>
<li><a href="generalization.html#deviance" id="toc-deviance"><span class="toc-section-number">7.7.3</span> Deviance</a></li>
<li><a href="generalization.html#diagnostic-plots" id="toc-diagnostic-plots"><span class="toc-section-number">7.7.4</span> Diagnostic Plots</a></li>
<li><a href="generalization.html#goodness-of-fit" id="toc-goodness-of-fit"><span class="toc-section-number">7.7.5</span> Goodness of Fit</a></li>
<li><a href="generalization.html#over-dispersion" id="toc-over-dispersion"><span class="toc-section-number">7.7.6</span> Over-Dispersion</a></li>
</ul></li>
</ul></li>
<li><a href="linear-mixed-models.html#linear-mixed-models" id="toc-linear-mixed-models"><span class="toc-section-number">8</span> Linear Mixed Models</a>
<ul>
<li><a href="dependent-data.html#dependent-data" id="toc-dependent-data"><span class="toc-section-number">8.1</span> Dependent Data</a>
<ul>
<li><a href="dependent-data.html#random-intercepts-model" id="toc-random-intercepts-model"><span class="toc-section-number">8.1.1</span> Random-Intercepts Model</a></li>
<li><a href="dependent-data.html#covariance-models" id="toc-covariance-models"><span class="toc-section-number">8.1.2</span> Covariance Models</a></li>
</ul></li>
<li><a href="estimation-2.html#estimation-2" id="toc-estimation-2"><span class="toc-section-number">8.2</span> Estimation</a>
<ul>
<li><a href="estimation-2.html#estimating-mathbfv" id="toc-estimating-mathbfv"><span class="toc-section-number">8.2.1</span> Estimating <span class="math inline">\(\mathbf{V}\)</span></a></li>
</ul></li>
<li><a href="inference-3.html#inference-3" id="toc-inference-3"><span class="toc-section-number">8.3</span> Inference</a>
<ul>
<li><a href="inference-3.html#parameters-beta" id="toc-parameters-beta"><span class="toc-section-number">8.3.1</span> Parameters <span class="math inline">\(\beta\)</span></a></li>
<li><a href="inference-3.html#variance-components" id="toc-variance-components"><span class="toc-section-number">8.3.2</span> Variance Components</a></li>
</ul></li>
<li><a href="information-criteria.html#information-criteria" id="toc-information-criteria"><span class="toc-section-number">8.4</span> Information Criteria</a>
<ul>
<li><a href="information-criteria.html#akaikes-information-criteria-aic" id="toc-akaikes-information-criteria-aic"><span class="toc-section-number">8.4.1</span> Akaike’s Information Criteria (AIC)</a></li>
<li><a href="information-criteria.html#corrected-aic-aicc" id="toc-corrected-aic-aicc"><span class="toc-section-number">8.4.2</span> Corrected AIC (AICC)</a></li>
<li><a href="information-criteria.html#bayesian-information-criteria-bic" id="toc-bayesian-information-criteria-bic"><span class="toc-section-number">8.4.3</span> Bayesian Information Criteria (BIC)</a></li>
</ul></li>
<li><a href="split-plot-designs.html#split-plot-designs" id="toc-split-plot-designs"><span class="toc-section-number">8.5</span> Split-Plot Designs</a>
<ul>
<li><a href="split-plot-designs.html#application-4" id="toc-application-4"><span class="toc-section-number">8.5.1</span> Application</a></li>
</ul></li>
<li><a href="repeated-measures-in-mixed-models.html#repeated-measures-in-mixed-models" id="toc-repeated-measures-in-mixed-models"><span class="toc-section-number">8.6</span> Repeated Measures in Mixed Models</a></li>
<li><a href="unbalanced-or-unequally-spaced-data.html#unbalanced-or-unequally-spaced-data" id="toc-unbalanced-or-unequally-spaced-data"><span class="toc-section-number">8.7</span> Unbalanced or Unequally Spaced Data</a></li>
<li><a href="application-5.html#application-5" id="toc-application-5"><span class="toc-section-number">8.8</span> Application</a>
<ul>
<li><a href="application-5.html#example-1-pulps" id="toc-example-1-pulps"><span class="toc-section-number">8.8.1</span> Example 1 (Pulps)</a></li>
<li><a href="application-5.html#example-2-rats" id="toc-example-2-rats"><span class="toc-section-number">8.8.2</span> Example 2 (Rats)</a></li>
<li><a href="application-5.html#example-3-agridat" id="toc-example-3-agridat"><span class="toc-section-number">8.8.3</span> Example 3 (Agridat)</a></li>
</ul></li>
</ul></li>
<li><a href="nonlinear-and-generalized-linear-mixed-models.html#nonlinear-and-generalized-linear-mixed-models" id="toc-nonlinear-and-generalized-linear-mixed-models"><span class="toc-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a>
<ul>
<li><a href="estimation-3.html#estimation-3" id="toc-estimation-3"><span class="toc-section-number">9.1</span> Estimation</a>
<ul>
<li><a href="estimation-3.html#estimation-by-numerical-integration" id="toc-estimation-by-numerical-integration"><span class="toc-section-number">9.1.1</span> Estimation by Numerical Integration</a></li>
<li><a href="estimation-3.html#estimation-by-linearization" id="toc-estimation-by-linearization"><span class="toc-section-number">9.1.2</span> Estimation by Linearization</a></li>
<li><a href="estimation-3.html#estimation-by-bayesian-hierarchical-models" id="toc-estimation-by-bayesian-hierarchical-models"><span class="toc-section-number">9.1.3</span> Estimation by Bayesian Hierarchical Models</a></li>
</ul></li>
<li><a href="application-6.html#application-6" id="toc-application-6"><span class="toc-section-number">9.2</span> Application</a>
<ul>
<li><a href="application-6.html#binomial-cbpp-data" id="toc-binomial-cbpp-data"><span class="toc-section-number">9.2.1</span> Binomial (CBPP Data)</a></li>
<li><a href="application-6.html#count-owl-data" id="toc-count-owl-data"><span class="toc-section-number">9.2.2</span> Count (Owl Data)</a></li>
<li><a href="application-6.html#binomial-1" id="toc-binomial-1"><span class="toc-section-number">9.2.3</span> Binomial</a></li>
<li><a href="application-6.html#example-from-schabenberger_2001-section-8.4.1" id="toc-example-from-schabenberger_2001-section-8.4.1"><span class="toc-section-number">9.2.4</span> Example from <span class="citation">(<span>Schabenberger and Pierce 2001</span>)</span> section 8.4.1</a></li>
</ul></li>
<li><a href="summary-2.html#summary-2" id="toc-summary-2"><span class="toc-section-number">9.3</span> Summary</a></li>
</ul></li>
<li><a href="#part-iii.-ramifications" id="toc-part-iii.-ramifications">(PART*) III. RAMIFICATIONS</a></li>
<li><a href="model-specification.html#model-specification" id="toc-model-specification"><span class="toc-section-number">10</span> Model Specification</a>
<ul>
<li><a href="nested-model.html#nested-model" id="toc-nested-model"><span class="toc-section-number">10.1</span> Nested Model</a>
<ul>
<li><a href="nested-model.html#chow-test" id="toc-chow-test"><span class="toc-section-number">10.1.1</span> Chow test</a></li>
</ul></li>
<li><a href="non-nested-model.html#non-nested-model" id="toc-non-nested-model"><span class="toc-section-number">10.2</span> Non-Nested Model</a>
<ul>
<li><a href="non-nested-model.html#davidson-mackinnon-test" id="toc-davidson-mackinnon-test"><span class="toc-section-number">10.2.1</span> Davidson-Mackinnon test</a></li>
</ul></li>
<li><a href="heteroskedasticity-1.html#heteroskedasticity-1" id="toc-heteroskedasticity-1"><span class="toc-section-number">10.3</span> Heteroskedasticity</a>
<ul>
<li><a href="heteroskedasticity-1.html#breusch-pagan-test" id="toc-breusch-pagan-test"><span class="toc-section-number">10.3.1</span> Breusch-Pagan test</a></li>
<li><a href="heteroskedasticity-1.html#white-test" id="toc-white-test"><span class="toc-section-number">10.3.2</span> White test</a></li>
</ul></li>
</ul></li>
<li><a href="imputation-missing-data.html#imputation-missing-data" id="toc-imputation-missing-data"><span class="toc-section-number">11</span> Imputation (Missing Data)</a>
<ul>
<li><a href="assumptions-1.html#assumptions-1" id="toc-assumptions-1"><span class="toc-section-number">11.1</span> Assumptions</a>
<ul>
<li><a href="assumptions-1.html#missing-completely-at-random-mcar" id="toc-missing-completely-at-random-mcar"><span class="toc-section-number">11.1.1</span> Missing Completely at Random (MCAR)</a></li>
<li><a href="assumptions-1.html#missing-at-random-mar" id="toc-missing-at-random-mar"><span class="toc-section-number">11.1.2</span> Missing at Random (MAR)</a></li>
<li><a href="assumptions-1.html#ignorable" id="toc-ignorable"><span class="toc-section-number">11.1.3</span> Ignorable</a></li>
<li><a href="assumptions-1.html#nonignorable" id="toc-nonignorable"><span class="toc-section-number">11.1.4</span> Nonignorable</a></li>
</ul></li>
<li><a href="solutions-to-missing-data.html#solutions-to-missing-data" id="toc-solutions-to-missing-data"><span class="toc-section-number">11.2</span> Solutions to Missing data</a>
<ul>
<li><a href="solutions-to-missing-data.html#listwise-deletion" id="toc-listwise-deletion"><span class="toc-section-number">11.2.1</span> Listwise Deletion</a></li>
<li><a href="solutions-to-missing-data.html#pairwise-deletion" id="toc-pairwise-deletion"><span class="toc-section-number">11.2.2</span> Pairwise Deletion</a></li>
<li><a href="solutions-to-missing-data.html#dummy-variable-adjustment" id="toc-dummy-variable-adjustment"><span class="toc-section-number">11.2.3</span> Dummy Variable Adjustment</a></li>
<li><a href="solutions-to-missing-data.html#imputation" id="toc-imputation"><span class="toc-section-number">11.2.4</span> Imputation</a></li>
<li><a href="solutions-to-missing-data.html#other-methods" id="toc-other-methods"><span class="toc-section-number">11.2.5</span> Other methods</a></li>
</ul></li>
<li><a href="criteria-for-choosing-an-effective-approach.html#criteria-for-choosing-an-effective-approach" id="toc-criteria-for-choosing-an-effective-approach"><span class="toc-section-number">11.3</span> Criteria for Choosing an Effective Approach</a></li>
<li><a href="another-perspective.html#another-perspective" id="toc-another-perspective"><span class="toc-section-number">11.4</span> Another Perspective</a></li>
<li><a href="diagnosing-the-mechanism.html#diagnosing-the-mechanism" id="toc-diagnosing-the-mechanism"><span class="toc-section-number">11.5</span> Diagnosing the Mechanism</a>
<ul>
<li><a href="diagnosing-the-mechanism.html#mar-vs.-mnar" id="toc-mar-vs.-mnar"><span class="toc-section-number">11.5.1</span> MAR vs. MNAR</a></li>
<li><a href="diagnosing-the-mechanism.html#mcar-vs.-mar" id="toc-mcar-vs.-mar"><span class="toc-section-number">11.5.2</span> MCAR vs. MAR</a></li>
</ul></li>
<li><a href="application-7.html#application-7" id="toc-application-7"><span class="toc-section-number">11.6</span> Application</a>
<ul>
<li><a href="application-7.html#imputation-with-mean-median-mode" id="toc-imputation-with-mean-median-mode"><span class="toc-section-number">11.6.1</span> Imputation with mean / median / mode</a></li>
<li><a href="application-7.html#knn" id="toc-knn"><span class="toc-section-number">11.6.2</span> KNN</a></li>
<li><a href="application-7.html#rpart" id="toc-rpart"><span class="toc-section-number">11.6.3</span> rpart</a></li>
<li><a href="application-7.html#mice-multivariate-imputation-via-chained-equations" id="toc-mice-multivariate-imputation-via-chained-equations"><span class="toc-section-number">11.6.4</span> MICE (Multivariate Imputation via Chained Equations)</a></li>
<li><a href="application-7.html#amelia" id="toc-amelia"><span class="toc-section-number">11.6.5</span> Amelia</a></li>
<li><a href="application-7.html#missforest" id="toc-missforest"><span class="toc-section-number">11.6.6</span> missForest</a></li>
<li><a href="application-7.html#hmisc" id="toc-hmisc"><span class="toc-section-number">11.6.7</span> Hmisc</a></li>
<li><a href="application-7.html#mi" id="toc-mi"><span class="toc-section-number">11.6.8</span> mi</a></li>
</ul></li>
</ul></li>
<li><a href="data.html#data" id="toc-data"><span class="toc-section-number">12</span> Data</a>
<ul>
<li><a href="cross-sectional.html#cross-sectional" id="toc-cross-sectional"><span class="toc-section-number">12.1</span> Cross-Sectional</a></li>
<li><a href="time-series-1.html#time-series-1" id="toc-time-series-1"><span class="toc-section-number">12.2</span> Time Series</a>
<ul>
<li><a href="time-series-1.html#deterministic-time-trend" id="toc-deterministic-time-trend"><span class="toc-section-number">12.2.1</span> Deterministic Time trend</a></li>
<li><a href="time-series-1.html#feedback-effect" id="toc-feedback-effect"><span class="toc-section-number">12.2.2</span> Feedback Effect</a></li>
<li><a href="time-series-1.html#dynamic-specification" id="toc-dynamic-specification"><span class="toc-section-number">12.2.3</span> Dynamic Specification</a></li>
<li><a href="time-series-1.html#dynamically-complete" id="toc-dynamically-complete"><span class="toc-section-number">12.2.4</span> Dynamically Complete</a></li>
<li><a href="time-series-1.html#highly-persistent-data" id="toc-highly-persistent-data"><span class="toc-section-number">12.2.5</span> Highly Persistent Data</a></li>
</ul></li>
<li><a href="repeated-cross-sections.html#repeated-cross-sections" id="toc-repeated-cross-sections"><span class="toc-section-number">12.3</span> Repeated Cross Sections</a>
<ul>
<li><a href="repeated-cross-sections.html#pooled-cross-section" id="toc-pooled-cross-section"><span class="toc-section-number">12.3.1</span> Pooled Cross Section</a></li>
</ul></li>
<li><a href="panel-data.html#panel-data" id="toc-panel-data"><span class="toc-section-number">12.4</span> Panel Data</a>
<ul>
<li><a href="panel-data.html#pooled-ols-estimator" id="toc-pooled-ols-estimator"><span class="toc-section-number">12.4.1</span> Pooled OLS Estimator</a></li>
<li><a href="panel-data.html#individual-specific-effects-model" id="toc-individual-specific-effects-model"><span class="toc-section-number">12.4.2</span> Individual-specific effects model</a></li>
<li><a href="panel-data.html#tests-for-assumptions" id="toc-tests-for-assumptions"><span class="toc-section-number">12.4.3</span> Tests for Assumptions</a></li>
<li><a href="panel-data.html#model-selection" id="toc-model-selection"><span class="toc-section-number">12.4.4</span> Model Selection</a></li>
<li><a href="panel-data.html#summary-3" id="toc-summary-3"><span class="toc-section-number">12.4.5</span> Summary</a></li>
<li><a href="panel-data.html#application-8" id="toc-application-8"><span class="toc-section-number">12.4.6</span> Application</a></li>
<li><a href="panel-data.html#other-estimators" id="toc-other-estimators"><span class="toc-section-number">12.4.7</span> Other Estimators</a></li>
</ul></li>
</ul></li>
<li><a href="hypothesis-testing.html#hypothesis-testing" id="toc-hypothesis-testing"><span class="toc-section-number">13</span> Hypothesis Testing</a>
<ul>
<li><a href="types-of-hypothesis-testing.html#types-of-hypothesis-testing" id="toc-types-of-hypothesis-testing"><span class="toc-section-number">13.1</span> Types of hypothesis testing</a></li>
<li><a href="wald-test.html#wald-test" id="toc-wald-test"><span class="toc-section-number">13.2</span> Wald test</a>
<ul>
<li><a href="wald-test.html#multiple-hypothesis" id="toc-multiple-hypothesis"><span class="toc-section-number">13.2.1</span> Multiple Hypothesis</a></li>
<li><a href="wald-test.html#linear-combination" id="toc-linear-combination"><span class="toc-section-number">13.2.2</span> Linear Combination</a></li>
<li><a href="wald-test.html#estimate-difference-in-coefficients" id="toc-estimate-difference-in-coefficients"><span class="toc-section-number">13.2.3</span> Estimate Difference in Coefficients</a></li>
<li><a href="wald-test.html#application-9" id="toc-application-9"><span class="toc-section-number">13.2.4</span> Application</a></li>
<li><a href="wald-test.html#nonlinear-1" id="toc-nonlinear-1"><span class="toc-section-number">13.2.5</span> Nonlinear</a></li>
</ul></li>
<li><a href="the-likelihood-ratio-test.html#the-likelihood-ratio-test" id="toc-the-likelihood-ratio-test"><span class="toc-section-number">13.3</span> The likelihood ratio test</a></li>
<li><a href="lagrange-multiplier-score.html#lagrange-multiplier-score" id="toc-lagrange-multiplier-score"><span class="toc-section-number">13.4</span> Lagrange Multiplier (Score)</a></li>
</ul></li>
<li><a href="prediction-and-estimation.html#prediction-and-estimation" id="toc-prediction-and-estimation"><span class="toc-section-number">14</span> Prediction and Estimation</a></li>
<li><a href="moderation.html#moderation" id="toc-moderation"><span class="toc-section-number">15</span> Moderation</a>
<ul>
<li><a href="emmeans-package.html#emmeans-package" id="toc-emmeans-package"><span class="toc-section-number">15.1</span> emmeans package</a>
<ul>
<li><a href="emmeans-package.html#continuous-by-continuous" id="toc-continuous-by-continuous"><span class="toc-section-number">15.1.1</span> Continuous by continuous</a></li>
<li><a href="emmeans-package.html#continuous-by-categorical" id="toc-continuous-by-categorical"><span class="toc-section-number">15.1.2</span> Continuous by categorical</a></li>
<li><a href="emmeans-package.html#categorical-by-categorical" id="toc-categorical-by-categorical"><span class="toc-section-number">15.1.3</span> Categorical by categorical</a></li>
</ul></li>
<li><a href="probmod-package.html#probmod-package" id="toc-probmod-package"><span class="toc-section-number">15.2</span> probmod package</a></li>
<li><a href="interactions-package.html#interactions-package" id="toc-interactions-package"><span class="toc-section-number">15.3</span> interactions package</a>
<ul>
<li><a href="interactions-package.html#continuous-interaction" id="toc-continuous-interaction"><span class="toc-section-number">15.3.1</span> Continuous interaction</a></li>
<li><a href="interactions-package.html#categorical-interaction" id="toc-categorical-interaction"><span class="toc-section-number">15.3.2</span> Categorical interaction</a></li>
</ul></li>
<li><a href="interactionr-package.html#interactionr-package" id="toc-interactionr-package"><span class="toc-section-number">15.4</span> interactionR package</a></li>
<li><a href="sjplot-package.html#sjplot-package" id="toc-sjplot-package"><span class="toc-section-number">15.5</span> sjPlot package</a></li>
</ul></li>
<li><a href="bootstrap.html#bootstrap" id="toc-bootstrap"><span class="toc-section-number">16</span> Bootstrap</a></li>
<li><a href="#part-iv.-causal-inference" id="toc-part-iv.-causal-inference">(PART*) IV. CAUSAL INFERENCE</a></li>
<li><a href="causal-inference.html#causal-inference" id="toc-causal-inference"><span class="toc-section-number">17</span> Causal Inference</a>
<ul>
<li><a href="intro-to-dag-framework.html#intro-to-dag-framework" id="toc-intro-to-dag-framework"><span class="toc-section-number">17.1</span> Intro to DAG Framework</a></li>
<li><a href="intro-to-po-framework.html#intro-to-po-framework" id="toc-intro-to-po-framework"><span class="toc-section-number">17.2</span> Intro to PO Framework</a>
<ul>
<li><a href="intro-to-po-framework.html#typical-assumptions" id="toc-typical-assumptions"><span class="toc-section-number">17.2.1</span> Typical Assumptions</a></li>
<li><a href="intro-to-po-framework.html#treatment-effect-types" id="toc-treatment-effect-types"><span class="toc-section-number">17.2.2</span> Treatment effect types</a></li>
</ul></li>
<li><a href="controls-under-causal-inference.html#controls-under-causal-inference" id="toc-controls-under-causal-inference"><span class="toc-section-number">17.3</span> Controls under causal inference</a>
<ul>
<li><a href="controls-under-causal-inference.html#good-controls" id="toc-good-controls"><span class="toc-section-number">17.3.1</span> Good Controls</a></li>
<li><a href="controls-under-causal-inference.html#bad-controls" id="toc-bad-controls"><span class="toc-section-number">17.3.2</span> Bad Controls</a></li>
</ul></li>
<li><a href="sensitivity-analysis.html#sensitivity-analysis" id="toc-sensitivity-analysis"><span class="toc-section-number">17.4</span> Sensitivity Analysis</a></li>
</ul></li>
<li><a href="#part-a.-experimental-design" id="toc-part-a.-experimental-design">(PART*) A. EXPERIMENTAL DESIGN</a></li>
<li><a href="experimental-design.html#experimental-design" id="toc-experimental-design"><span class="toc-section-number">18</span> Experimental Design</a>
<ul>
<li><a href="simulation.html#simulation" id="toc-simulation"><span class="toc-section-number">18.1</span> Simulation</a></li>
<li><a href="semi-random-experiment.html#semi-random-experiment" id="toc-semi-random-experiment"><span class="toc-section-number">18.2</span> Semi-random Experiment</a></li>
<li><a href="rerandomization.html#rerandomization" id="toc-rerandomization"><span class="toc-section-number">18.3</span> Rerandomization</a></li>
</ul></li>
<li><a href="sampling.html#sampling" id="toc-sampling"><span class="toc-section-number">19</span> Sampling</a>
<ul>
<li><a href="simple-sampling.html#simple-sampling" id="toc-simple-sampling"><span class="toc-section-number">19.1</span> Simple Sampling</a></li>
<li><a href="stratified-sampling.html#stratified-sampling" id="toc-stratified-sampling"><span class="toc-section-number">19.2</span> Stratified Sampling</a></li>
<li><a href="unequal-probability-sampling.html#unequal-probability-sampling" id="toc-unequal-probability-sampling"><span class="toc-section-number">19.3</span> Unequal Probability Sampling</a></li>
<li><a href="balanced-sampling.html#balanced-sampling" id="toc-balanced-sampling"><span class="toc-section-number">19.4</span> Balanced Sampling</a>
<ul>
<li><a href="balanced-sampling.html#cube" id="toc-cube"><span class="toc-section-number">19.4.1</span> Cube</a></li>
<li><a href="balanced-sampling.html#stratification" id="toc-stratification"><span class="toc-section-number">19.4.2</span> Stratification</a></li>
<li><a href="balanced-sampling.html#cluster-1" id="toc-cluster-1"><span class="toc-section-number">19.4.3</span> Cluster</a></li>
<li><a href="balanced-sampling.html#two-stage" id="toc-two-stage"><span class="toc-section-number">19.4.4</span> Two-stage</a></li>
</ul></li>
</ul></li>
<li><a href="analysis-of-variance-anova.html#analysis-of-variance-anova" id="toc-analysis-of-variance-anova"><span class="toc-section-number">20</span> Analysis of Variance (ANOVA)</a>
<ul>
<li><a href="completely-randomized-design-crd.html#completely-randomized-design-crd" id="toc-completely-randomized-design-crd"><span class="toc-section-number">20.1</span> Completely Randomized Design (CRD)</a>
<ul>
<li><a href="completely-randomized-design-crd.html#single-factor-fixed-effects-model" id="toc-single-factor-fixed-effects-model"><span class="toc-section-number">20.1.1</span> Single Factor Fixed Effects Model</a></li>
<li><a href="completely-randomized-design-crd.html#single-factor-random-effects-model" id="toc-single-factor-random-effects-model"><span class="toc-section-number">20.1.2</span> Single Factor Random Effects Model</a></li>
<li><a href="completely-randomized-design-crd.html#two-factor-fixed-effect-anova" id="toc-two-factor-fixed-effect-anova"><span class="toc-section-number">20.1.3</span> Two Factor Fixed Effect ANOVA</a></li>
<li><a href="completely-randomized-design-crd.html#two-way-random-effects-anova" id="toc-two-way-random-effects-anova"><span class="toc-section-number">20.1.4</span> Two-Way Random Effects ANOVA</a></li>
<li><a href="completely-randomized-design-crd.html#two-way-mixed-effects-anova" id="toc-two-way-mixed-effects-anova"><span class="toc-section-number">20.1.5</span> Two-Way Mixed Effects ANOVA</a></li>
</ul></li>
<li><a href="nonparametric-anova.html#nonparametric-anova" id="toc-nonparametric-anova"><span class="toc-section-number">20.2</span> Nonparametric ANOVA</a>
<ul>
<li><a href="nonparametric-anova.html#kruskal-wallis" id="toc-kruskal-wallis"><span class="toc-section-number">20.2.1</span> Kruskal-Wallis</a></li>
<li><a href="nonparametric-anova.html#friedman-test" id="toc-friedman-test"><span class="toc-section-number">20.2.2</span> Friedman Test</a></li>
</ul></li>
<li><a href="sample-size-planning-for-anova.html#sample-size-planning-for-anova" id="toc-sample-size-planning-for-anova"><span class="toc-section-number">20.3</span> Sample Size Planning for ANOVA</a>
<ul>
<li><a href="sample-size-planning-for-anova.html#balanced-designs" id="toc-balanced-designs"><span class="toc-section-number">20.3.1</span> Balanced Designs</a></li>
<li><a href="sample-size-planning-for-anova.html#randomized-block-experiments" id="toc-randomized-block-experiments"><span class="toc-section-number">20.3.2</span> Randomized Block Experiments</a></li>
</ul></li>
<li><a href="randomized-block-designs.html#randomized-block-designs" id="toc-randomized-block-designs"><span class="toc-section-number">20.4</span> Randomized Block Designs</a>
<ul>
<li><a href="randomized-block-designs.html#tukey-test-of-additivity" id="toc-tukey-test-of-additivity"><span class="toc-section-number">20.4.1</span> Tukey Test of Additivity</a></li>
</ul></li>
<li><a href="nested-designs.html#nested-designs" id="toc-nested-designs"><span class="toc-section-number">20.5</span> Nested Designs</a>
<ul>
<li><a href="nested-designs.html#two-factor-nested-designs" id="toc-two-factor-nested-designs"><span class="toc-section-number">20.5.1</span> Two-Factor Nested Designs</a></li>
</ul></li>
<li><a href="single-factor-covariance-model.html#single-factor-covariance-model" id="toc-single-factor-covariance-model"><span class="toc-section-number">20.6</span> Single Factor Covariance Model</a></li>
</ul></li>
<li><a href="multivariate-methods.html#multivariate-methods" id="toc-multivariate-methods"><span class="toc-section-number">21</span> Multivariate Methods</a>
<ul>
<li><a href="multivariate-methods.html#properties-of-mvn" id="toc-properties-of-mvn"><span class="toc-section-number">21.0.1</span> Properties of MVN</a></li>
<li><a href="multivariate-methods.html#mean-vector-inference" id="toc-mean-vector-inference"><span class="toc-section-number">21.0.2</span> Mean Vector Inference</a></li>
<li><a href="multivariate-methods.html#general-hypothesis-testing" id="toc-general-hypothesis-testing"><span class="toc-section-number">21.0.3</span> General Hypothesis Testing</a></li>
<li><a href="manova.html#manova" id="toc-manova"><span class="toc-section-number">21.1</span> MANOVA</a>
<ul>
<li><a href="manova.html#testing-general-hypotheses" id="toc-testing-general-hypotheses"><span class="toc-section-number">21.1.1</span> Testing General Hypotheses</a></li>
<li><a href="manova.html#profile-analysis" id="toc-profile-analysis"><span class="toc-section-number">21.1.2</span> Profile Analysis</a></li>
<li><a href="manova.html#summary-5" id="toc-summary-5"><span class="toc-section-number">21.1.3</span> Summary</a></li>
</ul></li>
<li><a href="principal-components.html#principal-components" id="toc-principal-components"><span class="toc-section-number">21.2</span> Principal Components</a>
<ul>
<li><a href="principal-components.html#population-principal-components" id="toc-population-principal-components"><span class="toc-section-number">21.2.1</span> Population Principal Components</a></li>
<li><a href="principal-components.html#sample-principal-components" id="toc-sample-principal-components"><span class="toc-section-number">21.2.2</span> Sample Principal Components</a></li>
<li><a href="principal-components.html#application-10" id="toc-application-10"><span class="toc-section-number">21.2.3</span> Application</a></li>
</ul></li>
<li><a href="factor-analysis.html#factor-analysis" id="toc-factor-analysis"><span class="toc-section-number">21.3</span> Factor Analysis</a>
<ul>
<li><a href="factor-analysis.html#methods-of-estimation" id="toc-methods-of-estimation"><span class="toc-section-number">21.3.1</span> Methods of Estimation</a></li>
<li><a href="factor-analysis.html#factor-rotation" id="toc-factor-rotation"><span class="toc-section-number">21.3.2</span> Factor Rotation</a></li>
<li><a href="factor-analysis.html#estimation-of-factor-scores" id="toc-estimation-of-factor-scores"><span class="toc-section-number">21.3.3</span> Estimation of Factor Scores</a></li>
<li><a href="factor-analysis.html#model-diagnostic" id="toc-model-diagnostic"><span class="toc-section-number">21.3.4</span> Model Diagnostic</a></li>
<li><a href="factor-analysis.html#application-11" id="toc-application-11"><span class="toc-section-number">21.3.5</span> Application</a></li>
</ul></li>
<li><a href="discriminant-analysis.html#discriminant-analysis" id="toc-discriminant-analysis"><span class="toc-section-number">21.4</span> Discriminant Analysis</a>
<ul>
<li><a href="discriminant-analysis.html#known-populations" id="toc-known-populations"><span class="toc-section-number">21.4.1</span> Known Populations</a></li>
<li><a href="discriminant-analysis.html#probabilities-of-misclassification" id="toc-probabilities-of-misclassification"><span class="toc-section-number">21.4.2</span> Probabilities of Misclassification</a></li>
<li><a href="discriminant-analysis.html#unknown-populations-nonparametric-discrimination" id="toc-unknown-populations-nonparametric-discrimination"><span class="toc-section-number">21.4.3</span> Unknown Populations/ Nonparametric Discrimination</a></li>
<li><a href="discriminant-analysis.html#application-12" id="toc-application-12"><span class="toc-section-number">21.4.4</span> Application</a></li>
</ul></li>
</ul></li>
<li><a href="#part-b.-quasi-experimental-design" id="toc-part-b.-quasi-experimental-design">(PART*) B. QUASI-EXPERIMENTAL DESIGN</a></li>
<li><a href="quasi-experimental.html#quasi-experimental" id="toc-quasi-experimental"><span class="toc-section-number">22</span> Quasi-experimental</a></li>
<li><a href="regression-discontinuity.html#regression-discontinuity" id="toc-regression-discontinuity"><span class="toc-section-number">23</span> Regression Discontinuity</a>
<ul>
<li><a href="specification-checks.html#specification-checks" id="toc-specification-checks"><span class="toc-section-number">23.1</span> Specification Checks</a>
<ul>
<li><a href="specification-checks.html#balance-checks" id="toc-balance-checks"><span class="toc-section-number">23.1.1</span> Balance Checks</a></li>
<li><a href="specification-checks.html#sortingbunchingmanipulation" id="toc-sortingbunchingmanipulation"><span class="toc-section-number">23.1.2</span> Sorting/Bunching/Manipulation</a></li>
<li><a href="specification-checks.html#placebo-tests" id="toc-placebo-tests"><span class="toc-section-number">23.1.3</span> Placebo Tests</a></li>
<li><a href="specification-checks.html#sensitivity-to-bandwidth-choice" id="toc-sensitivity-to-bandwidth-choice"><span class="toc-section-number">23.1.4</span> Sensitivity to Bandwidth Choice</a></li>
<li><a href="specification-checks.html#fuzzy-rd-design" id="toc-fuzzy-rd-design"><span class="toc-section-number">23.1.5</span> Fuzzy RD Design</a></li>
<li><a href="specification-checks.html#regression-kink-design" id="toc-regression-kink-design"><span class="toc-section-number">23.1.6</span> Regression Kink Design</a></li>
<li><a href="specification-checks.html#mutli-cutoff-multi-score-geographic-rd" id="toc-mutli-cutoff-multi-score-geographic-rd"><span class="toc-section-number">23.1.7</span> Mutli-cutoff, Multi-score, geographic RD</a></li>
</ul></li>
<li><a href="steps-for-sharp-rd.html#steps-for-sharp-rd" id="toc-steps-for-sharp-rd"><span class="toc-section-number">23.2</span> Steps for Sharp RD</a></li>
<li><a href="steps-for-fuzzy-rd.html#steps-for-fuzzy-rd" id="toc-steps-for-fuzzy-rd"><span class="toc-section-number">23.3</span> Steps for Fuzzy RD</a></li>
<li><a href="steps-for-rdit-regression-discontinuity-in-time.html#steps-for-rdit-regression-discontinuity-in-time" id="toc-steps-for-rdit-regression-discontinuity-in-time"><span class="toc-section-number">23.4</span> Steps for RDiT (Regression Discontinuity in Time)</a></li>
<li><a href="evaluation-of-an-rd.html#evaluation-of-an-rd" id="toc-evaluation-of-an-rd"><span class="toc-section-number">23.5</span> Evaluation of an RD</a></li>
<li><a href="applications.html#applications" id="toc-applications"><span class="toc-section-number">23.6</span> Applications</a>
<ul>
<li><a href="applications.html#example-1-1" id="toc-example-1-1"><span class="toc-section-number">23.6.1</span> Example 1</a></li>
<li><a href="applications.html#example-2" id="toc-example-2"><span class="toc-section-number">23.6.2</span> Example 2</a></li>
<li><a href="applications.html#example-3" id="toc-example-3"><span class="toc-section-number">23.6.3</span> Example 3</a></li>
<li><a href="applications.html#example-4" id="toc-example-4"><span class="toc-section-number">23.6.4</span> Example 4</a></li>
</ul></li>
</ul></li>
<li><a href="difference-in-differences.html#difference-in-differences" id="toc-difference-in-differences"><span class="toc-section-number">24</span> Difference-in-differences</a>
<ul>
<li><a href="simple-dif-n-dif.html#simple-dif-n-dif" id="toc-simple-dif-n-dif"><span class="toc-section-number">24.1</span> Simple Dif-n-dif</a>
<ul>
<li><a href="simple-dif-n-dif.html#assumptions-2" id="toc-assumptions-2"><span class="toc-section-number">24.1.1</span> Assumptions</a></li>
<li><a href="simple-dif-n-dif.html#examples" id="toc-examples"><span class="toc-section-number">24.1.2</span> Examples</a></li>
</ul></li>
<li><a href="multiple-periods-and-variation-in-treatment-timing.html#multiple-periods-and-variation-in-treatment-timing" id="toc-multiple-periods-and-variation-in-treatment-timing"><span class="toc-section-number">24.2</span> Multiple periods and variation in treatment timing</a>
<ul>
<li><a href="multiple-periods-and-variation-in-treatment-timing.html#staggered-dif-n-dif" id="toc-staggered-dif-n-dif"><span class="toc-section-number">24.2.1</span> Staggered Dif-n-dif</a></li>
</ul></li>
<li><a href="augmented-dif-n-dif.html#augmented-dif-n-dif" id="toc-augmented-dif-n-dif"><span class="toc-section-number">24.3</span> Augmented Dif-n-dif</a></li>
<li><a href="applications-1.html#applications-1" id="toc-applications-1"><span class="toc-section-number">24.4</span> Applications</a>
<ul>
<li><a href="applications-1.html#two-way-fixed-effects" id="toc-two-way-fixed-effects"><span class="toc-section-number">24.4.1</span> Two-way Fixed-effects</a></li>
</ul></li>
</ul></li>
<li><a href="synthetic-control.html#synthetic-control" id="toc-synthetic-control"><span class="toc-section-number">25</span> Synthetic Control</a>
<ul>
<li><a href="applications-2.html#applications-2" id="toc-applications-2"><span class="toc-section-number">25.1</span> Applications</a>
<ul>
<li><a href="applications-2.html#example-1-2" id="toc-example-1-2"><span class="toc-section-number">25.1.1</span> Example 1</a></li>
<li><a href="applications-2.html#example-2-1" id="toc-example-2-1"><span class="toc-section-number">25.1.2</span> Example 2</a></li>
<li><a href="applications-2.html#example-3-1" id="toc-example-3-1"><span class="toc-section-number">25.1.3</span> Example 3</a></li>
<li><a href="applications-2.html#example-4-1" id="toc-example-4-1"><span class="toc-section-number">25.1.4</span> Example 4</a></li>
</ul></li>
<li><a href="synthetic-difference-in-differences.html#synthetic-difference-in-differences" id="toc-synthetic-difference-in-differences"><span class="toc-section-number">25.2</span> Synthetic Difference-in-differences</a></li>
<li><a href="modified-synthetic-control.html#modified-synthetic-control" id="toc-modified-synthetic-control"><span class="toc-section-number">25.3</span> Modified Synthetic Control</a></li>
<li><a href="generalized-synthetic-control.html#generalized-synthetic-control" id="toc-generalized-synthetic-control"><span class="toc-section-number">25.4</span> Generalized Synthetic Control</a></li>
</ul></li>
<li><a href="panel-data-methods.html#panel-data-methods" id="toc-panel-data-methods"><span class="toc-section-number">26</span> Panel Data Methods</a></li>
<li><a href="event-studies.html#event-studies" id="toc-event-studies"><span class="toc-section-number">27</span> Event Studies</a>
<ul>
<li><a href="other-issues.html#other-issues" id="toc-other-issues"><span class="toc-section-number">27.1</span> Other Issues</a>
<ul>
<li><a href="other-issues.html#economic-significance" id="toc-economic-significance"><span class="toc-section-number">27.1.1</span> Economic significance</a></li>
<li><a href="other-issues.html#statistical-power" id="toc-statistical-power"><span class="toc-section-number">27.1.2</span> Statistical Power</a></li>
<li><a href="other-issues.html#testing" id="toc-testing"><span class="toc-section-number">27.1.3</span> Testing</a></li>
<li><a href="other-issues.html#confounders" id="toc-confounders"><span class="toc-section-number">27.1.4</span> Confounders</a></li>
<li><a href="other-issues.html#biases" id="toc-biases"><span class="toc-section-number">27.1.5</span> Biases</a></li>
<li><a href="other-issues.html#long-run-event-studies" id="toc-long-run-event-studies"><span class="toc-section-number">27.1.6</span> Long-run event studies</a></li>
</ul></li>
<li><a href="aggregation.html#aggregation" id="toc-aggregation"><span class="toc-section-number">27.2</span> Aggregation</a>
<ul>
<li><a href="aggregation.html#over-time" id="toc-over-time"><span class="toc-section-number">27.2.1</span> Over Time</a></li>
<li><a href="aggregation.html#across-firms-over-time" id="toc-across-firms-over-time"><span class="toc-section-number">27.2.2</span> Across Firms + Over Time</a></li>
</ul></li>
<li><a href="heterogeneity-in-the-event-effect.html#heterogeneity-in-the-event-effect" id="toc-heterogeneity-in-the-event-effect"><span class="toc-section-number">27.3</span> Heterogeneity in the event effect</a></li>
<li><a href="expected-return-calculation.html#expected-return-calculation" id="toc-expected-return-calculation"><span class="toc-section-number">27.4</span> Expected Return Calculation</a>
<ul>
<li><a href="expected-return-calculation.html#statistical-models" id="toc-statistical-models"><span class="toc-section-number">27.4.1</span> Statistical Models</a></li>
<li><a href="expected-return-calculation.html#economic-model" id="toc-economic-model"><span class="toc-section-number">27.4.2</span> Economic Model</a></li>
</ul></li>
<li><a href="application-13.html#application-13" id="toc-application-13"><span class="toc-section-number">27.5</span> Application</a></li>
</ul></li>
<li><a href="matching-methods.html#matching-methods" id="toc-matching-methods"><span class="toc-section-number">28</span> Matching Methods</a>
<ul>
<li><a href="matching-vs.-regression.html#matching-vs.-regression" id="toc-matching-vs.-regression"><span class="toc-section-number">28.1</span> Matching vs. Regression</a></li>
<li><a href="weighting-vs.-balancing.html#weighting-vs.-balancing" id="toc-weighting-vs.-balancing"><span class="toc-section-number">28.2</span> Weighting vs. Balancing</a></li>
<li><a href="matchit.html#matchit" id="toc-matchit"><span class="toc-section-number">28.3</span> MatchIt</a></li>
<li><a href="matchingfrontier.html#matchingfrontier" id="toc-matchingfrontier"><span class="toc-section-number">28.4</span> MatchingFrontier</a></li>
<li><a href="propensity-scores.html#propensity-scores" id="toc-propensity-scores"><span class="toc-section-number">28.5</span> Propensity Scores</a></li>
<li><a href="mahalanobis-distance.html#mahalanobis-distance" id="toc-mahalanobis-distance"><span class="toc-section-number">28.6</span> Mahalanobis Distance</a></li>
<li><a href="coarsened-exact-matching.html#coarsened-exact-matching" id="toc-coarsened-exact-matching"><span class="toc-section-number">28.7</span> Coarsened Exact Matching</a></li>
<li><a href="genetic-matching.html#genetic-matching" id="toc-genetic-matching"><span class="toc-section-number">28.8</span> Genetic Matching</a></li>
<li><a href="matching-for-time-series-cross-section-data.html#matching-for-time-series-cross-section-data" id="toc-matching-for-time-series-cross-section-data"><span class="toc-section-number">28.9</span> Matching for time series-cross-section data</a></li>
</ul></li>
<li><a href="interrupted-time-series.html#interrupted-time-series" id="toc-interrupted-time-series"><span class="toc-section-number">29</span> Interrupted Time Series</a></li>
<li><a href="#part-c.-other-concerns" id="toc-part-c.-other-concerns">(PART*) C. OTHER CONCERNS</a></li>
<li><a href="doubly-robust-estimator.html#doubly-robust-estimator" id="toc-doubly-robust-estimator"><span class="toc-section-number">30</span> Doubly Robust Estimator</a>
<ul>
<li><a href="regression-adjustments.html#regression-adjustments" id="toc-regression-adjustments"><span class="toc-section-number">30.1</span> Regression Adjustments</a></li>
<li><a href="inverse-propensity-weighting.html#inverse-propensity-weighting" id="toc-inverse-propensity-weighting"><span class="toc-section-number">30.2</span> Inverse Propensity Weighting</a></li>
<li><a href="augmented-inverse-propensity-weighting.html#augmented-inverse-propensity-weighting" id="toc-augmented-inverse-propensity-weighting"><span class="toc-section-number">30.3</span> Augmented Inverse Propensity Weighting</a></li>
</ul></li>
<li><a href="endogeneity.html#endogeneity" id="toc-endogeneity"><span class="toc-section-number">31</span> Endogeneity</a>
<ul>
<li><a href="measurement-error.html#measurement-error" id="toc-measurement-error"><span class="toc-section-number">31.1</span> Measurement Error</a>
<ul>
<li><a href="measurement-error.html#classical-measurement-errors" id="toc-classical-measurement-errors"><span class="toc-section-number">31.1.1</span> Classical Measurement Errors</a></li>
<li><a href="measurement-error.html#non-classical-measurement-errors" id="toc-non-classical-measurement-errors"><span class="toc-section-number">31.1.2</span> Non-classical Measurement Errors</a></li>
<li><a href="measurement-error.html#solution-to-measurement-errors" id="toc-solution-to-measurement-errors"><span class="toc-section-number">31.1.3</span> Solution to Measurement Errors</a></li>
</ul></li>
<li><a href="simultaneity.html#simultaneity" id="toc-simultaneity"><span class="toc-section-number">31.2</span> Simultaneity</a></li>
<li><a href="endogenous-treatment.html#endogenous-treatment" id="toc-endogenous-treatment"><span class="toc-section-number">31.3</span> Endogenous Treatment</a>
<ul>
<li><a href="endogenous-treatment.html#instrumental-variable" id="toc-instrumental-variable"><span class="toc-section-number">31.3.1</span> Instrumental Variable</a></li>
<li><a href="endogenous-treatment.html#internal-instrumental-variable" id="toc-internal-instrumental-variable"><span class="toc-section-number">31.3.2</span> Internal instrumental variable</a></li>
<li><a href="endogenous-treatment.html#proxy-variables" id="toc-proxy-variables"><span class="toc-section-number">31.3.3</span> Proxy Variables</a></li>
</ul></li>
<li><a href="endogenous-sample-selection.html#endogenous-sample-selection" id="toc-endogenous-sample-selection"><span class="toc-section-number">31.4</span> Endogenous Sample Selection</a>
<ul>
<li><a href="endogenous-sample-selection.html#tobit-2" id="toc-tobit-2"><span class="toc-section-number">31.4.1</span> Tobit-2</a></li>
<li><a href="endogenous-sample-selection.html#tobit-5" id="toc-tobit-5"><span class="toc-section-number">31.4.2</span> Tobit-5</a></li>
</ul></li>
</ul></li>
<li><a href="mediation.html#mediation" id="toc-mediation"><span class="toc-section-number">32</span> Mediation</a>
<ul>
<li><a href="traditional.html#traditional" id="toc-traditional"><span class="toc-section-number">32.1</span> Traditional</a>
<ul>
<li><a href="traditional.html#example-1-mediation-traditional" id="toc-example-1-mediation-traditional"><span class="toc-section-number">32.1.1</span> Example 1</a></li>
</ul></li>
<li><a href="model-based-causal-mediation-analysis.html#model-based-causal-mediation-analysis" id="toc-model-based-causal-mediation-analysis"><span class="toc-section-number">32.2</span> Model-based causal mediation analysis</a></li>
</ul></li>
<li><a href="directed-acyclic-graph.html#directed-acyclic-graph" id="toc-directed-acyclic-graph"><span class="toc-section-number">33</span> Directed Acyclic Graph</a></li>
<li><a href="#part-v.-miscellaneous" id="toc-part-v.-miscellaneous">(PART*) V. MISCELLANEOUS</a></li>
<li><a href="report.html#report" id="toc-report"><span class="toc-section-number">34</span> Report</a>
<ul>
<li><a href="one-summary-table.html#one-summary-table" id="toc-one-summary-table"><span class="toc-section-number">34.1</span> One summary table</a></li>
<li><a href="model-comparison.html#model-comparison" id="toc-model-comparison"><span class="toc-section-number">34.2</span> Model Comparison</a></li>
<li><a href="changes-in-an-estimate.html#changes-in-an-estimate" id="toc-changes-in-an-estimate"><span class="toc-section-number">34.3</span> Changes in an estimate</a></li>
</ul></li>
<li><a href="exploratory-data-analysis.html#exploratory-data-analysis" id="toc-exploratory-data-analysis"><span class="toc-section-number">35</span> Exploratory Data Analysis</a></li>
<li><a href="sensitivity-analysis-robustness-check.html#sensitivity-analysis-robustness-check" id="toc-sensitivity-analysis-robustness-check"><span class="toc-section-number">36</span> Sensitivity Analysis/ Robustness Check</a>
<ul>
<li><a href="specification-curve.html#specification-curve" id="toc-specification-curve"><span class="toc-section-number">36.1</span> Specification curve</a>
<ul>
<li><a href="specification-curve.html#starbility" id="toc-starbility"><span class="toc-section-number">36.1.1</span> starbility</a></li>
<li><a href="specification-curve.html#rdfanalysis" id="toc-rdfanalysis"><span class="toc-section-number">36.1.2</span> rdfanalysis</a></li>
</ul></li>
<li><a href="coefficient-stability.html#coefficient-stability" id="toc-coefficient-stability"><span class="toc-section-number">36.2</span> Coefficient stability</a></li>
</ul></li>
<li><a href="#appendix-appendix" id="toc-appendix-appendix">(APPENDIX) APPENDIX</a></li>
<li><a href="appendix.html#appendix" id="toc-appendix"><span class="toc-section-number">37</span> Appendix</a>
<ul>
<li><a href="git.html#git" id="toc-git"><span class="toc-section-number">37.1</span> Git</a></li>
<li><a href="short-cut.html#short-cut" id="toc-short-cut"><span class="toc-section-number">37.2</span> Short-cut</a></li>
<li><a href="function-short-cut.html#function-short-cut" id="toc-function-short-cut"><span class="toc-section-number">37.3</span> Function short-cut</a></li>
<li><a href="citation.html#citation" id="toc-citation"><span class="toc-section-number">37.4</span> Citation</a></li>
<li><a href="install-all-necessary-packageslibaries-on-your-local-machine.html#install-all-necessary-packageslibaries-on-your-local-machine" id="toc-install-all-necessary-packageslibaries-on-your-local-machine"><span class="toc-section-number">37.5</span> Install all necessary packages/libaries on your local machine</a></li>
</ul></li>
<li><a href="bookdown-cheat-sheet.html#bookdown-cheat-sheet" id="toc-bookdown-cheat-sheet"><span class="toc-section-number">38</span> Bookdown cheat sheet</a>
<ul>
<li><a href="operation.html#operation" id="toc-operation"><span class="toc-section-number">38.1</span> Operation</a></li>
<li><a href="math-expresssion-syntax.html#math-expresssion-syntax" id="toc-math-expresssion-syntax"><span class="toc-section-number">38.2</span> Math Expresssion/ Syntax</a>
<ul>
<li><a href="math-expresssion-syntax.html#statistics-notation" id="toc-statistics-notation"><span class="toc-section-number">38.2.1</span> Statistics Notation</a></li>
</ul></li>
<li><a href="table.html#table" id="toc-table"><span class="toc-section-number">38.3</span> Table</a></li>
</ul></li>
<li><a href="references.html#references" id="toc-references">References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Guide on Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="discriminant-analysis" class="section level2" number="21.4">
<h2><span class="header-section-number">21.4</span> Discriminant Analysis</h2>
<p>Suppose we have two or more different populations from which observations could come from. Discriminant analysis seeks to determine which of the possible population an observation comes from while making as few mistakes as possible</p>
<ul>
<li><p>This is an alternative to logistic approaches with the following advantages:</p>
<ul>
<li><p>when there is clear separation between classes, the parameter estimates for the logic regression model can be <strong>surprisingly</strong> unstable, while discriminant approaches do not suffer</p></li>
<li><p>If X is normal in each of the classes and the sample size is small, then discriminant approaches can be more accurate</p></li>
</ul></li>
</ul>
<p>Notation</p>
<p>SImilar to MANOVA, let <span class="math inline">\(\mathbf{y}_{j1},\mathbf{y}_{j2},\dots, \mathbf{y}_{in_j} \sim iid f_j (\mathbf{y})\)</span> for <span class="math inline">\(j = 1,\dots, h\)</span></p>
<p>Let <span class="math inline">\(f_j(\mathbf{y})\)</span> be the density function for population j . Note that each vector <span class="math inline">\(\mathbf{y}\)</span> contain measurements on all <span class="math inline">\(p\)</span> traits</p>
<ol style="list-style-type: decimal">
<li>Assume that each observation is from one of <span class="math inline">\(h\)</span> possible populations.</li>
<li>We want to form a discriminant rule that will allocate an observation <span class="math inline">\(\mathbf{y}\)</span> to population j when <span class="math inline">\(\mathbf{y}\)</span> is in fact from this population</li>
</ol>
<div id="known-populations" class="section level3" number="21.4.1">
<h3><span class="header-section-number">21.4.1</span> Known Populations</h3>
<p>The maximum likelihood discriminant rule for assigning an observation <span class="math inline">\(\mathbf{y}\)</span> to one of the <span class="math inline">\(h\)</span> populations allocates <span class="math inline">\(\mathbf{y}\)</span> to the population that gives the largest likelihood to <span class="math inline">\(\mathbf{y}\)</span></p>
<p>Consider the likelihood for a single observation <span class="math inline">\(\mathbf{y}\)</span>, which has the form <span class="math inline">\(f_j (\mathbf{y})\)</span> where j is the true population.</p>
<p>Since <span class="math inline">\(j\)</span> is unknown, to make the likelihood as large as possible, we should choose the value j which causes <span class="math inline">\(f_j (\mathbf{y})\)</span> to be as large as possible</p>
<p><br></p>
<p>Consider a simple univariate example. Suppose we have data from one of two binomial populations.</p>
<ul>
<li><p>The first population has <span class="math inline">\(n= 10\)</span> trials with success probability <span class="math inline">\(p = .5\)</span></p></li>
<li><p>The second population has <span class="math inline">\(n= 10\)</span> trials with success probability <span class="math inline">\(p = .7\)</span></p></li>
<li><p>to which population would we assign an observation of <span class="math inline">\(y = 7\)</span></p></li>
<li><p>Note:</p>
<ul>
<li><p><span class="math inline">\(f(y = 7|n = 10, p = .5) = .117\)</span></p></li>
<li><p><span class="math inline">\(f(y = 7|n = 10, p = .7) = .267\)</span> where <span class="math inline">\(f(.)\)</span> is the binomial likelihood.</p></li>
<li><p>Hence, we choose the second population</p></li>
</ul></li>
</ul>
<p><br></p>
<p>Another example</p>
<p>We have 2 populations, where</p>
<ul>
<li><p>First population: <span class="math inline">\(N(\mu_1, \sigma^2_1)\)</span></p></li>
<li><p>Second population: <span class="math inline">\(N(\mu_2, \sigma^2_2)\)</span></p></li>
</ul>
<p>The likelihood for a single observation is</p>
<p><span class="math display">\[
f_j (y) = (2\pi \sigma^2_j)^{-1/2} \exp\{ -\frac{1}{2}(\frac{y - \mu_j}{\sigma_j})^2\}
\]</span></p>
<p>Consider a likelihood ratio rule</p>
<p><span class="math display">\[
\begin{aligned}
\Lambda &amp;= \frac{\text{likelihood of y from pop 1}}{\text{likelihood of y from pop 2}} \\
&amp;= \frac{f_1(y)}{f_2(y)} \\
&amp;= \frac{\sigma_2}{\sigma_1} \exp\{-\frac{1}{2}[(\frac{y - \mu_1}{\sigma_1})^2- (\frac{y - \mu_2}{\sigma_2})^2] \}
\end{aligned}
\]</span></p>
<p>Hence, we classify into</p>
<ul>
<li><p>pop 1 if <span class="math inline">\(\Lambda &gt;1\)</span></p></li>
<li><p>pop 2 if <span class="math inline">\(\Lambda &lt;1\)</span></p></li>
<li><p>for ties, flip a coin</p></li>
</ul>
<p>Another way to think:</p>
<p>we classify into population 1 if the “standardized distance” of y from <span class="math inline">\(\mu_1\)</span> is less than the “standardized distance” of y from <span class="math inline">\(\mu_2\)</span> which is referred to as a <strong>quadratic discriminant rule</strong>.</p>
<p>(Significant simplification occurs in th special case where <span class="math inline">\(\sigma_1 = \sigma_2 = \sigma^2\)</span>)</p>
<p>Thus, we classify into population 1 if</p>
<p><span class="math display">\[
(y - \mu_2)^2 &gt; (y - \mu_1)^2
\]</span></p>
<p>or</p>
<p><span class="math display">\[
|y- \mu_2| &gt; |y - \mu_1|
\]</span></p>
<p>and</p>
<p><span class="math display">\[
-2 \log (\Lambda) = -2y  \frac{(\mu_1 - \mu_2)}{\sigma^2} + \frac{(\mu_1^2 - \mu_2^2)}{\sigma^2} = \beta y + \alpha
\]</span></p>
<p>Thus, we classify into population 1 if this is less than 0.</p>
<p>Discriminant classification rule is linear in y in this case.</p>
<p><br></p>
<div id="multivariate-expansion" class="section level4" number="21.4.1.1">
<h4><span class="header-section-number">21.4.1.1</span> Multivariate Expansion</h4>
<p>Suppose that there are 2 populations</p>
<ul>
<li><p><span class="math inline">\(N_p(\mathbf{\mu}_1, \mathbf{\Sigma}_1)\)</span></p></li>
<li><p><span class="math inline">\(N_p(\mathbf{\mu}_2, \mathbf{\Sigma}_2)\)</span></p></li>
</ul>
<p><span class="math display">\[
\begin{aligned}
-2 \log(\frac{f_1 (\mathbf{x})}{f_2 (\mathbf{x})}) &amp;= \log|\mathbf{\Sigma}_1| + (\mathbf{x} - \mathbf{\mu}_1)&#39; \mathbf{\Sigma}^{-1}_1 (\mathbf{x} - \mathbf{\mu}_1) \\
&amp;- [\log|\mathbf{\Sigma}_2|+ (\mathbf{x} - \mathbf{\mu}_2)&#39; \mathbf{\Sigma}^{-1}_2 (\mathbf{x} - \mathbf{\mu}_2) ]
\end{aligned}
\]</span></p>
<p>Again, we classify into population 1 if this is less than 0, otherwise, population 2. And like the univariate case with non-equal variances, this is a quadratic discriminant rule.</p>
<p>And if the covariance matrices are equal: <span class="math inline">\(\mathbf{\Sigma}_1 = \mathbf{\Sigma}_2 = \mathbf{\Sigma}_1\)</span> classify into population 1 if</p>
<p><span class="math display">\[
(\mathbf{\mu}_1 - \mathbf{\mu}_2)&#39; \mathbf{\Sigma}^{-1}\mathbf{x} - \frac{1}{2} (\mathbf{\mu}_1 - \mathbf{\mu}_2)&#39; \mathbf{\Sigma}^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2) \ge 0
\]</span></p>
<p>This linear discriminant rule is also referred to as <strong>Fisher’s linear discriminant function</strong></p>
<p>By <strong>assuming the covariance matrices are equal, we assume that the shape and orientation fo the two populations must be the same (which can be a strong restriction)</strong></p>
<p>In other words, for each variable, it can have different mean but the same variance.</p>
<ul>
<li>Note: LDA Bayes decision boundary is linear. Hence, quadratic decision boundary might lead to better classification. Moreover, the assumption of same variance/covariance matrix across all classes for Gaussian densities imposes the linear rule, if we allow the predictors in each class to follow MVN distribution with class-specific mean vectors and variance/covariance matrices, then it is <strong>Quadratic Discriminant Analysis.</strong> But then, you will have more parameters to estimate (which gives more flexibility than LDA) at the cost of more variance (bias -variance tradeoff).</li>
</ul>
<p>When <span class="math inline">\(\mathbf{\mu}_1, \mathbf{\mu}_2, \mathbf{\Sigma}\)</span> are known, the probability of misclassification can be determined:</p>
<p><span class="math display">\[
\begin{aligned}
P(2|1) &amp;= P(\text{calssify into pop 2| x is from pop 1}) \\
&amp;= P((\mathbf{\mu}_1 - \mathbf{\mu}_2)&#39; \mathbf{\Sigma}^{-1} \mathbf{x} \le \frac{1}{2} (\mathbf{\mu}_1 - \mathbf{\mu}_2)&#39; \mathbf{\Sigma}^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2)|\mathbf{x} \sim N(\mu_1, \mathbf{\Sigma}) \\
&amp;= \Phi(-\frac{1}{2} \delta)
\end{aligned}
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(\delta^2 = (\mathbf{\mu}_1 - \mathbf{\mu}_2)&#39; \mathbf{\Sigma}^{-1} (\mathbf{\mu}_1 - \mathbf{\mu}_2)\)</span></p></li>
<li><p><span class="math inline">\(\Phi\)</span> is the standard normal cdf</p></li>
</ul>
<p><br></p>
<p>Suppose there are <span class="math inline">\(h\)</span> possible populations, which are distributed as <span class="math inline">\(N_p (\mathbf{\mu}_p, \mathbf{\Sigma})\)</span>. Then, the maximum likelihood (linear) discriminant rule allocates <span class="math inline">\(\mathbf{y}\)</span> to population j where j minimizes the squared Mahalanobis distance</p>
<p><span class="math display">\[
(\mathbf{y} - \mathbf{\mu}_j)&#39; \mathbf{\Sigma}^{-1} (\mathbf{y} - \mathbf{\mu}_j)
\]</span></p>
<p><br></p>
</div>
<div id="bayes-discriminant-rules" class="section level4" number="21.4.1.2">
<h4><span class="header-section-number">21.4.1.2</span> Bayes Discriminant Rules</h4>
<p>If we know that population j has prior probabilities <span class="math inline">\(\pi_j\)</span> (assume <span class="math inline">\(\pi_j &gt;0\)</span>) we can form the Bayes discriminant rule.</p>
<p>This rule allocates an observation <span class="math inline">\(\mathbf{y}\)</span> to the population for which <span class="math inline">\(\pi_j f_j (\mathbf{y})\)</span> is maximized.</p>
<p>Note:</p>
<ul>
<li><strong>Maximum likelihood discriminant rule</strong> is a special case of the <strong>Bayes discriminant rule</strong>, where it sets all the <span class="math inline">\(\pi_j = 1/h\)</span></li>
</ul>
<p><br></p>
<p>Optimal Properties of Bayes Discriminant Rules</p>
<ul>
<li><p>let <span class="math inline">\(p_{ii}\)</span> be the probability of correctly assigning an observation from population i</p></li>
<li><p>then one rule (with probabilities <span class="math inline">\(p_{ii}\)</span> ) is as good as another rule (with probabilities <span class="math inline">\(p_{ii}&#39;\)</span> ) if <span class="math inline">\(p_{ii} \ge p_{ii}&#39;\)</span> for all <span class="math inline">\(i = 1,\dots, h\)</span></p></li>
<li><p>The first rule is better than the alternative if <span class="math inline">\(p_{ii} &gt; p_{ii}&#39;\)</span> for at least one i.</p></li>
<li><p>A rule for which there is no better alternative is called admissible</p></li>
<li><p>Bayes Discriminant Rules are admissible</p></li>
<li><p>If we utilized prior probabilities, then we can form the posterior probability of a correct allocation, <span class="math inline">\(\sum_{i=1}^h \pi_i p_{ii}\)</span></p></li>
<li><p>Bayes Discriminant Rules have the largest possible posterior probability of correct allocation with respect to the prior</p></li>
<li><p>These properties show that <strong>Bayes Discriminant rule is our best approach</strong>.</p></li>
</ul>
<p><br></p>
<p>Unequal Cost</p>
<ul>
<li><p>We want to consider the cost misallocation Define <span class="math inline">\(c_{ij}\)</span> to be the cost associated with allocation a member of population j to population i.</p></li>
<li><p>Assume that</p>
<ul>
<li><p><span class="math inline">\(c_{ij} &gt;0\)</span> for all <span class="math inline">\(i \neq j\)</span></p></li>
<li><p><span class="math inline">\(c_{ij} = 0\)</span> if <span class="math inline">\(i = j\)</span></p></li>
</ul></li>
<li><p>We could determine the expected amount of loss for an observation allocated to population i as <span class="math inline">\(\sum_j c_{ij} p_{ij}\)</span> where the <span class="math inline">\(p_{ij}s\)</span> are the probabilities of allocating an observation from population j into population i</p></li>
<li><p>We want to minimize the amount of loss expected for our rule. Using a Bayes Discrimination, allocate <span class="math inline">\(\mathbf{y}\)</span> to the population j which minimizes <span class="math inline">\(\sum_{k \neq j} c_{ij} \pi_k f_k(\mathbf{y})\)</span></p></li>
<li><p>We could assign equal probabilities to each group and get a maximum likelihood type rule. here, we would allocate <span class="math inline">\(\mathbf{y}\)</span> to population j which minimizes <span class="math inline">\(\sum_{k \neq j}c_{jk} f_k(\mathbf{y})\)</span></p></li>
</ul>
<p><strong>Example</strong>:</p>
<p>Two binomial populations, each of size 10, with probabilities <span class="math inline">\(p_1 = .5\)</span> and <span class="math inline">\(p_2 = .7\)</span></p>
<p>And the probability of being in the first population is .9</p>
<p>However, suppose the cost of inappropriately allocating into the first population is 1 and the cost of incorrectly allocating into the second population is 5.</p>
<p>In this case, we pick population 1 over population 2</p>
<p>In general, we consider two regions, <span class="math inline">\(R_1\)</span> and <span class="math inline">\(R_2\)</span> associated with population 1 and 2:</p>
<p><span class="math display">\[
R_1: \frac{f_1 (\mathbf{x})}{f_2 (\mathbf{x})} \ge \frac{c_{12} \pi_2}{c_{21} \pi_1}
\]</span></p>
<p><span class="math display">\[
R_2: \frac{f_1 (\mathbf{x})}{f_2 (\mathbf{x})} &lt; \frac{c_{12} \pi_2}{c_{21} \pi_1}
\]</span></p>
<p>where <span class="math inline">\(c_{12}\)</span> is the cost of assigning a member of population 2 to population 1.</p>
<p><br></p>
</div>
<div id="discrimination-under-estimation" class="section level4" number="21.4.1.3">
<h4><span class="header-section-number">21.4.1.3</span> Discrimination Under Estimation</h4>
<p>Suppose we know the form of the distributions for populations of interests, but we still have to estimate the parameters.</p>
<p>Example:</p>
<p>we know the distributions are multivariate normal, but we have to estimate the means and variances</p>
<p>The maximum likelihood discriminant rule allocates an observation <span class="math inline">\(\mathbf{y}\)</span> to population j when j maximizes the function</p>
<p><span class="math display">\[
f_j (\mathbf{y} |\hat{\theta})
\]</span></p>
<p>where <span class="math inline">\(\hat{\theta}\)</span> are the maximum likelihood estimates of the unknown parameters</p>
<p><br></p>
<p>For instance, we have 2 multivariate normal populations with distinct means, but common variance covariance matrix</p>
<p>MLEs for <span class="math inline">\(\mathbf{\mu}_1\)</span> and <span class="math inline">\(\mathbf{\mu}_2\)</span> are <span class="math inline">\(\mathbf{\bar{y}}_1\)</span> and <span class="math inline">\(\mathbf{\bar{y}}_2\)</span>and common <span class="math inline">\(\mathbf{\Sigma}\)</span> is <span class="math inline">\(\mathbf{S}\)</span>.</p>
<p>Thus, an estimated discriminant rule could be formed by substituting these sample values for the population values</p>
<p><br></p>
</div>
<div id="native-bayes" class="section level4" number="21.4.1.4">
<h4><span class="header-section-number">21.4.1.4</span> Native Bayes</h4>
<ul>
<li><p>The challenge with classification using Bayes’ is that we don’t know the (true) densities, <span class="math inline">\(f_k, k = 1, \dots, K\)</span>, while LDA and QDA make <strong>strong multivariate normality assumptions</strong> to deal with this.</p></li>
<li><p>Naive Bayes makes only one assumption: <strong>within the kth class, the p predictors are independent (i.e,, for</strong> <span class="math inline">\(k = 1,\dots, K\)</span></p></li>
</ul>
<p><span class="math display">\[
f_k(x) = f_{k1}(x_1) \times f_{k2}(x_2) \times \dots \times f_{kp}(x_p)
\]</span></p>
<p>where <span class="math inline">\(f_{kj}\)</span> is the density function of the j-th predictor among observation in the k-th class.</p>
<p>This assumption allows the use of joint distribution without the need to account for dependence between observations. However, this (native) assumption can be unrealistic, but still works well in cases where the number of sample (n) is not large relative to the number of features (p).</p>
<p>With this assumption, we have</p>
<p><span class="math display">\[
P(Y=k|X=x) = \frac{\pi_k \times f_{k1}(x_1) \times \dots \times f_{kp}(x_p)}{\sum_{l=1}^K \pi_l \times f_{l1}(x_1)\times \dots f_{lp}(x_p)}
\]</span></p>
<p>we only need to estimate the one-dimensional density function <span class="math inline">\(f_{kj}\)</span> with either of these approaches:</p>
<ul>
<li><p>When <span class="math inline">\(X_j\)</span> is quantitative, assume it has a univariate normal distribution (with independence): <span class="math inline">\(X_j | Y = k \sim N(\mu_{jk}, \sigma^2_{jk})\)</span> which is more restrictive than QDA because it assumes predictors are independent (e.g., a diagonal covariance matrix)</p></li>
<li><p>When <span class="math inline">\(X_j\)</span> is quantitative, use a kernel density estimator <a href="discriminant-analysis.html#kernel-methods">Kernel Methods</a> ; which is a smoothed histogram</p></li>
<li><p>When <span class="math inline">\(X_j\)</span> is qualitative, we count the promotion of training observations for the j-th predictor corresponding to each class.</p></li>
</ul>
<p><br></p>
</div>
<div id="comparison-of-classification-methods" class="section level4" number="21.4.1.5">
<h4><span class="header-section-number">21.4.1.5</span> Comparison of Classification Methods</h4>
<p>Assuming we have K classes and K is the baseline from (James , Witten, Hastie, and Tibshirani book)</p>
<p>Comparing the log odds relative to the K class</p>
<div id="logistic-regression-2" class="section level5" number="21.4.1.5.1">
<h5><span class="header-section-number">21.4.1.5.1</span> Logistic Regression</h5>
<p><span class="math display">\[
\log(\frac{P(Y=k|X = x)}{P(Y = K| X = x)}) = \beta_{k0} + \sum_{j=1}^p \beta_{kj}x_j
\]</span></p>
</div>
<div id="lda" class="section level5" number="21.4.1.5.2">
<h5><span class="header-section-number">21.4.1.5.2</span> LDA</h5>
<p><span class="math display">\[
\log(\frac{P(Y = k | X = x)}{P(Y = K | X = x)} = a_k + \sum_{j=1}^p b_{kj} x_j
\]</span></p>
<p>where <span class="math inline">\(a_k\)</span> and <span class="math inline">\(b_{kj}\)</span> are functions of <span class="math inline">\(\pi_k, \pi_K, \mu_k , \mu_K, \mathbf{\Sigma}\)</span></p>
<p>Similar to logistic regression, LDA assumes the log odds is linear in <span class="math inline">\(x\)</span></p>
<p>Even though they look like having the same form, the parameters in logistic regression are estimated by MLE, where as LDA linear parameters are specified by the prior and normal distributions</p>
<p>We expect LDA to outperform logistic regression when the normality assumption (approximately) holds, and logistic regression to perform better when it does not</p>
</div>
<div id="qda" class="section level5" number="21.4.1.5.3">
<h5><span class="header-section-number">21.4.1.5.3</span> QDA</h5>
<p><span class="math display">\[
\log(\frac{P(Y=k|X=x}{P(Y=K | X = x}) = a_k + \sum_{j=1}^{p}b_{kj}x_{j} + \sum_{j=1}^p \sum_{l=1}^p c_{kjl}x_j x_l
\]</span></p>
<p>where <span class="math inline">\(a_k, b_{kj}, c_{kjl}\)</span> are functions <span class="math inline">\(\pi_k , \pi_K, \mu_k, \mu_K ,\mathbf{\Sigma}_k, \mathbf{\Sigma}_K\)</span></p>
</div>
<div id="naive-bayes" class="section level5" number="21.4.1.5.4">
<h5><span class="header-section-number">21.4.1.5.4</span> Naive Bayes</h5>
<p><span class="math display">\[
\log (\frac{P(Y = k | X = x)}{P(Y = K | X = x}) = a_k + \sum_{j=1}^p g_{kj} (x_j)
\]</span></p>
<p>where <span class="math inline">\(a_k = \log (\pi_k / \pi_K)\)</span> and <span class="math inline">\(g_{kj}(x_j) = \log(\frac{f_{kj}(x_j)}{f_{Kj}(x_j)})\)</span> which is the form of generalized additive model</p>
<p><br></p>
</div>
<div id="summary-6" class="section level5" number="21.4.1.5.5">
<h5><span class="header-section-number">21.4.1.5.5</span> Summary</h5>
<ul>
<li><p>LDA is a special case of QDA</p></li>
<li><p>LDA is robust when it comes to high dimensions</p></li>
<li><p>Any classifier with a linear decision boundary is a special case of naive Bayes with <span class="math inline">\(g_{kj}(x_j) = b_{kj} x_j\)</span>, which means LDA is a special case of naive Bayes. LDA assumes that the features are normally distributed with a common within-class covariance matrix, and naive Bayes assumes independence of the features.</p></li>
<li><p>Naive bayes is also a special case of LDA with <span class="math inline">\(\mathbf{\Sigma}\)</span> restricted to a diagonal matrix with diagonals, <span class="math inline">\(\sigma^2\)</span> (another notation <span class="math inline">\(diag (\mathbf{\Sigma})\)</span> ) assuming <span class="math inline">\(f_{kj}(x_j) = N(\mu_{kj}, \sigma^2_j)\)</span></p></li>
<li><p>QDA and naive Bayes are not special case of each other. In principal,e naive Bayes can produce a more flexible fit by the choice of <span class="math inline">\(g_{kj}(x_j)\)</span> , but it’s restricted to only purely additive fit, but QDA includes multiplicative terms of the form <span class="math inline">\(c_{kjl}x_j x_l\)</span></p></li>
<li><p>None of these methods uniformly dominates the others: the choice of method depends on the true distribution of the predictors in each of the K classes, n and p (i.e., related to the bias-variance tradeoff).</p></li>
</ul>
<p>Compare to the non-parametric method (KNN)</p>
<ul>
<li><p>KNN would outperform both LDA and logistic regression when the decision boundary is highly nonlinear, but can’t say which predictors are most important, and requires many observations</p></li>
<li><p>KNN is also limited in high-dimensions due to the curse of dimensionality</p></li>
<li><p>Since QDA is a special type of nonlinear decision boundary (quadratic), it can be considered as a compromise between the linear methdos and KNN classification. QDA can have fewer training observations than KNN but not as flexible.</p></li>
</ul>
<p>From simulation:</p>
<table>
<colgroup>
<col width="64%" />
<col width="35%" />
</colgroup>
<thead>
<tr class="header">
<th>True decision boundary</th>
<th>Best performance</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear</td>
<td>LDA + Logistic regression</td>
</tr>
<tr class="even">
<td>Moderately nonlinear</td>
<td>QDA + Naive Bayes</td>
</tr>
<tr class="odd">
<td>Highly nonlinear (many training, p is not large)</td>
<td>KNN</td>
</tr>
</tbody>
</table>
<ul>
<li>like linear regression, we can also introduce flexibility by including transformed features <span class="math inline">\(\sqrt{X}, X^2, X^3\)</span></li>
</ul>
<p><br></p>
</div>
</div>
</div>
<div id="probabilities-of-misclassification" class="section level3" number="21.4.2">
<h3><span class="header-section-number">21.4.2</span> Probabilities of Misclassification</h3>
<p>When the distribution are exactly known, we can determine the misclassification probabilities exactly. however, when we need to estimate the population parameters, we have to estimate the probability of misclassification</p>
<ul>
<li><p>Naive method</p>
<ul>
<li><p>Plugging the parameters estimates into the form for the misclassification probabilities results to derive at the estimates of the misclassification probability.</p></li>
<li><p>But this will tend to be optimistic when the number of samples in one or more populations is small.</p></li>
</ul></li>
<li><p>Resubstitution method</p>
<ul>
<li><p>Use the proportion of the samples from population i that would be allocated to another population as an estimate of the misclassification probability</p></li>
<li><p>But also optimistic when the number of samples is small</p></li>
</ul></li>
<li><p>Jack-knife estimates:</p>
<ul>
<li><p>The above two methods use observation to estimate both parameters and also misclassification probabilities based upon the discriminant rule</p></li>
<li><p>Alternatively, we determine the discriminant rule based upon all of the data except the k-th observation from the j-th population</p></li>
<li><p>then, determine if the k-th observation would be misclassified under this rule</p></li>
<li><p>perform this process for all <span class="math inline">\(n_j\)</span> observation in population j . An estimate fo the misclassficaiton probability would be the fraction of <span class="math inline">\(n_j\)</span> observations which were misclassified</p></li>
<li><p>repeat the process for other <span class="math inline">\(i \neq j\)</span> populations</p></li>
<li><p>This method is more reliable than the others, but also computationally intensive</p></li>
</ul></li>
<li><p>Cross-Validation</p></li>
</ul>
<p><br></p>
<p><strong>Summary</strong></p>
<p>Consider the group-specific densities <span class="math inline">\(f_j (\mathbf{x})\)</span> for multivariate vector <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>Assume equal misclassification costs, the Bayes classification probability of <span class="math inline">\(\mathbf{x}\)</span> belonging to the j-th population is</p>
<p><span class="math display">\[
p(j |\mathbf{x}) = \frac{\pi_j f_j (\mathbf{x})}{\sum_{k=1}^h \pi_k f_k (\mathbf{x})}
\]</span></p>
<p><span class="math inline">\(j = 1,\dots, h\)</span></p>
<p>where there are <span class="math inline">\(h\)</span> possible groups.</p>
<p>We then classify into the group for which this probability of membership is largest</p>
<p>Alternatively, we can write this in terms of a <strong>generalized squared distance</strong> formation</p>
<p><span class="math display">\[
D_j^2 (\mathbf{x}) = d_j^2 (\mathbf{x})+ g_1(j) + g_2 (j)
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(d_j^2(\mathbf{x}) = (\mathbf{x} - \mathbf{\mu}_j)&#39; \mathbf{V}_j^{-1} (\mathbf{x} - \mathbf{\mu}_j)\)</span> is the squared Mahalanobis distance from <span class="math inline">\(\mathbf{x}\)</span> to the centroid of group j, and</p>
<ul>
<li><p><span class="math inline">\(\mathbf{V}_j = \mathbf{S}_j\)</span> if the within group covariance matrices are not equal</p></li>
<li><p><span class="math inline">\(\mathbf{V}_j = \mathbf{S}_p\)</span> if a pooled covariance estimate is appropriate</p></li>
</ul></li>
</ul>
<p>and</p>
<p><span class="math display">\[
g_1(j) =
\begin{cases}
\ln |\mathbf{S}_j| &amp; \text{within group covariances are not equal} \\
0 &amp; \text{pooled covariance}
\end{cases}
\]</span></p>
<p><span class="math display">\[
g_2(j) =
\begin{cases}
-2 \ln \pi_j &amp; \text{prior probabilities are not equal} \\
0 &amp; \text{prior probabilities are equal}
\end{cases}
\]</span></p>
<p>then, the posterior probability of belonging to group j is</p>
<p><span class="math display">\[
p(j| \mathbf{x})  = \frac{\exp(-.5 D_j^2(\mathbf{x}))}{\sum_{k=1}^h \exp(-.5 D^2_k (\mathbf{x}))}
\]</span></p>
<p>where <span class="math inline">\(j = 1,\dots , h\)</span></p>
<p>and <span class="math inline">\(\mathbf{x}\)</span> is classified into group j if <span class="math inline">\(p(j | \mathbf{x})\)</span> is largest for <span class="math inline">\(j = 1,\dots,h\)</span> (or, <span class="math inline">\(D_j^2(\mathbf{x})\)</span> is smallest).</p>
<p><br></p>
<div id="assessing-classification-performance" class="section level4" number="21.4.2.1">
<h4><span class="header-section-number">21.4.2.1</span> Assessing Classification Performance</h4>
<p>For binary classification, confusion matrix</p>
<table style="width:100%;">
<colgroup>
<col width="17%" />
<col width="24%" />
<col width="23%" />
<col width="23%" />
<col width="10%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Predicted class</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td>- or Null</td>
<td>+ or Null</td>
<td>Total</td>
</tr>
<tr class="even">
<td>True Class</td>
<td>- or Null</td>
<td>True Neg (TN)</td>
<td>False Pos (FP)</td>
<td>N</td>
</tr>
<tr class="odd">
<td></td>
<td>+ or Null</td>
<td>False Neg (FN)</td>
<td>True Pos (TP)</td>
<td>P</td>
</tr>
<tr class="even">
<td></td>
<td>Total</td>
<td>N*</td>
<td>P*</td>
<td></td>
</tr>
</tbody>
</table>
<p>and table 4.6 from <span class="citation">(<a href="#ref-james2013" role="doc-biblioref">James et al. 2013</a>)</span></p>
<table>
<colgroup>
<col width="23%" />
<col width="15%" />
<col width="61%" />
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Definition</th>
<th>Synonyms</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>False Pos rate</td>
<td>FP/N</td>
<td>Tyep I error, 1 0 SPecificity</td>
</tr>
<tr class="even">
<td>True Pos. rate</td>
<td>TP/P</td>
<td>1 - Type II error, power, sensitivity, recall</td>
</tr>
<tr class="odd">
<td>Pos Pred. value</td>
<td>TP/P*</td>
<td>Precision, 1 - false discovery promotion</td>
</tr>
<tr class="even">
<td>Neg. Pred. value</td>
<td>TN/N*</td>
<td></td>
</tr>
</tbody>
</table>
<p>ROC curve (receiver Operating Characteristics) is a graphical comparison between <strong>sensitivity</strong> (true positive) and <strong>specificity</strong> ( = 1 - false positive)</p>
<p>y-axis = true positive rate</p>
<p>x-axis = false positive rate</p>
<p>as we change the threshold rate for classifying an observation as from 0 to 1</p>
<p>AUC (area under the ROC) ideally would equal to 1, a bad classifier would have AUC = 0.5 (pure chance)</p>
<p><br></p>
</div>
</div>
<div id="unknown-populations-nonparametric-discrimination" class="section level3" number="21.4.3">
<h3><span class="header-section-number">21.4.3</span> Unknown Populations/ Nonparametric Discrimination</h3>
<p>When your multivariate data are not Gaussian, or known distributional form at all, we can use the following methods</p>
<div id="kernel-methods" class="section level4" number="21.4.3.1">
<h4><span class="header-section-number">21.4.3.1</span> Kernel Methods</h4>
<p>We approximate <span class="math inline">\(f_j (\mathbf{x})\)</span> by a kernel density estimate</p>
<p><span class="math display">\[
\hat{f}_j(\mathbf{x}) = \frac{1}{n_j} \sum_{i = 1}^{n_j} K_j (\mathbf{x} - \mathbf{x}_i)
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(K_j (.)\)</span> is a kernel function satisfying <span class="math inline">\(\int K_j(\mathbf{z})d\mathbf{z} =1\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{x}_i\)</span> , <span class="math inline">\(i = 1,\dots , n_j\)</span> is a random sample from the j-th population.</p></li>
</ul>
<p>Thus, after finding <span class="math inline">\(\hat{f}_j (\mathbf{x})\)</span> for each of the <span class="math inline">\(h\)</span> populations, the posterior probability of group membership is</p>
<p><span class="math display">\[
p(j |\mathbf{x}) = \frac{\pi_j \hat{f}_j (\mathbf{x})}{\sum_{k-1}^h \pi_k \hat{f}_k (\mathbf{x})}
\]</span></p>
<p>where <span class="math inline">\(j = 1,\dots, h\)</span></p>
<p>There are different choices for the kernel function:</p>
<ul>
<li><p>Uniform</p></li>
<li><p>Normal</p></li>
<li><p>Epanechnikov</p></li>
<li><p>Biweight</p></li>
<li><p>Triweight</p></li>
</ul>
<p>We these kernels, we have to pick the “radius” (or variance, width, window width, bandwidth) of the kernel, which is a smoothing parameter (the larger the radius, the more smooth the kernel estimate of the density).</p>
<p>To select the smoothness parameter, we can use the following method</p>
<p>If we believe the populations were close to multivariate normal, then</p>
<p><span class="math display">\[
R = (\frac{4/(2p+1)}{n_j})^{1/(p+1}
\]</span></p>
<p>But since we do not know for sure, we might choose several different values and select one that vies the best out of sample or cross-validation discrimination.</p>
<p>Moreover, you also have to decide whether to use different kernel smoothness for different populations, which is similar to the individual and pooled covariances in the classical methodology.</p>
<p><br></p>
</div>
<div id="nearest-neighbor-methods" class="section level4" number="21.4.3.2">
<h4><span class="header-section-number">21.4.3.2</span> Nearest Neighbor Methods</h4>
<p>The nearest neighbor (also known as k-nearest neighbor) method performs the classification of a new observation vector based on the group membership of its nearest neighbors. In practice, we find</p>
<p><span class="math display">\[
d_{ij}^2 (\mathbf{x}, \mathbf{x}_i) = (\mathbf{x}, \mathbf{x}_i) V_j^{-1}(\mathbf{x}, \mathbf{x}_i)
\]</span></p>
<p>which is the distance between the vector <span class="math inline">\(\mathbf{x}\)</span> and the i-th observation in group j</p>
<p>We consider different choices for <span class="math inline">\(\mathbf{V}_j\)</span></p>
<p>For example,</p>
<p><span class="math display">\[
\mathbf{V}_j = \mathbf{S}_p \\
\mathbf{V}_j = \mathbf{S}_j \\
\mathbf{V}_j = \mathbf{I} \\
\mathbf{V}_j = diag (\mathbf{S}_p)
\]</span></p>
<p>We find the <span class="math inline">\(k\)</span> observations that are closest to <span class="math inline">\(\mathbf{x}\)</span> (where users pick k). Then we classify into the most common population, weighted by the prior.</p>
<p><br></p>
</div>
<div id="modern-discriminant-methods" class="section level4" number="21.4.3.3">
<h4><span class="header-section-number">21.4.3.3</span> Modern Discriminant Methods</h4>
<p>Note:</p>
<p>Logistic regression (with or without random effects) is a flexible model-based procedure for classification between two populations.</p>
<p>The extension of logistic regression to the multi-group setting is polychotomous logistic regression (or, mulinomial regression).</p>
<p>The machine learning and pattern recognition are growing with strong focus on nonlienar discriminant analysis methods such as:</p>
<ul>
<li><p>radial basis function networks</p></li>
<li><p>support vector machines</p></li>
<li><p>multiplayer perceptrons (neural networks)</p></li>
</ul>
<p><br></p>
<p>The general framework</p>
<p><span class="math display">\[
g_j (\mathbf{x}) = \sum_{l = 1}^m w_{jl}\phi_l (\mathbf{x}; \mathbf{\theta}_l) + w_{j0}
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(j = 1,\dots, h\)</span></p></li>
<li><p><span class="math inline">\(m\)</span> nonlinear basis functions <span class="math inline">\(\phi_l\)</span>, each of which has <span class="math inline">\(n_m\)</span> parameters given by <span class="math inline">\(\theta_l = \{ \theta_{lk}: k = 1, \dots , n_m \}\)</span></p></li>
</ul>
<p>We assign <span class="math inline">\(\mathbf{x}\)</span> to the j-th population if <span class="math inline">\(g_j(\mathbf{x})\)</span> is the maximum for all <span class="math inline">\(j = 1,\dots, h\)</span></p>
<p>Development usually focuses on the choice and estimation of the basis functions, <span class="math inline">\(\phi_l\)</span> and the estimation of the weights <span class="math inline">\(w_{jl}\)</span></p>
<p>More details can be found <span class="citation">(<a href="#ref-webb2002" role="doc-biblioref">Webb 2002</a>)</span></p>
<p><br></p>
</div>
</div>
<div id="application-12" class="section level3" number="21.4.4">
<h3><span class="header-section-number">21.4.4</span> Application</h3>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb366-1"><a href="discriminant-analysis.html#cb366-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(class)</span>
<span id="cb366-2"><a href="discriminant-analysis.html#cb366-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(klaR)</span>
<span id="cb366-3"><a href="discriminant-analysis.html#cb366-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb366-4"><a href="discriminant-analysis.html#cb366-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb366-5"><a href="discriminant-analysis.html#cb366-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb366-6"><a href="discriminant-analysis.html#cb366-6" aria-hidden="true" tabindex="-1"></a><span class="do">## Read in the data</span></span>
<span id="cb366-7"><a href="discriminant-analysis.html#cb366-7" aria-hidden="true" tabindex="-1"></a>crops <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="st">&quot;images/crops.txt&quot;</span>)</span>
<span id="cb366-8"><a href="discriminant-analysis.html#cb366-8" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(crops) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;crop&quot;</span>, <span class="st">&quot;y1&quot;</span>, <span class="st">&quot;y2&quot;</span>, <span class="st">&quot;y3&quot;</span>, <span class="st">&quot;y4&quot;</span>)</span>
<span id="cb366-9"><a href="discriminant-analysis.html#cb366-9" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(crops)</span>
<span id="cb366-10"><a href="discriminant-analysis.html#cb366-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; &#39;data.frame&#39;:    36 obs. of  5 variables:</span></span>
<span id="cb366-11"><a href="discriminant-analysis.html#cb366-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ crop: chr  &quot;Corn&quot; &quot;Corn&quot; &quot;Corn&quot; &quot;Corn&quot; ...</span></span>
<span id="cb366-12"><a href="discriminant-analysis.html#cb366-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ y1  : int  16 15 16 18 15 15 12 20 24 21 ...</span></span>
<span id="cb366-13"><a href="discriminant-analysis.html#cb366-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ y2  : int  27 23 27 20 15 32 15 23 24 25 ...</span></span>
<span id="cb366-14"><a href="discriminant-analysis.html#cb366-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ y3  : int  31 30 27 25 31 32 16 23 25 23 ...</span></span>
<span id="cb366-15"><a href="discriminant-analysis.html#cb366-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ y4  : int  33 30 26 23 32 15 73 25 32 24 ...</span></span>
<span id="cb366-16"><a href="discriminant-analysis.html#cb366-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb366-17"><a href="discriminant-analysis.html#cb366-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb366-18"><a href="discriminant-analysis.html#cb366-18" aria-hidden="true" tabindex="-1"></a><span class="do">## Read in test data</span></span>
<span id="cb366-19"><a href="discriminant-analysis.html#cb366-19" aria-hidden="true" tabindex="-1"></a>crops_test <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="st">&quot;images/crops_test.txt&quot;</span>)</span>
<span id="cb366-20"><a href="discriminant-analysis.html#cb366-20" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(crops_test) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;crop&quot;</span>, <span class="st">&quot;y1&quot;</span>, <span class="st">&quot;y2&quot;</span>, <span class="st">&quot;y3&quot;</span>, <span class="st">&quot;y4&quot;</span>)</span>
<span id="cb366-21"><a href="discriminant-analysis.html#cb366-21" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(crops_test)</span>
<span id="cb366-22"><a href="discriminant-analysis.html#cb366-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; &#39;data.frame&#39;:    5 obs. of  5 variables:</span></span>
<span id="cb366-23"><a href="discriminant-analysis.html#cb366-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ crop: chr  &quot;Corn&quot; &quot;Soybeans&quot; &quot;Cotton&quot; &quot;Sugarbeets&quot; ...</span></span>
<span id="cb366-24"><a href="discriminant-analysis.html#cb366-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ y1  : int  16 21 29 54 32</span></span>
<span id="cb366-25"><a href="discriminant-analysis.html#cb366-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ y2  : int  27 25 24 23 32</span></span>
<span id="cb366-26"><a href="discriminant-analysis.html#cb366-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ y3  : int  31 23 26 21 62</span></span>
<span id="cb366-27"><a href="discriminant-analysis.html#cb366-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ y4  : int  33 24 28 54 16</span></span></code></pre></div>
<div id="lda-1" class="section level4" number="21.4.4.1">
<h4><span class="header-section-number">21.4.4.1</span> LDA</h4>
<p>Default prior is proportional to sample size and <code>lda</code> and <code>qda</code> do not fit a constant or intercept term</p>
<div class="sourceCode" id="cb367"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb367-1"><a href="discriminant-analysis.html#cb367-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Linear discriminant analysis</span></span>
<span id="cb367-2"><a href="discriminant-analysis.html#cb367-2" aria-hidden="true" tabindex="-1"></a>lda_mod <span class="ot">&lt;-</span> <span class="fu">lda</span>(crop <span class="sc">~</span> y1 <span class="sc">+</span> y2 <span class="sc">+</span> y3 <span class="sc">+</span> y4,</span>
<span id="cb367-3"><a href="discriminant-analysis.html#cb367-3" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> crops)</span>
<span id="cb367-4"><a href="discriminant-analysis.html#cb367-4" aria-hidden="true" tabindex="-1"></a>lda_mod</span>
<span id="cb367-5"><a href="discriminant-analysis.html#cb367-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb367-6"><a href="discriminant-analysis.html#cb367-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; lda(crop ~ y1 + y2 + y3 + y4, data = crops)</span></span>
<span id="cb367-7"><a href="discriminant-analysis.html#cb367-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb367-8"><a href="discriminant-analysis.html#cb367-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Prior probabilities of groups:</span></span>
<span id="cb367-9"><a href="discriminant-analysis.html#cb367-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     Clover       Corn     Cotton   Soybeans Sugarbeets </span></span>
<span id="cb367-10"><a href="discriminant-analysis.html#cb367-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  0.3055556  0.1944444  0.1666667  0.1666667  0.1666667 </span></span>
<span id="cb367-11"><a href="discriminant-analysis.html#cb367-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb367-12"><a href="discriminant-analysis.html#cb367-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Group means:</span></span>
<span id="cb367-13"><a href="discriminant-analysis.html#cb367-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;                  y1       y2       y3       y4</span></span>
<span id="cb367-14"><a href="discriminant-analysis.html#cb367-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Clover     46.36364 32.63636 34.18182 36.63636</span></span>
<span id="cb367-15"><a href="discriminant-analysis.html#cb367-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Corn       15.28571 22.71429 27.42857 33.14286</span></span>
<span id="cb367-16"><a href="discriminant-analysis.html#cb367-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Cotton     34.50000 32.66667 35.00000 39.16667</span></span>
<span id="cb367-17"><a href="discriminant-analysis.html#cb367-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Soybeans   21.00000 27.00000 23.50000 29.66667</span></span>
<span id="cb367-18"><a href="discriminant-analysis.html#cb367-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Sugarbeets 31.00000 32.16667 20.00000 40.50000</span></span>
<span id="cb367-19"><a href="discriminant-analysis.html#cb367-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb367-20"><a href="discriminant-analysis.html#cb367-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Coefficients of linear discriminants:</span></span>
<span id="cb367-21"><a href="discriminant-analysis.html#cb367-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;              LD1          LD2         LD3          LD4</span></span>
<span id="cb367-22"><a href="discriminant-analysis.html#cb367-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; y1 -6.147360e-02  0.009215431 -0.02987075 -0.014680566</span></span>
<span id="cb367-23"><a href="discriminant-analysis.html#cb367-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; y2 -2.548964e-02  0.042838972  0.04631489  0.054842132</span></span>
<span id="cb367-24"><a href="discriminant-analysis.html#cb367-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; y3  1.642126e-02 -0.079471595  0.01971222  0.008938745</span></span>
<span id="cb367-25"><a href="discriminant-analysis.html#cb367-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; y4  5.143616e-05 -0.013917423  0.05381787 -0.025717667</span></span>
<span id="cb367-26"><a href="discriminant-analysis.html#cb367-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb367-27"><a href="discriminant-analysis.html#cb367-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Proportion of trace:</span></span>
<span id="cb367-28"><a href="discriminant-analysis.html#cb367-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;    LD1    LD2    LD3    LD4 </span></span>
<span id="cb367-29"><a href="discriminant-analysis.html#cb367-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 0.7364 0.1985 0.0576 0.0075</span></span>
<span id="cb367-30"><a href="discriminant-analysis.html#cb367-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb367-31"><a href="discriminant-analysis.html#cb367-31" aria-hidden="true" tabindex="-1"></a><span class="do">## Look at accuracy on the training data</span></span>
<span id="cb367-32"><a href="discriminant-analysis.html#cb367-32" aria-hidden="true" tabindex="-1"></a>lda_fitted <span class="ot">&lt;-</span> <span class="fu">predict</span>(lda_mod,<span class="at">newdata =</span> crops)</span>
<span id="cb367-33"><a href="discriminant-analysis.html#cb367-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Contingency table</span></span>
<span id="cb367-34"><a href="discriminant-analysis.html#cb367-34" aria-hidden="true" tabindex="-1"></a>lda_table <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="at">truth =</span> crops<span class="sc">$</span>crop, <span class="at">fitted =</span> lda_fitted<span class="sc">$</span>class)</span>
<span id="cb367-35"><a href="discriminant-analysis.html#cb367-35" aria-hidden="true" tabindex="-1"></a>lda_table</span>
<span id="cb367-36"><a href="discriminant-analysis.html#cb367-36" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             fitted</span></span>
<span id="cb367-37"><a href="discriminant-analysis.html#cb367-37" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; truth        Clover Corn Cotton Soybeans Sugarbeets</span></span>
<span id="cb367-38"><a href="discriminant-analysis.html#cb367-38" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Clover          6    0      3        0          2</span></span>
<span id="cb367-39"><a href="discriminant-analysis.html#cb367-39" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Corn            0    6      0        1          0</span></span>
<span id="cb367-40"><a href="discriminant-analysis.html#cb367-40" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Cotton          3    0      1        2          0</span></span>
<span id="cb367-41"><a href="discriminant-analysis.html#cb367-41" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Soybeans        0    1      1        3          1</span></span>
<span id="cb367-42"><a href="discriminant-analysis.html#cb367-42" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Sugarbeets      1    1      0        2          2</span></span>
<span id="cb367-43"><a href="discriminant-analysis.html#cb367-43" aria-hidden="true" tabindex="-1"></a><span class="co"># accuracy of 0.5 is just random (not good)</span></span>
<span id="cb367-44"><a href="discriminant-analysis.html#cb367-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb367-45"><a href="discriminant-analysis.html#cb367-45" aria-hidden="true" tabindex="-1"></a><span class="do">## Posterior probabilities of membership</span></span>
<span id="cb367-46"><a href="discriminant-analysis.html#cb367-46" aria-hidden="true" tabindex="-1"></a>crops_post <span class="ot">&lt;-</span> <span class="fu">cbind.data.frame</span>(crops,</span>
<span id="cb367-47"><a href="discriminant-analysis.html#cb367-47" aria-hidden="true" tabindex="-1"></a>                               <span class="at">crop_pred =</span> lda_fitted<span class="sc">$</span>class,</span>
<span id="cb367-48"><a href="discriminant-analysis.html#cb367-48" aria-hidden="true" tabindex="-1"></a>                               lda_fitted<span class="sc">$</span>posterior)</span>
<span id="cb367-49"><a href="discriminant-analysis.html#cb367-49" aria-hidden="true" tabindex="-1"></a>crops_post <span class="ot">&lt;-</span> crops_post <span class="sc">%&gt;%</span></span>
<span id="cb367-50"><a href="discriminant-analysis.html#cb367-50" aria-hidden="true" tabindex="-1"></a>    <span class="fu">mutate</span>(<span class="at">missed =</span> crop <span class="sc">!=</span> crop_pred)</span>
<span id="cb367-51"><a href="discriminant-analysis.html#cb367-51" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(crops_post)</span>
<span id="cb367-52"><a href="discriminant-analysis.html#cb367-52" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   crop y1 y2 y3 y4 crop_pred     Clover      Corn    Cotton  Soybeans</span></span>
<span id="cb367-53"><a href="discriminant-analysis.html#cb367-53" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 Corn 16 27 31 33      Corn 0.08935164 0.4054296 0.1763189 0.2391845</span></span>
<span id="cb367-54"><a href="discriminant-analysis.html#cb367-54" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2 Corn 15 23 30 30      Corn 0.07690181 0.4558027 0.1420920 0.2530101</span></span>
<span id="cb367-55"><a href="discriminant-analysis.html#cb367-55" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 3 Corn 16 27 27 26      Corn 0.09817815 0.3422454 0.1365315 0.3073105</span></span>
<span id="cb367-56"><a href="discriminant-analysis.html#cb367-56" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 4 Corn 18 20 25 23      Corn 0.10521511 0.3633673 0.1078076 0.3281477</span></span>
<span id="cb367-57"><a href="discriminant-analysis.html#cb367-57" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 5 Corn 15 15 31 32      Corn 0.05879921 0.5753907 0.1173332 0.2086696</span></span>
<span id="cb367-58"><a href="discriminant-analysis.html#cb367-58" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 6 Corn 15 32 32 15  Soybeans 0.09723648 0.3278382 0.1318370 0.3419924</span></span>
<span id="cb367-59"><a href="discriminant-analysis.html#cb367-59" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Sugarbeets missed</span></span>
<span id="cb367-60"><a href="discriminant-analysis.html#cb367-60" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 0.08971545  FALSE</span></span>
<span id="cb367-61"><a href="discriminant-analysis.html#cb367-61" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2 0.07219340  FALSE</span></span>
<span id="cb367-62"><a href="discriminant-analysis.html#cb367-62" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 3 0.11573442  FALSE</span></span>
<span id="cb367-63"><a href="discriminant-analysis.html#cb367-63" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 4 0.09546233  FALSE</span></span>
<span id="cb367-64"><a href="discriminant-analysis.html#cb367-64" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 5 0.03980738  FALSE</span></span>
<span id="cb367-65"><a href="discriminant-analysis.html#cb367-65" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 6 0.10109590   TRUE</span></span>
<span id="cb367-66"><a href="discriminant-analysis.html#cb367-66" aria-hidden="true" tabindex="-1"></a><span class="co"># posterior shows that posterior of corn membershp is much higher than the prior</span></span>
<span id="cb367-67"><a href="discriminant-analysis.html#cb367-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb367-68"><a href="discriminant-analysis.html#cb367-68" aria-hidden="true" tabindex="-1"></a><span class="do">## LOOCV</span></span>
<span id="cb367-69"><a href="discriminant-analysis.html#cb367-69" aria-hidden="true" tabindex="-1"></a><span class="co"># leave-one-out cross validation for linear discriminant analysis</span></span>
<span id="cb367-70"><a href="discriminant-analysis.html#cb367-70" aria-hidden="true" tabindex="-1"></a><span class="co"># cannot run the prdecit function using the object with CV = TRUE because it returns the wihtin sample predictions</span></span>
<span id="cb367-71"><a href="discriminant-analysis.html#cb367-71" aria-hidden="true" tabindex="-1"></a>lda_cv <span class="ot">&lt;-</span> <span class="fu">lda</span>(crop <span class="sc">~</span> y1 <span class="sc">+</span> y2 <span class="sc">+</span> y3 <span class="sc">+</span> y4,</span>
<span id="cb367-72"><a href="discriminant-analysis.html#cb367-72" aria-hidden="true" tabindex="-1"></a>              <span class="at">data =</span> crops, <span class="at">CV =</span> <span class="cn">TRUE</span>)</span>
<span id="cb367-73"><a href="discriminant-analysis.html#cb367-73" aria-hidden="true" tabindex="-1"></a><span class="co"># Contingency table</span></span>
<span id="cb367-74"><a href="discriminant-analysis.html#cb367-74" aria-hidden="true" tabindex="-1"></a>lda_table_cv <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="at">truth =</span> crops<span class="sc">$</span>crop, <span class="at">fitted =</span> lda_cv<span class="sc">$</span>class)</span>
<span id="cb367-75"><a href="discriminant-analysis.html#cb367-75" aria-hidden="true" tabindex="-1"></a>lda_table_cv</span>
<span id="cb367-76"><a href="discriminant-analysis.html#cb367-76" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             fitted</span></span>
<span id="cb367-77"><a href="discriminant-analysis.html#cb367-77" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; truth        Clover Corn Cotton Soybeans Sugarbeets</span></span>
<span id="cb367-78"><a href="discriminant-analysis.html#cb367-78" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Clover          4    3      1        0          3</span></span>
<span id="cb367-79"><a href="discriminant-analysis.html#cb367-79" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Corn            0    4      1        2          0</span></span>
<span id="cb367-80"><a href="discriminant-analysis.html#cb367-80" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Cotton          3    0      0        2          1</span></span>
<span id="cb367-81"><a href="discriminant-analysis.html#cb367-81" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Soybeans        0    1      1        3          1</span></span>
<span id="cb367-82"><a href="discriminant-analysis.html#cb367-82" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Sugarbeets      2    1      0        2          1</span></span>
<span id="cb367-83"><a href="discriminant-analysis.html#cb367-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb367-84"><a href="discriminant-analysis.html#cb367-84" aria-hidden="true" tabindex="-1"></a><span class="do">## Predict the test data</span></span>
<span id="cb367-85"><a href="discriminant-analysis.html#cb367-85" aria-hidden="true" tabindex="-1"></a>lda_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(lda_mod, <span class="at">newdata =</span> crops_test)</span>
<span id="cb367-86"><a href="discriminant-analysis.html#cb367-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb367-87"><a href="discriminant-analysis.html#cb367-87" aria-hidden="true" tabindex="-1"></a><span class="do">## Make a contingency table with truth and most likely class</span></span>
<span id="cb367-88"><a href="discriminant-analysis.html#cb367-88" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="at">truth=</span>crops_test<span class="sc">$</span>crop, <span class="at">predict=</span>lda_pred<span class="sc">$</span>class)</span>
<span id="cb367-89"><a href="discriminant-analysis.html#cb367-89" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             predict</span></span>
<span id="cb367-90"><a href="discriminant-analysis.html#cb367-90" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; truth        Clover Corn Cotton Soybeans Sugarbeets</span></span>
<span id="cb367-91"><a href="discriminant-analysis.html#cb367-91" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Clover          0    0      1        0          0</span></span>
<span id="cb367-92"><a href="discriminant-analysis.html#cb367-92" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Corn            0    1      0        0          0</span></span>
<span id="cb367-93"><a href="discriminant-analysis.html#cb367-93" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Cotton          0    0      0        1          0</span></span>
<span id="cb367-94"><a href="discriminant-analysis.html#cb367-94" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Soybeans        0    0      0        1          0</span></span>
<span id="cb367-95"><a href="discriminant-analysis.html#cb367-95" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Sugarbeets      1    0      0        0          0</span></span></code></pre></div>
<p>LDA didn’t do well on both within sample and out-of-sample data.</p>
</div>
<div id="qda-1" class="section level4" number="21.4.4.2">
<h4><span class="header-section-number">21.4.4.2</span> QDA</h4>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb368-1"><a href="discriminant-analysis.html#cb368-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Quadratic discriminant analysis</span></span>
<span id="cb368-2"><a href="discriminant-analysis.html#cb368-2" aria-hidden="true" tabindex="-1"></a>qda_mod <span class="ot">&lt;-</span> <span class="fu">qda</span>(crop <span class="sc">~</span> y1 <span class="sc">+</span> y2 <span class="sc">+</span> y3 <span class="sc">+</span> y4,</span>
<span id="cb368-3"><a href="discriminant-analysis.html#cb368-3" aria-hidden="true" tabindex="-1"></a>               <span class="at">data =</span> crops)</span>
<span id="cb368-4"><a href="discriminant-analysis.html#cb368-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb368-5"><a href="discriminant-analysis.html#cb368-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Look at accuracy on the training data</span></span>
<span id="cb368-6"><a href="discriminant-analysis.html#cb368-6" aria-hidden="true" tabindex="-1"></a>qda_fitted <span class="ot">&lt;-</span> <span class="fu">predict</span>(qda_mod, <span class="at">newdata =</span> crops)</span>
<span id="cb368-7"><a href="discriminant-analysis.html#cb368-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Contingency table</span></span>
<span id="cb368-8"><a href="discriminant-analysis.html#cb368-8" aria-hidden="true" tabindex="-1"></a>qda_table <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="at">truth =</span> crops<span class="sc">$</span>crop, <span class="at">fitted =</span> qda_fitted<span class="sc">$</span>class)</span>
<span id="cb368-9"><a href="discriminant-analysis.html#cb368-9" aria-hidden="true" tabindex="-1"></a>qda_table</span>
<span id="cb368-10"><a href="discriminant-analysis.html#cb368-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             fitted</span></span>
<span id="cb368-11"><a href="discriminant-analysis.html#cb368-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; truth        Clover Corn Cotton Soybeans Sugarbeets</span></span>
<span id="cb368-12"><a href="discriminant-analysis.html#cb368-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Clover          9    0      0        0          2</span></span>
<span id="cb368-13"><a href="discriminant-analysis.html#cb368-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Corn            0    7      0        0          0</span></span>
<span id="cb368-14"><a href="discriminant-analysis.html#cb368-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Cotton          0    0      6        0          0</span></span>
<span id="cb368-15"><a href="discriminant-analysis.html#cb368-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Soybeans        0    0      0        6          0</span></span>
<span id="cb368-16"><a href="discriminant-analysis.html#cb368-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Sugarbeets      0    0      1        1          4</span></span>
<span id="cb368-17"><a href="discriminant-analysis.html#cb368-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb368-18"><a href="discriminant-analysis.html#cb368-18" aria-hidden="true" tabindex="-1"></a><span class="do">## LOOCV</span></span>
<span id="cb368-19"><a href="discriminant-analysis.html#cb368-19" aria-hidden="true" tabindex="-1"></a>qda_cv <span class="ot">&lt;-</span> <span class="fu">qda</span>(crop <span class="sc">~</span> y1 <span class="sc">+</span> y2 <span class="sc">+</span> y3 <span class="sc">+</span> y4,</span>
<span id="cb368-20"><a href="discriminant-analysis.html#cb368-20" aria-hidden="true" tabindex="-1"></a>              <span class="at">data =</span> crops, <span class="at">CV =</span> <span class="cn">TRUE</span>)</span>
<span id="cb368-21"><a href="discriminant-analysis.html#cb368-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Contingency table</span></span>
<span id="cb368-22"><a href="discriminant-analysis.html#cb368-22" aria-hidden="true" tabindex="-1"></a>qda_table_cv <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="at">truth =</span> crops<span class="sc">$</span>crop, <span class="at">fitted =</span> qda_cv<span class="sc">$</span>class)</span>
<span id="cb368-23"><a href="discriminant-analysis.html#cb368-23" aria-hidden="true" tabindex="-1"></a>qda_table_cv</span>
<span id="cb368-24"><a href="discriminant-analysis.html#cb368-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             fitted</span></span>
<span id="cb368-25"><a href="discriminant-analysis.html#cb368-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; truth        Clover Corn Cotton Soybeans Sugarbeets</span></span>
<span id="cb368-26"><a href="discriminant-analysis.html#cb368-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Clover          9    0      0        0          2</span></span>
<span id="cb368-27"><a href="discriminant-analysis.html#cb368-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Corn            3    2      0        0          2</span></span>
<span id="cb368-28"><a href="discriminant-analysis.html#cb368-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Cotton          3    0      2        0          1</span></span>
<span id="cb368-29"><a href="discriminant-analysis.html#cb368-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Soybeans        3    0      0        2          1</span></span>
<span id="cb368-30"><a href="discriminant-analysis.html#cb368-30" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Sugarbeets      3    0      1        1          1</span></span>
<span id="cb368-31"><a href="discriminant-analysis.html#cb368-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb368-32"><a href="discriminant-analysis.html#cb368-32" aria-hidden="true" tabindex="-1"></a><span class="do">## Predict the test data</span></span>
<span id="cb368-33"><a href="discriminant-analysis.html#cb368-33" aria-hidden="true" tabindex="-1"></a>qda_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(qda_mod, <span class="at">newdata =</span> crops_test)</span>
<span id="cb368-34"><a href="discriminant-analysis.html#cb368-34" aria-hidden="true" tabindex="-1"></a><span class="do">## Make a contingency table with truth and most likely class</span></span>
<span id="cb368-35"><a href="discriminant-analysis.html#cb368-35" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="at">truth =</span> crops_test<span class="sc">$</span>crop, <span class="at">predict =</span> qda_pred<span class="sc">$</span>class)</span>
<span id="cb368-36"><a href="discriminant-analysis.html#cb368-36" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             predict</span></span>
<span id="cb368-37"><a href="discriminant-analysis.html#cb368-37" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; truth        Clover Corn Cotton Soybeans Sugarbeets</span></span>
<span id="cb368-38"><a href="discriminant-analysis.html#cb368-38" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Clover          1    0      0        0          0</span></span>
<span id="cb368-39"><a href="discriminant-analysis.html#cb368-39" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Corn            0    1      0        0          0</span></span>
<span id="cb368-40"><a href="discriminant-analysis.html#cb368-40" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Cotton          0    0      1        0          0</span></span>
<span id="cb368-41"><a href="discriminant-analysis.html#cb368-41" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Soybeans        0    0      0        1          0</span></span>
<span id="cb368-42"><a href="discriminant-analysis.html#cb368-42" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Sugarbeets      0    0      0        0          1</span></span></code></pre></div>
</div>
<div id="knn-1" class="section level4" number="21.4.4.3">
<h4><span class="header-section-number">21.4.4.3</span> KNN</h4>
<p><code>knn</code> uses design matrices of the features.</p>
<div class="sourceCode" id="cb369"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb369-1"><a href="discriminant-analysis.html#cb369-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Design matrices</span></span>
<span id="cb369-2"><a href="discriminant-analysis.html#cb369-2" aria-hidden="true" tabindex="-1"></a>X_train <span class="ot">&lt;-</span> crops <span class="sc">%&gt;%</span></span>
<span id="cb369-3"><a href="discriminant-analysis.html#cb369-3" aria-hidden="true" tabindex="-1"></a>    dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>crop)</span>
<span id="cb369-4"><a href="discriminant-analysis.html#cb369-4" aria-hidden="true" tabindex="-1"></a>X_test <span class="ot">&lt;-</span> crops_test <span class="sc">%&gt;%</span></span>
<span id="cb369-5"><a href="discriminant-analysis.html#cb369-5" aria-hidden="true" tabindex="-1"></a>    dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>crop)</span>
<span id="cb369-6"><a href="discriminant-analysis.html#cb369-6" aria-hidden="true" tabindex="-1"></a>Y_train <span class="ot">&lt;-</span> crops<span class="sc">$</span>crop</span>
<span id="cb369-7"><a href="discriminant-analysis.html#cb369-7" aria-hidden="true" tabindex="-1"></a>Y_test <span class="ot">&lt;-</span> crops_test<span class="sc">$</span>crop</span>
<span id="cb369-8"><a href="discriminant-analysis.html#cb369-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb369-9"><a href="discriminant-analysis.html#cb369-9" aria-hidden="true" tabindex="-1"></a><span class="do">## Nearest neighbors with 2 neighbors</span></span>
<span id="cb369-10"><a href="discriminant-analysis.html#cb369-10" aria-hidden="true" tabindex="-1"></a>knn_2 <span class="ot">&lt;-</span> <span class="fu">knn</span>(X_train, X_train, Y_train, <span class="at">k =</span> <span class="dv">2</span>)</span>
<span id="cb369-11"><a href="discriminant-analysis.html#cb369-11" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="at">truth =</span> Y_train, <span class="at">fitted =</span> knn_2)</span>
<span id="cb369-12"><a href="discriminant-analysis.html#cb369-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             fitted</span></span>
<span id="cb369-13"><a href="discriminant-analysis.html#cb369-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; truth        Clover Corn Cotton Soybeans Sugarbeets</span></span>
<span id="cb369-14"><a href="discriminant-analysis.html#cb369-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Clover         10    0      1        0          0</span></span>
<span id="cb369-15"><a href="discriminant-analysis.html#cb369-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Corn            0    7      0        0          0</span></span>
<span id="cb369-16"><a href="discriminant-analysis.html#cb369-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Cotton          1    0      2        0          3</span></span>
<span id="cb369-17"><a href="discriminant-analysis.html#cb369-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Soybeans        0    0      0        5          1</span></span>
<span id="cb369-18"><a href="discriminant-analysis.html#cb369-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Sugarbeets      1    0      2        1          2</span></span>
<span id="cb369-19"><a href="discriminant-analysis.html#cb369-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb369-20"><a href="discriminant-analysis.html#cb369-20" aria-hidden="true" tabindex="-1"></a><span class="do">## Accuracy</span></span>
<span id="cb369-21"><a href="discriminant-analysis.html#cb369-21" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(Y_train<span class="sc">==</span>knn_2)</span>
<span id="cb369-22"><a href="discriminant-analysis.html#cb369-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.7222222</span></span>
<span id="cb369-23"><a href="discriminant-analysis.html#cb369-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb369-24"><a href="discriminant-analysis.html#cb369-24" aria-hidden="true" tabindex="-1"></a><span class="do">## Performance on test data</span></span>
<span id="cb369-25"><a href="discriminant-analysis.html#cb369-25" aria-hidden="true" tabindex="-1"></a>knn_2_test <span class="ot">&lt;-</span> <span class="fu">knn</span>(X_train, X_test, Y_train, <span class="at">k =</span> <span class="dv">2</span>)</span>
<span id="cb369-26"><a href="discriminant-analysis.html#cb369-26" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="at">truth =</span> Y_test, <span class="at">predict =</span> knn_2_test)</span>
<span id="cb369-27"><a href="discriminant-analysis.html#cb369-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             predict</span></span>
<span id="cb369-28"><a href="discriminant-analysis.html#cb369-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; truth        Clover Corn Cotton Soybeans Sugarbeets</span></span>
<span id="cb369-29"><a href="discriminant-analysis.html#cb369-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Clover          1    0      0        0          0</span></span>
<span id="cb369-30"><a href="discriminant-analysis.html#cb369-30" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Corn            0    1      0        0          0</span></span>
<span id="cb369-31"><a href="discriminant-analysis.html#cb369-31" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Cotton          0    0      0        0          1</span></span>
<span id="cb369-32"><a href="discriminant-analysis.html#cb369-32" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Soybeans        0    0      0        1          0</span></span>
<span id="cb369-33"><a href="discriminant-analysis.html#cb369-33" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Sugarbeets      0    0      0        0          1</span></span>
<span id="cb369-34"><a href="discriminant-analysis.html#cb369-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb369-35"><a href="discriminant-analysis.html#cb369-35" aria-hidden="true" tabindex="-1"></a><span class="do">## Accuracy</span></span>
<span id="cb369-36"><a href="discriminant-analysis.html#cb369-36" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(Y_test<span class="sc">==</span>knn_2_test)</span>
<span id="cb369-37"><a href="discriminant-analysis.html#cb369-37" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.8</span></span>
<span id="cb369-38"><a href="discriminant-analysis.html#cb369-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb369-39"><a href="discriminant-analysis.html#cb369-39" aria-hidden="true" tabindex="-1"></a><span class="do">## Nearest neighbors with 3 neighbors</span></span>
<span id="cb369-40"><a href="discriminant-analysis.html#cb369-40" aria-hidden="true" tabindex="-1"></a>knn_3 <span class="ot">&lt;-</span> <span class="fu">knn</span>(X_train, X_train, Y_train, <span class="at">k =</span> <span class="dv">3</span>)</span>
<span id="cb369-41"><a href="discriminant-analysis.html#cb369-41" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="at">truth =</span> Y_train, <span class="at">fitted =</span> knn_3)</span>
<span id="cb369-42"><a href="discriminant-analysis.html#cb369-42" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             fitted</span></span>
<span id="cb369-43"><a href="discriminant-analysis.html#cb369-43" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; truth        Clover Corn Cotton Soybeans Sugarbeets</span></span>
<span id="cb369-44"><a href="discriminant-analysis.html#cb369-44" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Clover          8    0      3        0          0</span></span>
<span id="cb369-45"><a href="discriminant-analysis.html#cb369-45" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Corn            0    5      0        2          0</span></span>
<span id="cb369-46"><a href="discriminant-analysis.html#cb369-46" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Cotton          1    1      3        0          1</span></span>
<span id="cb369-47"><a href="discriminant-analysis.html#cb369-47" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Soybeans        1    0      2        2          1</span></span>
<span id="cb369-48"><a href="discriminant-analysis.html#cb369-48" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Sugarbeets      0    0      0        3          3</span></span>
<span id="cb369-49"><a href="discriminant-analysis.html#cb369-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb369-50"><a href="discriminant-analysis.html#cb369-50" aria-hidden="true" tabindex="-1"></a><span class="do">## Accuracy</span></span>
<span id="cb369-51"><a href="discriminant-analysis.html#cb369-51" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(Y_train<span class="sc">==</span>knn_3)</span>
<span id="cb369-52"><a href="discriminant-analysis.html#cb369-52" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 0.5833333</span></span>
<span id="cb369-53"><a href="discriminant-analysis.html#cb369-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb369-54"><a href="discriminant-analysis.html#cb369-54" aria-hidden="true" tabindex="-1"></a><span class="do">## Performance on test data</span></span>
<span id="cb369-55"><a href="discriminant-analysis.html#cb369-55" aria-hidden="true" tabindex="-1"></a>knn_3_test <span class="ot">&lt;-</span> <span class="fu">knn</span>(X_train, X_test, Y_train, <span class="at">k =</span> <span class="dv">3</span>)</span>
<span id="cb369-56"><a href="discriminant-analysis.html#cb369-56" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="at">truth =</span> Y_test, <span class="at">predict =</span> knn_3_test)</span>
<span id="cb369-57"><a href="discriminant-analysis.html#cb369-57" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             predict</span></span>
<span id="cb369-58"><a href="discriminant-analysis.html#cb369-58" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; truth        Clover Corn Cotton Soybeans Sugarbeets</span></span>
<span id="cb369-59"><a href="discriminant-analysis.html#cb369-59" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Clover          1    0      0        0          0</span></span>
<span id="cb369-60"><a href="discriminant-analysis.html#cb369-60" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Corn            0    1      0        0          0</span></span>
<span id="cb369-61"><a href="discriminant-analysis.html#cb369-61" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Cotton          0    0      1        0          0</span></span>
<span id="cb369-62"><a href="discriminant-analysis.html#cb369-62" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Soybeans        0    0      0        1          0</span></span>
<span id="cb369-63"><a href="discriminant-analysis.html#cb369-63" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   Sugarbeets      0    0      0        0          1</span></span>
<span id="cb369-64"><a href="discriminant-analysis.html#cb369-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb369-65"><a href="discriminant-analysis.html#cb369-65" aria-hidden="true" tabindex="-1"></a><span class="do">## Accuracy</span></span>
<span id="cb369-66"><a href="discriminant-analysis.html#cb369-66" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(Y_test<span class="sc">==</span>knn_3_test)</span>
<span id="cb369-67"><a href="discriminant-analysis.html#cb369-67" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] 1</span></span></code></pre></div>
</div>
<div id="stepwise" class="section level4" number="21.4.4.4">
<h4><span class="header-section-number">21.4.4.4</span> Stepwise</h4>
<p>Stepwise discriminant analysis using the <code>stepclass</code> in function in the <code>klaR</code> package.</p>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb370-1"><a href="discriminant-analysis.html#cb370-1" aria-hidden="true" tabindex="-1"></a>step <span class="ot">&lt;-</span> <span class="fu">stepclass</span>(</span>
<span id="cb370-2"><a href="discriminant-analysis.html#cb370-2" aria-hidden="true" tabindex="-1"></a>    crop <span class="sc">~</span> y1 <span class="sc">+</span> y2 <span class="sc">+</span> y3 <span class="sc">+</span> y4,</span>
<span id="cb370-3"><a href="discriminant-analysis.html#cb370-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">data =</span> crops,</span>
<span id="cb370-4"><a href="discriminant-analysis.html#cb370-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">&quot;qda&quot;</span>,</span>
<span id="cb370-5"><a href="discriminant-analysis.html#cb370-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">improvement =</span> <span class="fl">0.15</span></span>
<span id="cb370-6"><a href="discriminant-analysis.html#cb370-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb370-7"><a href="discriminant-analysis.html#cb370-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; correctness rate: 0.475;  in: &quot;y1&quot;;  variables (1): y1 </span></span>
<span id="cb370-8"><a href="discriminant-analysis.html#cb370-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb370-9"><a href="discriminant-analysis.html#cb370-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  hr.elapsed min.elapsed sec.elapsed </span></span>
<span id="cb370-10"><a href="discriminant-analysis.html#cb370-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;        0.00        0.00        0.12</span></span>
<span id="cb370-11"><a href="discriminant-analysis.html#cb370-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb370-12"><a href="discriminant-analysis.html#cb370-12" aria-hidden="true" tabindex="-1"></a>step<span class="sc">$</span>process</span>
<span id="cb370-13"><a href="discriminant-analysis.html#cb370-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;    step var varname result.pm</span></span>
<span id="cb370-14"><a href="discriminant-analysis.html#cb370-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 0 start   0      --     0.000</span></span>
<span id="cb370-15"><a href="discriminant-analysis.html#cb370-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1    in   1      y1     0.475</span></span>
<span id="cb370-16"><a href="discriminant-analysis.html#cb370-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb370-17"><a href="discriminant-analysis.html#cb370-17" aria-hidden="true" tabindex="-1"></a>step<span class="sc">$</span>performance.measure</span>
<span id="cb370-18"><a href="discriminant-analysis.html#cb370-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] &quot;correctness rate&quot;</span></span></code></pre></div>
<p>Iris Data</p>
<div class="sourceCode" id="cb371"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb371-1"><a href="discriminant-analysis.html#cb371-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb371-2"><a href="discriminant-analysis.html#cb371-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb371-3"><a href="discriminant-analysis.html#cb371-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&#39;iris&#39;</span>)</span>
<span id="cb371-4"><a href="discriminant-analysis.html#cb371-4" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb371-5"><a href="discriminant-analysis.html#cb371-5" aria-hidden="true" tabindex="-1"></a>samp <span class="ot">&lt;-</span></span>
<span id="cb371-6"><a href="discriminant-analysis.html#cb371-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">sample.int</span>(<span class="fu">nrow</span>(iris), <span class="at">size =</span> <span class="fu">floor</span>(<span class="fl">0.70</span> <span class="sc">*</span> <span class="fu">nrow</span>(iris)), <span class="at">replace =</span> F)</span>
<span id="cb371-7"><a href="discriminant-analysis.html#cb371-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb371-8"><a href="discriminant-analysis.html#cb371-8" aria-hidden="true" tabindex="-1"></a>train.iris <span class="ot">&lt;-</span> iris[samp,] <span class="sc">%&gt;%</span> <span class="fu">mutate_if</span>(is.numeric,scale)</span>
<span id="cb371-9"><a href="discriminant-analysis.html#cb371-9" aria-hidden="true" tabindex="-1"></a>test.iris <span class="ot">&lt;-</span> iris[<span class="sc">-</span>samp,] <span class="sc">%&gt;%</span> <span class="fu">mutate_if</span>(is.numeric,scale)</span>
<span id="cb371-10"><a href="discriminant-analysis.html#cb371-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb371-11"><a href="discriminant-analysis.html#cb371-11" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb371-12"><a href="discriminant-analysis.html#cb371-12" aria-hidden="true" tabindex="-1"></a>iris.model <span class="ot">&lt;-</span> <span class="fu">lda</span>(Species <span class="sc">~</span> ., <span class="at">data =</span> train.iris)</span>
<span id="cb371-13"><a href="discriminant-analysis.html#cb371-13" aria-hidden="true" tabindex="-1"></a><span class="co">#pred</span></span>
<span id="cb371-14"><a href="discriminant-analysis.html#cb371-14" aria-hidden="true" tabindex="-1"></a>pred.lda <span class="ot">&lt;-</span> <span class="fu">predict</span>(iris.model, test.iris)</span>
<span id="cb371-15"><a href="discriminant-analysis.html#cb371-15" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="at">truth =</span> test.iris<span class="sc">$</span>Species, <span class="at">prediction =</span> pred.lda<span class="sc">$</span>class)</span>
<span id="cb371-16"><a href="discriminant-analysis.html#cb371-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             prediction</span></span>
<span id="cb371-17"><a href="discriminant-analysis.html#cb371-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; truth        setosa versicolor virginica</span></span>
<span id="cb371-18"><a href="discriminant-analysis.html#cb371-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   setosa         15          0         0</span></span>
<span id="cb371-19"><a href="discriminant-analysis.html#cb371-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   versicolor      0         17         0</span></span>
<span id="cb371-20"><a href="discriminant-analysis.html#cb371-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   virginica       0          0        13</span></span>
<span id="cb371-21"><a href="discriminant-analysis.html#cb371-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb371-22"><a href="discriminant-analysis.html#cb371-22" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(iris.model)</span></code></pre></div>
<p><img src="21-multivariate_files/figure-html/unnamed-chunk-22-1.png" width="90%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb372"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb372-1"><a href="discriminant-analysis.html#cb372-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb372-2"><a href="discriminant-analysis.html#cb372-2" aria-hidden="true" tabindex="-1"></a>iris.model.qda <span class="ot">&lt;-</span> <span class="fu">qda</span>(Species<span class="sc">~</span>.,<span class="at">data=</span>train.iris)</span>
<span id="cb372-3"><a href="discriminant-analysis.html#cb372-3" aria-hidden="true" tabindex="-1"></a><span class="co">#pred</span></span>
<span id="cb372-4"><a href="discriminant-analysis.html#cb372-4" aria-hidden="true" tabindex="-1"></a>pred.qda <span class="ot">&lt;-</span> <span class="fu">predict</span>(iris.model.qda,test.iris)</span>
<span id="cb372-5"><a href="discriminant-analysis.html#cb372-5" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="at">truth=</span>test.iris<span class="sc">$</span>Species,<span class="at">prediction=</span>pred.qda<span class="sc">$</span>class)</span>
<span id="cb372-6"><a href="discriminant-analysis.html#cb372-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;             prediction</span></span>
<span id="cb372-7"><a href="discriminant-analysis.html#cb372-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; truth        setosa versicolor virginica</span></span>
<span id="cb372-8"><a href="discriminant-analysis.html#cb372-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   setosa         15          0         0</span></span>
<span id="cb372-9"><a href="discriminant-analysis.html#cb372-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   versicolor      0         16         1</span></span>
<span id="cb372-10"><a href="discriminant-analysis.html#cb372-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   virginica       0          0        13</span></span></code></pre></div>
</div>
<div id="pca-with-discriminant-analysis" class="section level4" number="21.4.4.5">
<h4><span class="header-section-number">21.4.4.5</span> PCA with Discriminant Analysis</h4>
<p>we can use both PCA for dimension reduction in discriminant analysis</p>
<div class="sourceCode" id="cb373"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb373-1"><a href="discriminant-analysis.html#cb373-1" aria-hidden="true" tabindex="-1"></a>zeros <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">read.table</span>(<span class="st">&quot;images/mnist0_train_b.txt&quot;</span>))</span>
<span id="cb373-2"><a href="discriminant-analysis.html#cb373-2" aria-hidden="true" tabindex="-1"></a>nines <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">read.table</span>(<span class="st">&quot;images/mnist9_train_b.txt&quot;</span>))</span>
<span id="cb373-3"><a href="discriminant-analysis.html#cb373-3" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">rbind</span>(zeros[<span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>, ], nines[<span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>, ])</span>
<span id="cb373-4"><a href="discriminant-analysis.html#cb373-4" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> train <span class="sc">/</span> <span class="dv">255</span> <span class="co">#divide by 255 per notes (so ranges from 0 to 1)</span></span>
<span id="cb373-5"><a href="discriminant-analysis.html#cb373-5" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">t</span>(train) <span class="co">#each column is an observation</span></span>
<span id="cb373-6"><a href="discriminant-analysis.html#cb373-6" aria-hidden="true" tabindex="-1"></a><span class="fu">image</span>(<span class="fu">matrix</span>(train[, <span class="dv">1</span>], <span class="at">nrow =</span> <span class="dv">28</span>), <span class="at">main =</span> <span class="st">&#39;Example image, unrotated&#39;</span>)</span></code></pre></div>
<p><img src="21-multivariate_files/figure-html/unnamed-chunk-23-1.png" width="90%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb374"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb374-1"><a href="discriminant-analysis.html#cb374-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb374-2"><a href="discriminant-analysis.html#cb374-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb374-3"><a href="discriminant-analysis.html#cb374-3" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> <span class="fu">rbind</span>(zeros[<span class="dv">2501</span><span class="sc">:</span><span class="dv">3000</span>, ], nines[<span class="dv">2501</span><span class="sc">:</span><span class="dv">3000</span>, ])</span>
<span id="cb374-4"><a href="discriminant-analysis.html#cb374-4" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> test <span class="sc">/</span> <span class="dv">255</span></span>
<span id="cb374-5"><a href="discriminant-analysis.html#cb374-5" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> <span class="fu">t</span>(test)</span>
<span id="cb374-6"><a href="discriminant-analysis.html#cb374-6" aria-hidden="true" tabindex="-1"></a>y.train <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">1000</span>), <span class="fu">rep</span>(<span class="dv">9</span>, <span class="dv">1000</span>))</span>
<span id="cb374-7"><a href="discriminant-analysis.html#cb374-7" aria-hidden="true" tabindex="-1"></a>y.test <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">500</span>), <span class="fu">rep</span>(<span class="dv">9</span>, <span class="dv">500</span>))</span>
<span id="cb374-8"><a href="discriminant-analysis.html#cb374-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb374-9"><a href="discriminant-analysis.html#cb374-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb374-10"><a href="discriminant-analysis.html#cb374-10" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb374-11"><a href="discriminant-analysis.html#cb374-11" aria-hidden="true" tabindex="-1"></a>pc <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(<span class="fu">t</span>(train))</span>
<span id="cb374-12"><a href="discriminant-analysis.html#cb374-12" aria-hidden="true" tabindex="-1"></a>train.large <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">cbind</span>(y.train, pc<span class="sc">$</span>x[, <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]))</span>
<span id="cb374-13"><a href="discriminant-analysis.html#cb374-13" aria-hidden="true" tabindex="-1"></a>large <span class="ot">&lt;-</span> <span class="fu">lda</span>(y.train <span class="sc">~</span> ., <span class="at">data =</span> train.large)</span>
<span id="cb374-14"><a href="discriminant-analysis.html#cb374-14" aria-hidden="true" tabindex="-1"></a><span class="co">#the test data set needs to be constucted w/ the same 10 princomps</span></span>
<span id="cb374-15"><a href="discriminant-analysis.html#cb374-15" aria-hidden="true" tabindex="-1"></a>test.large <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="fu">cbind</span>(y.test, <span class="fu">predict</span>(pc, <span class="fu">t</span>(test))[, <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]))</span>
<span id="cb374-16"><a href="discriminant-analysis.html#cb374-16" aria-hidden="true" tabindex="-1"></a>pred.lda <span class="ot">&lt;-</span> <span class="fu">predict</span>(large, test.large)</span>
<span id="cb374-17"><a href="discriminant-analysis.html#cb374-17" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="at">truth =</span> test.large<span class="sc">$</span>y.test, <span class="at">prediction =</span> pred.lda<span class="sc">$</span>class)</span>
<span id="cb374-18"><a href="discriminant-analysis.html#cb374-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      prediction</span></span>
<span id="cb374-19"><a href="discriminant-analysis.html#cb374-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; truth   0   9</span></span>
<span id="cb374-20"><a href="discriminant-analysis.html#cb374-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     0 491   9</span></span>
<span id="cb374-21"><a href="discriminant-analysis.html#cb374-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     9   5 495</span></span>
<span id="cb374-22"><a href="discriminant-analysis.html#cb374-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb374-23"><a href="discriminant-analysis.html#cb374-23" aria-hidden="true" tabindex="-1"></a>large.qda <span class="ot">&lt;-</span> <span class="fu">qda</span>(y.train<span class="sc">~</span>.,<span class="at">data=</span>train.large)</span>
<span id="cb374-24"><a href="discriminant-analysis.html#cb374-24" aria-hidden="true" tabindex="-1"></a><span class="co">#prediction</span></span>
<span id="cb374-25"><a href="discriminant-analysis.html#cb374-25" aria-hidden="true" tabindex="-1"></a>pred.qda <span class="ot">&lt;-</span> <span class="fu">predict</span>(large.qda,test.large)</span>
<span id="cb374-26"><a href="discriminant-analysis.html#cb374-26" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(<span class="at">truth=</span>test.large<span class="sc">$</span>y.test,<span class="at">prediction=</span>pred.qda<span class="sc">$</span>class)</span>
<span id="cb374-27"><a href="discriminant-analysis.html#cb374-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      prediction</span></span>
<span id="cb374-28"><a href="discriminant-analysis.html#cb374-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; truth   0   9</span></span>
<span id="cb374-29"><a href="discriminant-analysis.html#cb374-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     0 493   7</span></span>
<span id="cb374-30"><a href="discriminant-analysis.html#cb374-30" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;     9   3 497</span></span></code></pre></div>

</div>
</div>
</div>
<!-- </div> -->



</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-james2013" class="csl-entry">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <span>“Statistical Learning.”</span> In, 15–57. Springer New York. <a href="https://doi.org/10.1007/978-1-4614-7138-7_2">https://doi.org/10.1007/978-1-4614-7138-7_2</a>.
</div>
<div id="ref-webb2002" class="csl-entry">
Webb, Andrew R. 2002. <span>“Statistical Pattern Recognition,”</span> July. <a href="https://doi.org/10.1002/0470854774">https://doi.org/10.1002/0470854774</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="factor-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="quasi-experimental.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mikenguyen13/data_analysis/edit/main/21-multivariate.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/mikenguyen13/data_analysis/blob/main/21-multivariate.Rmd",
"text": null
},
"download": ["data_analysis.pdf", "data_analysis.epub", "data_analysis.mobi"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true,
"sharing": {
"facebook": true,
"github": true,
"twitter": true,
"linkedin": true
},
"info": true,
"edit": "https://github.com/mikenguyen13/data_analysis/edit/main/%s"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
