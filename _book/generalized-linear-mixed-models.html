<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6.4 Generalized Linear Mixed Models | A Guide on Data Analysis</title>
  <meta name="description" content="This is a guide on how to conduct data analysis" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="6.4 Generalized Linear Mixed Models | A Guide on Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a guide on how to conduct data analysis" />
  <meta name="github-repo" content="mikenguyen13/data_analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6.4 Generalized Linear Mixed Models | A Guide on Data Analysis" />
  
  <meta name="twitter:description" content="This is a guide on how to conduct data analysis" />
  

<meta name="author" content="Mike Nguyen" />


<meta name="date" content="2021-03-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="generalized-linear-models.html"/>
<link rel="next" href="generalized-method-of-moments.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-3.5.0/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/proj4js-2.3.15/proj4.js"></script>
<link href="libs/highcharts-8.1.2/css/motion.css" rel="stylesheet" />
<script src="libs/highcharts-8.1.2/highcharts.js"></script>
<script src="libs/highcharts-8.1.2/highcharts-3d.js"></script>
<script src="libs/highcharts-8.1.2/highcharts-more.js"></script>
<script src="libs/highcharts-8.1.2/modules/stock.js"></script>
<script src="libs/highcharts-8.1.2/modules/map.js"></script>
<script src="libs/highcharts-8.1.2/modules/annotations.js"></script>
<script src="libs/highcharts-8.1.2/modules/data.js"></script>
<script src="libs/highcharts-8.1.2/modules/drilldown.js"></script>
<script src="libs/highcharts-8.1.2/modules/item-series.js"></script>
<script src="libs/highcharts-8.1.2/modules/offline-exporting.js"></script>
<script src="libs/highcharts-8.1.2/modules/overlapping-datalabels.js"></script>
<script src="libs/highcharts-8.1.2/modules/exporting.js"></script>
<script src="libs/highcharts-8.1.2/modules/export-data.js"></script>
<script src="libs/highcharts-8.1.2/modules/funnel.js"></script>
<script src="libs/highcharts-8.1.2/modules/heatmap.js"></script>
<script src="libs/highcharts-8.1.2/modules/treemap.js"></script>
<script src="libs/highcharts-8.1.2/modules/sankey.js"></script>
<script src="libs/highcharts-8.1.2/modules/dependency-wheel.js"></script>
<script src="libs/highcharts-8.1.2/modules/organization.js"></script>
<script src="libs/highcharts-8.1.2/modules/solid-gauge.js"></script>
<script src="libs/highcharts-8.1.2/modules/streamgraph.js"></script>
<script src="libs/highcharts-8.1.2/modules/sunburst.js"></script>
<script src="libs/highcharts-8.1.2/modules/vector.js"></script>
<script src="libs/highcharts-8.1.2/modules/wordcloud.js"></script>
<script src="libs/highcharts-8.1.2/modules/xrange.js"></script>
<script src="libs/highcharts-8.1.2/modules/tilemap.js"></script>
<script src="libs/highcharts-8.1.2/modules/venn.js"></script>
<script src="libs/highcharts-8.1.2/modules/gantt.js"></script>
<script src="libs/highcharts-8.1.2/modules/timeline.js"></script>
<script src="libs/highcharts-8.1.2/modules/parallel-coordinates.js"></script>
<script src="libs/highcharts-8.1.2/modules/bullet.js"></script>
<script src="libs/highcharts-8.1.2/modules/coloraxis.js"></script>
<script src="libs/highcharts-8.1.2/modules/dumbbell.js"></script>
<script src="libs/highcharts-8.1.2/modules/lollipop.js"></script>
<script src="libs/highcharts-8.1.2/modules/series-label.js"></script>
<script src="libs/highcharts-8.1.2/plugins/motion.js"></script>
<script src="libs/highcharts-8.1.2/custom/reset.js"></script>
<script src="libs/highcharts-8.1.2/modules/boost.js"></script>
<script src="libs/highchart-binding-0.8.2/highchart.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Guide on Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a>
<ul>
<li class="chapter" data-level="2.1" data-path="matrix-theory.html"><a href="matrix-theory.html"><i class="fa fa-check"></i><b>2.1</b> Matrix Theory</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="matrix-theory.html"><a href="matrix-theory.html#rank"><i class="fa fa-check"></i><b>2.1.1</b> Rank</a></li>
<li class="chapter" data-level="2.1.2" data-path="matrix-theory.html"><a href="matrix-theory.html#inverse"><i class="fa fa-check"></i><b>2.1.2</b> Inverse</a></li>
<li class="chapter" data-level="2.1.3" data-path="matrix-theory.html"><a href="matrix-theory.html#definiteness"><i class="fa fa-check"></i><b>2.1.3</b> Definiteness</a></li>
<li class="chapter" data-level="2.1.4" data-path="matrix-theory.html"><a href="matrix-theory.html#matrix-calculus"><i class="fa fa-check"></i><b>2.1.4</b> Matrix Calculus</a></li>
<li class="chapter" data-level="2.1.5" data-path="matrix-theory.html"><a href="matrix-theory.html#optimization"><i class="fa fa-check"></i><b>2.1.5</b> Optimization</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>2.2</b> Probability Theory</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="probability-theory.html"><a href="probability-theory.html#axiom-and-theorems-of-probability"><i class="fa fa-check"></i><b>2.2.1</b> Axiom and Theorems of Probability</a></li>
<li class="chapter" data-level="2.2.2" data-path="probability-theory.html"><a href="probability-theory.html#central-limit-theorem"><i class="fa fa-check"></i><b>2.2.2</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="2.2.3" data-path="probability-theory.html"><a href="probability-theory.html#random-variable"><i class="fa fa-check"></i><b>2.2.3</b> Random variable</a></li>
<li class="chapter" data-level="2.2.4" data-path="probability-theory.html"><a href="probability-theory.html#moment-generating-function"><i class="fa fa-check"></i><b>2.2.4</b> Moment generating function</a></li>
<li class="chapter" data-level="2.2.5" data-path="probability-theory.html"><a href="probability-theory.html#moment"><i class="fa fa-check"></i><b>2.2.5</b> Moment</a></li>
<li class="chapter" data-level="2.2.6" data-path="probability-theory.html"><a href="probability-theory.html#distributions"><i class="fa fa-check"></i><b>2.2.6</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="general-math.html"><a href="general-math.html"><i class="fa fa-check"></i><b>2.3</b> General Math</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="general-math.html"><a href="general-math.html#law-of-large-numbers"><i class="fa fa-check"></i><b>2.3.1</b> Law of large numbers</a></li>
<li class="chapter" data-level="2.3.2" data-path="general-math.html"><a href="general-math.html#law-of-iterated-expectation"><i class="fa fa-check"></i><b>2.3.2</b> Law of Iterated Expectation</a></li>
<li class="chapter" data-level="2.3.3" data-path="general-math.html"><a href="general-math.html#convergence"><i class="fa fa-check"></i><b>2.3.3</b> Convergence</a></li>
<li class="chapter" data-level="2.3.4" data-path="general-math.html"><a href="general-math.html#sufficient-statistics"><i class="fa fa-check"></i><b>2.3.4</b> Sufficient Statistics</a></li>
<li class="chapter" data-level="2.3.5" data-path="general-math.html"><a href="general-math.html#parameter-transformations"><i class="fa fa-check"></i><b>2.3.5</b> Parameter transformations</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>2.4</b> Methods</a></li>
<li class="chapter" data-level="2.5" data-path="data-manipulation.html"><a href="data-manipulation.html"><i class="fa fa-check"></i><b>2.5</b> Data Manipulation</a></li>
</ul></li>
<li class="part"><span><b>I BASIC</b></span></li>
<li class="chapter" data-level="3" data-path="descriptive-stat.html"><a href="descriptive-stat.html"><i class="fa fa-check"></i><b>3</b> Descriptive Statistics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="numerical-measures.html"><a href="numerical-measures.html"><i class="fa fa-check"></i><b>3.1</b> Numerical Measures</a></li>
<li class="chapter" data-level="3.2" data-path="graphical-measures.html"><a href="graphical-measures.html"><i class="fa fa-check"></i><b>3.2</b> Graphical Measures</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="graphical-measures.html"><a href="graphical-measures.html#shape"><i class="fa fa-check"></i><b>3.2.1</b> Shape</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="normality-assessment.html"><a href="normality-assessment.html"><i class="fa fa-check"></i><b>3.3</b> Normality Assessment</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="normality-assessment.html"><a href="normality-assessment.html#graphical-assessment"><i class="fa fa-check"></i><b>3.3.1</b> Graphical Assessment</a></li>
<li class="chapter" data-level="3.3.2" data-path="normality-assessment.html"><a href="normality-assessment.html#summary-statistics"><i class="fa fa-check"></i><b>3.3.2</b> Summary Statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="basic-statistical-inference.html"><a href="basic-statistical-inference.html"><i class="fa fa-check"></i><b>4</b> Basic Statistical Inference</a>
<ul>
<li class="chapter" data-level="4.1" data-path="one-sample-inference.html"><a href="one-sample-inference.html"><i class="fa fa-check"></i><b>4.1</b> One Sample Inference</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="one-sample-inference.html"><a href="one-sample-inference.html#the-mean"><i class="fa fa-check"></i><b>4.1.1</b> The Mean</a></li>
<li class="chapter" data-level="4.1.2" data-path="one-sample-inference.html"><a href="one-sample-inference.html#single-variance"><i class="fa fa-check"></i><b>4.1.2</b> Single Variance</a></li>
<li class="chapter" data-level="4.1.3" data-path="one-sample-inference.html"><a href="one-sample-inference.html#single-proportion-p"><i class="fa fa-check"></i><b>4.1.3</b> Single Proportion (p)</a></li>
<li class="chapter" data-level="4.1.4" data-path="one-sample-inference.html"><a href="one-sample-inference.html#power"><i class="fa fa-check"></i><b>4.1.4</b> Power</a></li>
<li class="chapter" data-level="4.1.5" data-path="one-sample-inference.html"><a href="one-sample-inference.html#sample-size"><i class="fa fa-check"></i><b>4.1.5</b> Sample Size</a></li>
<li class="chapter" data-level="4.1.6" data-path="one-sample-inference.html"><a href="one-sample-inference.html#note"><i class="fa fa-check"></i><b>4.1.6</b> Note</a></li>
<li class="chapter" data-level="4.1.7" data-path="one-sample-inference.html"><a href="one-sample-inference.html#one-sample-non-parametric-methods"><i class="fa fa-check"></i><b>4.1.7</b> One-sample Non-parametric Methods</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="two-sample-inference.html"><a href="two-sample-inference.html"><i class="fa fa-check"></i><b>4.2</b> Two Sample Inference</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="two-sample-inference.html"><a href="two-sample-inference.html#means"><i class="fa fa-check"></i><b>4.2.1</b> Means</a></li>
<li class="chapter" data-level="4.2.2" data-path="two-sample-inference.html"><a href="two-sample-inference.html#variances"><i class="fa fa-check"></i><b>4.2.2</b> Variances</a></li>
<li class="chapter" data-level="4.2.3" data-path="two-sample-inference.html"><a href="two-sample-inference.html#power-1"><i class="fa fa-check"></i><b>4.2.3</b> Power</a></li>
<li class="chapter" data-level="4.2.4" data-path="two-sample-inference.html"><a href="two-sample-inference.html#sample-size-1"><i class="fa fa-check"></i><b>4.2.4</b> Sample Size</a></li>
<li class="chapter" data-level="4.2.5" data-path="two-sample-inference.html"><a href="two-sample-inference.html#matched-pair-designs"><i class="fa fa-check"></i><b>4.2.5</b> Matched Pair Designs</a></li>
<li class="chapter" data-level="4.2.6" data-path="two-sample-inference.html"><a href="two-sample-inference.html#nonparametric-tests-for-two-samples"><i class="fa fa-check"></i><b>4.2.6</b> Nonparametric Tests for Two Samples</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html"><i class="fa fa-check"></i><b>4.3</b> Categorical Data Analysis</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#inferences-for-small-samples"><i class="fa fa-check"></i><b>4.3.1</b> Inferences for Small Samples</a></li>
<li class="chapter" data-level="4.3.2" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#test-of-association"><i class="fa fa-check"></i><b>4.3.2</b> Test of Association</a></li>
<li class="chapter" data-level="4.3.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#ordinal-association"><i class="fa fa-check"></i><b>4.3.3</b> Ordinal Association</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II REGRESSION</b></span></li>
<li class="chapter" data-level="5" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html"><i class="fa fa-check"></i><b>5.1</b> Ordinary Least Squares</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#simple-regression-basic-model"><i class="fa fa-check"></i><b>5.1.1</b> Simple Regression (Basic Model)</a></li>
<li class="chapter" data-level="5.1.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#multiple-linear-regression"><i class="fa fa-check"></i><b>5.1.2</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="5.1.3" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#ols-assumptions"><i class="fa fa-check"></i><b>5.1.3</b> OLS Assumptions</a></li>
<li class="chapter" data-level="5.1.4" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#theorems"><i class="fa fa-check"></i><b>5.1.4</b> Theorems</a></li>
<li class="chapter" data-level="5.1.5" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#variable-selection"><i class="fa fa-check"></i><b>5.1.5</b> Variable Selection</a></li>
<li class="chapter" data-level="5.1.6" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#diagnostics-1"><i class="fa fa-check"></i><b>5.1.6</b> Diagnostics</a></li>
<li class="chapter" data-level="5.1.7" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#model-validation"><i class="fa fa-check"></i><b>5.1.7</b> Model Validation</a></li>
<li class="chapter" data-level="5.1.8" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#finite-sample-properties"><i class="fa fa-check"></i><b>5.1.8</b> Finite Sample Properties</a></li>
<li class="chapter" data-level="5.1.9" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#large-sample-properties"><i class="fa fa-check"></i><b>5.1.9</b> Large Sample Properties</a></li>
<li class="chapter" data-level="5.1.10" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#application"><i class="fa fa-check"></i><b>5.1.10</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="feasible-generalized-least-squares.html"><a href="feasible-generalized-least-squares.html"><i class="fa fa-check"></i><b>5.2</b> Feasible Generalized Least Squares</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="feasible-generalized-least-squares.html"><a href="feasible-generalized-least-squares.html#heteroskedasticity"><i class="fa fa-check"></i><b>5.2.1</b> Heteroskedasticity</a></li>
<li class="chapter" data-level="5.2.2" data-path="feasible-generalized-least-squares.html"><a href="feasible-generalized-least-squares.html#serial-correlation"><i class="fa fa-check"></i><b>5.2.2</b> Serial Correlation</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="weighted-least-squares.html"><a href="weighted-least-squares.html"><i class="fa fa-check"></i><b>5.3</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="5.4" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html"><i class="fa fa-check"></i><b>5.4</b> Generalized Least Squares</a></li>
<li class="chapter" data-level="5.5" data-path="feasiable-prais-winsten.html"><a href="feasiable-prais-winsten.html"><i class="fa fa-check"></i><b>5.5</b> Feasiable Prais Winsten</a></li>
<li class="chapter" data-level="5.6" data-path="feasible-group-level-random-effects.html"><a href="feasible-group-level-random-effects.html"><i class="fa fa-check"></i><b>5.6</b> Feasible group level Random Effects</a></li>
<li class="chapter" data-level="5.7" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>5.7</b> Ridge Regression</a></li>
<li class="chapter" data-level="5.8" data-path="principal-component-regression.html"><a href="principal-component-regression.html"><i class="fa fa-check"></i><b>5.8</b> Principal Component Regression</a></li>
<li class="chapter" data-level="5.9" data-path="robust-regression.html"><a href="robust-regression.html"><i class="fa fa-check"></i><b>5.9</b> Robust Regression</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="robust-regression.html"><a href="robust-regression.html#least-absolute-residuals-lar-regression"><i class="fa fa-check"></i><b>5.9.1</b> Least Absolute Residuals (LAR) Regression</a></li>
<li class="chapter" data-level="5.9.2" data-path="robust-regression.html"><a href="robust-regression.html#least-median-of-squares-lms-regression"><i class="fa fa-check"></i><b>5.9.2</b> Least Median of Squares (LMS) Regression</a></li>
<li class="chapter" data-level="5.9.3" data-path="robust-regression.html"><a href="robust-regression.html#iteratively-reweighted-least-squares-irls-robust-regression"><i class="fa fa-check"></i><b>5.9.3</b> Iteratively Reweighted Least Squares (IRLS) Robust Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html"><i class="fa fa-check"></i><b>5.10</b> Maximum Likelihood</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#motivation-for-mle"><i class="fa fa-check"></i><b>5.10.1</b> Motivation for MLE</a></li>
<li class="chapter" data-level="5.10.2" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#assumption"><i class="fa fa-check"></i><b>5.10.2</b> Assumption</a></li>
<li class="chapter" data-level="5.10.3" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#properties"><i class="fa fa-check"></i><b>5.10.3</b> Properties</a></li>
<li class="chapter" data-level="5.10.4" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#compare-to-ols"><i class="fa fa-check"></i><b>5.10.4</b> Compare to OLS</a></li>
<li class="chapter" data-level="5.10.5" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#application-1"><i class="fa fa-check"></i><b>5.10.5</b> Application</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="non-linear-regression.html"><a href="non-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Non-linear Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="inference-1.html"><a href="inference-1.html"><i class="fa fa-check"></i><b>6.1</b> Inference</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="inference-1.html"><a href="inference-1.html#linear-function-of-the-parameters"><i class="fa fa-check"></i><b>6.1.1</b> Linear Function of the Parameters</a></li>
<li class="chapter" data-level="6.1.2" data-path="inference-1.html"><a href="inference-1.html#nonlinear"><i class="fa fa-check"></i><b>6.1.2</b> Nonlinear</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="non-linear-least-squares.html"><a href="non-linear-least-squares.html"><i class="fa fa-check"></i><b>6.2</b> Non-linear Least Squares</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="non-linear-least-squares.html"><a href="non-linear-least-squares.html#alternative-of-gauss-newton-algorithm"><i class="fa fa-check"></i><b>6.2.1</b> Alternative of Gauss-Newton Algorithm</a></li>
<li class="chapter" data-level="6.2.2" data-path="non-linear-least-squares.html"><a href="non-linear-least-squares.html#practical-considerations"><i class="fa fa-check"></i><b>6.2.2</b> Practical Considerations</a></li>
<li class="chapter" data-level="6.2.3" data-path="non-linear-least-squares.html"><a href="non-linear-least-squares.html#modelestiamtion-adequcy"><i class="fa fa-check"></i><b>6.2.3</b> Model/Estiamtion Adequcy</a></li>
<li class="chapter" data-level="6.2.4" data-path="non-linear-least-squares.html"><a href="non-linear-least-squares.html#application-2"><i class="fa fa-check"></i><b>6.2.4</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>6.3</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#logistic-regression"><i class="fa fa-check"></i><b>6.3.1</b> Logistic Regression</a></li>
<li class="chapter" data-level="6.3.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#probit-regression"><i class="fa fa-check"></i><b>6.3.2</b> Probit Regression</a></li>
<li class="chapter" data-level="6.3.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#binomial-regression"><i class="fa fa-check"></i><b>6.3.3</b> Binomial Regression</a></li>
<li class="chapter" data-level="6.3.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson-regression"><i class="fa fa-check"></i><b>6.3.4</b> Poisson Regression</a></li>
<li class="chapter" data-level="6.3.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#negative-binomial-regression"><i class="fa fa-check"></i><b>6.3.5</b> Negative Binomial Regression</a></li>
<li class="chapter" data-level="6.3.6" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#multinomial"><i class="fa fa-check"></i><b>6.3.6</b> Multinomial</a></li>
<li class="chapter" data-level="6.3.7" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#generalization"><i class="fa fa-check"></i><b>6.3.7</b> Generalization</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="generalized-linear-mixed-models.html"><a href="generalized-linear-mixed-models.html"><i class="fa fa-check"></i><b>6.4</b> Generalized Linear Mixed Models</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="generalized-linear-mixed-models.html"><a href="generalized-linear-mixed-models.html#dependent-data"><i class="fa fa-check"></i><b>6.4.1</b> Dependent Data</a></li>
<li class="chapter" data-level="6.4.2" data-path="generalized-linear-mixed-models.html"><a href="generalized-linear-mixed-models.html#estimation-2"><i class="fa fa-check"></i><b>6.4.2</b> Estimation</a></li>
<li class="chapter" data-level="6.4.3" data-path="generalized-linear-mixed-models.html"><a href="generalized-linear-mixed-models.html#inference-3"><i class="fa fa-check"></i><b>6.4.3</b> Inference</a></li>
<li class="chapter" data-level="6.4.4" data-path="generalized-linear-mixed-models.html"><a href="generalized-linear-mixed-models.html#information-criteria"><i class="fa fa-check"></i><b>6.4.4</b> Information Criteria</a></li>
<li class="chapter" data-level="6.4.5" data-path="generalized-linear-mixed-models.html"><a href="generalized-linear-mixed-models.html#split-plot-designs"><i class="fa fa-check"></i><b>6.4.5</b> Split-Plot Designs</a></li>
<li class="chapter" data-level="6.4.6" data-path="generalized-linear-mixed-models.html"><a href="generalized-linear-mixed-models.html#repeated-measures-in-mixed-models"><i class="fa fa-check"></i><b>6.4.6</b> Repeated Measures in Mixed Models</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="generalized-method-of-moments.html"><a href="generalized-method-of-moments.html"><i class="fa fa-check"></i><b>6.5</b> Generalized Method of Moments</a></li>
<li class="chapter" data-level="6.6" data-path="minimum-distance.html"><a href="minimum-distance.html"><i class="fa fa-check"></i><b>6.6</b> Minimum Distance</a></li>
<li class="chapter" data-level="6.7" data-path="spline-regression.html"><a href="spline-regression.html"><i class="fa fa-check"></i><b>6.7</b> Spline Regression</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="spline-regression.html"><a href="spline-regression.html#regression-splines"><i class="fa fa-check"></i><b>6.7.1</b> Regression Splines</a></li>
<li class="chapter" data-level="6.7.2" data-path="spline-regression.html"><a href="spline-regression.html#natural-splines"><i class="fa fa-check"></i><b>6.7.2</b> Natural splines</a></li>
<li class="chapter" data-level="6.7.3" data-path="spline-regression.html"><a href="spline-regression.html#smoothing-splines"><i class="fa fa-check"></i><b>6.7.3</b> Smoothing splines</a></li>
<li class="chapter" data-level="6.7.4" data-path="spline-regression.html"><a href="spline-regression.html#application-5"><i class="fa fa-check"></i><b>6.7.4</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="generalized-additive-models.html"><a href="generalized-additive-models.html"><i class="fa fa-check"></i><b>6.8</b> Generalized Additive Models</a></li>
<li class="chapter" data-level="6.9" data-path="quantile-regression.html"><a href="quantile-regression.html"><i class="fa fa-check"></i><b>6.9</b> Quantile Regression</a>
<ul>
<li class="chapter" data-level="6.9.1" data-path="quantile-regression.html"><a href="quantile-regression.html#application-6"><i class="fa fa-check"></i><b>6.9.1</b> Application</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="model-specification.html"><a href="model-specification.html"><i class="fa fa-check"></i><b>7</b> Model Specification</a>
<ul>
<li class="chapter" data-level="7.1" data-path="nested-model.html"><a href="nested-model.html"><i class="fa fa-check"></i><b>7.1</b> Nested Model</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="nested-model.html"><a href="nested-model.html#chow-test"><i class="fa fa-check"></i><b>7.1.1</b> Chow test</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="non-nested-model.html"><a href="non-nested-model.html"><i class="fa fa-check"></i><b>7.2</b> Non-Nested Model</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="non-nested-model.html"><a href="non-nested-model.html#davidson-mackinnon-test"><i class="fa fa-check"></i><b>7.2.1</b> Davidson-Mackinnon test</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="heteroskedasticity-1.html"><a href="heteroskedasticity-1.html"><i class="fa fa-check"></i><b>7.3</b> Heteroskedasticity</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="heteroskedasticity-1.html"><a href="heteroskedasticity-1.html#breusch-pagan-test"><i class="fa fa-check"></i><b>7.3.1</b> Breusch-Pagan test</a></li>
<li class="chapter" data-level="7.3.2" data-path="heteroskedasticity-1.html"><a href="heteroskedasticity-1.html#white-test"><i class="fa fa-check"></i><b>7.3.2</b> White test</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="endogeneity.html"><a href="endogeneity.html"><i class="fa fa-check"></i><b>8</b> Endogeneity</a>
<ul>
<li class="chapter" data-level="8.1" data-path="endogenous-treatment.html"><a href="endogenous-treatment.html"><i class="fa fa-check"></i><b>8.1</b> Endogenous Treatment</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="endogenous-treatment.html"><a href="endogenous-treatment.html#instrumental-variable"><i class="fa fa-check"></i><b>8.1.1</b> Instrumental Variable</a></li>
<li class="chapter" data-level="8.1.2" data-path="endogenous-treatment.html"><a href="endogenous-treatment.html#internal-instrumental-variable"><i class="fa fa-check"></i><b>8.1.2</b> Internal instrumental variable</a></li>
<li class="chapter" data-level="8.1.3" data-path="endogenous-treatment.html"><a href="endogenous-treatment.html#proxy-variables"><i class="fa fa-check"></i><b>8.1.3</b> Proxy Variables</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="endogenous-sample-selection.html"><a href="endogenous-sample-selection.html"><i class="fa fa-check"></i><b>8.2</b> Endogenous Sample Selection</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="endogenous-sample-selection.html"><a href="endogenous-sample-selection.html#tobit-2"><i class="fa fa-check"></i><b>8.2.1</b> Tobit-2</a></li>
<li class="chapter" data-level="8.2.2" data-path="endogenous-sample-selection.html"><a href="endogenous-sample-selection.html#tobit-5"><i class="fa fa-check"></i><b>8.2.2</b> Tobit-5</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="imputation-missing-data.html"><a href="imputation-missing-data.html"><i class="fa fa-check"></i><b>9</b> Imputation (Missing Data)</a>
<ul>
<li class="chapter" data-level="9.1" data-path="assumptions-1.html"><a href="assumptions-1.html"><i class="fa fa-check"></i><b>9.1</b> Assumptions</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="assumptions-1.html"><a href="assumptions-1.html#missing-completely-at-random-mcar"><i class="fa fa-check"></i><b>9.1.1</b> Missing Completely at Random (MCAR)</a></li>
<li class="chapter" data-level="9.1.2" data-path="assumptions-1.html"><a href="assumptions-1.html#missing-at-random-mar"><i class="fa fa-check"></i><b>9.1.2</b> Missing at Random (MAR)</a></li>
<li class="chapter" data-level="9.1.3" data-path="assumptions-1.html"><a href="assumptions-1.html#ignorable"><i class="fa fa-check"></i><b>9.1.3</b> Ignorable</a></li>
<li class="chapter" data-level="9.1.4" data-path="assumptions-1.html"><a href="assumptions-1.html#nonignorable"><i class="fa fa-check"></i><b>9.1.4</b> Nonignorable</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html"><i class="fa fa-check"></i><b>9.2</b> Solutions to Missing data</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#listwise-deletion"><i class="fa fa-check"></i><b>9.2.1</b> Listwise Deletion</a></li>
<li class="chapter" data-level="9.2.2" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#pairwise-deletion"><i class="fa fa-check"></i><b>9.2.2</b> Pairwise Deletion</a></li>
<li class="chapter" data-level="9.2.3" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#dummy-variable-adjustment"><i class="fa fa-check"></i><b>9.2.3</b> Dummy Variable Adjustment</a></li>
<li class="chapter" data-level="9.2.4" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#imputation"><i class="fa fa-check"></i><b>9.2.4</b> Imputation</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="criteria-for-choosing-an-effective-approach.html"><a href="criteria-for-choosing-an-effective-approach.html"><i class="fa fa-check"></i><b>9.3</b> Criteria for Choosing an Effective Approach</a></li>
<li class="chapter" data-level="9.4" data-path="another-perspective.html"><a href="another-perspective.html"><i class="fa fa-check"></i><b>9.4</b> Another Perspective</a></li>
<li class="chapter" data-level="9.5" data-path="diagnosing-the-mechanism.html"><a href="diagnosing-the-mechanism.html"><i class="fa fa-check"></i><b>9.5</b> Diagnosing the Mechanism</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="diagnosing-the-mechanism.html"><a href="diagnosing-the-mechanism.html#mar-vs.-mnar"><i class="fa fa-check"></i><b>9.5.1</b> MAR vs. MNAR</a></li>
<li class="chapter" data-level="9.5.2" data-path="diagnosing-the-mechanism.html"><a href="diagnosing-the-mechanism.html#mcar-vs.-mar"><i class="fa fa-check"></i><b>9.5.2</b> MCAR vs. MAR</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="application-7.html"><a href="application-7.html"><i class="fa fa-check"></i><b>9.6</b> Application</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="application-7.html"><a href="application-7.html#imputation-with-mean-median-mode"><i class="fa fa-check"></i><b>9.6.1</b> Imputation with mean / median / mode</a></li>
<li class="chapter" data-level="9.6.2" data-path="application-7.html"><a href="application-7.html#knn"><i class="fa fa-check"></i><b>9.6.2</b> KNN</a></li>
<li class="chapter" data-level="9.6.3" data-path="application-7.html"><a href="application-7.html#rpart"><i class="fa fa-check"></i><b>9.6.3</b> rpart</a></li>
<li class="chapter" data-level="9.6.4" data-path="application-7.html"><a href="application-7.html#mice-multivariate-imputation-via-chained-equations"><i class="fa fa-check"></i><b>9.6.4</b> MICE (Multivariate Imputation via Chained Equations)</a></li>
<li class="chapter" data-level="9.6.5" data-path="application-7.html"><a href="application-7.html#amelia"><i class="fa fa-check"></i><b>9.6.5</b> Amelia</a></li>
<li class="chapter" data-level="9.6.6" data-path="application-7.html"><a href="application-7.html#missforest"><i class="fa fa-check"></i><b>9.6.6</b> missForest</a></li>
<li class="chapter" data-level="9.6.7" data-path="application-7.html"><a href="application-7.html#hmisc"><i class="fa fa-check"></i><b>9.6.7</b> Hmisc</a></li>
<li class="chapter" data-level="9.6.8" data-path="application-7.html"><a href="application-7.html#mi"><i class="fa fa-check"></i><b>9.6.8</b> mi</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>10</b> Data</a>
<ul>
<li class="chapter" data-level="10.1" data-path="cross-sectional.html"><a href="cross-sectional.html"><i class="fa fa-check"></i><b>10.1</b> Cross-Sectional</a></li>
<li class="chapter" data-level="10.2" data-path="time-series-1.html"><a href="time-series-1.html"><i class="fa fa-check"></i><b>10.2</b> Time Series</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="time-series-1.html"><a href="time-series-1.html#deterministic-time-trend"><i class="fa fa-check"></i><b>10.2.1</b> Deterministic Time trend</a></li>
<li class="chapter" data-level="10.2.2" data-path="time-series-1.html"><a href="time-series-1.html#feedback-effect"><i class="fa fa-check"></i><b>10.2.2</b> Feedback Effect</a></li>
<li class="chapter" data-level="10.2.3" data-path="time-series-1.html"><a href="time-series-1.html#dynamic-specification"><i class="fa fa-check"></i><b>10.2.3</b> Dynamic Specification</a></li>
<li class="chapter" data-level="10.2.4" data-path="time-series-1.html"><a href="time-series-1.html#dynamically-complete"><i class="fa fa-check"></i><b>10.2.4</b> Dynamically Complete</a></li>
<li class="chapter" data-level="10.2.5" data-path="time-series-1.html"><a href="time-series-1.html#highly-persistent-data"><i class="fa fa-check"></i><b>10.2.5</b> Highly Persistent Data</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="repeated-cross-sections.html"><a href="repeated-cross-sections.html"><i class="fa fa-check"></i><b>10.3</b> Repeated Cross Sections</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="repeated-cross-sections.html"><a href="repeated-cross-sections.html#pooled-cross-section"><i class="fa fa-check"></i><b>10.3.1</b> Pooled Cross Section</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="panel-data.html"><a href="panel-data.html"><i class="fa fa-check"></i><b>10.4</b> Panel Data</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="panel-data.html"><a href="panel-data.html#pooled-ols-estimator"><i class="fa fa-check"></i><b>10.4.1</b> Pooled OLS Estimator</a></li>
<li class="chapter" data-level="10.4.2" data-path="panel-data.html"><a href="panel-data.html#individual-specific-effects-model"><i class="fa fa-check"></i><b>10.4.2</b> Individual-specific effects model</a></li>
<li class="chapter" data-level="10.4.3" data-path="panel-data.html"><a href="panel-data.html#tests-for-assumptions"><i class="fa fa-check"></i><b>10.4.3</b> Tests for Assumptions</a></li>
<li class="chapter" data-level="10.4.4" data-path="panel-data.html"><a href="panel-data.html#model-selection"><i class="fa fa-check"></i><b>10.4.4</b> Model Selection</a></li>
<li class="chapter" data-level="10.4.5" data-path="panel-data.html"><a href="panel-data.html#summary-1"><i class="fa fa-check"></i><b>10.4.5</b> Summary</a></li>
<li class="chapter" data-level="10.4.6" data-path="panel-data.html"><a href="panel-data.html#application-8"><i class="fa fa-check"></i><b>10.4.6</b> Application</a></li>
<li class="chapter" data-level="10.4.7" data-path="panel-data.html"><a href="panel-data.html#other-estimators"><i class="fa fa-check"></i><b>10.4.7</b> Other Estimators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>11</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="11.1" data-path="types-of-hypothesis-testing.html"><a href="types-of-hypothesis-testing.html"><i class="fa fa-check"></i><b>11.1</b> Types of hypothesis testing</a></li>
<li class="chapter" data-level="11.2" data-path="wald-test.html"><a href="wald-test.html"><i class="fa fa-check"></i><b>11.2</b> Wald test</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="wald-test.html"><a href="wald-test.html#multiple-hypothesis"><i class="fa fa-check"></i><b>11.2.1</b> Multiple Hypothesis</a></li>
<li class="chapter" data-level="11.2.2" data-path="wald-test.html"><a href="wald-test.html#linear-combination"><i class="fa fa-check"></i><b>11.2.2</b> Linear Combination</a></li>
<li class="chapter" data-level="11.2.3" data-path="wald-test.html"><a href="wald-test.html#application-9"><i class="fa fa-check"></i><b>11.2.3</b> Application</a></li>
<li class="chapter" data-level="11.2.4" data-path="wald-test.html"><a href="wald-test.html#nonlinear-1"><i class="fa fa-check"></i><b>11.2.4</b> Nonlinear</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="the-likelihood-ratio-test.html"><a href="the-likelihood-ratio-test.html"><i class="fa fa-check"></i><b>11.3</b> The likelihood ratio test</a></li>
<li class="chapter" data-level="11.4" data-path="lagrange-multiplier-score.html"><a href="lagrange-multiplier-score.html"><i class="fa fa-check"></i><b>11.4</b> Lagrange Multiplier (Score)</a></li>
</ul></li>
<li class="part"><span><b>III EXPERIMENTAL DESIGN</b></span></li>
<li class="chapter" data-level="12" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html"><i class="fa fa-check"></i><b>12</b> Analysis of Variance (ANOVA)</a>
<ul>
<li class="chapter" data-level="12.1" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html"><i class="fa fa-check"></i><b>12.1</b> Completely Randomized Design (CRD)</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#single-factor-fixed-effects-model"><i class="fa fa-check"></i><b>12.1.1</b> Single Factor Fixed Effects Model</a></li>
<li class="chapter" data-level="12.1.2" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#single-factor-random-effects-model"><i class="fa fa-check"></i><b>12.1.2</b> Single Factor Random Effects Model</a></li>
<li class="chapter" data-level="12.1.3" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#two-factor-fixed-effect-anova"><i class="fa fa-check"></i><b>12.1.3</b> Two Factor Fixed Effect ANOVA</a></li>
<li class="chapter" data-level="12.1.4" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#two-way-random-effects-anova"><i class="fa fa-check"></i><b>12.1.4</b> Two-Way Random Effects ANOVA</a></li>
<li class="chapter" data-level="12.1.5" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#two-way-mixed-effects-anova"><i class="fa fa-check"></i><b>12.1.5</b> Two-Way Mixed Effects ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="nonparametric-anova.html"><a href="nonparametric-anova.html"><i class="fa fa-check"></i><b>12.2</b> Nonparametric ANOVA</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="nonparametric-anova.html"><a href="nonparametric-anova.html#kruskal-wallis"><i class="fa fa-check"></i><b>12.2.1</b> Kruskal-Wallis</a></li>
<li class="chapter" data-level="12.2.2" data-path="nonparametric-anova.html"><a href="nonparametric-anova.html#friedman-test"><i class="fa fa-check"></i><b>12.2.2</b> Friedman Test</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sample-size-planning-for-anova.html"><a href="sample-size-planning-for-anova.html"><i class="fa fa-check"></i><b>12.3</b> Sample Size Planning for ANOVA</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="sample-size-planning-for-anova.html"><a href="sample-size-planning-for-anova.html#balanced-designs"><i class="fa fa-check"></i><b>12.3.1</b> Balanced Designs</a></li>
<li class="chapter" data-level="12.3.2" data-path="sample-size-planning-for-anova.html"><a href="sample-size-planning-for-anova.html#randomized-block-experiments"><i class="fa fa-check"></i><b>12.3.2</b> Randomized Block Experiments</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="randomized-block-designs.html"><a href="randomized-block-designs.html"><i class="fa fa-check"></i><b>12.4</b> Randomized Block Designs</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="randomized-block-designs.html"><a href="randomized-block-designs.html#tukey-test-of-additivity"><i class="fa fa-check"></i><b>12.4.1</b> Tukey Test of Additivity</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="nested-designs.html"><a href="nested-designs.html"><i class="fa fa-check"></i><b>12.5</b> Nested Designs</a>
<ul>
<li class="chapter" data-level="12.5.1" data-path="nested-designs.html"><a href="nested-designs.html#two-factor-nested-designs"><i class="fa fa-check"></i><b>12.5.1</b> Two-Factor Nested Designs</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="single-factor-covariance-model.html"><a href="single-factor-covariance-model.html"><i class="fa fa-check"></i><b>12.6</b> Single Factor Covariance Model</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="causality.html"><a href="causality.html"><i class="fa fa-check"></i><b>13</b> Causality</a></li>
<li class="chapter" data-level="14" data-path="report.html"><a href="report.html"><i class="fa fa-check"></i><b>14</b> Report</a>
<ul>
<li class="chapter" data-level="14.1" data-path="one-summary-table.html"><a href="one-summary-table.html"><i class="fa fa-check"></i><b>14.1</b> One summary table</a></li>
<li class="chapter" data-level="14.2" data-path="model-comparison.html"><a href="model-comparison.html"><i class="fa fa-check"></i><b>14.2</b> Model Comparison</a></li>
<li class="chapter" data-level="14.3" data-path="changes-in-an-estimate.html"><a href="changes-in-an-estimate.html"><i class="fa fa-check"></i><b>14.3</b> Changes in an estimate</a></li>
</ul></li>
<li class="appendix"><span><b>APPENDIX</b></span></li>
<li class="chapter" data-level="A" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>A</b> Appendix</a>
<ul>
<li class="chapter" data-level="A.1" data-path="git.html"><a href="git.html"><i class="fa fa-check"></i><b>A.1</b> Git</a></li>
<li class="chapter" data-level="A.2" data-path="short-cut.html"><a href="short-cut.html"><i class="fa fa-check"></i><b>A.2</b> Short-cut</a></li>
<li class="chapter" data-level="A.3" data-path="function-short-cut.html"><a href="function-short-cut.html"><i class="fa fa-check"></i><b>A.3</b> Function short-cut</a></li>
<li class="chapter" data-level="A.4" data-path="citation.html"><a href="citation.html"><i class="fa fa-check"></i><b>A.4</b> Citation</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="bookdown-cheat-sheet.html"><a href="bookdown-cheat-sheet.html"><i class="fa fa-check"></i><b>B</b> Bookdown cheat sheet</a>
<ul>
<li class="chapter" data-level="B.1" data-path="operation.html"><a href="operation.html"><i class="fa fa-check"></i><b>B.1</b> Operation</a></li>
<li class="chapter" data-level="B.2" data-path="math-expresssion-syntax.html"><a href="math-expresssion-syntax.html"><i class="fa fa-check"></i><b>B.2</b> Math Expresssion/ Syntax</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="math-expresssion-syntax.html"><a href="math-expresssion-syntax.html#statistics-notation"><i class="fa fa-check"></i><b>B.2.1</b> Statistics Notation</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="table.html"><a href="table.html"><i class="fa fa-check"></i><b>B.3</b> Table</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Guide on Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="generalized-linear-mixed-models" class="section level2" number="6.4">
<h2><span class="header-section-number">6.4</span> Generalized Linear Mixed Models</h2>
<div id="dependent-data" class="section level3" number="6.4.1">
<h3><span class="header-section-number">6.4.1</span> Dependent Data</h3>
<p>Forms of dependent data:</p>
<ul>
<li>Multivariate measurements on different individuals: (e.g., a person’s blood pressure, fat, etc are correlated)</li>
<li>Clustered measurements: (e.g., blood pressure measurements of people in the same family can be correlated).</li>
<li>Repeated measurements: (e.g., measurement of cholesterol over time can be correlated) “If data are collected repeatedly on experimental material to which treatments were applied initially, the data is a repeated measure.” <span class="citation">(<a href="references.html#ref-Schabenberger_2001" role="doc-biblioref">Schabenberger and Pierce 2001</a>)</span></li>
<li>Longitudinal data: (e.g., individual’s cholesterol tracked over time are correlated): “data collected repeatedly over time in an observational study are termed longitudinal.” <span class="citation">(<a href="references.html#ref-Schabenberger_2001" role="doc-biblioref">Schabenberger and Pierce 2001</a>)</span></li>
<li>Spatial data: (e.g., measurement of individuals living in the same neighborhood are correlated)</li>
</ul>
<p>Hence, we like to account for these correlations.</p>
<p><strong>Linear Mixed Model</strong> (LMM), also known as <strong>Mixed Linear Model</strong> has 2 components:</p>
<ul>
<li><p><strong>Fixed effect</strong> (e.g, gender, age, diet, time)</p></li>
<li><p><strong>Random effects</strong> representing individual variation or auto correlation/spatial effects that imply <strong>dependent (correlated) errors</strong></p></li>
</ul>
<p>Review <a href="completely-randomized-design-crd.html#two-way-mixed-effects-anova">Two-Way Mixed Effects ANOVA</a></p>
<p><strong>LLM Motivation</strong></p>
<p>In a repeated measurements analysis where <span class="math inline">\(Y_{ij}\)</span> is the response for the i-th individual measured at the j-th time,</p>
<p><span class="math inline">\(i =1,…,N\)</span> ; <span class="math inline">\(j = 1,…,n_i\)</span></p>
<p><span class="math display">\[
\mathbf{Y}_i = 
\left(
\begin{array}
{c}
Y_{i1} \\
. \\
.\\
.\\
Y_{in_i}
\end{array}
\right)
\]</span></p>
<p>is all measurements for subject i.</p>
<p><u><em>Stage 1: (Regression Model)</em></u> how the response changes over time for the ith subject</p>
<p><span class="math display">\[
\mathbf{Y_i = Z_i \beta_i + \epsilon_i}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(Z_i\)</span> is an <span class="math inline">\(n_i \times q\)</span> matrix of known covariates</li>
<li><span class="math inline">\(\beta_i\)</span> is an unknown q x 1 vector of subjective -specific coefficients (regression coefficients different for each subject)</li>
<li><span class="math inline">\(\epsilon_i\)</span> are the random errors (typically <span class="math inline">\(\sim N(0, \sigma^2 I)\)</span>)</li>
</ul>
<p>We notice that there are two many <span class="math inline">\(\beta\)</span> to estimate here. Hence, this is the motivation for the second stage</p>
<p><u><em>Stage 2: (Parameter Model)</em></u></p>
<p><span class="math display">\[
\mathbf{\beta_i = K_i \beta + b_i}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(K_i\)</span> is a q x p matrix of known covariates</li>
<li><span class="math inline">\(\beta\)</span> is a p x 1 vector of unknown parameter</li>
<li><span class="math inline">\(\mathbf{b}_i\)</span> are independent <span class="math inline">\(N(0,D)\)</span> random variables</li>
</ul>
<p>This model explain the observed variability between subjects with respect to the subject-specific regression coefficients, <span class="math inline">\(\beta_i\)</span>. We model our different coefficient (<span class="math inline">\(\beta_i\)</span>) with respect to <span class="math inline">\(\beta\)</span>.</p>
<p>Example:</p>
<p>Stage 1:</p>
<p><span class="math display">\[
Y_{ij} = \beta_{1i} + \beta_{2i}t_{ij} + \epsilon_{ij}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(j = 1,..,n_i\)</span></li>
</ul>
<p>In the matrix notation,</p>
<p><span class="math display">\[
\mathbf{Y_i} = 
\left(
\begin{array}
{c}
Y_{i1} \\
.\\
Y_{in_i}
\end{array}
\right)
\]</span></p>
<p><span class="math display">\[
\mathbf{Z}_i = 
\left(
\begin{array}
{cc}
1 &amp; t_{i1} \\
. &amp; . \\
1 &amp; t_{in_i} 
\end{array}
\right)
\]</span></p>
<p><span class="math display">\[
\beta_i =
\left(
\begin{array}
{c}
\beta_{1i} \\
\beta_{2i}
\end{array}
\right)
\]</span></p>
<p><span class="math display">\[
\epsilon_i = 
\left(
\begin{array}
{c}
\epsilon_{i1} \\
. \\
\epsilon_{in_i}
\end{array}
\right)
\]</span></p>
<p>Thus,</p>
<p><span class="math display">\[
\mathbf{Y_i = Z_i \beta_i + \epsilon_i}
\]</span></p>
<p>Stage 2:</p>
<p><span class="math display">\[
\beta_{1i} = \beta_0 + b_{1i} \\
\beta_{2i} = \beta_1 L_i + \beta_2 H_i + \beta_3 C_i + b_{2i}
\]</span></p>
<p>where <span class="math inline">\(L_i, H_i, C_i\)</span> are indicator variables defined to 1 as the subject falls into different categories.</p>
<p>Subject specific intercepts do not depend upon treatment, with <span class="math inline">\(\beta_0\)</span> (the average response at the start of treatment), and <span class="math inline">\(\beta_1 , \beta_2, \beta_3\)</span> (the average time effects for each of three treatment groups).</p>
<p><span class="math display">\[
\mathbf{K}_i = \left(
\begin{array}
{cccc}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; L_i &amp; H_i &amp; C_i 
\end{array}
\right) \\ \beta = (\beta_0 , \beta_1, \beta_2, \beta_3)&#39; \\ 
\mathbf{b}_i = 
\left(
\begin{array}
{c}
b_{1i} \\
b_{2i} \\
\end{array}
\right) \\ 
\beta_i = \mathbf{K_i \beta + b_i}
\]</span></p>
<p>To get <span class="math inline">\(\hat{\beta}\)</span>, we can fit the model sequentially:</p>
<ol style="list-style-type: decimal">
<li>Estimate <span class="math inline">\(\hat{\beta_i}\)</span> in the first stage</li>
<li>Estimate <span class="math inline">\(\hat{\beta}\)</span> in the second stage by replacing <span class="math inline">\(\beta_i\)</span> with <span class="math inline">\(\hat{\beta}_i\)</span></li>
</ol>
<p>However, problems arise from this method:</p>
<ul>
<li>information is lost by summarizing the vector <span class="math inline">\(\mathbf{Y}_i\)</span> solely by <span class="math inline">\(\hat{\beta}_i\)</span></li>
<li>we need to account for variability when replacing <span class="math inline">\(\beta_i\)</span> with its estimate</li>
<li>different subjects might have different number of observations.</li>
</ul>
<p>To address these problems, we can use <strong>Linear Mixed Model <span class="citation">(<a href="references.html#ref-Laird_1982" role="doc-biblioref">Laird and Ware 1982</a>)</span></strong></p>
<p>Substituting stage 2 into stage 1:</p>
<p><span class="math display">\[
\mathbf{Y}_i = \mathbf{Z}_i \mathbf{K}_i \beta + \mathbf{Z}_i \mathbf{b}_i + \mathbf{\epsilon}_i
\]</span></p>
<p>Let <span class="math inline">\(\mathbf{X}_i = \mathbf{Z}_i \mathbf{K}_i\)</span> be an <span class="math inline">\(n_i \times p\)</span> matrix . Then, the LMM is</p>
<p><span class="math display">\[
\mathbf{Y}_i = \mathbf{X}_i \beta + \mathbf{Z}_i \mathbf{b}_i + \mathbf{\epsilon}_i
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(i = 1,..,N\)</span></li>
<li><span class="math inline">\(\beta\)</span> are the fixed effects, which are common to all subjects</li>
<li><span class="math inline">\(\mathbf{b}_i\)</span> are the subject specific random effects. <span class="math inline">\(\mathbf{b}_i \sim N_q (\mathbf{0,D})\)</span></li>
<li><span class="math inline">\(\mathbf{\epsilon}_i \sim N_{n_i}(\mathbf{0,\Sigma_i})\)</span></li>
<li><span class="math inline">\(\mathbf{b}_i\)</span> and <span class="math inline">\(\epsilon_i\)</span> are independent</li>
<li><span class="math inline">\(\mathbf{Z}_{i(n_i \times q})\)</span> and <span class="math inline">\(\mathbf{X}_{i(n_i \times p})\)</span> are matrices of known covariates.</li>
</ul>
<p>Equivalently, in the hierarchical form, we call <strong>conditional</strong> or <strong>hierarchical</strong> formulation of the linear mixed model</p>
<p><span class="math display">\[
\mathbf{Y}_i | \mathbf{b}_i \sim N(\mathbf{X}_i \beta+ \mathbf{Z}_i \mathbf{b}_i, \mathbf{\Sigma}_i) \\
\mathbf{b}_i \sim N(\mathbf{0,D})
\]</span></p>
<p>for <span class="math inline">\(i = 1,..,N\)</span>. denote the respective functions by <span class="math inline">\(f(\mathbf{Y}_i |\mathbf{b}_i)\)</span> and <span class="math inline">\(f(\mathbf{b}_i)\)</span></p>
<p>In general,</p>
<p><span class="math display">\[
f(A,B) = f(A|B)f(B) \\
f(A) = \int f(A,B)dB = \int f(A|B) f(B) dB
\]</span></p>
<p>In the LMM, the marginal density of <span class="math inline">\(\mathbf{Y}_i\)</span> is</p>
<p><span class="math display">\[
f(\mathbf{Y}_i) = \int f(\mathbf{Y}_i | \mathbf{b}_i) f(\mathbf{b}_i) d\mathbf{b}_i
\]</span></p>
<p>which can be shown</p>
<p><span class="math display">\[
\mathbf{Y}_i \sim N(\mathbf{X_i \beta, Z_i DZ&#39;_i + \Sigma_i})
\]</span></p>
<p>This is the <strong>marginal</strong> formulation of the linear mixed model</p>
<p>Notes:</p>
<p>We no longer have <span class="math inline">\(Z_i b_i\)</span> in the mean, but add error in the variance (marginal dependence in Y). kinda of averaging out the common effect. Technically, we shouldn’t call it averaging the error b (adding it to the variance covariance matrix), it should be called adding random effect</p>
<p>Continue with our example</p>
<p><span class="math display">\[
Y_{ij} = (\beta_0 + b_{1i}) + (\beta_1L_i + \beta_2 H_i + \beta_3 C_i + b_{2i})t_{ij} + \epsilon_{ij}
\]</span></p>
<p>for each treatment group</p>
<p><span class="math display">\[
Y_{ik}= 
\begin{cases}
\beta_0 + b_{1i} + (\beta_1 + \ b_{2i})t_{ij} + \epsilon_{ij} &amp;&amp; L \\
\beta_0 + b_{1i} + (\beta_2 + \ b_{2i})t_{ij} + \epsilon_{ij} &amp;&amp; H\\
\beta_0 + b_{1i} + (\beta_3 + \ b_{2i})t_{ij} + \epsilon_{ij} &amp;&amp; C
\end{cases}
\]</span></p>
<ul>
<li>Intercepts and slopes are all subject specific</li>
<li>Different treatment groups have different slops, but the same intercept.</li>
</ul>
<p><strong>In the hierarchical model form</strong></p>
<p><span class="math display">\[
\mathbf{Y}_i | \mathbf{b}_i \sim N(\mathbf{X}_i \beta + \mathbf{Z}_i \mathbf{b}_i, \mathbf{\Sigma}_i)\\
\mathbf{b}_i \sim N(\mathbf{0,D})
\]</span></p>
<p>X will be in the form of</p>
<p><span class="math display">\[
\mathbf{X}_i = \mathbf{Z}_i \mathbf{K}_i \\
= 
\left[
\begin{array}
{cc}
1 &amp; t_{i1} \\
1 &amp; t_{i2} \\
. &amp; . \\
1 &amp; t_{in_i}
\end{array}
\right]
\times
\left[
\begin{array}
{cccc}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; L_i &amp; H_i &amp; C_i \\
\end{array}
\right] \\
=
\left[ 
\begin{array}
{cccc}
1 &amp; t_{i1}L_i &amp; t_{i1}H_i &amp; T_{i1}C_i \\
1 &amp; t_{i2}L_i &amp; t_{i2}H_i &amp; T_{i2}C_i \\
. &amp;. &amp;. &amp;. \\
1 &amp; t_{in_i}L_i &amp; t_{in_i}H_i &amp; T_{in_i}C_i \\
\end{array}
\right]
\]</span></p>
<p><span class="math display">\[
\beta = (\beta_0, \beta_1, \beta_2, \beta_3)&#39; \\
\mathbf{b}_i = 
\left(
\begin{array}
{c}
b_{1i} \\
b_{2i}
\end{array}
\right)
,
D = 
\left(
\begin{array}
{cc}
d_{11} &amp; d_{12}\\
d_{12} &amp; d_{22}
\end{array}
\right)
\]</span></p>
<p>Assuming <span class="math inline">\(\mathbf{\Sigma}_i = \sigma^2 \mathbf{I}_{n_i}\)</span>, which is called <strong>conditional independence</strong>, meaning the response on subject i are independent conditional on <span class="math inline">\(\mathbf{b}_i\)</span> and <span class="math inline">\(\beta\)</span></p>
<p><br></p>
<p><strong>In the marginal model form</strong></p>
<p><span class="math display">\[
Y_{ij} = \beta_0 + \beta_1 L_i t_{ij} + \beta_2 H_i t_{ij} + \beta_3 C_i t_{ij} + \eta_{ij}
\]</span></p>
<p>where <span class="math inline">\(\eta_i \sim N(\mathbf{0},\mathbf{Z}_i\mathbf{DZ}_i&#39;+ \mathbf{\Sigma}_i)\)</span></p>
<p>Equivalently,</p>
<p><span class="math display">\[
\mathbf{Y_i \sim N(X_i \beta, Z_i DZ_i&#39; + \Sigma_i})
\]</span></p>
<p>In this case that <span class="math inline">\(n_i = 2\)</span></p>
<p><span class="math display">\[
\mathbf{Z_iDZ_i&#39;} = 
\left(
\begin{array}
{cc}
1 &amp; t_{i1} \\
1 &amp; t_{i2} 
\end{array}
\right)
\left(
\begin{array}
{cc}
d_{11} &amp; d_{12} \\
d_{12} &amp; d_{22} 
\end{array}
\right)
\left(
\begin{array}
{cc}
1 &amp; 1 \\
t_{i1} &amp; t_{i2} 
\end{array}
\right) \\
=
\left(
\begin{array}
{cc}
d_{11} + 2d_{12}t_{i1} + d_{22}t_{i1}^2 &amp; d_{11} + d_{12}(t_{i1} + t_{i2}) + d_{22}t_{i1}t_{i2} \\
d_{11} + d_{12}(t_{i1} + t_{i2}) + d_{22} t_{i1} t_{i2} &amp; d_{11} + 2d_{12}t_{i2} + d_{22}t_{i2}^2  
\end{array}
\right)
\]</span></p>
<p><span class="math display">\[
var(Y_{i1}) = d_{11} + 2d_{12}t_{i1} + d_{22} t_{i1}^2 + \sigma^2
\]</span></p>
<p>On top of correlation in the errors, the marginal implies that the variance function of the response is quadratic over time, with positive curvature <span class="math inline">\(d_{22}\)</span></p>
<div id="random-intercepts-model" class="section level4" number="6.4.1.1">
<h4><span class="header-section-number">6.4.1.1</span> <strong>Random-Intercepts Model</strong></h4>
<p>If we remove the random slopes,</p>
<ul>
<li>the assumption is that all variability in subject-specific slopes can be attributed to treatment differences</li>
<li>the model is random-intercepts model. This has subject specific intercepts, but the same slopes within each treatment group.</li>
</ul>
<p><span class="math display">\[
\mathbf{Y}_i | b_i \sim N(\mathbf{X}_i \beta + 1 b_i , \Sigma_i) \\
b_i \sim N(0,d_{11})
\]</span></p>
<p>The marginal model is then (<span class="math inline">\(\mathbf{\Sigma}_i = \sigma^2 \mathbf{I}\)</span>)</p>
<p><span class="math display">\[
\mathbf{Y}_i \sim N(\mathbf{X}_i \beta, 11&#39;d_{11} + \sigma^2 \mathbf{I})
\]</span></p>
<p>The marginal covariance matrix is</p>
<p><span class="math display">\[
cov(\mathbf{Y}_i)  = 11&#39;d_{11} + \sigma^2I \\
=
\left(
\begin{array}
{ccc}
d_{11}+ \sigma^2 &amp; d_{11} &amp; ... &amp; d_{11} \\
d_{11} &amp; d_{11} + \sigma^2 &amp; d_{11} &amp; ... \\
. &amp; . &amp; . &amp; . \\
d_{11} &amp; ... &amp; ... &amp; d_{11} + \sigma^2
\end{array}
\right)
\]</span></p>
<p>the associated correlation matrix is</p>
<p><span class="math display">\[
corr(\mathbf{Y}_i) = 
\left(
\begin{array}
{cccc}
1 &amp; \rho &amp; ... &amp; \rho \\
\rho &amp; 1 &amp; \rho &amp; ... \\
. &amp; . &amp; . &amp; . \\
\rho &amp; ... &amp; ... &amp; 1 \\
\end{array}
\right)
\]</span></p>
<p>where <span class="math inline">\(\rho \equiv \frac{d_{11}}{d_{11} + \sigma^2}\)</span></p>
<p>Thu, we have</p>
<ul>
<li>constant variance over time</li>
<li>equal, positive correlation between any two measurements from the same subject</li>
<li>a covariance structure that is called <strong>compound symmetry</strong>, and <span class="math inline">\(\rho\)</span> is called the <strong>intra-class correlation</strong></li>
<li>that when <span class="math inline">\(\rho\)</span> is large, the <strong>inter-subject variability</strong> (<span class="math inline">\(d_{11}\)</span>) is large relative to the intra-subject variability (<span class="math inline">\(\sigma^2\)</span>)</li>
</ul>
</div>
<div id="covariance-models" class="section level4" number="6.4.1.2">
<h4><span class="header-section-number">6.4.1.2</span> <strong>Covariance Models</strong></h4>
<p>If the conditional independence assumption, (<span class="math inline">\(\mathbf{\Sigma_i= \sigma^2 I_{n_i}}\)</span>). Consider, <span class="math inline">\(\epsilon_i = \epsilon_{(1)i} + \epsilon_{(2)i}\)</span>, where</p>
<ul>
<li><span class="math inline">\(\epsilon_{(1)i}\)</span> is a “serial correlation” component. That is, part of the individual’s profile is a response to time-varying stochastic processes.</li>
<li><span class="math inline">\(\epsilon_{(2)i}\)</span> is the measurement error component, and is independent of <span class="math inline">\(\epsilon_{(1)i}\)</span></li>
</ul>
<p>Then</p>
<p><span class="math display">\[
\mathbf{Y_i = X_i \beta + Z_i b_i + \epsilon_{(1)i} + \epsilon_{(2)i}}
\]</span></p>
<p>where</p>
<ul>
<li><p><span class="math inline">\(\mathbf{b_i} \sim N(\mathbf{0,D})\)</span></p></li>
<li><p><span class="math inline">\(\epsilon_{(2)i} \sim N(\mathbf{0,\sigma^2 I_{n_i}})\)</span></p></li>
<li><p><span class="math inline">\(\epsilon_{(1)i} \sim N(\mathbf{0,\tau^2H_i})\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{b}_i\)</span> and <span class="math inline">\(\epsilon_i\)</span> are mutually independent</p></li>
</ul>
<p>To model the structure of the <span class="math inline">\(n_i \times n_i\)</span> correlation (or covariance ) matrix <span class="math inline">\(\mathbf{H}_i\)</span>. Let the (j,k)th element of <span class="math inline">\(\mathbf{H}_i\)</span> be <span class="math inline">\(h_{ijk}= g(t_{ij}t_{ik})\)</span>. that is a function of the times <span class="math inline">\(t_{ij}\)</span> and <span class="math inline">\(t_{ik}\)</span> , which is assumed to be some function of the "distance’ between the times.</p>
<p><span class="math display">\[
h_{ijk} = g(|t_{ij}-t_{ik}|)
\]</span></p>
<p>for some decreasing function <span class="math inline">\(g(.)\)</span> with <span class="math inline">\(g(0)=1\)</span> (for correlation matrices).</p>
<p>Examples of this type of function:</p>
<ul>
<li>Exponential function: <span class="math inline">\(g(|t_{ij}-t_{ik}|) = \exp(-\phi|t_{ij} - t_{ik}|)\)</span></li>
<li>Gaussian function: <span class="math inline">\(g(|t_{ij} - t_{ik}|) = \exp(-\phi(t_{ij} - t_{ik})^2)\)</span></li>
</ul>
<p>Similar structures could also be used for <span class="math inline">\(\mathbf{D}\)</span> matrix (of <span class="math inline">\(\mathbf{b}\)</span>)</p>
<p>Example: Autoregressive Covariance Structure</p>
<p>A first order Autoregressive Model (AR(1)) has the form</p>
<p><span class="math display">\[
\alpha_t = \phi \alpha_{t-1} + \eta_t
\]</span></p>
<p>where <span class="math inline">\(\eta_t \sim iid N (0,\sigma^2_\eta)\)</span></p>
<p>Then, the covariance between two observations is</p>
<p><span class="math display">\[
cov(\alpha_t, \alpha_{t+h}) = \frac{\sigma^2_\eta \phi^{|h|}}{1- \phi^2}
\]</span></p>
<p>for <span class="math inline">\(h = 0, \pm 1, \pm 2, ...; |\phi|&lt;1\)</span></p>
<p>Hence,</p>
<p><span class="math display">\[
corr(\alpha_t, \alpha_{t+h}) = \phi^{|h|}
\]</span></p>
<p>If we let <span class="math inline">\(\alpha_T = (\alpha_1,...\alpha_T)&#39;\)</span>, then</p>
<p><span class="math display">\[
corr(\alpha_T) = 
\left[
\begin{array}
{ccccc}
1 &amp; \phi^1 &amp; \phi^2 &amp; ... &amp; \phi^2 \\
\phi^1 &amp; 1 &amp; \phi^1 &amp; ... &amp; \phi^{T-1} \\
\phi^2 &amp; \phi^1 &amp; 1 &amp; ... &amp; \phi^{T-2} \\
. &amp; . &amp; . &amp; . &amp;. \\
\phi^T &amp; \phi^{T-1} &amp; \phi^{T-2} &amp; ... &amp; 1
\end{array}
\right]
\]</span></p>
<p>Notes:</p>
<ul>
<li>The correlation decreases as time lag increases</li>
<li>This matrix structure is known as a <strong>Toeplitz</strong> structure</li>
<li>More complicated covariance structures are possible, which is critical component of spatial random effects models and time series models.</li>
<li>Often, we don’t need both random effects <span class="math inline">\(\mathbf{b}\)</span> and <span class="math inline">\(\epsilon_{(1)i}\)</span></li>
</ul>
<p>More in the <a href="time-series-1.html#time-series-1">Time Series</a> section</p>
</div>
</div>
<div id="estimation-2" class="section level3" number="6.4.2">
<h3><span class="header-section-number">6.4.2</span> Estimation</h3>
<p><span class="math display">\[
\mathbf{Y}_i = \mathbf{X}_i \beta + \mathbf{Z}_i \mathbf{b}_i + \epsilon_i
\]</span></p>
<p>where <span class="math inline">\(\beta, \mathbf{b}_i, \mathbf{D}, \mathbf{\Sigma}_i\)</span> we must obtain estimation from the data</p>
<ul>
<li><span class="math inline">\(\mathbf{\beta}, \mathbf{D}, \mathbf{\Sigma}_i\)</span> are unknown, but fixed, parameters, and must be estimated from the data</li>
<li><span class="math inline">\(\mathbf{b}_i\)</span> is a random variable. Thus, we can’t estimate these values, but we can predict them. (i.e., you can’t estimate a random thing).</li>
</ul>
<p>If we have</p>
<ul>
<li><span class="math inline">\(\hat{\beta}\)</span> as an estimator of <span class="math inline">\(\beta\)</span></li>
<li><span class="math inline">\(\mathbf{b}_i\)</span> as a predictor of <span class="math inline">\(\mathbf{b}_i\)</span></li>
</ul>
<p>Then,</p>
<ul>
<li>The population average estimate of <span class="math inline">\(\mathbf{Y}_i\)</span> is <span class="math inline">\(\hat{\mathbf{Y}_i} = \mathbf{X}_i \hat{\beta}\)</span></li>
<li>The subject-specific prediction is <span class="math inline">\(\hat{\mathbf{Y}_i} = \mathbf{X}_i \hat{\beta} + \mathbf{Z}_i \hat{b}_i\)</span></li>
</ul>
<p>According to <span class="citation">(<a href="references.html#ref-Henderson_1950" role="doc-biblioref">Henderson 1950</a>)</span>, estimating equations known as the mixed model equations:</p>
<p><span class="math display">\[
\left[
\begin{array}
{c}
\hat{\beta} \\
\hat{\mathbf{b}}
\end{array}
\right]
=
\left[
\begin{array}
{cc}
\mathbf{X&#39;\Sigma^{-1}X} &amp; \mathbf{X&#39;\Sigma^{-1}Z} \\
\mathbf{Z&#39;\Sigma^{-1}X} &amp; \mathbf{Z&#39;\Sigma^{-1}Z +B^{-1}}
\end{array}
\right]
\left[
\begin{array}
{cc}
\mathbf{X&#39;\Sigma^{-1}Y} \\
\mathbf{Z&#39;\Sigma^{-1}Y}
\end{array}
\right]
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\mathbf{Y}
=
\left[
\begin{array}
{c}
\mathbf{y}_1 \\
. \\
\mathbf{y}_N
\end{array}
\right] ;
\mathbf{X}
=
\left[
\begin{array}
{c}
\mathbf{X}_1 \\
. \\
\mathbf{X}_N
\end{array}
\right];
\mathbf{b} = 
\left[
\begin{array}
{c}
\mathbf{b}_1 \\
. \\
\mathbf{b}_N
\end{array}
\right] ;
\epsilon = 
\left[
\begin{array}
{c}
\epsilon_1 \\
. \\
\epsilon_N
\end{array}
\right]
\\
cov(\epsilon) = \mathbf{\Sigma},
\mathbf{Z} = 
\left[
\begin{array}
{cccc}
\mathbf{Z}_1 &amp; 0 &amp;  ... &amp; 0 \\
0 &amp; \mathbf{Z}_2 &amp; ... &amp; 0 \\
. &amp; . &amp; . &amp; . \\
0 &amp; 0 &amp; ... &amp; \mathbf{Z}_n
\end{array}
\right],
\mathbf{B} =
\left[
\begin{array}
{cccc}
\mathbf{D} &amp; 0 &amp; ... &amp; 0 \\
0 &amp; \mathbf{D} &amp; ... &amp; 0 \\
. &amp; . &amp; . &amp; . \\
0 &amp; 0 &amp; ... &amp; \mathbf{D}
\end{array}
\right]
\]</span></p>
<p>The model has the form</p>
<p><span class="math display">\[
\mathbf{Y = X \beta + Z b + \epsilon} \\
\mathbf{Y} \sim N(\mathbf{X \beta, ZBZ&#39; + \Sigma})
\]</span></p>
<p>If <span class="math inline">\(\mathbf{V = ZBZ&#39; + \Sigma}\)</span>, then the solutions to the estimating equations can be</p>
<p><span class="math display">\[
\hat{\beta} = \mathbf{(X&#39;V^{-1}X)^{-1}X&#39;V^{-1}Y} \\
\hat{\mathbf{b}} = \mathbf{BZ&#39;V^{-1}(Y-X\hat{\beta}})
\]</span></p>
<p>The estimate <span class="math inline">\(\hat{\beta}\)</span> is a generalized least squares estimate.</p>
<p>The predictor, <span class="math inline">\(\hat{\mathbf{b}}\)</span> is the best linear unbiased predictor (BLUP), for <span class="math inline">\(\mathbf{b}\)</span></p>
<p><span class="math display">\[
E(\hat{\beta}) = \beta \\
var(\hat{\beta}) = (\mathbf{X&#39;V^{-1}X})^{-1} \\
E(\hat{\mathbf{b}}) = 0 \\
var(\mathbf{\hat{b}-b}) = \mathbf{B-BZ&#39;V^{-1}ZB + BZ&#39;V^{-1}X(X&#39;V^{-1}X)^{-1}X&#39;V^{-1}B}
\]</span></p>
<p>The variance here is the variance of the prediction error (mean squared prediction error, MSPE), which is more meaningful than <span class="math inline">\(var(\hat{\mathbf{b}})\)</span>, since MSPE accounts for both variance and bias in the prediction.</p>
<p>To derive the mixed model equations, consider</p>
<p><span class="math display">\[
\mathbf{\epsilon = Y - X\beta - Zb}
\]</span></p>
<p>Let <span class="math inline">\(T = \sum_{i=1}^N n_i\)</span> be the total number of observations (i.e., the length of <span class="math inline">\(\mathbf{Y},\epsilon\)</span>) and <span class="math inline">\(Nq\)</span> the length of <span class="math inline">\(\mathbf{b}\)</span>. The joint distribution of <span class="math inline">\(\mathbf{b, \epsilon}\)</span> is</p>
<p><span class="math display">\[
f(\mathbf{b,\epsilon})= \frac{1}{(2\pi)^{(T+ Nq)/2}}
\left|
\begin{array}
{cc}
\mathbf{B} &amp; 0 \\
0 &amp; \mathbf{\Sigma}
\end{array}
\right| ^{-1/2}
\exp
\left(
-\frac{1}{2}
\left[
\begin{array}
{c}
\mathbf{b} \\
\mathbf{Y - X \beta - Zb}
\end{array}
\right]&#39;
\left[
\begin{array}
{cc}
\mathbf{B} &amp; 0 \\
0 &amp; \mathbf{\Sigma}
\end{array}
\right]^{-1}
\left[
\begin{array}
{c}
\mathbf{b} \\
\mathbf{Y - X \beta - Zb}
\end{array}
\right]
\right)
\]</span></p>
<p>Maximization of <span class="math inline">\(f(\mathbf{b},\epsilon)\)</span> with respect to <span class="math inline">\(\mathbf{b}\)</span> and <span class="math inline">\(\beta\)</span> requires minimization of</p>
<p><span class="math display">\[
Q = 
\left[
\begin{array}
{c}
\mathbf{b} \\
\mathbf{Y - X \beta - Zb}
\end{array}
\right]&#39;
\left[
\begin{array}
{cc}
\mathbf{B} &amp; 0 \\
0 &amp; \mathbf{\Sigma}
\end{array}
\right]^{-1}
\left[
\begin{array}
{c}
\mathbf{b} \\
\mathbf{Y - X \beta - Zb}
\end{array}
\right] \\
= \mathbf{b&#39;B^{-1}b+(Y-X \beta-Zb)&#39;\Sigma^{-1}(Y-X \beta-Zb)}
\]</span></p>
<p>Setting the derivatives of Q with respect to <span class="math inline">\(\mathbf{b}\)</span> and <span class="math inline">\(\mathbf{\beta}\)</span> to zero leads to the system of equations:</p>
<p><span class="math display">\[
\begin{align}
\mathbf{X&#39;\Sigma^{-1}X\beta + X&#39;\Sigma^{-1}Zb} &amp;= \mathbf{X&#39;\Sigma^{-1}Y}\\
\mathbf{(Z&#39;\Sigma^{-1}Z + B^{-1})b + Z&#39;\Sigma^{-1}X\beta} &amp;= \mathbf{Z&#39;\Sigma^{-1}Y}
\end{align}
\]</span></p>
<p>Rearranging</p>
<p><span class="math display">\[
\left[
\begin{array}
{cc}
\mathbf{X&#39;\Sigma^{-1}X} &amp; \mathbf{X&#39;\Sigma^{-1}Z} \\
\mathbf{Z&#39;\Sigma^{-1}X} &amp; \mathbf{Z&#39;\Sigma^{-1}Z + B^{-1}}
\end{array}
\right]
\left[
\begin{array}
{c}
\beta \\
\mathbf{b}
\end{array}
\right]
= 
\left[
\begin{array}
{c}
\mathbf{X&#39;\Sigma^{-1}Y} \\
\mathbf{Z&#39;\Sigma^{-1}Y}
\end{array}
\right]
\]</span></p>
<p>Thus, the solution to the mixed model equations give:</p>
<p><span class="math display">\[
\left[
\begin{array}
{c}
\hat{\beta} \\
\hat{\mathbf{b}}
\end{array}
\right]
= 
\left[
\begin{array}
{cc}
\mathbf{X&#39;\Sigma^{-1}X} &amp; \mathbf{X&#39;\Sigma^{-1}Z} \\
\mathbf{Z&#39;\Sigma^{-1}X} &amp; \mathbf{Z&#39;\Sigma^{-1}Z + B^{-1}}
\end{array}
\right]
\left[
\begin{array}
{c}
\mathbf{X&#39;\Sigma^{-1}Y} \\
\mathbf{Z&#39;\Sigma^{-1}Y}
\end{array}
\right]
\]</span></p>
<p><br></p>
<p>Equivalently,</p>
<p>Bayes’ theorem</p>
<p><span class="math display">\[
f(\mathbf{b}| \mathbf{Y}) = \frac{f(\mathbf{Y}|\mathbf{b})f(\mathbf{b})}{\int f(\mathbf{Y}|\mathbf{b})f(\mathbf{b}) d\mathbf{b}}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(f(\mathbf{Y}|\mathbf{b})\)</span> is the “likelihood”</li>
<li><span class="math inline">\(f(\mathbf{b})\)</span> is the prior</li>
<li>the denominator is the “normalizing constant”</li>
<li><span class="math inline">\(f(\mathbf{b}|\mathbf{Y})\)</span> is the posterior distribution</li>
</ul>
<p>In this case</p>
<p><span class="math display">\[
\mathbf{Y} | \mathbf{b} \sim N(\mathbf{X\beta+Zb,\Sigma}) \\
\mathbf{b} \sim N(\mathbf{0,B})
\]</span></p>
<p>The posterior distribution has the form</p>
<p><span class="math display">\[
\mathbf{b}|\mathbf{Y} \sim N(\mathbf{BZ&#39;V^{-1}(Y-X\beta),(Z&#39;\Sigma^{-1}Z + B^{-1})^{-1}})
\]</span></p>
<p>Hence, the best predictor (based on squared error loss)</p>
<p><span class="math display">\[
E(\mathbf{b}|\mathbf{Y}) = \mathbf{BZ&#39;V^{-1}(Y-X\beta)}
\]</span></p>
<div id="estimating-mathbfv" class="section level4" number="6.4.2.1">
<h4><span class="header-section-number">6.4.2.1</span> Estimating <span class="math inline">\(\mathbf{V}\)</span></h4>
<p>If we have <span class="math inline">\(\tilde{\mathbf{V}}\)</span> (estimate of <span class="math inline">\(\mathbf{V}\)</span>), then we can estimate:</p>
<p><span class="math display">\[
\hat{\beta} = \mathbf{(X&#39;\tilde{V}^{-1}X)^{-1}X&#39;\tilde{V}^{-1}Y} \\
\hat{\mathbf{b}} = \mathbf{BZ&#39;\tilde{V}^{-1}(Y-X\hat{\beta})}
\]</span></p>
<p>where <span class="math inline">\({\mathbf{b}}\)</span> is <strong>EBLUP</strong> (estimated BLUP) or <strong>empirical Bayes estimate</strong></p>
<p>Note:</p>
<ul>
<li><span class="math inline">\(\hat{var}(\hat{\beta})\)</span> is a consistent estimator of <span class="math inline">\(var(\hat{\beta})\)</span> if <span class="math inline">\(\tilde{\mathbf{V}}\)</span> is a consistent estimator of <span class="math inline">\(\mathbf{V}\)</span></li>
<li>However, <span class="math inline">\(\hat{var}(\hat{\beta})\)</span> is biased since the variability arises from estimating <span class="math inline">\(\mathbf{V}\)</span> is not accounted for in the estimate.</li>
<li>Hence, <span class="math inline">\(\hat{var}(\hat{\beta})\)</span> underestimates the true variability</li>
</ul>
<p>Ways to estimate <span class="math inline">\(\mathbf{V}\)</span></p>
<ul>
<li><a href="generalized-linear-mixed-models.html#maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</a></li>
<li><a href="generalized-linear-mixed-models.html#restricted-maximum-likelihood-reml">Restricted Maximum Likelihood (REML)</a></li>
<li><a href="generalized-linear-mixed-models.html#estimated-generalized-least-squares">Estimated Generalized Least Squares</a></li>
<li><a href="generalized-linear-mixed-models.html#bayesian-hierarchical-models-bhm">Bayesian Hierarchical Models (BHM)</a></li>
</ul>
<div id="maximum-likelihood-estimation-mle" class="section level5" number="6.4.2.1.1">
<h5><span class="header-section-number">6.4.2.1.1</span> Maximum Likelihood Estimation (MLE)</h5>
<p>Grouping unknown parameters in <span class="math inline">\(\Sigma\)</span> and <span class="math inline">\(B\)</span> under a parameter vector <span class="math inline">\(\theta\)</span>. Under MLE, <span class="math inline">\(\hat{\theta}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> maximize the likelihood <span class="math inline">\(\mathbf{y} \sim N(\mathbf{X\beta, V(\theta))}\)</span>. Synonymously, <span class="math inline">\(-2\log L(\mathbf{y;\theta,\beta})\)</span>:</p>
<p><span class="math display">\[
-2l(\mathbf{\beta,\theta,y}) = \log |\mathbf{V(\theta)}| + \mathbf{(y-X\beta)&#39;V(\theta)^{-1}(y-X\beta)} + N \log(2\pi)
\]</span></p>
<ul>
<li>Step 1: Replace <span class="math inline">\(\beta\)</span> with its maximum likelihood (where <span class="math inline">\(\theta\)</span> is known <span class="math inline">\(\hat{\beta}= (\mathbf{X&#39;V(\theta)^{-1}X)^{-1}X&#39;V(\theta)^{-1}y}\)</span></li>
<li>Step 2: Minimize the above equation with respect to <span class="math inline">\(\theta\)</span> to get the estimator <span class="math inline">\(\hat{\theta}_{MLE}\)</span></li>
<li>Step 3: Substitute <span class="math inline">\(\hat{\theta}_{MLE}\)</span> back to get <span class="math inline">\(\hat{\beta}_{MLE} = (\mathbf{X&#39;V(\theta_{MLE})^{-1}X)^{-1}X&#39;V(\theta_{MLE})^{-1}y}\)</span></li>
<li>Step 4: Get <span class="math inline">\(\hat{\mathbf{b}}_{MLE} = \mathbf{B(\hat{\theta}_{MLE})Z&#39;V(\hat{\theta}_{MLE})^{-1}(y-X\hat{\beta}_{MLE})}\)</span></li>
</ul>
<p>Note:</p>
<ul>
<li><span class="math inline">\(\hat{\theta}\)</span> are typically negatively biased due to unaccounted fixed effects being estimated, which we could try to account for.</li>
</ul>
<p><br></p>
</div>
<div id="restricted-maximum-likelihood-reml" class="section level5" number="6.4.2.1.2">
<h5><span class="header-section-number">6.4.2.1.2</span> Restricted Maximum Likelihood (REML)</h5>
<p>REML accounts for the number of estimated mean parameters by adjusting the objective function. Specifically, the likelihood of linear combination of the elements of <span class="math inline">\(\mathbf{y}\)</span> is accounted for.</p>
<p>We have <span class="math inline">\(\mathbf{K&#39;y}\)</span>, where <span class="math inline">\(\mathbf{K}\)</span> is any <span class="math inline">\(N \times (N - p)\)</span> full-rank contrast matrix, which has columns orthogonal to the <span class="math inline">\(\mathbf{X}\)</span> matrix (that is <span class="math inline">\(\mathbf{K&#39;X} = 0\)</span>). Then,</p>
<p><span class="math display">\[
\mathbf{K&#39;y} \sim N(0,\mathbf{K&#39;V(\theta)K})
\]</span></p>
<p>where <span class="math inline">\(\beta\)</span> is no longer in the distribution</p>
<p>We can proceed to maximize this likelihood for the contrasts to get <span class="math inline">\(\hat{\theta}_{REML}\)</span>, which does not depend on the choice of <span class="math inline">\(\mathbf{K}\)</span>. And <span class="math inline">\(\hat{\beta}\)</span> are based on <span class="math inline">\(\hat{\theta}\)</span></p>
<p>Comparison REML and MLE</p>
<ul>
<li><p>Both methods are based upon the likelihood principle, and have desired properties for the estimates:</p>
<ul>
<li><p>consistency</p></li>
<li><p>asymptotic normality</p></li>
<li><p>efficiency</p></li>
</ul></li>
<li><p>ML estimation provides estimates for fixed effects, while REML can’t</p></li>
<li><p>In balanced models, REML is identical to ANOVA</p></li>
<li><p>REML accounts for df for the fixed effects int eh model, which is important when <span class="math inline">\(\mathbf{X}\)</span> is large relative to the sample size</p></li>
<li><p>Changing <span class="math inline">\(\mathbf{\beta}\)</span> has no effect on the REML estimates of <span class="math inline">\(\theta\)</span></p></li>
<li><p>REML is less sensitive to outliers than MLE</p></li>
<li><p>MLE is better than REML regarding model comparisons (e.g., AIC or BIC)</p></li>
</ul>
<p><br></p>
</div>
<div id="estimated-generalized-least-squares" class="section level5" number="6.4.2.1.3">
<h5><span class="header-section-number">6.4.2.1.3</span> Estimated Generalized Least Squares</h5>
<p>MLE and REML rely upon the Gaussian assumption. To overcome this issue, EGLS uses the first and second moments.</p>
<p><span class="math display">\[
\mathbf{Y}_i = \mathbf{X}_i \beta + \mathbf{Z}_i \mathbf{b}_i + \epsilon_i
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\epsilon_i \sim (\mathbf{0,\Sigma_i})\)</span></li>
<li><span class="math inline">\(\mathbf{b}_i \sim (\mathbf{0,D})\)</span></li>
<li><span class="math inline">\(cov(\epsilon_i, \mathbf{b}_i) = 0\)</span></li>
</ul>
<p>Then the EGLS estimator is</p>
<p><span class="math display">\[
\begin{align}
\hat{\beta}_{GLS} &amp;= \{\sum_{i=1}^n \mathbf{X&#39;_iV_i(\theta)^{-1}X_i}  \}^{-1} \sum_{i=1}^n \mathbf{X&#39;_iV_i(\theta)^{-1}Y_i} \\
&amp;=\{\mathbf{X&#39;V(\theta)^{-1}X} \}^{-1} \mathbf{X&#39;V(\theta)^{-1}Y}
\end{align}
\]</span></p>
<p>depends on the first two moments</p>
<ul>
<li><span class="math inline">\(E(\mathbf{Y}_i) = \mathbf{X}_i \beta\)</span></li>
<li><span class="math inline">\(var(\mathbf{Y}_i)= \mathbf{V}_i\)</span></li>
</ul>
<p>EGLS use <span class="math inline">\(\hat{\mathbf{V}}\)</span> for <span class="math inline">\(\mathbf{V(\theta)}\)</span></p>
<p><span class="math display">\[
\hat{\beta}_{EGLS} = \{ \mathbf{X&#39;\hat{V}^{-1}X} \}^{-1} \mathbf{X&#39;\hat{V}^{-1}Y}
\]</span></p>
<p>Hence, the fixed effects estimators for the MLE, REML, and EGLS are of the same form, except for the estimate of <span class="math inline">\(\mathbf{V}\)</span></p>
<p>In case of non-iterative approach, EGLS can be appealing when <span class="math inline">\(\mathbf{V}\)</span> can be estimated without much computational burden.</p>
<p><br></p>
</div>
<div id="bayesian-hierarchical-models-bhm" class="section level5" number="6.4.2.1.4">
<h5><span class="header-section-number">6.4.2.1.4</span> Bayesian Hierarchical Models (BHM)</h5>
<p>Joint distribution cane be decomposed hierarchically in terms of the product of conditional distributions and a marginal distribution</p>
<p><span class="math display">\[
f(A,B,C) = f(A|B,C) f(B|C)f(C)
\]</span></p>
<p>Applying to estimate <span class="math inline">\(\mathbf{V}\)</span></p>
<p><span class="math display">\[
\begin{align}
f(\mathbf{Y, \beta, b, \theta}) &amp;= f(\mathbf{Y|\beta,b, \theta})f(\mathbf{b|\theta,\beta})f(\mathbf{\beta|\theta})f(\mathbf{\theta}) &amp;&amp; \text{based on probability decomposition} \\
&amp;= f(\mathbf{Y|\beta,b, \theta})f(\mathbf{b|\theta})f(\mathbf{\beta})f(\mathbf{\theta}) &amp;&amp; \text{based on simplifying modeling assumptions}
\end{align}
\]</span></p>
<p>elaborate on the second equality, if we assume conditional independence (e.g., given <span class="math inline">\(\theta\)</span>, no additional info about <span class="math inline">\(\mathbf{b}\)</span> is given by knowing <span class="math inline">\(\beta\)</span>), then we can simply from the first equality</p>
<p>Using Bayes’ rule</p>
<p><span class="math display">\[
f(\mathbf{\beta, b, \theta|Y}) \propto f(\mathbf{Y|\beta,b, \theta})f(\mathbf{b|\theta})f(\mathbf{\beta})f(\mathbf{\theta})
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\mathbf{Y| \beta, b, \theta \sim N(X\beta+ Zb, \Sigma(\theta))} \\
\mathbf{b | \theta \sim N(0, B(\theta))}
\]</span></p>
<p>and we also have to have prior distributions for <span class="math inline">\(f(\beta), f(\theta)\)</span></p>
<p>With normalizing constant, we can obtain the posterior distribution. Typically, we can’t get analytical solution right away. Hence, we can use Markov Chain Monte Carlo (MCMC) to obtain samples from the posterior distribution.</p>
<p>Bayesian Methods:</p>
<ul>
<li>account for the uncertainty in parameters estimates and accommodate the propagation of that uncertainty through the model</li>
<li>can adjust prior information (i.e., priori) in parameters</li>
<li>Can extend beyond Gaussian distributions</li>
<li>but hard to implement algorithms and might have problem converging</li>
</ul>
</div>
</div>
</div>
<div id="inference-3" class="section level3" number="6.4.3">
<h3><span class="header-section-number">6.4.3</span> Inference</h3>
<div id="parameters-beta" class="section level4" number="6.4.3.1">
<h4><span class="header-section-number">6.4.3.1</span> Parameters <span class="math inline">\(\beta\)</span></h4>
<div id="wald-test-GLMM" class="section level5" number="6.4.3.1.1">
<h5><span class="header-section-number">6.4.3.1.1</span> Wald test</h5>
<p>We have</p>
<p><span class="math display">\[
\mathbf{\hat{\beta}(\theta) = \{X&#39;V^{-1}(\theta) X\}^{-1}X&#39;V^{-1}(\theta) Y} \\
var(\hat{\beta}(\theta)) = \mathbf{\{X&#39;V^{-1}(\theta) X\}^{-1}}
\]</span></p>
<p>We can use <span class="math inline">\(\hat{\theta}\)</span> in place of <span class="math inline">\(\theta\)</span> to approximate Wald test</p>
<p><span class="math display">\[
H_0: \mathbf{A \beta =d} 
\]</span></p>
<p>With</p>
<p><span class="math display">\[
W = \mathbf{(A\hat{\beta} - d)&#39;[A(X&#39;\hat{V}^{-1}X)^{-1}A&#39;]^{-1}(A\hat{\beta} - d)}
\]</span></p>
<p>where <span class="math inline">\(W \sim \chi^2_{rank(A)}\)</span> under <span class="math inline">\(H_0\)</span> is true. However, it does not take into account variability from using <span class="math inline">\(\hat{\theta}\)</span> in place of <span class="math inline">\(\theta\)</span>, hence the standard errors are underestimated</p>
<p><br></p>
</div>
<div id="f-test-1" class="section level5" number="6.4.3.1.2">
<h5><span class="header-section-number">6.4.3.1.2</span> F-test</h5>
<p>Alternatively, we can use the modified F-test, suppose we have <span class="math inline">\(var(\mathbf{Y}) = \sigma^2 \mathbf{V}(\theta)\)</span>, then</p>
<p><span class="math display">\[
F^* = \frac{\mathbf{(A\hat{\beta} - d)&#39;[A(X&#39;\hat{V}^{-1}X)^{-1}A&#39;]^{-1}(A\hat{\beta} - d)}}{\hat{\sigma}^2 \text{rank}(A)}
\]</span></p>
<p>where <span class="math inline">\(F^* \sim f_{rank(A), den(df)}\)</span> under the null hypothesis. And den(df) needs to be approximated from the data by either:</p>
<ul>
<li>Satterthwaite method</li>
<li>Kenward-Roger approximation</li>
</ul>
<p>Under balanced cases, the Wald and F tests are similar. But for small sample sizes, they can differ in p-values. And both can be reduced to t-test for a single <span class="math inline">\(\beta\)</span></p>
<p><br></p>
</div>
<div id="likelihood-ratio-test" class="section level5" number="6.4.3.1.3">
<h5><span class="header-section-number">6.4.3.1.3</span> Likelihood Ratio Test</h5>
<p><span class="math display">\[
H_0: \beta \in \Theta_{\beta,0}
\]</span></p>
<p>where <span class="math inline">\(\Theta_{\beta, 0}\)</span> is a subspace of the parameter space, <span class="math inline">\(\Theta_{\beta}\)</span> of the fixed effects <span class="math inline">\(\beta\)</span> . Then</p>
<p><span class="math display">\[
-2\log \lambda_N = -2\log\{\frac{\hat{L}_{ML,0}}{\hat{L}_{ML}}\}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\hat{L}_{ML,0}\)</span> , <span class="math inline">\(\hat{L}_{ML}\)</span> are the maximized likelihood obtained from maximizing over <span class="math inline">\(\Theta_{\beta,0}\)</span> and <span class="math inline">\(\Theta_{\beta}\)</span></li>
<li><span class="math inline">\(-2 \log \lambda_N \dot{\sim} \chi^2_{df}\)</span> where df is the difference in the dimension (i.e., number of parameters) of <span class="math inline">\(\Theta_{\beta,0}\)</span> and <span class="math inline">\(\Theta_{\beta}\)</span></li>
</ul>
<p>This method is not applicable for REML. But REML can still be used to test for covariance parameters between nested models.</p>
<p><br></p>
</div>
</div>
<div id="variance-components" class="section level4" number="6.4.3.2">
<h4><span class="header-section-number">6.4.3.2</span> Variance Components</h4>
<ul>
<li><p>For ML and REML estimator, <span class="math inline">\(\hat{\theta} \sim N(\theta, I(\theta))\)</span> for large samples</p></li>
<li><p>Wald test in variance components is analogous to the fixed effects case (see <a href="generalized-linear-mixed-models.html#wald-test-GLMM">6.4.3.1.1</a> )</p>
<ul>
<li><p>However, the normal approximation depends largely on the true value of <span class="math inline">\(\theta\)</span>. It will fail if the true value of <span class="math inline">\(\theta\)</span> is close to the boundary of the parameter space <span class="math inline">\(\Theta_{\theta}\)</span> (i.e., <span class="math inline">\(\sigma^2 \approx 0\)</span>)</p></li>
<li><p>Typically works better for covariance parameter, than vairance prarmetesr.</p></li>
</ul></li>
<li><p>The likelihood ratio tests can also be used with ML or REML estimates. However, the same problem of parameters</p></li>
</ul>
<p><br></p>
</div>
</div>
<div id="information-criteria" class="section level3" number="6.4.4">
<h3><span class="header-section-number">6.4.4</span> Information Criteria</h3>
<ul>
<li>account for the likelihood and the number of parameters to assess model comparison.</li>
</ul>
<div id="akaikes-information-criteria-aic" class="section level4" number="6.4.4.1">
<h4><span class="header-section-number">6.4.4.1</span> Akaike’s Information Criteria (AIC)</h4>
<p>Derived as an estimator of the expected Kullback discrepancy between the true model and a fitted candidate model</p>
<p><span class="math display">\[
AIC = -2l(\hat{\theta}, \hat{\beta}) + 2q
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(l(\hat{\theta}, \hat{\beta})\)</span> is the log-likelihood</li>
<li>q = the effective number of parameters; total of fixed and those associated with random effects (variance/covariance; those not estimated to be on a boundary constraint)</li>
</ul>
<p>Note:</p>
<ul>
<li>In comparing models that differ in their random effects, this method is not advised to due the inability to get the correct number of effective parameters).</li>
<li>We prefer smaller AIC values.</li>
<li>If your program uses <span class="math inline">\(l-q\)</span> then we prefer larger AIC values (but rarely).</li>
<li>can be used for mixed model section, (e.g., selection of the covariance structure), but the sample size must be very large to have adequate comparison based on the criterion</li>
<li>Can have a large negative bias (e.g., when sample size is small but the number of parameters is large) due to the penalty term can’t approximate the bias adjustment adequately</li>
</ul>
<p><br></p>
</div>
<div id="corrected-aic-aicc" class="section level4" number="6.4.4.2">
<h4><span class="header-section-number">6.4.4.2</span> Corrected AIC (AICC)</h4>
<ul>
<li>developed by <span class="citation">(<a href="references.html#ref-HURVICH_1989" role="doc-biblioref">HURVICH and TSAI 1989</a>)</span></li>
<li>correct small-sample adjustment</li>
<li>depends on the candidate model class</li>
<li>Only if you have fixed covariance structure, then AICC is justified, but not general covariance structure</li>
</ul>
<p><br></p>
</div>
<div id="bayesian-information-criteria-bic" class="section level4" number="6.4.4.3">
<h4><span class="header-section-number">6.4.4.3</span> Bayesian Information Criteria (BIC)</h4>
<p><span class="math display">\[
BIC = -2l(\hat{\theta}, \hat{\beta}) + q \log n
\]</span></p>
<p>where n = number of observations.</p>
<ul>
<li>we prefer smaller BIC value</li>
<li>BIC and AIC are used for both REML and MLE if we have the same mean structure. Otherwise, in general, we should prefer MLE</li>
</ul>
<p><br></p>
<p>With our example presented at the beginning of <a href="generalized-linear-mixed-models.html#generalized-linear-mixed-models">Generalized Linear Mixed Models</a>,</p>
<p><span class="math display">\[
Y_{ik}= 
\begin{cases}
\beta_0 + b_{1i} + (\beta_1 + \ b_{2i})t_{ij} + \epsilon_{ij} &amp;&amp; L \\
\beta_0 + b_{1i} + (\beta_2 + \ b_{2i})t_{ij} + \epsilon_{ij} &amp;&amp; H\\
\beta_0 + b_{1i} + (\beta_3 + \ b_{2i})t_{ij} + \epsilon_{ij} &amp;&amp; C
\end{cases}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(i = 1,..,N\)</span></li>
<li><span class="math inline">\(j = 1,..,n_i\)</span> (measures at time <span class="math inline">\(t_{ij}\)</span>)</li>
</ul>
<p>Note:</p>
<ul>
<li>we have subject-specific intercepts,</li>
</ul>
<p><span class="math display">\[
\mathbf{Y}_i |b_i \sim N(\mathbf{X}_i \beta + 1 b_i, \sigma^2 \mathbf{I}) \\
b_i \sim N(0,d_{11})
\]</span></p>
<p>here, we want to estimate <span class="math inline">\(\beta, \sigma^2, d_{11}\)</span> and predict <span class="math inline">\(b_i\)</span></p>
<p><br></p>
</div>
</div>
<div id="split-plot-designs" class="section level3" number="6.4.5">
<h3><span class="header-section-number">6.4.5</span> Split-Plot Designs</h3>
<ul>
<li>Typically used in the case that you have two factors where one needs much larger units than the other.</li>
</ul>
<p>Example:</p>
<p>A: 3 levels (large units)</p>
<p>B: 2 levels (small units)</p>
<ul>
<li>A and B levels are randomized into 4 blocks.</li>
<li>But it differs from <a href="randomized-block-designs.html#randomized-block-designs">Randomized Block Designs</a>. In each block, both have one of the 6 (3x2) treatment combinations. But <a href="randomized-block-designs.html#randomized-block-designs">Randomized Block Designs</a> assign in each block randomly, while split-plot does not randomize this step.</li>
<li>Moreover, because A needs to be applied in large units, factor A is applied only once in each block while B can be applied multiple times.</li>
</ul>
<p>Hence, we have our model</p>
<p>If A is our factor of interest</p>
<p><span class="math display">\[
Y_{ij} = \mu + \rho_i + \alpha_j + e_{ij}
\]</span></p>
<p>where</p>
<ul>
<li>i = replication (block or subject)</li>
<li>j = level of Factor A</li>
<li><span class="math inline">\(\mu\)</span> = overall mean</li>
<li><span class="math inline">\(\rho_i\)</span> = variation due to the i-th block</li>
<li><span class="math inline">\(e_{ij} \sim N(0, \sigma^2_e)\)</span> = whole plot error</li>
</ul>
<p>If B is our factor of interest</p>
<p><span class="math display">\[
Y_{ijk} = \mu + \phi_{ij} + \beta_k + \epsilon_{ijk}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\phi_{ij}\)</span> = variation due to the ij-th main plot</li>
<li><span class="math inline">\(\beta_k\)</span> = Factor B effect</li>
<li><span class="math inline">\(\epsilon_{ijk} \sim N(0, \sigma^2_\epsilon)\)</span> = subplot error</li>
<li><span class="math inline">\(\phi_{ij} = \rho_i + \alpha_j + e_{ij}\)</span></li>
</ul>
<p>Together, the split-plot model</p>
<p><span class="math display">\[
Y_{ijk} = \mu + \rho_i + \alpha_j + e_{ij} + \beta_k + (\alpha \beta)_{jk} + \epsilon_{ijk}
\]</span></p>
<p>where</p>
<ul>
<li>i = replicate (blocks or subjects)</li>
<li>j = level of factor A</li>
<li>k = level of factor B</li>
<li><span class="math inline">\(\mu\)</span> = overall mean</li>
<li><span class="math inline">\(\rho_i\)</span> = effect of the block</li>
<li><span class="math inline">\(\alpha_j\)</span> = main effect of factor A (fixed)</li>
<li><span class="math inline">\(e_{ij} = (\rho \alpha)_{ij}\)</span> = block by factor A interaction (the whole plot error, random)</li>
<li><span class="math inline">\(\beta_k\)</span> = main effect of factor B (fixed)</li>
<li><span class="math inline">\((\alpha \beta)_{jk}\)</span> = interaction between factors A and B (fixed)</li>
<li><span class="math inline">\(\epsilon_{ijk}\)</span> = subplot error (random)</li>
</ul>
<p>We can approach sub-plot analysis based on</p>
<ul>
<li><p>the ANOVA perspective</p>
<ul>
<li><p>Whole plot comparisons</p>
<ul>
<li><p>Compare factor A to the whole plot error (i.e., <span class="math inline">\(\alpha_j\)</span> to <span class="math inline">\(e_{ij}\)</span>)</p></li>
<li><p>Compare the block to the whole plot error (i.e., <span class="math inline">\(\rho_i\)</span> to <span class="math inline">\(e_{ij}\)</span>)</p></li>
</ul></li>
<li><p>Sub-plot comparisons:</p>
<ul>
<li><p>Compare factor B to the subplot error (<span class="math inline">\(\beta\)</span> to <span class="math inline">\(\epsilon_{ijk}\)</span>)</p></li>
<li><p>Compare the AB interaction to the subplot error (<span class="math inline">\((\alpha \beta)_{jk}\)</span> to <span class="math inline">\(\epsilon_{ijk}\)</span>)</p></li>
</ul></li>
</ul></li>
<li><p>the mixed model perspective</p></li>
</ul>
<p><span class="math display">\[
\mathbf{Y = X \beta + Zb + \epsilon}
\]</span></p>
<p><br></p>
</div>
<div id="repeated-measures-in-mixed-models" class="section level3" number="6.4.6">
<h3><span class="header-section-number">6.4.6</span> Repeated Measures in Mixed Models</h3>
<p><span class="math display">\[
Y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \delta_{i(k)}+ \epsilon_{ijk}
\]</span></p>
<p>where</p>
<ul>
<li>i-th group (fixed)</li>
<li>j-th (repeated measure) time effect (fixed)</li>
<li>k-th subject</li>
<li><span class="math inline">\(\delta_{i(k)} \sim N(0,\sigma^2_\delta)\)</span> (k-th subject in the i-th group) and <span class="math inline">\(\epsilon_{ijk} \sim N(0,\sigma^2)\)</span> (independent error) are random effects (<span class="math inline">\(i = 1,..,n_A, j = 1,..,n_B, k = 1,...,n_i\)</span>)</li>
</ul>
<p>hence, the variance-covariance matrix of the repeated observations on the k-th subject of the i-th group, <span class="math inline">\(\mathbf{Y}_{ik} = (Y_{i1k},..,Y_{in_Bk})&#39;\)</span>, will be</p>
<p><span class="math display">\[
\begin{align}
\mathbf{\Sigma}_{subject} &amp;=
\left(
\begin{array}
{cccc}
\sigma^2_\delta + \sigma^2 &amp; \sigma^2_\delta &amp; ... &amp; \sigma^2_\delta \\
\sigma^2_\delta &amp; \sigma^2_\delta +\sigma^2 &amp; ... &amp; \sigma^2_\delta \\
. &amp; . &amp; . &amp; . \\
\sigma^2_\delta &amp; \sigma^2_\delta &amp; ... &amp; \sigma^2_\delta + \sigma^2 \\
\end{array}
\right) \\
&amp;= (\sigma^2_\delta + \sigma^2)
\left(
\begin{array}
{cccc}
1 &amp; \rho &amp; ... &amp; \rho \\
\rho &amp; 1 &amp; ... &amp; \rho \\
. &amp; . &amp; . &amp; . \\
\rho &amp; \rho &amp; ... &amp; 1 \\
\end{array}
\right) 
&amp;&amp; \text{product of a scalar and a correlation matrix}
\end{align}
\]</span></p>
<p>where <span class="math inline">\(\rho = \frac{\sigma^2_\delta}{\sigma^2_\delta + \sigma^2}\)</span>, which is the compound symmetry structure that we discussed in [Random-Intercepts Model]</p>
<p>But if you only have repeated measurements on the subject over time, AR(1) structure might be more appropriate</p>
<p>Mixed model for a repeated measure</p>
<p><span class="math display">\[
Y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijk}
\]</span> where</p>
<ul>
<li><span class="math inline">\(\epsilon_{ijk}\)</span> combines random error of both the whole and subplots.</li>
</ul>
<p>In general,</p>
<p><span class="math display">\[
\mathbf{Y = X \beta + \epsilon}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\epsilon \sim N(0, \sigma^2 \mathbf{\Sigma})\)</span> where <span class="math inline">\(\mathbf{\Sigma}\)</span> is block diagonal if the random error covariance is the same for each subject</li>
</ul>
<p>The variance covariance matrix with AR(1) structure is</p>
<p><span class="math display">\[
\mathbf{\Sigma}_{subject} =
\sigma^2
\left(
\begin{array}
{ccccc}
1  &amp; \rho &amp; \rho^2 &amp; ... &amp; \rho^{n_B-1} \\
\rho &amp; 1 &amp; \rho &amp; ... &amp; \rho^{n_B-2} \\
. &amp; . &amp; . &amp; . &amp; . \\
\rho^{n_B-1} &amp; \rho^{n_B-2} &amp; \rho^{n_B-3} &amp; ... &amp; 1 \\
\end{array}
\right)
\]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="generalized-linear-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="generalized-method-of-moments.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mikenguyen13/data_analysis/edit/main/06-2-nonlinear_regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Data Analysis.pdf", "Data Analysis.epub", "Data Analysis.mobi"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true,
"sharing": {
"facebook": true,
"github": true,
"twitter": true,
"linkedin": true
},
"info": true,
"edit": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
