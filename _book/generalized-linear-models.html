<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6.3 Generalized Linear Models | A Guide on Data Analysis</title>
  <meta name="description" content="This is a guide on how to conduct a data analysis routine" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="6.3 Generalized Linear Models | A Guide on Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a guide on how to conduct a data analysis routine" />
  <meta name="github-repo" content="mikenguyen13/data_analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6.3 Generalized Linear Models | A Guide on Data Analysis" />
  
  <meta name="twitter:description" content="This is a guide on how to conduct a data analysis routine" />
  

<meta name="author" content="Mike Nguyen" />


<meta name="date" content="2021-02-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="non-linear-least-squares.html"/>
<link rel="next" href="genelized-method-of-moments.html"/>
<script src="libs/jquery-3.5.0/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/proj4js-2.3.15/proj4.js"></script>
<link href="libs/highcharts-8.1.2/css/motion.css" rel="stylesheet" />
<script src="libs/highcharts-8.1.2/highcharts.js"></script>
<script src="libs/highcharts-8.1.2/highcharts-3d.js"></script>
<script src="libs/highcharts-8.1.2/highcharts-more.js"></script>
<script src="libs/highcharts-8.1.2/modules/stock.js"></script>
<script src="libs/highcharts-8.1.2/modules/map.js"></script>
<script src="libs/highcharts-8.1.2/modules/annotations.js"></script>
<script src="libs/highcharts-8.1.2/modules/data.js"></script>
<script src="libs/highcharts-8.1.2/modules/drilldown.js"></script>
<script src="libs/highcharts-8.1.2/modules/item-series.js"></script>
<script src="libs/highcharts-8.1.2/modules/offline-exporting.js"></script>
<script src="libs/highcharts-8.1.2/modules/overlapping-datalabels.js"></script>
<script src="libs/highcharts-8.1.2/modules/exporting.js"></script>
<script src="libs/highcharts-8.1.2/modules/export-data.js"></script>
<script src="libs/highcharts-8.1.2/modules/funnel.js"></script>
<script src="libs/highcharts-8.1.2/modules/heatmap.js"></script>
<script src="libs/highcharts-8.1.2/modules/treemap.js"></script>
<script src="libs/highcharts-8.1.2/modules/sankey.js"></script>
<script src="libs/highcharts-8.1.2/modules/dependency-wheel.js"></script>
<script src="libs/highcharts-8.1.2/modules/organization.js"></script>
<script src="libs/highcharts-8.1.2/modules/solid-gauge.js"></script>
<script src="libs/highcharts-8.1.2/modules/streamgraph.js"></script>
<script src="libs/highcharts-8.1.2/modules/sunburst.js"></script>
<script src="libs/highcharts-8.1.2/modules/vector.js"></script>
<script src="libs/highcharts-8.1.2/modules/wordcloud.js"></script>
<script src="libs/highcharts-8.1.2/modules/xrange.js"></script>
<script src="libs/highcharts-8.1.2/modules/tilemap.js"></script>
<script src="libs/highcharts-8.1.2/modules/venn.js"></script>
<script src="libs/highcharts-8.1.2/modules/gantt.js"></script>
<script src="libs/highcharts-8.1.2/modules/timeline.js"></script>
<script src="libs/highcharts-8.1.2/modules/parallel-coordinates.js"></script>
<script src="libs/highcharts-8.1.2/modules/bullet.js"></script>
<script src="libs/highcharts-8.1.2/modules/coloraxis.js"></script>
<script src="libs/highcharts-8.1.2/modules/dumbbell.js"></script>
<script src="libs/highcharts-8.1.2/modules/lollipop.js"></script>
<script src="libs/highcharts-8.1.2/modules/series-label.js"></script>
<script src="libs/highcharts-8.1.2/plugins/motion.js"></script>
<script src="libs/highcharts-8.1.2/custom/reset.js"></script>
<script src="libs/highcharts-8.1.2/modules/boost.js"></script>
<script src="libs/highchart-binding-0.8.2/highchart.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Guide on Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a><ul>
<li class="chapter" data-level="2.1" data-path="matrix-theory.html"><a href="matrix-theory.html"><i class="fa fa-check"></i><b>2.1</b> Matrix Theory</a><ul>
<li class="chapter" data-level="2.1.1" data-path="matrix-theory.html"><a href="matrix-theory.html#rank"><i class="fa fa-check"></i><b>2.1.1</b> Rank</a></li>
<li class="chapter" data-level="2.1.2" data-path="matrix-theory.html"><a href="matrix-theory.html#inverse"><i class="fa fa-check"></i><b>2.1.2</b> Inverse</a></li>
<li class="chapter" data-level="2.1.3" data-path="matrix-theory.html"><a href="matrix-theory.html#definiteness"><i class="fa fa-check"></i><b>2.1.3</b> Definiteness</a></li>
<li class="chapter" data-level="2.1.4" data-path="matrix-theory.html"><a href="matrix-theory.html#matrix-calculus"><i class="fa fa-check"></i><b>2.1.4</b> Matrix Calculus</a></li>
<li class="chapter" data-level="2.1.5" data-path="matrix-theory.html"><a href="matrix-theory.html#optimization"><i class="fa fa-check"></i><b>2.1.5</b> Optimization</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>2.2</b> Probability Theory</a><ul>
<li class="chapter" data-level="2.2.1" data-path="probability-theory.html"><a href="probability-theory.html#axiom-and-theorems-of-probability"><i class="fa fa-check"></i><b>2.2.1</b> Axiom and Theorems of Probability</a></li>
<li class="chapter" data-level="2.2.2" data-path="probability-theory.html"><a href="probability-theory.html#central-limit-theorem"><i class="fa fa-check"></i><b>2.2.2</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="2.2.3" data-path="probability-theory.html"><a href="probability-theory.html#random-variable"><i class="fa fa-check"></i><b>2.2.3</b> Random variable</a></li>
<li class="chapter" data-level="2.2.4" data-path="probability-theory.html"><a href="probability-theory.html#moment-generating-function"><i class="fa fa-check"></i><b>2.2.4</b> Moment generating function</a></li>
<li class="chapter" data-level="2.2.5" data-path="probability-theory.html"><a href="probability-theory.html#moment"><i class="fa fa-check"></i><b>2.2.5</b> Moment</a></li>
<li class="chapter" data-level="2.2.6" data-path="probability-theory.html"><a href="probability-theory.html#distributions"><i class="fa fa-check"></i><b>2.2.6</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="general-math.html"><a href="general-math.html"><i class="fa fa-check"></i><b>2.3</b> General Math</a><ul>
<li class="chapter" data-level="2.3.1" data-path="general-math.html"><a href="general-math.html#law-of-large-numbers"><i class="fa fa-check"></i><b>2.3.1</b> Law of large numbers</a></li>
<li class="chapter" data-level="2.3.2" data-path="general-math.html"><a href="general-math.html#law-of-iterated-expectation"><i class="fa fa-check"></i><b>2.3.2</b> Law of Iterated Expectation</a></li>
<li class="chapter" data-level="2.3.3" data-path="general-math.html"><a href="general-math.html#convergence"><i class="fa fa-check"></i><b>2.3.3</b> Convergence</a></li>
<li class="chapter" data-level="2.3.4" data-path="general-math.html"><a href="general-math.html#sufficient-statistics"><i class="fa fa-check"></i><b>2.3.4</b> Sufficient Statistics</a></li>
<li class="chapter" data-level="2.3.5" data-path="general-math.html"><a href="general-math.html#parameter-transformations"><i class="fa fa-check"></i><b>2.3.5</b> Parameter transformations</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>2.4</b> Methods</a></li>
<li class="chapter" data-level="2.5" data-path="data-manipulation.html"><a href="data-manipulation.html"><i class="fa fa-check"></i><b>2.5</b> Data Manipulation</a></li>
</ul></li>
<li class="part"><span><b>I BASIC</b></span></li>
<li class="chapter" data-level="3" data-path="descriptive-stat.html"><a href="descriptive-stat.html"><i class="fa fa-check"></i><b>3</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="3.1" data-path="numerical-measures.html"><a href="numerical-measures.html"><i class="fa fa-check"></i><b>3.1</b> Numerical Measures</a></li>
<li class="chapter" data-level="3.2" data-path="graphical-measures.html"><a href="graphical-measures.html"><i class="fa fa-check"></i><b>3.2</b> Graphical Measures</a><ul>
<li class="chapter" data-level="3.2.1" data-path="graphical-measures.html"><a href="graphical-measures.html#shape"><i class="fa fa-check"></i><b>3.2.1</b> Shape</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="normality-assessment.html"><a href="normality-assessment.html"><i class="fa fa-check"></i><b>3.3</b> Normality Assessment</a><ul>
<li class="chapter" data-level="3.3.1" data-path="normality-assessment.html"><a href="normality-assessment.html#graphical-assessment"><i class="fa fa-check"></i><b>3.3.1</b> Graphical Assessment</a></li>
<li class="chapter" data-level="3.3.2" data-path="normality-assessment.html"><a href="normality-assessment.html#summary-statistics"><i class="fa fa-check"></i><b>3.3.2</b> Summary Statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="basic-statistical-inference.html"><a href="basic-statistical-inference.html"><i class="fa fa-check"></i><b>4</b> Basic Statistical Inference</a><ul>
<li class="chapter" data-level="4.1" data-path="one-sample-inference.html"><a href="one-sample-inference.html"><i class="fa fa-check"></i><b>4.1</b> One Sample Inference</a><ul>
<li class="chapter" data-level="4.1.1" data-path="one-sample-inference.html"><a href="one-sample-inference.html#the-mean"><i class="fa fa-check"></i><b>4.1.1</b> The Mean</a></li>
<li class="chapter" data-level="4.1.2" data-path="one-sample-inference.html"><a href="one-sample-inference.html#single-variance"><i class="fa fa-check"></i><b>4.1.2</b> Single Variance</a></li>
<li class="chapter" data-level="4.1.3" data-path="one-sample-inference.html"><a href="one-sample-inference.html#single-proportion-p"><i class="fa fa-check"></i><b>4.1.3</b> Single Proportion (p)</a></li>
<li class="chapter" data-level="4.1.4" data-path="one-sample-inference.html"><a href="one-sample-inference.html#power"><i class="fa fa-check"></i><b>4.1.4</b> Power</a></li>
<li class="chapter" data-level="4.1.5" data-path="one-sample-inference.html"><a href="one-sample-inference.html#sample-size"><i class="fa fa-check"></i><b>4.1.5</b> Sample Size</a></li>
<li class="chapter" data-level="4.1.6" data-path="one-sample-inference.html"><a href="one-sample-inference.html#note"><i class="fa fa-check"></i><b>4.1.6</b> Note</a></li>
<li class="chapter" data-level="4.1.7" data-path="one-sample-inference.html"><a href="one-sample-inference.html#one-sample-non-parametric-methods"><i class="fa fa-check"></i><b>4.1.7</b> One-sample Non-parametric Methods</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="two-sample-inference.html"><a href="two-sample-inference.html"><i class="fa fa-check"></i><b>4.2</b> Two Sample Inference</a><ul>
<li class="chapter" data-level="4.2.1" data-path="two-sample-inference.html"><a href="two-sample-inference.html#means"><i class="fa fa-check"></i><b>4.2.1</b> Means</a></li>
<li class="chapter" data-level="4.2.2" data-path="two-sample-inference.html"><a href="two-sample-inference.html#variances"><i class="fa fa-check"></i><b>4.2.2</b> Variances</a></li>
<li class="chapter" data-level="4.2.3" data-path="two-sample-inference.html"><a href="two-sample-inference.html#power-1"><i class="fa fa-check"></i><b>4.2.3</b> Power</a></li>
<li class="chapter" data-level="4.2.4" data-path="two-sample-inference.html"><a href="two-sample-inference.html#sample-size-1"><i class="fa fa-check"></i><b>4.2.4</b> Sample Size</a></li>
<li class="chapter" data-level="4.2.5" data-path="two-sample-inference.html"><a href="two-sample-inference.html#matched-pair-designs"><i class="fa fa-check"></i><b>4.2.5</b> Matched Pair Designs</a></li>
<li class="chapter" data-level="4.2.6" data-path="two-sample-inference.html"><a href="two-sample-inference.html#nonparametric-tests-for-two-samples"><i class="fa fa-check"></i><b>4.2.6</b> Nonparametric Tests for Two Samples</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html"><i class="fa fa-check"></i><b>4.3</b> Categorical Data Analysis</a><ul>
<li class="chapter" data-level="4.3.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#inferences-for-small-samples"><i class="fa fa-check"></i><b>4.3.1</b> Inferences for Small Samples</a></li>
<li class="chapter" data-level="4.3.2" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#test-of-association"><i class="fa fa-check"></i><b>4.3.2</b> Test of Association</a></li>
<li class="chapter" data-level="4.3.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#ordinal-association"><i class="fa fa-check"></i><b>4.3.3</b> Ordinal Association</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II REGRESSION</b></span></li>
<li class="chapter" data-level="5" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html"><i class="fa fa-check"></i><b>5.1</b> Ordinary Least Squares</a><ul>
<li class="chapter" data-level="5.1.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#simple-regression-basic-model"><i class="fa fa-check"></i><b>5.1.1</b> Simple Regression (Basic Model)</a></li>
<li class="chapter" data-level="5.1.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#multiple-linear-regression"><i class="fa fa-check"></i><b>5.1.2</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="5.1.3" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#ols-assumptions"><i class="fa fa-check"></i><b>5.1.3</b> OLS Assumptions</a></li>
<li class="chapter" data-level="5.1.4" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#theorems"><i class="fa fa-check"></i><b>5.1.4</b> Theorems</a></li>
<li class="chapter" data-level="5.1.5" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#variable-selection"><i class="fa fa-check"></i><b>5.1.5</b> Variable Selection</a></li>
<li class="chapter" data-level="5.1.6" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#diagnostics-1"><i class="fa fa-check"></i><b>5.1.6</b> Diagnostics</a></li>
<li class="chapter" data-level="5.1.7" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#model-validation"><i class="fa fa-check"></i><b>5.1.7</b> Model Validation</a></li>
<li class="chapter" data-level="5.1.8" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#finite-sample-properties"><i class="fa fa-check"></i><b>5.1.8</b> Finite Sample Properties</a></li>
<li class="chapter" data-level="5.1.9" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#large-sample-properties"><i class="fa fa-check"></i><b>5.1.9</b> Large Sample Properties</a></li>
<li class="chapter" data-level="5.1.10" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#application"><i class="fa fa-check"></i><b>5.1.10</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="feasible-generalized-least-squares.html"><a href="feasible-generalized-least-squares.html"><i class="fa fa-check"></i><b>5.2</b> Feasible Generalized Least Squares</a><ul>
<li class="chapter" data-level="5.2.1" data-path="feasible-generalized-least-squares.html"><a href="feasible-generalized-least-squares.html#heteroskedasticity"><i class="fa fa-check"></i><b>5.2.1</b> Heteroskedasticity</a></li>
<li class="chapter" data-level="5.2.2" data-path="feasible-generalized-least-squares.html"><a href="feasible-generalized-least-squares.html#serial-correlation"><i class="fa fa-check"></i><b>5.2.2</b> Serial Correlation</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="weighted-least-squares.html"><a href="weighted-least-squares.html"><i class="fa fa-check"></i><b>5.3</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="5.4" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html"><i class="fa fa-check"></i><b>5.4</b> Generalized Least Squares</a></li>
<li class="chapter" data-level="5.5" data-path="feasiable-prais-winsten.html"><a href="feasiable-prais-winsten.html"><i class="fa fa-check"></i><b>5.5</b> Feasiable Prais Winsten</a></li>
<li class="chapter" data-level="5.6" data-path="feasible-group-level-random-effects.html"><a href="feasible-group-level-random-effects.html"><i class="fa fa-check"></i><b>5.6</b> Feasible group level Random Effects</a></li>
<li class="chapter" data-level="5.7" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>5.7</b> Ridge Regression</a></li>
<li class="chapter" data-level="5.8" data-path="principal-component-regression.html"><a href="principal-component-regression.html"><i class="fa fa-check"></i><b>5.8</b> Principal Component Regression</a></li>
<li class="chapter" data-level="5.9" data-path="robust-regression.html"><a href="robust-regression.html"><i class="fa fa-check"></i><b>5.9</b> Robust Regression</a><ul>
<li class="chapter" data-level="5.9.1" data-path="robust-regression.html"><a href="robust-regression.html#least-absolute-residuals-lar-regression"><i class="fa fa-check"></i><b>5.9.1</b> Least Absolute Residuals (LAR) Regression</a></li>
<li class="chapter" data-level="5.9.2" data-path="robust-regression.html"><a href="robust-regression.html#least-median-of-squares-lms-regression"><i class="fa fa-check"></i><b>5.9.2</b> Least Median of Squares (LMS) Regression</a></li>
<li class="chapter" data-level="5.9.3" data-path="robust-regression.html"><a href="robust-regression.html#iteratively-reweighted-least-squares-irls-robust-regression"><i class="fa fa-check"></i><b>5.9.3</b> Iteratively Reweighted Least Squares (IRLS) Robust Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html"><i class="fa fa-check"></i><b>5.10</b> Maximum Likelihood</a><ul>
<li class="chapter" data-level="5.10.1" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#motivation-for-mle"><i class="fa fa-check"></i><b>5.10.1</b> Motivation for MLE</a></li>
<li class="chapter" data-level="5.10.2" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#assumption"><i class="fa fa-check"></i><b>5.10.2</b> Assumption</a></li>
<li class="chapter" data-level="5.10.3" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#properties"><i class="fa fa-check"></i><b>5.10.3</b> Properties</a></li>
<li class="chapter" data-level="5.10.4" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#application-1"><i class="fa fa-check"></i><b>5.10.4</b> Application</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="non-linear-regression.html"><a href="non-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Non-linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="inference-1.html"><a href="inference-1.html"><i class="fa fa-check"></i><b>6.1</b> Inference</a><ul>
<li class="chapter" data-level="6.1.1" data-path="inference-1.html"><a href="inference-1.html#linear-function-of-the-parameters"><i class="fa fa-check"></i><b>6.1.1</b> Linear Function of the Parameters</a></li>
<li class="chapter" data-level="6.1.2" data-path="inference-1.html"><a href="inference-1.html#nonlinear"><i class="fa fa-check"></i><b>6.1.2</b> Nonlinear</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="non-linear-least-squares.html"><a href="non-linear-least-squares.html"><i class="fa fa-check"></i><b>6.2</b> Non-linear Least Squares</a><ul>
<li class="chapter" data-level="6.2.1" data-path="non-linear-least-squares.html"><a href="non-linear-least-squares.html#alternative-of-gauss-newton-algorithm"><i class="fa fa-check"></i><b>6.2.1</b> Alternative of Gauss-Newton Algorithm</a></li>
<li class="chapter" data-level="6.2.2" data-path="non-linear-least-squares.html"><a href="non-linear-least-squares.html#practical-considerations"><i class="fa fa-check"></i><b>6.2.2</b> Practical Considerations</a></li>
<li class="chapter" data-level="6.2.3" data-path="non-linear-least-squares.html"><a href="non-linear-least-squares.html#modelestiamtion-adequcy"><i class="fa fa-check"></i><b>6.2.3</b> Model/Estiamtion Adequcy</a></li>
<li class="chapter" data-level="6.2.4" data-path="non-linear-least-squares.html"><a href="non-linear-least-squares.html#application-2"><i class="fa fa-check"></i><b>6.2.4</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>6.3</b> Generalized Linear Models</a><ul>
<li class="chapter" data-level="6.3.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#logistic-regression"><i class="fa fa-check"></i><b>6.3.1</b> Logistic Regression</a></li>
<li class="chapter" data-level="6.3.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#probit-regression"><i class="fa fa-check"></i><b>6.3.2</b> Probit Regression</a></li>
<li class="chapter" data-level="6.3.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson-regression"><i class="fa fa-check"></i><b>6.3.3</b> Poisson Regression</a></li>
<li class="chapter" data-level="6.3.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#generalization"><i class="fa fa-check"></i><b>6.3.4</b> Generalization</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="genelized-method-of-moments.html"><a href="genelized-method-of-moments.html"><i class="fa fa-check"></i><b>6.4</b> Genelized Method of Moments</a></li>
<li class="chapter" data-level="6.5" data-path="minimum-distance.html"><a href="minimum-distance.html"><i class="fa fa-check"></i><b>6.5</b> Minimum Distance</a></li>
<li class="chapter" data-level="6.6" data-path="spline-regression.html"><a href="spline-regression.html"><i class="fa fa-check"></i><b>6.6</b> Spline Regression</a><ul>
<li class="chapter" data-level="6.6.1" data-path="spline-regression.html"><a href="spline-regression.html#regression-splines"><i class="fa fa-check"></i><b>6.6.1</b> Regression Splines</a></li>
<li class="chapter" data-level="6.6.2" data-path="spline-regression.html"><a href="spline-regression.html#natural-splines"><i class="fa fa-check"></i><b>6.6.2</b> Natural splines</a></li>
<li class="chapter" data-level="6.6.3" data-path="spline-regression.html"><a href="spline-regression.html#smoothing-spliness"><i class="fa fa-check"></i><b>6.6.3</b> Smoothing spliness</a></li>
<li class="chapter" data-level="6.6.4" data-path="spline-regression.html"><a href="spline-regression.html#application-3"><i class="fa fa-check"></i><b>6.6.4</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="generalized-additive-models.html"><a href="generalized-additive-models.html"><i class="fa fa-check"></i><b>6.7</b> Generalized Additive Models</a></li>
<li class="chapter" data-level="6.8" data-path="quantile-regression.html"><a href="quantile-regression.html"><i class="fa fa-check"></i><b>6.8</b> Quantile Regression</a><ul>
<li class="chapter" data-level="6.8.1" data-path="quantile-regression.html"><a href="quantile-regression.html#application-4"><i class="fa fa-check"></i><b>6.8.1</b> Application</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="model-specification.html"><a href="model-specification.html"><i class="fa fa-check"></i><b>7</b> Model Specification</a><ul>
<li class="chapter" data-level="7.1" data-path="nested-model.html"><a href="nested-model.html"><i class="fa fa-check"></i><b>7.1</b> Nested Model</a><ul>
<li class="chapter" data-level="7.1.1" data-path="nested-model.html"><a href="nested-model.html#chow-test"><i class="fa fa-check"></i><b>7.1.1</b> Chow test</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="non-nested-model.html"><a href="non-nested-model.html"><i class="fa fa-check"></i><b>7.2</b> Non-Nested Model</a><ul>
<li class="chapter" data-level="7.2.1" data-path="non-nested-model.html"><a href="non-nested-model.html#davidson-mackinnon-test"><i class="fa fa-check"></i><b>7.2.1</b> Davidson-Mackinnon test</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="heteroskedasticity-1.html"><a href="heteroskedasticity-1.html"><i class="fa fa-check"></i><b>7.3</b> Heteroskedasticity</a><ul>
<li class="chapter" data-level="7.3.1" data-path="heteroskedasticity-1.html"><a href="heteroskedasticity-1.html#breusch-pagan-test"><i class="fa fa-check"></i><b>7.3.1</b> Breusch-Pagan test</a></li>
<li class="chapter" data-level="7.3.2" data-path="heteroskedasticity-1.html"><a href="heteroskedasticity-1.html#white-test"><i class="fa fa-check"></i><b>7.3.2</b> White test</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="endogeneity.html"><a href="endogeneity.html"><i class="fa fa-check"></i><b>8</b> Endogeneity</a><ul>
<li class="chapter" data-level="8.1" data-path="endogenous-treatment.html"><a href="endogenous-treatment.html"><i class="fa fa-check"></i><b>8.1</b> Endogenous Treatment</a><ul>
<li class="chapter" data-level="8.1.1" data-path="endogenous-treatment.html"><a href="endogenous-treatment.html#instrumental-variable"><i class="fa fa-check"></i><b>8.1.1</b> Instrumental Variable</a></li>
<li class="chapter" data-level="8.1.2" data-path="endogenous-treatment.html"><a href="endogenous-treatment.html#internal-instrumental-variable"><i class="fa fa-check"></i><b>8.1.2</b> Internal instrumental variable</a></li>
<li class="chapter" data-level="8.1.3" data-path="endogenous-treatment.html"><a href="endogenous-treatment.html#proxy-variables"><i class="fa fa-check"></i><b>8.1.3</b> Proxy Variables</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="endogenous-sample-selection.html"><a href="endogenous-sample-selection.html"><i class="fa fa-check"></i><b>8.2</b> Endogenous Sample Selection</a><ul>
<li class="chapter" data-level="8.2.1" data-path="endogenous-sample-selection.html"><a href="endogenous-sample-selection.html#tobit-2"><i class="fa fa-check"></i><b>8.2.1</b> Tobit-2</a></li>
<li class="chapter" data-level="8.2.2" data-path="endogenous-sample-selection.html"><a href="endogenous-sample-selection.html#tobit-5"><i class="fa fa-check"></i><b>8.2.2</b> Tobit-5</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>9</b> Data</a><ul>
<li class="chapter" data-level="9.1" data-path="cross-sectional.html"><a href="cross-sectional.html"><i class="fa fa-check"></i><b>9.1</b> Cross-Sectional</a></li>
<li class="chapter" data-level="9.2" data-path="time-series-1.html"><a href="time-series-1.html"><i class="fa fa-check"></i><b>9.2</b> Time Series</a><ul>
<li class="chapter" data-level="9.2.1" data-path="time-series-1.html"><a href="time-series-1.html#deterministic-time-trend"><i class="fa fa-check"></i><b>9.2.1</b> Deterministic Time trend</a></li>
<li class="chapter" data-level="9.2.2" data-path="time-series-1.html"><a href="time-series-1.html#feedback-effect"><i class="fa fa-check"></i><b>9.2.2</b> Feedback Effect</a></li>
<li class="chapter" data-level="9.2.3" data-path="time-series-1.html"><a href="time-series-1.html#dynamic-specification"><i class="fa fa-check"></i><b>9.2.3</b> Dynamic Specification</a></li>
<li class="chapter" data-level="9.2.4" data-path="time-series-1.html"><a href="time-series-1.html#dynamically-complete"><i class="fa fa-check"></i><b>9.2.4</b> Dynamically Complete</a></li>
<li class="chapter" data-level="9.2.5" data-path="time-series-1.html"><a href="time-series-1.html#highly-persistent-data"><i class="fa fa-check"></i><b>9.2.5</b> Highly Persistent Data</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="repeated-cross-sections.html"><a href="repeated-cross-sections.html"><i class="fa fa-check"></i><b>9.3</b> Repeated Cross Sections</a><ul>
<li class="chapter" data-level="9.3.1" data-path="repeated-cross-sections.html"><a href="repeated-cross-sections.html#pooled-cross-section"><i class="fa fa-check"></i><b>9.3.1</b> Pooled Cross Section</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="panel-data.html"><a href="panel-data.html"><i class="fa fa-check"></i><b>9.4</b> Panel Data</a><ul>
<li class="chapter" data-level="9.4.1" data-path="panel-data.html"><a href="panel-data.html#pooled-ols-esimator"><i class="fa fa-check"></i><b>9.4.1</b> Pooled OLS Esimator</a></li>
<li class="chapter" data-level="9.4.2" data-path="panel-data.html"><a href="panel-data.html#individual-specific-effects-model"><i class="fa fa-check"></i><b>9.4.2</b> Individual-specific effects model</a></li>
<li class="chapter" data-level="9.4.3" data-path="panel-data.html"><a href="panel-data.html#tests-for-assumptions"><i class="fa fa-check"></i><b>9.4.3</b> Tests for Assumptions</a></li>
<li class="chapter" data-level="9.4.4" data-path="panel-data.html"><a href="panel-data.html#model-selection"><i class="fa fa-check"></i><b>9.4.4</b> Model Selection</a></li>
<li class="chapter" data-level="9.4.5" data-path="panel-data.html"><a href="panel-data.html#summary-1"><i class="fa fa-check"></i><b>9.4.5</b> Summary</a></li>
<li class="chapter" data-level="9.4.6" data-path="panel-data.html"><a href="panel-data.html#application-5"><i class="fa fa-check"></i><b>9.4.6</b> Application</a></li>
<li class="chapter" data-level="9.4.7" data-path="panel-data.html"><a href="panel-data.html#other-estimators"><i class="fa fa-check"></i><b>9.4.7</b> Other Estimators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>10</b> Hypothesis Testing</a><ul>
<li class="chapter" data-level="10.1" data-path="types-of-hypothesis-testing.html"><a href="types-of-hypothesis-testing.html"><i class="fa fa-check"></i><b>10.1</b> Types of hypothesis testing</a></li>
<li class="chapter" data-level="10.2" data-path="wald-test.html"><a href="wald-test.html"><i class="fa fa-check"></i><b>10.2</b> Wald test</a><ul>
<li class="chapter" data-level="10.2.1" data-path="wald-test.html"><a href="wald-test.html#multiple-hypothesis"><i class="fa fa-check"></i><b>10.2.1</b> Multiple Hypothesis</a></li>
<li class="chapter" data-level="10.2.2" data-path="wald-test.html"><a href="wald-test.html#linear-combination"><i class="fa fa-check"></i><b>10.2.2</b> Linear Combination</a></li>
<li class="chapter" data-level="10.2.3" data-path="wald-test.html"><a href="wald-test.html#application-6"><i class="fa fa-check"></i><b>10.2.3</b> Application</a></li>
<li class="chapter" data-level="10.2.4" data-path="wald-test.html"><a href="wald-test.html#nonlinear-1"><i class="fa fa-check"></i><b>10.2.4</b> Nonlinear</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="the-likelihood-ratio-test.html"><a href="the-likelihood-ratio-test.html"><i class="fa fa-check"></i><b>10.3</b> The likelihood ratio test</a></li>
<li class="chapter" data-level="10.4" data-path="lagrange-multiplier-score.html"><a href="lagrange-multiplier-score.html"><i class="fa fa-check"></i><b>10.4</b> Lagrange Multiplier (Score)</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="imputation-missing-data.html"><a href="imputation-missing-data.html"><i class="fa fa-check"></i><b>11</b> Imputation (Missing Data)</a><ul>
<li class="chapter" data-level="11.1" data-path="assumptions-1.html"><a href="assumptions-1.html"><i class="fa fa-check"></i><b>11.1</b> Assumptions</a><ul>
<li class="chapter" data-level="11.1.1" data-path="assumptions-1.html"><a href="assumptions-1.html#missing-completely-at-random-mcar"><i class="fa fa-check"></i><b>11.1.1</b> Missing Completely at Random (MCAR)</a></li>
<li class="chapter" data-level="11.1.2" data-path="assumptions-1.html"><a href="assumptions-1.html#missing-at-random-mar"><i class="fa fa-check"></i><b>11.1.2</b> Missing at Random (MAR)</a></li>
<li class="chapter" data-level="11.1.3" data-path="assumptions-1.html"><a href="assumptions-1.html#ignorable"><i class="fa fa-check"></i><b>11.1.3</b> Ignorable</a></li>
<li class="chapter" data-level="11.1.4" data-path="assumptions-1.html"><a href="assumptions-1.html#nonignorable"><i class="fa fa-check"></i><b>11.1.4</b> Nonignorable</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html"><i class="fa fa-check"></i><b>11.2</b> Solutions to Missing data</a><ul>
<li class="chapter" data-level="11.2.1" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#listwise-deletion"><i class="fa fa-check"></i><b>11.2.1</b> Listwise Deletion</a></li>
<li class="chapter" data-level="11.2.2" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#pairwise-deletion"><i class="fa fa-check"></i><b>11.2.2</b> Pairwise Deletion</a></li>
<li class="chapter" data-level="11.2.3" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#dummy-variable-adjustment"><i class="fa fa-check"></i><b>11.2.3</b> Dummy Variable Adjustment</a></li>
<li class="chapter" data-level="11.2.4" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#imputation"><i class="fa fa-check"></i><b>11.2.4</b> Imputation</a></li>
<li class="chapter" data-level="11.2.5" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#heckmans-sample-selection-model"><i class="fa fa-check"></i><b>11.2.5</b> Heckman’s Sample Selection Model</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="criteria-for-choosing-an-effective-approach.html"><a href="criteria-for-choosing-an-effective-approach.html"><i class="fa fa-check"></i><b>11.3</b> Criteria for Choosing an Effective Approach</a></li>
<li class="chapter" data-level="11.4" data-path="another-perspective.html"><a href="another-perspective.html"><i class="fa fa-check"></i><b>11.4</b> Another Perspective</a></li>
<li class="chapter" data-level="11.5" data-path="diagnosing-the-mechanism.html"><a href="diagnosing-the-mechanism.html"><i class="fa fa-check"></i><b>11.5</b> Diagnosing the Mechanism</a><ul>
<li class="chapter" data-level="11.5.1" data-path="diagnosing-the-mechanism.html"><a href="diagnosing-the-mechanism.html#mar-vs.-mnar"><i class="fa fa-check"></i><b>11.5.1</b> MAR vs. MNAR</a></li>
<li class="chapter" data-level="11.5.2" data-path="diagnosing-the-mechanism.html"><a href="diagnosing-the-mechanism.html#mcar-vs.-mar"><i class="fa fa-check"></i><b>11.5.2</b> MCAR vs. MAR</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="application-7.html"><a href="application-7.html"><i class="fa fa-check"></i><b>11.6</b> Application</a><ul>
<li class="chapter" data-level="11.6.1" data-path="application-7.html"><a href="application-7.html#imputation-with-mean-median-mode"><i class="fa fa-check"></i><b>11.6.1</b> Imputation with mean / median / mode</a></li>
<li class="chapter" data-level="11.6.2" data-path="application-7.html"><a href="application-7.html#knn"><i class="fa fa-check"></i><b>11.6.2</b> KNN</a></li>
<li class="chapter" data-level="11.6.3" data-path="application-7.html"><a href="application-7.html#rpart"><i class="fa fa-check"></i><b>11.6.3</b> rpart</a></li>
<li class="chapter" data-level="11.6.4" data-path="application-7.html"><a href="application-7.html#mice-multivariate-imputation-via-chained-equations"><i class="fa fa-check"></i><b>11.6.4</b> MICE (Multivariate Imputation via Chained Equations)</a></li>
<li class="chapter" data-level="11.6.5" data-path="application-7.html"><a href="application-7.html#amelia"><i class="fa fa-check"></i><b>11.6.5</b> Amelia</a></li>
<li class="chapter" data-level="11.6.6" data-path="application-7.html"><a href="application-7.html#missforest"><i class="fa fa-check"></i><b>11.6.6</b> missForest</a></li>
<li class="chapter" data-level="11.6.7" data-path="application-7.html"><a href="application-7.html#hmisc"><i class="fa fa-check"></i><b>11.6.7</b> Hmisc</a></li>
<li class="chapter" data-level="11.6.8" data-path="application-7.html"><a href="application-7.html#mi"><i class="fa fa-check"></i><b>11.6.8</b> mi</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III EXPERIMENTAL DESIGN</b></span></li>
<li class="chapter" data-level="12" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html"><i class="fa fa-check"></i><b>12</b> Analysis of Variance (ANOVA)</a><ul>
<li class="chapter" data-level="12.1" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html"><i class="fa fa-check"></i><b>12.1</b> Completely Randomized Design (CRD)</a><ul>
<li class="chapter" data-level="12.1.1" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#single-factor-fixed-effects-model"><i class="fa fa-check"></i><b>12.1.1</b> Single Factor Fixed Effects Model</a></li>
<li class="chapter" data-level="12.1.2" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#single-factor-random-effects-model"><i class="fa fa-check"></i><b>12.1.2</b> Single Factor Random Effects Model</a></li>
<li class="chapter" data-level="12.1.3" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#two-factor-fixed-effect-anova"><i class="fa fa-check"></i><b>12.1.3</b> Two Factor Fixed Effect ANOVA</a></li>
<li class="chapter" data-level="12.1.4" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#two-way-random-effects-anova"><i class="fa fa-check"></i><b>12.1.4</b> Two-Way Random Effects ANOVA</a></li>
<li class="chapter" data-level="12.1.5" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#two-way-mixed-effects-anova"><i class="fa fa-check"></i><b>12.1.5</b> Two-Way Mixed Effects ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="nonparametric-anova.html"><a href="nonparametric-anova.html"><i class="fa fa-check"></i><b>12.2</b> Nonparametric ANOVA</a><ul>
<li class="chapter" data-level="12.2.1" data-path="nonparametric-anova.html"><a href="nonparametric-anova.html#kruskal-wallis"><i class="fa fa-check"></i><b>12.2.1</b> Kruskal-Wallis</a></li>
<li class="chapter" data-level="12.2.2" data-path="nonparametric-anova.html"><a href="nonparametric-anova.html#friedman-test"><i class="fa fa-check"></i><b>12.2.2</b> Friedman Test</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="sample-size-planning-for-anova.html"><a href="sample-size-planning-for-anova.html"><i class="fa fa-check"></i><b>12.3</b> Sample Size Planning for ANOVA</a><ul>
<li class="chapter" data-level="12.3.1" data-path="sample-size-planning-for-anova.html"><a href="sample-size-planning-for-anova.html#balanced-designs"><i class="fa fa-check"></i><b>12.3.1</b> Balanced Designs</a></li>
<li class="chapter" data-level="12.3.2" data-path="sample-size-planning-for-anova.html"><a href="sample-size-planning-for-anova.html#randomized-block-experiments"><i class="fa fa-check"></i><b>12.3.2</b> Randomized Block Experiments</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="randomized-block-designs.html"><a href="randomized-block-designs.html"><i class="fa fa-check"></i><b>12.4</b> Randomized Block Designs</a><ul>
<li class="chapter" data-level="12.4.1" data-path="randomized-block-designs.html"><a href="randomized-block-designs.html#tukey-test-of-additivity"><i class="fa fa-check"></i><b>12.4.1</b> Tukey Test of Additivity</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="nested-designs.html"><a href="nested-designs.html"><i class="fa fa-check"></i><b>12.5</b> Nested Designs</a><ul>
<li class="chapter" data-level="12.5.1" data-path="nested-designs.html"><a href="nested-designs.html#two-factor-nested-designs"><i class="fa fa-check"></i><b>12.5.1</b> Two-Factor Nested Designs</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="analysis-of-covariance.html"><a href="analysis-of-covariance.html"><i class="fa fa-check"></i><b>13</b> Analysis of Covariance</a><ul>
<li class="chapter" data-level="13.1" data-path="single-factor-covariance-model.html"><a href="single-factor-covariance-model.html"><i class="fa fa-check"></i><b>13.1</b> Single Factor Covariance Model</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="causality.html"><a href="causality.html"><i class="fa fa-check"></i><b>14</b> Causality</a></li>
<li class="chapter" data-level="15" data-path="report.html"><a href="report.html"><i class="fa fa-check"></i><b>15</b> Report</a><ul>
<li class="chapter" data-level="15.1" data-path="one-summary-table.html"><a href="one-summary-table.html"><i class="fa fa-check"></i><b>15.1</b> One summary table</a></li>
<li class="chapter" data-level="15.2" data-path="model-comparison.html"><a href="model-comparison.html"><i class="fa fa-check"></i><b>15.2</b> Model Comparison</a></li>
<li class="chapter" data-level="15.3" data-path="changes-in-an-estimate.html"><a href="changes-in-an-estimate.html"><i class="fa fa-check"></i><b>15.3</b> Changes in an estimate</a></li>
</ul></li>
<li class="appendix"><span><b>APPENDIX</b></span></li>
<li class="chapter" data-level="A" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>A</b> Appendix</a><ul>
<li class="chapter" data-level="A.1" data-path="short-cut.html"><a href="short-cut.html"><i class="fa fa-check"></i><b>A.1</b> Short-cut</a></li>
<li class="chapter" data-level="A.2" data-path="function-short-cut.html"><a href="function-short-cut.html"><i class="fa fa-check"></i><b>A.2</b> Function short-cut</a></li>
<li class="chapter" data-level="A.3" data-path="citation.html"><a href="citation.html"><i class="fa fa-check"></i><b>A.3</b> Citation</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="bookdown-cheat-sheet.html"><a href="bookdown-cheat-sheet.html"><i class="fa fa-check"></i><b>B</b> Bookdown cheat sheet</a><ul>
<li class="chapter" data-level="B.1" data-path="operation.html"><a href="operation.html"><i class="fa fa-check"></i><b>B.1</b> Operation</a></li>
<li class="chapter" data-level="B.2" data-path="math-expresssion-syntax.html"><a href="math-expresssion-syntax.html"><i class="fa fa-check"></i><b>B.2</b> Math Expresssion/ Syntax</a><ul>
<li class="chapter" data-level="B.2.1" data-path="math-expresssion-syntax.html"><a href="math-expresssion-syntax.html#statistics-notation"><i class="fa fa-check"></i><b>B.2.1</b> Statistics Notation</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="table.html"><a href="table.html"><i class="fa fa-check"></i><b>B.3</b> Table</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Guide on Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="generalized-linear-models" class="section level2">
<h2><span class="header-section-number">6.3</span> Generalized Linear Models</h2>
<p>Even though we call it generalized linear model, it is still under the paradigm of non-linear regression, because the form of the regression model is non-linear. The name generalized linear model derived from the fact that we have <span class="math inline">\(\mathbf{x&#39;_i \beta}\)</span> (which is linear form) in the model.</p>
<div id="logistic-regression" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Logistic Regression</h3>
<p><span class="math display">\[
p_i = f(\mathbf{x}_i ; \beta) = \frac{exp(\mathbf{x_i&#39;\beta})}{1 + exp(\mathbf{x_i&#39;\beta})}
\]</span>
Equivalently,</p>
<p><span class="math display">\[
logit(p_i) = log(\frac{p_i}{1+p_i}) = \mathbf{x_i&#39;\beta}
\]</span>
where <span class="math inline">\(\frac{p_i}{1+p_i}\)</span>is the <strong>odds</strong>.</p>
<p>In this form, the model is specified such that <strong>a function of the mean response is linear</strong>. Hence, <strong>Generalized Linear Models</strong></p>
<p>The likelihood function</p>
<p><span class="math display">\[
L(p_i) = \prod_{i=1}^{n} p_i^{Y_i}(1-p_i)^{1-Y_i}
\]</span>
where <span class="math inline">\(p_i = \frac{\mathbf{x&#39;_i \beta}}{1+\mathbf{x&#39;_i \beta}}\)</span> and <span class="math inline">\(1-p_i = (1+ exp(\mathbf{x&#39;_i \beta}))^{-1}\)</span></p>
<p>Hence, our objective function is</p>
<p><span class="math display">\[
Q(\beta) = log(L(\beta)) = \sum_{i=1}^n Y_i \mathbf{x&#39;_i \beta} - \sum_{i=1}^n  log(1+ exp(\mathbf{x&#39;_i \beta}))
\]</span></p>
<p>we could maximize this function numerically using the optimization method above, which allows us to find numerical MLE for <span class="math inline">\(\hat{\beta}\)</span>. Then we can use the standard asymptotic properties of MLEs to make inference.</p>
<p>Property of MLEs is that parameters are asymptotically unbiased with sample variance-covariance matrix given by the <strong>inverse Fisher information matrix</strong></p>
<p><span class="math display">\[
\hat{\beta} \dot{\sim} AN(\beta,[\mathbf{I}(\beta)]^{-1})
\]</span>
where the <strong>Fisher Information matrix</strong>, <span class="math inline">\(\mathbf{I}(\beta)\)</span> is</p>
<p><span class="math display">\[
\begin{align}
\mathbf{I}(\beta) &amp;= E[\frac{\partial \log(L(\beta))}{\partial (\beta)}\frac{\partial \log(L(\beta))}{\partial \beta&#39;}] \\
&amp;= E[(\frac{\partial \log(L(\beta))}{\partial \beta_i} \frac{\partial \log(L(\beta))}{\partial \beta_j})_{ij}]
\end{align}
\]</span>
Under <strong>regularity conditions</strong>, this is equivalent to the negative of the expected value of the Hessian Matrix</p>
<p><span class="math display">\[
\begin{align}
\mathbf{I}(\beta) &amp;= -E[\frac{\partial^2 \log(L(\beta))}{\partial \beta \partial \beta&#39;}] \\
&amp;= -E[(\frac{\partial^2 \log(L(\beta))}{\partial \beta_i \partial \beta_j})_{ij}]
\end{align}
\]</span></p>
<p>Example:</p>
<p><span class="math display">\[
x_i&#39; \beta = \beta_0 + \beta_1 x_i
\]</span></p>
<p><span class="math display">\[
- \frac{\partial^2 \ln(L(\beta))}{\partial \beta^2_0} = \sum_{i=1}^n \frac{\exp(x&#39;_i \beta)}{1 + \exp(x&#39;_i \beta)} - [\frac{\exp(x_i&#39; \beta)}{1+ \exp(x&#39;_i \beta)}]^2 = \sum_{i=1}^n p_i (1-p_i) \\
- \frac{\partial^2 \ln(L(\beta))}{\partial \beta^2_1} = \sum_{i=1}^n \frac{x_i^2\exp(x&#39;_i \beta)}{1 + \exp(x&#39;_i \beta)} - [\frac{x_i\exp(x_i&#39; \beta)}{1+ \exp(x&#39;_i \beta)}]^2 = \sum_{i=1}^n x_i^2p_i (1-p_i) \\
- \frac{\partial^2 \ln(L(\beta))}{\partial \beta_0 \partial \beta_1} = \sum_{i=1}^n \frac{x_i\exp(x&#39;_i \beta)}{1 + \exp(x&#39;_i \beta)} - x_i[\frac{\exp(x_i&#39; \beta)}{1+ \exp(x&#39;_i \beta)}]^2 = \sum_{i=1}^n x_ip_i (1-p_i) \\
\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[
\mathbf{I} (\beta) = 
\left[
\begin{array}
{cc}
\sum_i p_i(1-p_i) &amp;&amp; \sum_i x_i p_i(1-p_i) \\
\sum_i x_i p_i(1-p_i) &amp;&amp; \sum_i x_i^2 p_i(1-p_i)
\end{array}
\right]
\]</span></p>
<p><strong>Inference</strong></p>
<p><strong>Likelihood Ratio Tests</strong></p>
<p>To formulate the test, let <span class="math inline">\(\beta = [\beta_1&#39;, \beta_2&#39;]&#39;\)</span>. If you are interested in testing a hypothesis about <span class="math inline">\(\beta_1\)</span>, then we leave <span class="math inline">\(\beta_2\)</span> unspecified (called <strong>nuisance parameters</strong>). <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> can either a <strong>vector</strong> or <strong>scalar</strong>, or <span class="math inline">\(\beta_2\)</span> can be null.</p>
<p>Example: <span class="math inline">\(H_0: \beta_1 = \beta_{1,0}\)</span> (where <span class="math inline">\(\beta_{1,0}\)</span> is specified) and <span class="math inline">\(\hat{\beta}_{2,0}\)</span> be the MLE of <span class="math inline">\(\beta_2\)</span> under the restriction that <span class="math inline">\(\beta_1 = \beta_{1,0}\)</span>. The likelihood ratio test statistic is</p>
<p><span class="math display">\[
-2\log\Lambda = -2[\log(L(\beta_{1,0},\hat{\beta}_{2,0})) - \log(L(\hat{\beta}_1,\hat{\beta}_2))]
\]</span>
where</p>
<ul>
<li>the first term is the value fo the likelihood for the fitted restricted model<br />
</li>
<li>the second term is the likelihood value of the fitted unrestricted model</li>
</ul>
<p>Under the null,</p>
<p><span class="math display">\[
-2 \log \Lambda \sim \chi^2_{\upsilon}
\]</span>
where <span class="math inline">\(\upsilon\)</span> is the dimension of <span class="math inline">\(\beta_1\)</span></p>
<p>We reject the null when <span class="math inline">\(-2\log \Lambda &gt; \chi_{\upsilon,1-\alpha}^2\)</span></p>
<p><strong>Wald Statistics</strong></p>
<p>Based on</p>
<p><span class="math display">\[
\hat{\beta} \sim AN (\beta, [\mathbf{I}(\beta)^{-1}])
\]</span>
<span class="math display">\[
H_0: \mathbf{L}\hat{\beta} = 0 
\]</span>
where <span class="math inline">\(\mathbf{L}\)</span> is a q x p matrix with q linearly independent rows. Then</p>
<p><span class="math display">\[
W = (\mathbf{L\hat{\beta}})&#39;(\mathbf{L[I(\hat{\beta})]^{-1}L&#39;})^{-1}(\mathbf{L\hat{\beta}})
\]</span>
under the null hypothesis</p>
<p>Confidence interval</p>
<p><span class="math display">\[
\hat{\beta}_i \pm 1.96 \hat{s}_{ii}^2
\]</span>
where <span class="math inline">\(\hat{s}_{ii}^2\)</span> is the i-th diagonal of <span class="math inline">\(\mathbf{[I(\hat{\beta})]}^{-1}\)</span></p>
<p>If you have</p>
<ul>
<li>large sample size, the likelihood ratio and Wald tests have similar results.<br />
</li>
<li>small sample size, the likelihood ratio test is better.</li>
</ul>
<p><strong>Logistic Regression: Interpretation of <span class="math inline">\(\beta\)</span></strong></p>
<p>For single regressor, the model is</p>
<p><span class="math display">\[
logit\{\hat{p}_{x_i}\} \equiv logit (\hat{p}_i) = \log(\frac{\hat{p}_i}{1 - \hat{p}_i}) = \hat{\beta}_0 + \hat{\beta}_1 x_i
\]</span></p>
<p>When <span class="math inline">\(x= x_i + 1\)</span></p>
<p><span class="math display">\[
logit\{\hat{p}_{x_i +1}\} = \hat{\beta}_0 + \hat{\beta}(x_i + 1) = logit\{\hat{p}_{x_i}\} + \hat{\beta}_1
\]</span></p>
<p>Then,</p>
<p><span class="math display">\[
logit\{\hat{p}_{x_i +1}\} - logit\{\hat{p}_{x_i}\} = log\{odds[\hat{p}_{x_i +1}]\} - log\{odds[\hat{p}_{x_i}]\} \\
= log(\frac{odds[\hat{p}_{x_i + 1}]}{odds[\hat{p}_{x_i}]}) = \hat{\beta}_1
\]</span></p>
<p>and</p>
<p><span class="math display">\[
exp(\hat{\beta}_1) = \frac{odds[\hat{p}_{x_i + 1}]}{odds[\hat{p}_{x_i}]}
\]</span>
the estimated <strong>odds ratio</strong></p>
<p>the estimated odds ratio, when there is a difference of c units in the regressor x, is <span class="math inline">\(exp(c\hat{\beta}_1)\)</span>. When there are multiple covariates, <span class="math inline">\(exp(\hat{\beta}_k)\)</span> is the estimated odds ratio for the variable <span class="math inline">\(x_k\)</span>, assuming that all of the other variables are held constant.</p>
<p><strong>Inference on the Mean Response</strong></p>
<p>Let <span class="math inline">\(x_h = (1, x_{h1}, ...,x_{h,p-1})&#39;\)</span>. Then</p>
<p><span class="math display">\[
\hat{p}_h = \frac{exp(\mathbf{x&#39;_h \hat{\beta}})}{1 + exp(\mathbf{x&#39;_h \hat{\beta}})}
\]</span></p>
<p>and <span class="math inline">\(s^2(\hat{p}_h) = \mathbf{x&#39;_h[I(\hat{\beta})]^{-1}x_h}\)</span></p>
<p>For new observation, we can have a cutoff point to decide whether y = 0 or 1.</p>
</div>
<div id="probit-regression" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Probit Regression</h3>
<p><span class="math display">\[
E(Y_i) = p_i = \Phi(\mathbf{x_i&#39;\theta})
\]</span>
where <span class="math inline">\(\Phi()\)</span> is the CDF of a N(0,1) random variable.</p>
<p>Other models (e..g, t–distribution; log-log; I complimentary log-log)</p>
<p>We let <span class="math inline">\(Y_i = 1\)</span> success, <span class="math inline">\(Y_i =0\)</span> no success. We assume <span class="math inline">\(Y \sim Ber\)</span> and <span class="math inline">\(p_i = P(Y_i =1)\)</span>, the success probability. We cosnider a logistic regression with the response function <span class="math inline">\(logit(p_i) = x&#39;_i \beta\)</span></p>
<p><strong>Confusion matrix</strong></p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Predicted</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Truth</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>1</td>
<td>True Positive (TP)</td>
<td>False Negative (FN)</td>
</tr>
<tr class="odd">
<td>0</td>
<td>False Positive (FP)</td>
<td>True Negative (TN)</td>
</tr>
</tbody>
</table>
<p>Sensitivity: ability to identify positive results</p>
<p><span class="math display">\[
\text{Sensitivity} = \frac{TP}{TP + FN}
\]</span></p>
<p>Specificity: ability to identify negative results</p>
<p><span class="math display">\[
\text{Specificity} = \frac{TN}{TN + FP}
\]</span></p>
<p>False positive rate: Type I error (1- specificity)</p>
<p><span class="math display">\[
\text{ False Positive Rate} = \frac{FP}{TN+ FP}
\]</span></p>
<p>False Negative Rate: Type II error (1-sensitivity)</p>
<p><span class="math display">\[
\text{False Negative Rate} = \frac{FN}{TP + FN}
\]</span></p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Predicted</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Truth</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>1</td>
<td>Sensitivity</td>
<td>False Negative Rate</td>
</tr>
<tr class="odd">
<td>0</td>
<td>False Positive Rate</td>
<td>Specificity</td>
</tr>
</tbody>
</table>
</div>
<div id="poisson-regression" class="section level3">
<h3><span class="header-section-number">6.3.3</span> Poisson Regression</h3>
<p>From the Poisson distribution</p>
<p><span class="math display">\[
f(Y_i) = \frac{\mu_i^{Y_i}exp(-\mu_i)}{Y_i!}, Y_i = 0,1,.. \\
E(Y_i) = \mu_i  \\
var(Y_i) = \mu_i
\]</span>
which is a natural distribution for counts. We can see that the variance is a function of the mean. If we let <span class="math inline">\(\mu_i = f(\mathbf{x_i; \theta})\)</span>, it would be similar to <a href="generalized-linear-models.html#logistic-regression">Logistic Regression</a> since we can choose <span class="math inline">\(f()\)</span> as <span class="math inline">\(\mu_i = \mathbf{x_i&#39;\theta}, \mu_i = \exp(\mathbf{x_i&#39;\theta}), \mu_i = \log(\mathbf{x_i&#39;\theta})\)</span></p>
</div>
<div id="generalization" class="section level3">
<h3><span class="header-section-number">6.3.4</span> Generalization</h3>
<p>We can see that Poisson regression looks similar to logistic regression. Hence, we can generalize to a class of modeling. Thanks to <span class="citation">(Nelder and Wedderburn <a href="#ref-Nelder_1972" role="doc-biblioref">1972</a>)</span>, we have the <strong>generalized linear models</strong> (GLMs). Estimation is generalize in these models.</p>
<p><strong>Exponential Family</strong><br />
The theory of GLMs is developed for data with distribution given y the <strong>exponential family</strong>.<br />
The form of the data distribution that is useful for GLMs is</p>
<p><span class="math display">\[
f(y;\theta, \phi) = \exp(\frac{\theta y - b(\theta)}{a(\phi)} + c(y, \phi))
\]</span>
where</p>
<ul>
<li><span class="math inline">\(\theta\)</span> is called the natural parameter<br />
</li>
<li><span class="math inline">\(\phi\)</span> is called the dispersion parameter</li>
</ul>
<p><strong>Note</strong>:</p>
<p>This family includes the <a href="probability-theory.html#gamma">Gamma</a>, <a href="probability-theory.html#normal">Normal</a>, <a href="probability-theory.html#poisson">Poisson</a>, and other.</p>
<p><strong>Example</strong></p>
<p>if we have <span class="math inline">\(Y \sim N(\mu, \sigma^2)\)</span></p>
<p><span class="math display">\[
\begin{align}
f(y; \mu, \sigma^2) &amp;= \frac{1}{(2\pi \sigma^2)^{1/2}}\exp(-\frac{1}{2\sigma^2}(y- \mu)^2) \\
&amp;= \exp(-\frac{1}{2\sigma^2}(y^2 - 2y \mu +\mu^2)- \frac{1}{2}\log(2\pi \sigma^2)) \\
&amp;= \exp(\frac{y \mu - \mu^2/2}{\sigma^2} - \frac{y^2}{2\sigma^2} - \frac{1}{2}\log(2\pi \sigma^2)) \\
&amp;= \exp(\frac{\theta y - b(\theta)}{a(\phi)} + c(y , \phi))
\end{align}
\]</span>
where</p>
<ul>
<li><span class="math inline">\(\theta = \mu\)</span><br />
</li>
<li><span class="math inline">\(b(\theta) = \frac{\mu^2}{2}\)</span><br />
</li>
<li><span class="math inline">\(a(\phi) = \sigma^2 = \phi\)</span><br />
</li>
<li><span class="math inline">\(c(y , \phi) = - \frac{1}{2}(\frac{y^2}{\phi}+\log(2\pi \sigma^2))\)</span></li>
</ul>
<p><strong>Properties of GLM exponential families</strong></p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(E(Y) = b&#39; (\theta)\)</span> where <span class="math inline">\(b&#39;(\theta) = \frac{\partial b(\theta)}{\partial \theta}\)</span> (here <code>'</code> is “prime”, not transpose)<br />
</li>
<li><span class="math inline">\(var(Y) = a(\phi)b&#39;&#39;(\theta)= a(\phi)V(\mu)\)</span>.
<ul>
<li><span class="math inline">\(V(\mu)\)</span> is the <em>variance function</em>; however, it is only the variance in the case that <span class="math inline">\(a(\phi) =1\)</span><br />
</li>
</ul></li>
<li>If <span class="math inline">\(a(), b(), c()\)</span> are identifiable, we will derive expected value and variance of Y.</li>
</ol>
<p>Example</p>
<p>Normal distribution</p>
<p><span class="math display">\[
b&#39;(\theta) = \frac{\partial b(\mu^2/2)}{\partial \mu} = \mu \\
V(\mu) = \frac{\partial^2 (\mu^2/2)}{\partial \mu^2} = 1 \\
\to var(Y) = a(\phi) = \sigma^2
\]</span></p>
<p>Poisson distribution</p>
<p><span class="math display">\[
\begin{align}
f(y, \theta, \phi) &amp;= \frac{\mu^y \exp(-\mu)}{y!} \\
&amp;= \exp(y\log(\mu) - \mu - \log(y!)) \\
&amp;= \exp(y\theta - \exp(\theta) - \log(y!))
\end{align}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\theta = \log(\mu)\)</span><br />
</li>
<li><span class="math inline">\(a(\phi) = 1\)</span><br />
</li>
<li><span class="math inline">\(b(\theta) = \exp(\theta)\)</span><br />
</li>
<li><span class="math inline">\(c(y, \phi) = \log(y!)\)</span></li>
</ul>
<p>Hence,</p>
<p><span class="math display">\[
E(Y) = \frac{\partial b(\theta)}{\partial \theta} = \exp(\theta) = \mu \\
var(Y) = \frac{\partial^2 b(\theta)}{\partial \theta^2} = \mu
\]</span></p>
<p>Since <span class="math inline">\(\mu = E(Y) = b&#39;(\theta)\)</span></p>
<p>In GLM, we take some monotone function (typically nonlinear) of <span class="math inline">\(\mu\)</span> to be linear in the set of covariates</p>
<p><span class="math display">\[
g(\mu) = g(b&#39;(\theta)) = \mathbf{x&#39;\beta}
\]</span>
Equivalently,</p>
<p><span class="math display">\[
\mu = g^{-1}(\mathbf{x&#39;\beta})
\]</span>
where <span class="math inline">\(g(.)\)</span> is the <strong>link function</strong> since it links mean response (<span class="math inline">\(\mu = E(Y)\)</span>) and a linear expression of the covariates</p>
<p>Some people use <span class="math inline">\(\eta = \mathbf{x&#39;\beta}\)</span> where <span class="math inline">\(\eta\)</span> = the “linear predictor”</p>
<p><strong>GLM is composed of 2 components</strong></p>
<p>The <strong>random component</strong>:</p>
<ul>
<li>is the distribution chosen to model the response variables <span class="math inline">\(Y_1,...,Y_n\)</span></li>
<li>is specified by the choice fo <span class="math inline">\(a(), b(), c()\)</span> in the exponential form<br />
</li>
<li>Notation:
<ul>
<li>Assume that there are n <strong>independent</strong> response variables <span class="math inline">\(Y_1,...,Y_n\)</span> with densities<br />
<span class="math display">\[
f(y_i ; \theta_i, \phi) = \exp(\frac{\theta_i y_i - b(\theta_i)}{a(\phi)}+ c(y_i, \phi))
\]</span>
notice each observation might have different densities</li>
<li>Assume that <span class="math inline">\(\phi\)</span> is constant for all <span class="math inline">\(i = 1,...,n\)</span>, but <span class="math inline">\(\theta_i\)</span> will vary. <span class="math inline">\(\mu_i = E(Y_i)\)</span> for all i.</li>
</ul></li>
</ul>
<p>The <strong>systematic component</strong></p>
<ul>
<li>is the portion of the model that gives the relation between <span class="math inline">\(\mu\)</span> and the covariates <span class="math inline">\(\mathbf{x}\)</span><br />
</li>
<li>consists of 2 parts:
<ul>
<li>the <em>link</em> function, <span class="math inline">\(g(.)\)</span><br />
</li>
<li>the <em>linear predictor</em>, <span class="math inline">\(\eta = \mathbf{x&#39;\beta}\)</span><br />
</li>
</ul></li>
<li>Notation:
<ul>
<li>assume <span class="math inline">\(g(\mu_i) = \mathbf{x&#39;\beta} = \eta_i\)</span> where <span class="math inline">\(\mathbf{\beta} = (\beta_1,..., \beta_p)&#39;\)</span><br />
</li>
<li>The parameters to be estimated are <span class="math inline">\(\beta_1,...\beta_p , \phi\)</span></li>
</ul></li>
</ul>
<p><strong>The Canonical Link</strong></p>
<p>To choose <span class="math inline">\(g(.)\)</span>, we can use <strong>canonical link function</strong></p>
<p>If the link function <span class="math inline">\(g(.)\)</span> is such <span class="math inline">\(g(\mu_i) = \theta_i\)</span>, the natural parameter, then <span class="math inline">\(g(.)\)</span> is the canonical link.</p>
<p>Example</p>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th>Distribution</th>
<th>Mean Response</th>
<th>Canonical link</th>
<th>name</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Normal random component</td>
<td><span class="math inline">\(\mu_i = \theta_i\)</span></td>
<td><span class="math inline">\(g(\mu_i) = \mu_i\)</span></td>
<td>the identity link</td>
</tr>
<tr class="even">
<td>Binomial random component</td>
<td><span class="math inline">\(\mu_i = \frac{n_i \exp(\theta)}{1+\exp(\theta_i)} \\ \theta(\mu_i) = \log(\frac{p_i}{1-p_i}) = \log (\frac{\mu_i}{n_i - \mu_i})\)</span></td>
<td><span class="math inline">\(g(\mu_i) = \log(\frac{\mu_i}{n_i - \mu_i})\)</span></td>
<td>the logit link</td>
</tr>
<tr class="odd">
<td>Poisson random component</td>
<td><span class="math inline">\(\mu_i = \exp(\theta_i)\)</span></td>
<td><span class="math inline">\(g(\mu_i) = \log(\mu_i)\)</span></td>
<td></td>
</tr>
<tr class="even">
<td>Gamma random component</td>
<td><span class="math inline">\(\mu_i = -\frac{1}{\theta_i} \\ \theta(\mu_i) = - \mu_i^{-1}\)</span></td>
<td><span class="math inline">\(g(\mu_i) = - \frac{1}{\mu_i}\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td>Inverse Gaussian random</td>
<td></td>
<td><span class="math inline">\(g(\mu_i) = \frac{1}{\mu_i^2}\)</span></td>
<td></td>
</tr>
</tbody>
</table>
<div id="estimation-1" class="section level4">
<h4><span class="header-section-number">6.3.4.1</span> Estimation</h4>
<ul>
<li>MLE for parameters of the <strong>systematic component (<span class="math inline">\(\beta\)</span>)</strong><br />
</li>
<li>Unification of derivation and computation (thanks to the exponential forms)<br />
</li>
<li>No unification for estimation of the dispersion parameter (<span class="math inline">\(\phi\)</span>)</li>
</ul>
<div id="estimation-of-beta" class="section level5">
<h5><span class="header-section-number">6.3.4.1.1</span> Estimation of <span class="math inline">\(\beta\)</span></h5>
<p>We have</p>
<p><span class="math display">\[
f(y_i ; \theta_i, \phi) = \exp(\frac{\theta_i y_i - b(\theta_i)}{a(\phi)}+ c(y_i, \phi)) \\
E(Y_i) = \mu_i = b&#39;(\theta) \\
var(Y_i) = b&#39;&#39;(\theta)a(\phi) = V(\mu_i)a(\phi) \\
g(\mu_i) = \mathbf{x}_i&#39;\beta = \eta_i
\]</span></p>
<p>If the log-likelihood for a single observation is <span class="math inline">\(l_i (\beta,\phi)\)</span>. The log-likelihood for all n observations is</p>
<p><span class="math display">\[
\begin{align}
l(\beta,\phi) &amp;= \sum_{i=1}^n l_i (\beta,\phi) \\
&amp;= \sum_{i=1}^n (\frac{\theta_i y_i - b(\theta_i)}{a(\phi)}+ c(y_i, \phi))
\end{align}
\]</span></p>
<p>Using MLE to find <span class="math inline">\(\beta\)</span>, we use the chain rule to get the derivatives</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial l_i (\beta,\phi)}{\partial \beta_j} &amp;=  \frac{\partial l_i (\beta, \phi)}{\partial \theta_i} \times \frac{\partial \theta_i}{\partial \mu_i} \times \frac{\partial \mu_i}{\partial \eta_i}\times \frac{\partial \eta_i}{\partial \beta_j} \\
&amp;= \sum_{i=1}^{n}(\frac{ y_i - \mu_i}{a(\phi)} \times \frac{1}{V(\mu_i)} \times \frac{\partial \mu_i}{\partial \eta_i} \times x_{ij})
\end{align}
\]</span></p>
<p>If we let</p>
<p><span class="math display">\[
w_i \equiv ((\frac{\partial \eta_i}{\partial \mu_i})^2 V(\mu_i))^{-1}
\]</span></p>
<p>Then,</p>
<p><span class="math display">\[
\frac{\partial l_i (\beta,\phi)}{\partial \beta_j} = \sum_{i=1}^n (\frac{y_i \mu_i}{a(\phi)} \times w_i \times \frac{\partial \eta_i}{\partial \mu_i} \times x_{ij})
\]</span></p>
<p>We can also get the second derivatives using the chain rule.</p>
<p>Example:</p>
<p>For the <a href="non-linear-least-squares.html#newton-raphson">Newton-Raphson</a> algorithm, we need</p>
<p><span class="math display">\[
- E(\frac{\partial^2 l(\beta,\phi)}{\partial \beta_j \partial \beta_k})
\]</span>
where <span class="math inline">\((j,k)\)</span>th element of the <strong>Fisher information matrix</strong> <span class="math inline">\(\mathbf{I}(\beta)\)</span></p>
<p>Hence,</p>
<p><span class="math display">\[
- E(\frac{\partial^2 l(\beta,\phi)}{\partial \beta_j \partial \beta_k}) = \sum_{i=1}^n \frac{w_i}{a(\phi)}x_{ij}x_{ik}
\]</span>
for the (j,k)th element</p>
<p>If Bernoulli model with logit link function (which is the canonical link)</p>
<p><span class="math display">\[
b(\theta) = \log(1 + \exp(\theta)) = \log(1 + \exp(\mathbf{x&#39;\beta})) \\
a(\phi) = 1  \\
c(y_i, \phi) = 0 \\
E(Y) = b&#39;(\theta) = \frac{\exp(\theta)}{1 + \exp(\theta)} = \mu = p \\
\eta = g(\mu) = \log(\frac{\mu}{1-\mu}) = \theta = \log(\frac{p}{1-p}) = \mathbf{x&#39;\beta} 
\]</span></p>
<p>For <span class="math inline">\(Y_i\)</span>, i = 1,.., the log-likelihood is</p>
<p><span class="math display">\[
l_i (\beta, \phi) = \frac{y_i \theta_i - b(\theta_i)}{a(\phi)} + c(y_i, \phi) = y_i \mathbf{x}&#39;_i \beta - \log(1+ \exp(\mathbf{x&#39;\beta}))
\]</span>
Additionally,</p>
<p><span class="math display">\[
V(\mu_i) = \mu_i(1-\mu_i)= p_i (1-p_i) \\
\frac{\partial \mu_i}{\partial \eta_i} = p_i(1-p_i)
\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial l(\beta, \phi)}{\partial \beta_j} &amp;= \sum_{i=1}^n[\frac{y_i - \mu_i}{a(\phi)} \times \frac{1}{V(\mu_i)}\times \frac{\partial \mu_i}{\partial \eta_i} \times x_{ij}] \\
&amp;= \sum_{i=1}^n (y_i - p_i) \times \frac{1}{p_i(1-p_i)} \times p_i(1-p_i) \times x_{ij} \\
&amp;= \sum_{i=1}^n (y_i - p_i) x_{ij} \\
&amp;= \sum_{i=1}^n (y_i - \frac{\exp(\mathbf{x&#39;_i\beta})}{1+ \exp(\mathbf{x&#39;_i\beta})})x_{ij}
\end{align}
\]</span>
then</p>
<p><span class="math display">\[
w_i = ((\frac{\partial \eta_i}{\partial \mu_i})^2 V(\mu_i))^{-1} = p_i (1-p_i)
\]</span></p>
<p><span class="math display">\[
\mathbf{I}_{jk}(\mathbf{\beta}) = \sum_{i=1}^n \frac{w_i}{a(\phi)} x_{ij}x_{ik} = \sum_{i=1}^n p_i (1-p_i)x_{ij}x_{ik}
\]</span></p>
<p>The <strong>Fisher-scoring</strong> algorithm for the MLE of <span class="math inline">\(\mathbf{\beta}\)</span> is</p>
<p><span class="math display">\[
\left(
\begin{array}
{c}
\beta_1 \\
\beta_2 \\
. \\
. \\
. \\
\beta_p \\
\end{array}
\right)^{(m+1)}
=
\left(
\begin{array}
{c}
\beta_1 \\
\beta_2 \\
. \\
. \\
. \\
\beta_p \\
\end{array}
\right)^{(m)} +
\mathbf{I}^{-1}(\mathbf{\beta})
\left(
\begin{array}
{c}
\frac{\partial l (\beta, \phi)}{\partial \beta_1} \\
\frac{\partial l (\beta, \phi)}{\partial \beta_2} \\
. \\
. \\
. \\
\frac{\partial l (\beta, \phi)}{\partial \beta_p} \\
\end{array}
\right)|_{\beta = \beta^{(m)}}
\]</span>
Similar to <a href="non-linear-least-squares.html#newton-raphson">Newton-Raphson</a> expect the matrix of second derivatives by the expected value of the second derivative matrix.</p>
<p>In matrix notation,</p>
<p><span class="math display">\[
\begin{align}
\frac{\partial l }{\partial \beta} &amp;= \frac{1}{a(\phi)}\mathbf{X&#39;W\Delta(y - \mu)} \\
&amp;= \frac{1}{a(\phi)}\mathbf{F&#39;V^{-1}(y - \mu)} \\
\end{align}
\]</span></p>
<p><span class="math display">\[
\mathbf{I}(\beta) = \frac{1}{a(\phi)}\mathbf{X&#39;WX} = \frac{1}{a(\phi)}\mathbf{F&#39;V^{-1}F} \\
\]</span>
where</p>
<ul>
<li><span class="math inline">\(\mathbf{X}\)</span> is an n x p matrix of covariates<br />
</li>
<li><span class="math inline">\(\mathbf{W}\)</span> is an n x n diagonal matrix with (i,i)th element given by <span class="math inline">\(w_i\)</span><br />
</li>
<li><span class="math inline">\(\mathbf{\Delta}\)</span> an n x n diagonal matrix with (i,i)th element given by <span class="math inline">\(\frac{\partial \eta_i}{\partial \mu_i}\)</span><br />
</li>
<li><span class="math inline">\(\mathbf{F} = \mathbf{\frac{\partial \mu}{\partial \beta}}\)</span> an n x p matrix with ith row <span class="math inline">\(\frac{\partial \mu_i}{\partial \beta} = (\frac{\partial \mu_i}{\partial \eta_i})\mathbf{x}&#39;_i\)</span><br />
</li>
<li><span class="math inline">\(\mathbf{V}\)</span> an n x n diagonal matrix with (i,i)th element given by <span class="math inline">\(V(\mu_i)\)</span></li>
</ul>
<p>Setting the derivative of the log-likelihood equal to 0, ML estimating equations are</p>
<p><span class="math display">\[
\mathbf{F&#39;V^{-1}y= F&#39;V^{-1}\mu}
\]</span>
where all components of this equation expect y depends on the parameters <span class="math inline">\(\beta\)</span></p>
<p><strong>Special Cases</strong></p>
<p>If one has a canonical link, the estimating equations reduce to</p>
<p><span class="math display">\[
\mathbf{X&#39;y= X&#39;\mu}
\]</span>
If one has an identity link, then</p>
<p><span class="math display">\[
\mathbf{X&#39;V^{-1}y = X&#39;V^{-1}X\hat{\beta}}
\]</span>
which gives the generalized least squares estimator</p>
<p>Generally, we can rewrite the Fisher-scoring algorithm as</p>
<p><span class="math display">\[
\beta^{(m+1)} = \beta^{(m)} + \mathbf{(\hat{F}&#39;\hat{V}^{-1}\hat{F})^{-1}\hat{F}&#39;\hat{V}^{-1}(y- \hat{\mu})}
\]</span></p>
<p>Since <span class="math inline">\(\hat{F},\hat{V}, \hat{\mu}\)</span> depend on <span class="math inline">\(\beta\)</span>, we evaluate at <span class="math inline">\(\beta^{(m)}\)</span></p>
<p>From starting values <span class="math inline">\(\beta^{(0)}\)</span>, we can iterate until convergence.</p>
<p>Notes:</p>
<ul>
<li>if <span class="math inline">\(a(\phi)\)</span> is a constant or of the form <span class="math inline">\(m_i \phi\)</span> with known <span class="math inline">\(m_i\)</span>, then <span class="math inline">\(\phi\)</span> cancels.</li>
</ul>
</div>
<div id="estimation-of-phi" class="section level5">
<h5><span class="header-section-number">6.3.4.1.2</span> Estimation of <span class="math inline">\(\phi\)</span></h5>
<p>2 approaches:</p>
<ol style="list-style-type: decimal">
<li>MLE</li>
</ol>
<p><span class="math display">\[
\frac{\partial l_i}{\partial \phi} = \frac{(\theta_i y_i - b(\theta_i)a&#39;(\phi))}{a^2(\phi)} + \frac{\partial c(y_i,\phi)}{\partial \phi}
\]</span></p>
<p>the MLE of <span class="math inline">\(\phi\)</span> solves</p>
<p><span class="math display">\[
\frac{a^2(\phi)}{a&#39;(\phi)}\sum_{i=1}^n \frac{\partial c(y_i, \phi)}{\partial \phi} = \sum_{i=1}^n(\theta_i y_i - b(\theta_i))
\]</span>
* Situation others than normal error case, expression for <span class="math inline">\(\frac{\partial c(y,\phi)}{\partial \phi}\)</span> are not simple<br />
* Even for the canonical link and <span class="math inline">\(a(\phi)\)</span> constant, there is no nice general expression for <span class="math inline">\(-E(\frac{\partial^2 l}{\partial \phi^2})\)</span>, so the unification GLMs provide for estimation of <span class="math inline">\(\beta\)</span> breaks down for <span class="math inline">\(\phi\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li><p>Moment Estimation (“Bias Corrected <span class="math inline">\(\chi^2\)</span>”)</p>
<ul>
<li>The MLE is not conventional approach to estimation of <span class="math inline">\(\phi\)</span> in GLMS.<br />
</li>
<li>For the exponential family <span class="math inline">\(var(Y) =V(\mu)a(\phi)\)</span>. This implies<br />
<span class="math display">\[
a(\phi) = \frac{var(Y)}{V(\mu)} = \frac{E(Y- \mu)^2}{V(\mu)} \\
a(\hat{\phi})  = \frac{1}{n-p} \sum_{i=1}^n \frac{(y_i -\hat{\mu}_i)^2}{V(\hat{\mu})}
\]</span>
where p is the dimension of <span class="math inline">\(\beta\)</span><br />
</li>
<li>GLM with canonical link function <span class="math inline">\(g(.)= (b&#39;(.))^{-1}\)</span><br />
<span class="math display">\[
g(\mu) = \theta = \eta = \mathbf{x&#39;\beta} \\
\mu = g^{-1}(\eta)= b&#39;(\eta)
\]</span></li>
<li>so the method estimator for <span class="math inline">\(a(\phi)=\phi\)</span> is</li>
</ul></li>
</ol>
<p><span class="math display">\[
\hat{\phi} = \frac{1}{n-p} \sum_{i=1}^n \frac{(y_i - g^{-1}(\hat{\eta}_i))^2}{V(g^{-1}(\hat{\eta}_i))}
\]</span></p>
</div>
</div>
<div id="inference-2" class="section level4">
<h4><span class="header-section-number">6.3.4.2</span> Inference</h4>
<p>We have</p>
<p><span class="math display">\[
\hat{var}(\beta) = a(\phi)(\mathbf{\hat{F}&#39;\hat{V}\hat{F}})^{-1}
\]</span>
where</p>
<ul>
<li><span class="math inline">\(\mathbf{V}\)</span> is an n x n diagonal matrix with diagonal elements given by <span class="math inline">\(V(\mu_i)\)</span><br />
</li>
<li><span class="math inline">\(\mathbf{F}\)</span> is an n x p matrix given by <span class="math inline">\(\mathbf{F} = \frac{\partial \mu}{\partial \beta}\)</span><br />
</li>
<li>Both <span class="math inline">\(\mathbf{V,F}\)</span> are dependent on the mean <span class="math inline">\(\mu\)</span>, and thus <span class="math inline">\(\beta\)</span>. Hence, their estimates (<span class="math inline">\(\mathbf{\hat{V},\hat{F}}\)</span>) depend on <span class="math inline">\(\hat{\beta}\)</span>.</li>
</ul>
<p><span class="math display">\[
H_0: \mathbf{L\beta = d}
\]</span>
where <span class="math inline">\(\mathbf{L}\)</span> is a q x p matrix with a <strong>Wald</strong> test</p>
<p><span class="math display">\[
W = \mathbf{(L \hat{\beta}-d)&#39;(a(\phi)L(\hat{F}&#39;\hat{V}^{-1}\hat{F})L&#39;)^{-1}(L \hat{\beta}-d)}
\]</span>
which follows <span class="math inline">\(\chi_q^2\)</span> distribution (asymptotically), where q is the rank of <span class="math inline">\(\mathbf{L}\)</span></p>
<p>In the simple case <span class="math inline">\(H_0: \beta_j = 0\)</span> gives <span class="math inline">\(W = \frac{\hat{\beta}^2_j}{\hat{var}(\hat{\beta}_j)} \sim \chi^2_1\)</span> asymptotically</p>
<p>Likelihood ratio test</p>
<p><span class="math display">\[
\Lambda = 2 (l(\hat{\beta}_f)-l(\hat{\beta}_r)) \sim \chi^2_q
\]</span>
where</p>
<ul>
<li>q is the number of constraints used to fit the reduced model <span class="math inline">\(\hat{\beta}_r\)</span>, and <span class="math inline">\(\hat{\beta}_r\)</span> is the fit under the full model.</li>
</ul>
<p>Wald test is easier to implement, but likelihood ratio test is better (especially for small samples).</p>
</div>
<div id="deviance" class="section level4">
<h4><span class="header-section-number">6.3.4.3</span> Deviance</h4>
<p><a href="generalized-linear-models.html#deviance">Deviance</a> is necessary for goodness of fit, inference and for alternative estimation of the dispersion parameter. We define and consider <a href="generalized-linear-models.html#deviance">Deviance</a> from a likelihood ratio perspective.</p>
<ul>
<li><p>Assume that <span class="math inline">\(\phi\)</span> is known. Let <span class="math inline">\(\tilde{\theta}\)</span> denote the full and <span class="math inline">\(\hat{\theta}\)</span> denote the reduced model MLEs. Then, the likelihood ratio (2 times the difference in log-likelihoods) is
<span class="math display">\[
2\sum_{i=1}^{n} \frac{y_i (\tilde{\theta}_i- \hat{\theta}_i)-b(\tilde{\theta}_i) + b(\hat{\theta}_i)}{a_i(\phi)}
\]</span></p></li>
<li><p>For exponential families, <span class="math inline">\(\mu = E(y) = b&#39;(\theta)\)</span>, so the natural parameter is a function of <span class="math inline">\(\mu: \theta = \theta(\mu) = b&#39;^{-1}(\mu)\)</span>, and the likelihood ratio turns into<br />
<span class="math display">\[
2 \sum_{i=1}^m \frac{y_i\{\theta(\tilde{\mu}_i - \theta(\hat{\mu}_i)\} - b(\theta(\tilde{\mu}_i)) + b(\theta(\hat{\mu}_i))}{a_i(\phi)}
\]</span></p></li>
<li><p>Comparing a fitted model to “the fullest possible model”, which is the <strong>saturated model</strong>: <span class="math inline">\(\tilde{\mu}_i = y_i\)</span>, i = 1,..,n. If <span class="math inline">\(\tilde{\theta}_i^* = \theta(y_i), \hat{\theta}_i^* = \theta (\hat{\mu})\)</span>, the likelihood ratio is<br />
<span class="math display">\[
2 \sum_{i=1}^{n} \frac{y_i (\tilde{\theta}_i^* - \hat{\theta}_i^* + b(\hat{\theta}_i^*))}{a_i(\phi)}
\]</span></p></li>
<li><p><span class="citation">(McCullagh and Nelder <a href="#ref-McCullagh_2019" role="doc-biblioref">2019</a>)</span> specify <span class="math inline">\(a(\phi) = \phi\)</span>, then the likelihood ratio can be written as<br />
<span class="math display">\[
D^*(\mathbf{y, \hat{\mu}}) = \frac{2}{\phi}\sum_{i=1}^n\{y_i (\tilde{\theta}_i^*- \hat{\theta}_i^*)- b(\tilde{\theta}_i^*) +b(\hat{\theta}_i^*)  \}  
\]</span>
where</p></li>
<li><p><span class="math inline">\(D^*(\mathbf{y, \hat{\mu}})\)</span> = <strong>scaled deviance</strong><br />
</p></li>
<li><p><span class="math inline">\(D(\mathbf{y, \hat{\mu}}) = \phi D^*(\mathbf{y, \hat{\mu}})\)</span> = <strong>deviance</strong></p></li>
</ul>
<p><br></p>
<p><strong>Note</strong>:</p>
<ul>
<li>in some random component distributions, we can write <span class="math inline">\(a_i(\phi) = \phi m_i\)</span>, where
<ul>
<li><span class="math inline">\(m_i\)</span> is some known scalar that may change with the observations. THen, the scaled deviance components are divided by <span class="math inline">\(m_i\)</span>:<br />
<span class="math display">\[
D^*(\mathbf{y, \hat{\mu}}) \equiv 2\sum_{i=1}^n\{y_i (\tilde{\theta}_i^*- \hat{\theta}_i^*)- b(\tilde{\theta}_i^*) +b(\hat{\theta}_i^*)\} / (\phi m_i)  
\]</span></li>
</ul></li>
<li><span class="math inline">\(D^*(\mathbf{y, \hat{\mu}}) = \sum_{i=1}^n d_i\)</span>m where <span class="math inline">\(d_i\)</span> is the deviance contribution from the ith observation.<br />
</li>
<li>D is used in model selection<br />
</li>
<li><span class="math inline">\(D^*\)</span> is used in goodness of fit tests (as it is a likelihood ratio statistic).
<span class="math display">\[
D^*(\mathbf{y, \hat{\mu}}) = 2\{l(\mathbf{y,\tilde{\mu}})-l(\mathbf{y,\hat{\mu}})\}
\]</span></li>
<li><span class="math inline">\(d_i\)</span> are used to form <strong>deviance residuals</strong></li>
</ul>
<p><strong>Example</strong>:</p>
<p><strong>Normal</strong></p>
<p>We have</p>
<p><span class="math display">\[
\theta = \mu \\
\phi = \sigma^2 \\
b(\theta) = \frac{1}{2} \theta^2 \\
a(\phi) = \phi
\]</span>
Hence,</p>
<p><span class="math display">\[
\tilde{\theta}_i = y_i \\
\hat{\theta}_i = \hat{\mu}_i = g^{-1}(\hat{\eta}_i) 
\]</span></p>
<p>And</p>
<p><span class="math display">\[
\begin{align}
D &amp;= 2 \sum_{1=1}^n Y^2_i - y_i \hat{\mu}_i - \frac{1}{2}y^2_i + \frac{1}{2} \hat{\mu}_i^2 \\
&amp;= \sum_{i=1}^n y_i^2 - 2y_i \hat{\mu}_i + \hat{\mu}_i^2 \\
&amp;= \sum_{i=1}^n (y_i - \hat{\mu}_i)^2
\end{align}
\]</span>
which is the <strong>residual sum of squares</strong></p>
<p><strong>Poisson</strong></p>
<p><span class="math display">\[
f(y) = \exp\{y\log(\mu) - \mu - \log(y!)\} \\
\theta = \log(\mu) \\
b(\theta) = \exp(\theta) \\
a(\phi) = 1 \\
\tilde{\theta}_i = \log(y_i) \\
\hat{\theta}_i = \log(\hat{\mu}_i) \\
\hat{\mu}_i = g^{-1}(\hat{\eta}_i)
\]</span>
Then,</p>
<p><span class="math display">\[
\begin{align}
D &amp;= 2 \sum_{i = 1}^n y_i \log(y_i) - y_i \log(\hat{\mu}_i) - y_i + \hat{\mu}_i \\
&amp;= 2 \sum_{i = 1}^n y_i \log(\frac{y_i}{\hat{\mu}_i}) - (y_i - \hat{\mu}_i)
\end{align}
\]</span>
and</p>
<p><span class="math display">\[
d_i = 2\{y_i \log(\frac{y_i}{\hat{\mu}})- (y_i - \hat{\mu}_i)\}
\]</span></p>
<div id="analysis-of-deviance" class="section level5">
<h5><span class="header-section-number">6.3.4.3.1</span> Analysis of Deviance</h5>
<p>The difference in deviance between a reduced and full model, where q is the difference in the number of free parameters, has an asymptotic <span class="math inline">\(\chi^2_q\)</span>. The likelihood ratio test</p>
<p><span class="math display">\[
D^*(\mathbf{y;\hat{\mu}_r}) - D^*(\mathbf{y;\hat{\mu}_f}) = 2\{l(\mathbf{y;\hat{\mu}_f})-l(\mathbf{y;\hat{\mu}_r})\}
\]</span></p>
<p>this comparison of models is <strong>Analysis of Deviance</strong></p>
</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-McCullagh_2019">
<p>McCullagh, P., and J. A. Nelder. 2019. “An Outline of Generalized Linear Models.” In <em>Generalized Linear Models</em>, 21–47. Routledge. <a href="https://doi.org/10.1201/9780203753736-2">https://doi.org/10.1201/9780203753736-2</a>.</p>
</div>
<div id="ref-Nelder_1972">
<p>Nelder, J. A., and R. W. M. Wedderburn. 1972. “Generalized Linear Models.” <em>Journal of the Royal Statistical Society. Series A (General)</em> 135 (3): 370. <a href="https://doi.org/10.2307/2344614">https://doi.org/10.2307/2344614</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="non-linear-least-squares.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="genelized-method-of-moments.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mikenguyen13/data_analysis/edit/main/06-2-nonlinear_regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Data Analysis.pdf", "Data Analysis.epub", "Data Analysis.mobi"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true,
"sharing": {
"facebook": true,
"github": true,
"twitter": true,
"linkedin": true
},
"info": true,
"edit": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
