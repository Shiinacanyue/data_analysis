<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.1 Linear Regression | A Guide on Data Analysis</title>
  <meta name="description" content="This is a guide on how to conduct a data analysis routine" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="5.1 Linear Regression | A Guide on Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a guide on how to conduct a data analysis routine" />
  <meta name="github-repo" content="mikenguyen13/data_analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.1 Linear Regression | A Guide on Data Analysis" />
  
  <meta name="twitter:description" content="This is a guide on how to conduct a data analysis routine" />
  

<meta name="author" content="Mike Nguyen" />


<meta name="date" content="2020-12-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regression-analysis.html"/>
<link rel="next" href="non-linear-regression.html"/>
<script src="libs/jquery-3.5.0/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/proj4js-2.3.15/proj4.js"></script>
<link href="libs/highcharts-8.1.2/css/motion.css" rel="stylesheet" />
<script src="libs/highcharts-8.1.2/highcharts.js"></script>
<script src="libs/highcharts-8.1.2/highcharts-3d.js"></script>
<script src="libs/highcharts-8.1.2/highcharts-more.js"></script>
<script src="libs/highcharts-8.1.2/modules/stock.js"></script>
<script src="libs/highcharts-8.1.2/modules/map.js"></script>
<script src="libs/highcharts-8.1.2/modules/annotations.js"></script>
<script src="libs/highcharts-8.1.2/modules/data.js"></script>
<script src="libs/highcharts-8.1.2/modules/drilldown.js"></script>
<script src="libs/highcharts-8.1.2/modules/item-series.js"></script>
<script src="libs/highcharts-8.1.2/modules/offline-exporting.js"></script>
<script src="libs/highcharts-8.1.2/modules/overlapping-datalabels.js"></script>
<script src="libs/highcharts-8.1.2/modules/exporting.js"></script>
<script src="libs/highcharts-8.1.2/modules/export-data.js"></script>
<script src="libs/highcharts-8.1.2/modules/funnel.js"></script>
<script src="libs/highcharts-8.1.2/modules/heatmap.js"></script>
<script src="libs/highcharts-8.1.2/modules/treemap.js"></script>
<script src="libs/highcharts-8.1.2/modules/sankey.js"></script>
<script src="libs/highcharts-8.1.2/modules/dependency-wheel.js"></script>
<script src="libs/highcharts-8.1.2/modules/organization.js"></script>
<script src="libs/highcharts-8.1.2/modules/solid-gauge.js"></script>
<script src="libs/highcharts-8.1.2/modules/streamgraph.js"></script>
<script src="libs/highcharts-8.1.2/modules/sunburst.js"></script>
<script src="libs/highcharts-8.1.2/modules/vector.js"></script>
<script src="libs/highcharts-8.1.2/modules/wordcloud.js"></script>
<script src="libs/highcharts-8.1.2/modules/xrange.js"></script>
<script src="libs/highcharts-8.1.2/modules/tilemap.js"></script>
<script src="libs/highcharts-8.1.2/modules/venn.js"></script>
<script src="libs/highcharts-8.1.2/modules/gantt.js"></script>
<script src="libs/highcharts-8.1.2/modules/timeline.js"></script>
<script src="libs/highcharts-8.1.2/modules/parallel-coordinates.js"></script>
<script src="libs/highcharts-8.1.2/modules/bullet.js"></script>
<script src="libs/highcharts-8.1.2/modules/coloraxis.js"></script>
<script src="libs/highcharts-8.1.2/modules/dumbbell.js"></script>
<script src="libs/highcharts-8.1.2/modules/lollipop.js"></script>
<script src="libs/highcharts-8.1.2/modules/series-label.js"></script>
<script src="libs/highcharts-8.1.2/plugins/motion.js"></script>
<script src="libs/highcharts-8.1.2/custom/reset.js"></script>
<script src="libs/highcharts-8.1.2/modules/boost.js"></script>
<script src="libs/highchart-binding-0.8.2/highchart.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Guide on Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a><ul>
<li class="chapter" data-level="2.1" data-path="matrix-theory.html"><a href="matrix-theory.html"><i class="fa fa-check"></i><b>2.1</b> Matrix Theory</a><ul>
<li class="chapter" data-level="2.1.1" data-path="matrix-theory.html"><a href="matrix-theory.html#rank"><i class="fa fa-check"></i><b>2.1.1</b> Rank</a></li>
<li class="chapter" data-level="2.1.2" data-path="matrix-theory.html"><a href="matrix-theory.html#inverse"><i class="fa fa-check"></i><b>2.1.2</b> Inverse</a></li>
<li class="chapter" data-level="2.1.3" data-path="matrix-theory.html"><a href="matrix-theory.html#definiteness"><i class="fa fa-check"></i><b>2.1.3</b> Definiteness</a></li>
<li class="chapter" data-level="2.1.4" data-path="matrix-theory.html"><a href="matrix-theory.html#matrix-calculus"><i class="fa fa-check"></i><b>2.1.4</b> Matrix Calculus</a></li>
<li class="chapter" data-level="2.1.5" data-path="matrix-theory.html"><a href="matrix-theory.html#optimization"><i class="fa fa-check"></i><b>2.1.5</b> Optimization</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>2.2</b> Probability Theory</a><ul>
<li class="chapter" data-level="2.2.1" data-path="probability-theory.html"><a href="probability-theory.html#axiom-and-theorems-of-probability"><i class="fa fa-check"></i><b>2.2.1</b> Axiom and Theorems of Probability</a></li>
<li class="chapter" data-level="2.2.2" data-path="probability-theory.html"><a href="probability-theory.html#central-limit-theorem"><i class="fa fa-check"></i><b>2.2.2</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="2.2.3" data-path="probability-theory.html"><a href="probability-theory.html#random-variable"><i class="fa fa-check"></i><b>2.2.3</b> Random variable</a></li>
<li class="chapter" data-level="2.2.4" data-path="probability-theory.html"><a href="probability-theory.html#moment"><i class="fa fa-check"></i><b>2.2.4</b> Moment</a></li>
<li class="chapter" data-level="2.2.5" data-path="probability-theory.html"><a href="probability-theory.html#distributions"><i class="fa fa-check"></i><b>2.2.5</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="general-math.html"><a href="general-math.html"><i class="fa fa-check"></i><b>2.3</b> General Math</a><ul>
<li class="chapter" data-level="2.3.1" data-path="general-math.html"><a href="general-math.html#law-of-large-numbers"><i class="fa fa-check"></i><b>2.3.1</b> Law of large numbers</a></li>
<li class="chapter" data-level="2.3.2" data-path="general-math.html"><a href="general-math.html#law-of-iterated-expectation"><i class="fa fa-check"></i><b>2.3.2</b> Law of Iterated Expectation</a></li>
<li class="chapter" data-level="2.3.3" data-path="general-math.html"><a href="general-math.html#convergence"><i class="fa fa-check"></i><b>2.3.3</b> Convergence</a></li>
<li class="chapter" data-level="2.3.4" data-path="general-math.html"><a href="general-math.html#sufficient-statistics"><i class="fa fa-check"></i><b>2.3.4</b> Sufficient Statistics</a></li>
<li class="chapter" data-level="2.3.5" data-path="general-math.html"><a href="general-math.html#parameter-transformations"><i class="fa fa-check"></i><b>2.3.5</b> Parameter transformations</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>2.4</b> Methods</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="descriptive-stat.html"><a href="descriptive-stat.html"><i class="fa fa-check"></i><b>3</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="3.1" data-path="numerical-measures.html"><a href="numerical-measures.html"><i class="fa fa-check"></i><b>3.1</b> Numerical Measures</a></li>
<li class="chapter" data-level="3.2" data-path="graphical-measures.html"><a href="graphical-measures.html"><i class="fa fa-check"></i><b>3.2</b> Graphical Measures</a><ul>
<li class="chapter" data-level="3.2.1" data-path="graphical-measures.html"><a href="graphical-measures.html#shape"><i class="fa fa-check"></i><b>3.2.1</b> Shape</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="normality-assessment.html"><a href="normality-assessment.html"><i class="fa fa-check"></i><b>3.3</b> Normality Assessment</a><ul>
<li class="chapter" data-level="3.3.1" data-path="normality-assessment.html"><a href="normality-assessment.html#graphical-assessment"><i class="fa fa-check"></i><b>3.3.1</b> Graphical Assessment</a></li>
<li class="chapter" data-level="3.3.2" data-path="normality-assessment.html"><a href="normality-assessment.html#summary-statistics"><i class="fa fa-check"></i><b>3.3.2</b> Summary Statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="basic-statistical-inference.html"><a href="basic-statistical-inference.html"><i class="fa fa-check"></i><b>4</b> Basic Statistical Inference</a><ul>
<li class="chapter" data-level="4.1" data-path="one-sample-inference.html"><a href="one-sample-inference.html"><i class="fa fa-check"></i><b>4.1</b> One Sample Inference</a></li>
<li class="chapter" data-level="4.2" data-path="two-sample-inference.html"><a href="two-sample-inference.html"><i class="fa fa-check"></i><b>4.2</b> Two Sample Inference</a></li>
<li class="chapter" data-level="4.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html"><i class="fa fa-check"></i><b>4.3</b> Categorical Data Analysis</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regression-analysis.html"><a href="regression-analysis.html"><i class="fa fa-check"></i><b>5</b> Regression Analysis</a><ul>
<li class="chapter" data-level="5.1" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>5.1</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1.1" data-path="linear-regression.html"><a href="linear-regression.html#ordinary-least-squares"><i class="fa fa-check"></i><b>5.1.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="5.1.2" data-path="linear-regression.html"><a href="linear-regression.html#feasible-generalized-least-squares"><i class="fa fa-check"></i><b>5.1.2</b> Feasible Generalized Least Squares</a></li>
<li class="chapter" data-level="5.1.3" data-path="linear-regression.html"><a href="linear-regression.html#weighted-least-squares"><i class="fa fa-check"></i><b>5.1.3</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="5.1.4" data-path="linear-regression.html"><a href="linear-regression.html#feasiable-prais-winsten"><i class="fa fa-check"></i><b>5.1.4</b> Feasiable Prais Winsten</a></li>
<li class="chapter" data-level="5.1.5" data-path="linear-regression.html"><a href="linear-regression.html#feasible-group-level-random-effects"><i class="fa fa-check"></i><b>5.1.5</b> Feasible group level Random Effects</a></li>
<li class="chapter" data-level="5.1.6" data-path="linear-regression.html"><a href="linear-regression.html#generalized-least-squares"><i class="fa fa-check"></i><b>5.1.6</b> Generalized Least Squares</a></li>
<li class="chapter" data-level="5.1.7" data-path="linear-regression.html"><a href="linear-regression.html#maximum-likelihood"><i class="fa fa-check"></i><b>5.1.7</b> Maximum Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="non-linear-regression.html"><a href="non-linear-regression.html"><i class="fa fa-check"></i><b>5.2</b> Non-linear Regression</a><ul>
<li class="chapter" data-level="5.2.1" data-path="non-linear-regression.html"><a href="non-linear-regression.html#non-linear-least-sqaures"><i class="fa fa-check"></i><b>5.2.1</b> Non-linear Least Sqaures</a></li>
<li class="chapter" data-level="5.2.2" data-path="non-linear-regression.html"><a href="non-linear-regression.html#genelized-method-of-moments"><i class="fa fa-check"></i><b>5.2.2</b> Genelized Method of Moments</a></li>
<li class="chapter" data-level="5.2.3" data-path="non-linear-regression.html"><a href="non-linear-regression.html#minimum-distance"><i class="fa fa-check"></i><b>5.2.3</b> Minimum Distance</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="quantile-regression.html"><a href="quantile-regression.html"><i class="fa fa-check"></i><b>5.3</b> Quantile Regression</a><ul>
<li class="chapter" data-level="5.3.1" data-path="quantile-regression.html"><a href="quantile-regression.html#application-2"><i class="fa fa-check"></i><b>5.3.1</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="spline-regression.html"><a href="spline-regression.html"><i class="fa fa-check"></i><b>5.4</b> Spline Regression</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>6</b> Data</a><ul>
<li class="chapter" data-level="6.1" data-path="cross-sectional.html"><a href="cross-sectional.html"><i class="fa fa-check"></i><b>6.1</b> Cross-Sectional</a></li>
<li class="chapter" data-level="6.2" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>6.2</b> Time Series</a><ul>
<li class="chapter" data-level="6.2.1" data-path="time-series.html"><a href="time-series.html#deterministic-time-trend"><i class="fa fa-check"></i><b>6.2.1</b> Deterministic Time trend</a></li>
<li class="chapter" data-level="6.2.2" data-path="time-series.html"><a href="time-series.html#feedback-effect"><i class="fa fa-check"></i><b>6.2.2</b> Feedback Effect</a></li>
<li class="chapter" data-level="6.2.3" data-path="time-series.html"><a href="time-series.html#dynamic-specification"><i class="fa fa-check"></i><b>6.2.3</b> Dynamic Specification</a></li>
<li class="chapter" data-level="6.2.4" data-path="time-series.html"><a href="time-series.html#dynamically-complete"><i class="fa fa-check"></i><b>6.2.4</b> Dynamically Complete</a></li>
<li class="chapter" data-level="6.2.5" data-path="time-series.html"><a href="time-series.html#highly-persistent-data"><i class="fa fa-check"></i><b>6.2.5</b> Highly Persistent Data</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="repeated-cross-sections.html"><a href="repeated-cross-sections.html"><i class="fa fa-check"></i><b>6.3</b> Repeated Cross Sections</a><ul>
<li class="chapter" data-level="6.3.1" data-path="repeated-cross-sections.html"><a href="repeated-cross-sections.html#pooled-cross-section"><i class="fa fa-check"></i><b>6.3.1</b> Pooled Cross Section</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="panel-data.html"><a href="panel-data.html"><i class="fa fa-check"></i><b>6.4</b> Panel Data</a><ul>
<li class="chapter" data-level="6.4.1" data-path="panel-data.html"><a href="panel-data.html#pooled-ols-esimator"><i class="fa fa-check"></i><b>6.4.1</b> Pooled OLS Esimator</a></li>
<li class="chapter" data-level="6.4.2" data-path="panel-data.html"><a href="panel-data.html#individual-specific-effects-model"><i class="fa fa-check"></i><b>6.4.2</b> Individual-specific effects model</a></li>
<li class="chapter" data-level="6.4.3" data-path="panel-data.html"><a href="panel-data.html#tests-for-assumptions"><i class="fa fa-check"></i><b>6.4.3</b> Tests for Assumptions</a></li>
<li class="chapter" data-level="6.4.4" data-path="panel-data.html"><a href="panel-data.html#model-selection"><i class="fa fa-check"></i><b>6.4.4</b> Model Selection</a></li>
<li class="chapter" data-level="6.4.5" data-path="panel-data.html"><a href="panel-data.html#summary-1"><i class="fa fa-check"></i><b>6.4.5</b> Summary</a></li>
<li class="chapter" data-level="6.4.6" data-path="panel-data.html"><a href="panel-data.html#application-3"><i class="fa fa-check"></i><b>6.4.6</b> Application</a></li>
<li class="chapter" data-level="6.4.7" data-path="panel-data.html"><a href="panel-data.html#other-estimators"><i class="fa fa-check"></i><b>6.4.7</b> Other Estimators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="imputation-missing-data.html"><a href="imputation-missing-data.html"><i class="fa fa-check"></i><b>7</b> Imputation (Missing Data)</a><ul>
<li class="chapter" data-level="7.1" data-path="assumptions.html"><a href="assumptions.html"><i class="fa fa-check"></i><b>7.1</b> Assumptions</a><ul>
<li class="chapter" data-level="7.1.1" data-path="assumptions.html"><a href="assumptions.html#missing-completely-at-random-mcar"><i class="fa fa-check"></i><b>7.1.1</b> Missing Completely at Random (MCAR)</a></li>
<li class="chapter" data-level="7.1.2" data-path="assumptions.html"><a href="assumptions.html#missing-at-random-mar"><i class="fa fa-check"></i><b>7.1.2</b> Missing at Random (MAR)</a></li>
<li class="chapter" data-level="7.1.3" data-path="assumptions.html"><a href="assumptions.html#ignorable"><i class="fa fa-check"></i><b>7.1.3</b> Ignorable</a></li>
<li class="chapter" data-level="7.1.4" data-path="assumptions.html"><a href="assumptions.html#nonignorable"><i class="fa fa-check"></i><b>7.1.4</b> Nonignorable</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html"><i class="fa fa-check"></i><b>7.2</b> Solutions to Missing data</a><ul>
<li class="chapter" data-level="7.2.1" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#listwise-deletion"><i class="fa fa-check"></i><b>7.2.1</b> Listwise Deletion</a></li>
<li class="chapter" data-level="7.2.2" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#pairwise-deletion"><i class="fa fa-check"></i><b>7.2.2</b> Pairwise Deletion</a></li>
<li class="chapter" data-level="7.2.3" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#dummy-variable-adjustment"><i class="fa fa-check"></i><b>7.2.3</b> Dummy Variable Adjustment</a></li>
<li class="chapter" data-level="7.2.4" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#imputation"><i class="fa fa-check"></i><b>7.2.4</b> Imputation</a></li>
<li class="chapter" data-level="7.2.5" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#heckmans-sample-selection-model"><i class="fa fa-check"></i><b>7.2.5</b> Heckman’s Sample Selection Model</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="criteria-for-choosing-an-effective-approach.html"><a href="criteria-for-choosing-an-effective-approach.html"><i class="fa fa-check"></i><b>7.3</b> Criteria for Choosing an Effective Approach</a></li>
<li class="chapter" data-level="7.4" data-path="another-perspective.html"><a href="another-perspective.html"><i class="fa fa-check"></i><b>7.4</b> Another Perspective</a></li>
<li class="chapter" data-level="7.5" data-path="diagnosing-the-mechanism.html"><a href="diagnosing-the-mechanism.html"><i class="fa fa-check"></i><b>7.5</b> Diagnosing the Mechanism</a><ul>
<li class="chapter" data-level="7.5.1" data-path="diagnosing-the-mechanism.html"><a href="diagnosing-the-mechanism.html#mar-vs.-mnar"><i class="fa fa-check"></i><b>7.5.1</b> MAR vs. MNAR</a></li>
<li class="chapter" data-level="7.5.2" data-path="diagnosing-the-mechanism.html"><a href="diagnosing-the-mechanism.html#mcar-vs.-mar"><i class="fa fa-check"></i><b>7.5.2</b> MCAR vs. MAR</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="application-4.html"><a href="application-4.html"><i class="fa fa-check"></i><b>7.6</b> Application</a><ul>
<li class="chapter" data-level="7.6.1" data-path="application-4.html"><a href="application-4.html#imputation-with-mean-median-mode"><i class="fa fa-check"></i><b>7.6.1</b> Imputation with mean / median / mode</a></li>
<li class="chapter" data-level="7.6.2" data-path="application-4.html"><a href="application-4.html#knn"><i class="fa fa-check"></i><b>7.6.2</b> KNN</a></li>
<li class="chapter" data-level="7.6.3" data-path="application-4.html"><a href="application-4.html#rpart"><i class="fa fa-check"></i><b>7.6.3</b> rpart</a></li>
<li class="chapter" data-level="7.6.4" data-path="application-4.html"><a href="application-4.html#mice-multivariate-imputation-via-chained-equations"><i class="fa fa-check"></i><b>7.6.4</b> MICE (Multivariate Imputation via Chained Equations)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="experimental-design.html"><a href="experimental-design.html"><i class="fa fa-check"></i><b>8</b> Experimental Design</a><ul>
<li class="chapter" data-level="8.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html"><i class="fa fa-check"></i><b>8.1</b> Analysis of Variance (ANOVA)</a><ul>
<li class="chapter" data-level="8.1.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#completely-randomized-design-crd"><i class="fa fa-check"></i><b>8.1.1</b> Completely Randomized Design (CRD)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>9</b> Deep Learning</a><ul>
<li class="chapter" data-level="9.1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>9.1</b> Overview</a></li>
<li class="chapter" data-level="9.2" data-path="tensor-flow-and-r.html"><a href="tensor-flow-and-r.html"><i class="fa fa-check"></i><b>9.2</b> Tensor Flow and R</a><ul>
<li class="chapter" data-level="9.2.1" data-path="tensor-flow-and-r.html"><a href="tensor-flow-and-r.html#r-packages"><i class="fa fa-check"></i><b>9.2.1</b> R packages</a></li>
<li class="chapter" data-level="9.2.2" data-path="tensor-flow-and-r.html"><a href="tensor-flow-and-r.html#keras-layers"><i class="fa fa-check"></i><b>9.2.2</b> Keras layers</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="compiling-models.html"><a href="compiling-models.html"><i class="fa fa-check"></i><b>9.3</b> Compiling models</a></li>
<li class="chapter" data-level="9.4" data-path="losses-optimizers-and-metrics.html"><a href="losses-optimizers-and-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Losses, Optimizers, and Metrics</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="research-methods.html"><a href="research-methods.html"><i class="fa fa-check"></i><b>10</b> Research Methods</a><ul>
<li class="chapter" data-level="10.1" data-path="but-for-world.html"><a href="but-for-world.html"><i class="fa fa-check"></i><b>10.1</b> But-for World</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="causality.html"><a href="causality.html"><i class="fa fa-check"></i><b>11</b> Causality</a></li>
<li class="chapter" data-level="12" data-path="report.html"><a href="report.html"><i class="fa fa-check"></i><b>12</b> Report</a><ul>
<li class="chapter" data-level="12.1" data-path="one-summary-table.html"><a href="one-summary-table.html"><i class="fa fa-check"></i><b>12.1</b> One summary table</a></li>
<li class="chapter" data-level="12.2" data-path="model-comparison.html"><a href="model-comparison.html"><i class="fa fa-check"></i><b>12.2</b> Model Comparison</a></li>
<li class="chapter" data-level="12.3" data-path="changes-in-an-estimate.html"><a href="changes-in-an-estimate.html"><i class="fa fa-check"></i><b>12.3</b> Changes in an estimate</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>13</b> Appendix</a><ul>
<li class="chapter" data-level="13.1" data-path="short-cut.html"><a href="short-cut.html"><i class="fa fa-check"></i><b>13.1</b> Short-cut</a></li>
<li class="chapter" data-level="13.2" data-path="function-short-cut.html"><a href="function-short-cut.html"><i class="fa fa-check"></i><b>13.2</b> Function short-cut</a></li>
<li class="chapter" data-level="13.3" data-path="citation.html"><a href="citation.html"><i class="fa fa-check"></i><b>13.3</b> Citation</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="bookdown-cheat-sheet.html"><a href="bookdown-cheat-sheet.html"><i class="fa fa-check"></i><b>14</b> Bookdown cheat sheet</a><ul>
<li class="chapter" data-level="14.1" data-path="math-expresssion-syntax.html"><a href="math-expresssion-syntax.html"><i class="fa fa-check"></i><b>14.1</b> Math Expresssion/ Syntax</a><ul>
<li class="chapter" data-level="14.1.1" data-path="math-expresssion-syntax.html"><a href="math-expresssion-syntax.html#statistics-notation"><i class="fa fa-check"></i><b>14.1.1</b> Statistics Notation</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="table.html"><a href="table.html"><i class="fa fa-check"></i><b>14.2</b> Table</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="part"><span><b>I Group</b></span></li>
<li class="chapter" data-level="A" data-path="endogeneity.html"><a href="endogeneity.html"><i class="fa fa-check"></i><b>A</b> Endogeneity</a><ul>
<li class="chapter" data-level="A.1" data-path="testing-assumption.html"><a href="testing-assumption.html"><i class="fa fa-check"></i><b>A.1</b> Testing Assumption</a><ul>
<li class="chapter" data-level="A.1.1" data-path="testing-assumption.html"><a href="testing-assumption.html#test-of-endogeneity"><i class="fa fa-check"></i><b>A.1.1</b> Test of Endogeneity</a></li>
<li class="chapter" data-level="A.1.2" data-path="testing-assumption.html"><a href="testing-assumption.html#testing-instruments-assumptions"><i class="fa fa-check"></i><b>A.1.2</b> Testing Instrument’s assumptions</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="omitted-variables-bias.html"><a href="omitted-variables-bias.html"><i class="fa fa-check"></i><b>A.2</b> Omitted Variables Bias</a></li>
<li class="chapter" data-level="A.3" data-path="feedback-effect-simultaneity.html"><a href="feedback-effect-simultaneity.html"><i class="fa fa-check"></i><b>A.3</b> Feedback Effect (Simultaneity)</a></li>
<li class="chapter" data-level="A.4" data-path="endogenous-sample-design-sample-selection.html"><a href="endogenous-sample-design-sample-selection.html"><i class="fa fa-check"></i><b>A.4</b> Endogenous sample design (sample selection)</a></li>
<li class="chapter" data-level="A.5" data-path="measurement-error.html"><a href="measurement-error.html"><i class="fa fa-check"></i><b>A.5</b> Measurement Error</a></li>
<li class="chapter" data-level="A.6" data-path="proxy-variables.html"><a href="proxy-variables.html"><i class="fa fa-check"></i><b>A.6</b> Proxy Variables</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>B</b> Hypothesis Testing</a><ul>
<li class="chapter" data-level="B.1" data-path="confidence-intervals-hypothesis-testing.html"><a href="confidence-intervals-hypothesis-testing.html"><i class="fa fa-check"></i><b>B.1</b> Confidence Intervals &amp; Hypothesis Testing</a><ul>
<li class="chapter" data-level="B.1.1" data-path="confidence-intervals-hypothesis-testing.html"><a href="confidence-intervals-hypothesis-testing.html#for-the-mean-mu"><i class="fa fa-check"></i><b>B.1.1</b> For the Mean (<span class="math inline">\(\mu\)</span>)</a></li>
<li class="chapter" data-level="B.1.2" data-path="confidence-intervals-hypothesis-testing.html"><a href="confidence-intervals-hypothesis-testing.html#for-single-proportions-p"><i class="fa fa-check"></i><b>B.1.2</b> For Single Proportions (p)</a></li>
<li class="chapter" data-level="B.1.3" data-path="confidence-intervals-hypothesis-testing.html"><a href="confidence-intervals-hypothesis-testing.html#for-difference-of-two-proportions-p_1---p_2"><i class="fa fa-check"></i><b>B.1.3</b> For Difference of Two Proportions (<span class="math inline">\(p_1 - p_2\)</span>)</a></li>
<li class="chapter" data-level="B.1.4" data-path="confidence-intervals-hypothesis-testing.html"><a href="confidence-intervals-hypothesis-testing.html#for-a-signle-variance-sigma2"><i class="fa fa-check"></i><b>B.1.4</b> For a signle variance (<span class="math inline">\(\sigma^2\)</span>)</a></li>
<li class="chapter" data-level="B.1.5" data-path="confidence-intervals-hypothesis-testing.html"><a href="confidence-intervals-hypothesis-testing.html#for-two-variances-sigma2_1sigma2_2"><i class="fa fa-check"></i><b>B.1.5</b> For Two Variances (<span class="math inline">\(\sigma^2_1,\sigma^2_2\)</span>)</a></li>
<li class="chapter" data-level="B.1.6" data-path="confidence-intervals-hypothesis-testing.html"><a href="confidence-intervals-hypothesis-testing.html#for-difference-of-means-mu_1-mu_2-independent-samples"><i class="fa fa-check"></i><b>B.1.6</b> For Difference of Means (<span class="math inline">\(\mu_1-\mu_2\)</span>), Independent Samples</a></li>
<li class="chapter" data-level="B.1.7" data-path="confidence-intervals-hypothesis-testing.html"><a href="confidence-intervals-hypothesis-testing.html#for-difference-of-means-mu_1---mu_2-paired-samples-d-x-y"><i class="fa fa-check"></i><b>B.1.7</b> For Difference of Means (<span class="math inline">\(\mu_1 - \mu_2\)</span>), Paired Samples (D = X-Y)</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="wald-test.html"><a href="wald-test.html"><i class="fa fa-check"></i><b>B.2</b> Wald Test</a><ul>
<li class="chapter" data-level="B.2.1" data-path="wald-test.html"><a href="wald-test.html#multiple-hypothesis"><i class="fa fa-check"></i><b>B.2.1</b> Multiple Hypothesis</a></li>
<li class="chapter" data-level="B.2.2" data-path="wald-test.html"><a href="wald-test.html#linear-combination"><i class="fa fa-check"></i><b>B.2.2</b> Linear Combination</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="application-5.html"><a href="application-5.html"><i class="fa fa-check"></i><b>B.3</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="model-specification.html"><a href="model-specification.html"><i class="fa fa-check"></i><b>C</b> Model Specification</a><ul>
<li class="chapter" data-level="C.1" data-path="nested-model.html"><a href="nested-model.html"><i class="fa fa-check"></i><b>C.1</b> Nested Model</a><ul>
<li class="chapter" data-level="C.1.1" data-path="nested-model.html"><a href="nested-model.html#chow-test"><i class="fa fa-check"></i><b>C.1.1</b> Chow test</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="non-nested-model.html"><a href="non-nested-model.html"><i class="fa fa-check"></i><b>C.2</b> Non-Nested Model</a><ul>
<li class="chapter" data-level="C.2.1" data-path="non-nested-model.html"><a href="non-nested-model.html#davidson-mackinnon-test"><i class="fa fa-check"></i><b>C.2.1</b> Davidson-Mackinnon test</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="heteroskedasticity-2.html"><a href="heteroskedasticity-2.html"><i class="fa fa-check"></i><b>C.3</b> Heteroskedasticity</a><ul>
<li class="chapter" data-level="C.3.1" data-path="heteroskedasticity-2.html"><a href="heteroskedasticity-2.html#breusch-pagan-test"><i class="fa fa-check"></i><b>C.3.1</b> Breusch-Pagan test</a></li>
<li class="chapter" data-level="C.3.2" data-path="heteroskedasticity-2.html"><a href="heteroskedasticity-2.html#white-test"><i class="fa fa-check"></i><b>C.3.2</b> White test</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="D" data-path="tests-for-difference-among-nested-models.html"><a href="tests-for-difference-among-nested-models.html"><i class="fa fa-check"></i><b>D</b> Tests for difference among nested models</a><ul>
<li class="chapter" data-level="D.1" data-path="wald-test-1.html"><a href="wald-test-1.html"><i class="fa fa-check"></i><b>D.1</b> Wald test</a></li>
<li class="chapter" data-level="D.2" data-path="the-likelihood-ratio-test.html"><a href="the-likelihood-ratio-test.html"><i class="fa fa-check"></i><b>D.2</b> The likelihood ratio test</a></li>
<li class="chapter" data-level="D.3" data-path="lagrange-multiplier-score.html"><a href="lagrange-multiplier-score.html"><i class="fa fa-check"></i><b>D.3</b> Lagrange Multiplier (Score)</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Guide on Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression" class="section level2">
<h2><span class="header-section-number">5.1</span> Linear Regression</h2>
<div id="ordinary-least-squares" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Ordinary Least Squares</h3>
<p>The most fundamental model in statistics or econometric is a OLS linear regression.
OLS = Maximum likelihood when the error term is assumed to be normally distributed.</p>
<div id="ols-assumptions" class="section level4">
<h4><span class="header-section-number">5.1.1.1</span> OLS Assumptions</h4>
<ul>
<li><a href="linear-regression.html#a1-linearity">A1 Linearity</a></li>
<li><a href="linear-regression.html#a2-full-rank">A2 Full rank</a></li>
<li><a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3 Exogeneity of Independent Variables</a></li>
<li><a href="linear-regression.html#a4-homoskedasticity">A4 Homoskedasticity</a></li>
<li><a href="linear-regression.html#a5-data-generation-random-sampling">A5 Data Generation (random Sampling)</a></li>
<li><a href="linear-regression.html#a6-normal-distribution">A6 Normal Distribution</a></li>
</ul>
<div id="a1-linearity" class="section level5">
<h5><span class="header-section-number">5.1.1.1.1</span> A1 Linearity</h5>
<p><span class="math display" id="eq:A1">\[
\begin{equation}
A1: y=\mathbf{x}\beta + \epsilon
\tag{5.1}
\end{equation}
\]</span></p>
<p>Not restrictive</p>
<ul>
<li>x can be nonlinear transformation including interactions, natural log, quadratic</li>
</ul>
<p>With A3 (Exogeneity of Independent), linearity can be restrictive</p>
<div id="log-model" class="section level6">
<h6><span class="header-section-number">5.1.1.1.1.1</span> Log Model</h6>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Form</th>
<th>Interpretation of <span class="math inline">\(\beta\)</span></th>
<th>In words</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Level-Level</td>
<td><span class="math inline">\(y =\beta_0+\beta_1x+\epsilon\)</span></td>
<td><span class="math inline">\(\Delta y = \beta_1 \Delta x\)</span></td>
<td>A unit change in x will result in <span class="math inline">\(\beta_1\)</span> unit change in y</td>
</tr>
<tr class="even">
<td>Log-Level</td>
<td><span class="math inline">\(ln(y)= \beta_0 + \beta_1x +epsilon\)</span></td>
<td><span class="math inline">\(\% \Delta y=100 \beta_1 \Delta x\)</span></td>
<td>A unit change in x result in 100 <span class="math inline">\(\beta_1\)</span> % change in y</td>
</tr>
<tr class="odd">
<td>Level-Log</td>
<td><span class="math inline">\(y = beta_0 + \beta_1 ln(x) + \epsilon\)</span></td>
<td><span class="math inline">\(\Delta y = (\beta_1/100)\%\Delta x\)</span></td>
<td>One percent change in x result in <span class="math inline">\(\beta_1/100\)</span> units change in y</td>
</tr>
<tr class="even">
<td>Log-Log</td>
<td><span class="math inline">\(ln(y) = \beta_0 + \beta_1 ln(x) +\epsilon\)</span></td>
<td><span class="math inline">\(\% \Delta y= \beta_1 \% \Delta x\)</span></td>
<td>One percent change in x result in <span class="math inline">\(\beta_1\)</span> percent change in y</td>
</tr>
</tbody>
</table>
</div>
<div id="higher-orders" class="section level6">
<h6><span class="header-section-number">5.1.1.1.1.2</span> Higher Orders</h6>
<p><span class="math inline">\(y=\beta_0 + x_1\beta_1 + x_1^2\beta_2 + \epsilon\)</span>
<span class="math display">\[
\frac{\partial y}{\partial x_1}=\beta_1 + 2x_1\beta_2
\]</span></p>
<ul>
<li>The effect of <span class="math inline">\(x_1\)</span> on y depends on the level of <span class="math inline">\(x_1\)</span></li>
<li>The partial effect at the average = <span class="math inline">\(\beta_1+2E(x_1)\beta_2\)</span></li>
<li>Average Partial Effect = <span class="math inline">\(E(\beta_1 + 2x_1\beta_2)\)</span></li>
</ul>
</div>
<div id="interactions" class="section level6">
<h6><span class="header-section-number">5.1.1.1.1.3</span> Interactions</h6>
<p><span class="math inline">\(y=\beta_0 + x_1\beta_1 + x_2\beta_2 + x_1x_2\beta_3 + \epsilon\)</span></p>
<ul>
<li><span class="math inline">\(\beta_1\)</span> is the average effect on y for a unit change in <span class="math inline">\(x_1\)</span> when <span class="math inline">\(x_2=0\)</span></li>
<li><span class="math inline">\(\beta_1 + x_2\beta_3\)</span> is the partial effect of <span class="math inline">\(x_1\)</span> on y which depends on the level of <span class="math inline">\(x_2\)</span></li>
</ul>
<p><br>
<br></p>
</div>
</div>
<div id="a2-full-rank" class="section level5">
<h5><span class="header-section-number">5.1.1.1.2</span> A2 Full rank</h5>
<p><span class="math display" id="eq:A2">\[
\begin{equation}
A2: rank(E(x&#39;x))=k
\tag{5.2}
\end{equation}
\]</span></p>
<p>also known as <strong>identification condition</strong></p>
<ul>
<li>columns of <span class="math inline">\(\mathbf{x}\)</span> cannot be written as a linear function of the other columns</li>
<li>which ensures that each parameter is unique and exists in the population regression equation</li>
</ul>
<p><br>
<br></p>
</div>
<div id="a3-exogeneity-of-independent-variables" class="section level5">
<h5><span class="header-section-number">5.1.1.1.3</span> A3 Exogeneity of Independent Variables</h5>
<p><span class="math display" id="eq:A3">\[\begin{equation}
A3: E[\epsilon|x_1,x_2,...,x_k]=E[\epsilon|\mathbf{x}]=0
\tag{5.3}
\end{equation}\]</span></p>
<p><strong>strict exogeneity</strong></p>
<ul>
<li>also known as <strong>mean independence</strong> check back on <a href="probability-theory.html#correlation-and-independence">Correlation and Independence</a></li>
<li>by the <a href="probability-theory.html#law-of-iterated-expectations">Law of Iterated Expectations</a> <span class="math inline">\(E(\epsilon)=0\)</span>, which can be satisfied by always including an intercept.</li>
<li>independent variables do not carry information for prediction of <span class="math inline">\(\epsilon\)</span></li>
<li>A3 implies <span class="math inline">\(E(y|x)=x\beta\)</span>, which means the conditional mean function must be a linear function of x <a href="linear-regression.html#a1-linearity">A1 Linearity</a></li>
</ul>
<div id="a3a" class="section level6">
<h6><span class="header-section-number">5.1.1.1.3.1</span> A3a</h6>
<p>Weaker Exogeneity Assumption</p>
<p><strong>Exogeneity of Independent variables</strong></p>
<p>A3a: <span class="math inline">\(E(\mathbf{x_i&#39;}\epsilon_i)=0\)</span></p>
<ul>
<li><span class="math inline">\(x_i\)</span> is <strong>uncorrelated</strong> with <span class="math inline">\(\epsilon_i\)</span> <a href="probability-theory.html#correlation-and-independence">Correlation and Independence</a></li>
<li>Weaker than <strong>mean independence</strong> A3
<ul>
<li>A3 implies A3a, not the reverse</li>
<li>No causality interpretations</li>
<li>Cannot test the difference</li>
</ul></li>
</ul>
<p><br>
<br></p>
</div>
</div>
<div id="a4-homoskedasticity" class="section level5">
<h5><span class="header-section-number">5.1.1.1.4</span> A4 Homoskedasticity</h5>
<p><span class="math display" id="eq:A4">\[
\begin{equation}
A4: Var(\epsilon|x)=Var(\epsilon)=\sigma^2
\tag{5.4}
\end{equation}
\]</span></p>
<ul>
<li>Variation in the disturbance to be the same over the independent variables</li>
</ul>
<p><br>
<br></p>
</div>
<div id="a5-data-generation-random-sampling" class="section level5">
<h5><span class="header-section-number">5.1.1.1.5</span> A5 Data Generation (random Sampling)</h5>
<p><span class="math display" id="eq:A5">\[
\begin{equation}
A5: {y_i,x_{i1},...,x_{ik-1}: i = 1,..., n}
\tag{5.5}
\end{equation}
\]</span>
is a random sample</p>
<ul>
<li>random sample mean samples are independent and identically distributed (iid) from a joint distribution of <span class="math inline">\((y,\mathbf{x})\)</span></li>
<li>with <a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3</a> and <a href="linear-regression.html#a4-homoskedasticity">A4</a>, we have
<ul>
<li><strong>Strict Exogeneity</strong>: <span class="math inline">\(E(\epsilon_i|x_1,...,x_n)=0\)</span>. independent variables do not carry information for prediction of <span class="math inline">\(\epsilon\)</span></li>
<li><strong>Non-autocorrelation</strong>: <span class="math inline">\(E(\epsilon_i\epsilon_j|x_1,...,x_n)=0\)</span> The error term is uncorrelated across the draws conditional on the independent variables <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(A4: Var(\epsilon|\mathbf{X})=Var(\epsilon)=\sigma^2I_n\)</span></li>
</ul></li>
<li>In times series and spatial settings, A5 is less likely to hold.</li>
</ul>
<div id="a5a" class="section level6">
<h6><span class="header-section-number">5.1.1.1.5.1</span> A5a</h6>
<p>A stochastic process <span class="math inline">\(\{x_t\}_{t=1}^T\)</span> is <strong>stationary</strong> if for every collection fo time indices <span class="math inline">\(\{t_1,t_2,...,t_m\}\)</span>, the joint distribution of</p>
<p><span class="math display">\[
x_{t_1},x_{t_2},...,x_{t_m}
\]</span>
is the same as the joint distribution of</p>
<p><span class="math display">\[
x_{t_1+h},x_{t_2+h},...,x_{t_m+h}
\]</span>
for any <span class="math inline">\(h \ge 1\)</span></p>
<ul>
<li>The joint distribution for the first ten observation is the same for the next ten, etc.</li>
<li>Independent draws automatically satisfies this</li>
</ul>
<p><br></p>
<p>A stochastic process <span class="math inline">\(\{x_t\}_{t=1}^T\)</span> is <strong>weakly stationary</strong> if <span class="math inline">\(x_t\)</span> and <span class="math inline">\(x_{t+h}\)</span> are “almost independent” as h increases without bounds.<br />
* two observation that are very far apart should be “almost independent”</p>
<p>Common Weakly Dependent Processes</p>
<ol style="list-style-type: decimal">
<li>Moving Average process of order 1 (MA(1))</li>
</ol>
<p>MA(1) means that there is only one period lag.</p>
<p><span class="math display">\[
y_t = u_t + \alpha_1 u_{t-1} \\
E(y_t) = E(u_t) + \alpha_1E(u_{t-1}) = 0 \\
Var(y_t) = var(u_t) + \alpha_1 var(u_{t-1}) = \sigma^2 + \alpha_1^2 \sigma^2 = \sigma^2(1+\alpha_1^2)
\]</span>
where <span class="math inline">\(u_t\)</span> is drawn iid over t with variance <span class="math inline">\(\sigma^2\)</span></p>
<p>An increase in the absolute value of <span class="math inline">\(\alpha_1\)</span> increases the variance</p>
<p>When the MA(1) process can be <strong>inverted</strong> (<span class="math inline">\(|\alpha|&lt;1\)</span> then</p>
<p><span class="math display">\[
u_t = y_t - \alpha_1u_{t-1}
\]</span>
called the autoregressive representation (express current observation in term of past observation).</p>
<p>We can expand it to more than 1 lag, then we have MA(q) process</p>
<p><span class="math display">\[
y_t = u_t + \alpha_1 u_{t-1} + ... + \alpha_q u_{t-q}
\]</span></p>
<p>where <span class="math inline">\(u_t \sim WN(0,\sigma^2)\)</span></p>
<ul>
<li>Covariance stationary: irrespective of the value of the parameters.</li>
<li>Invertibility when <span class="math inline">\(\alpha &lt; 1\)</span></li>
<li>The conditional mean of MA(q) depends on the q lags (long-term memory).</li>
<li>In MA(q), all autorcorrealtions beyond q are 0.</li>
</ul>
<p><span class="math display">\[
\begin{align}
Cov(y_t,y_{t-1}) &amp;= Cov(u_t + \alpha_1 u_{t-1},u_{t-1}+\alpha_1u_{t-2}) \\
&amp;= \alpha_1var(u_{t-1}) \\
&amp;= \alpha_1\sigma^2
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
Cov(y_t,y_{t-2}) &amp;= Cov(u_t + \alpha_1 u_{t-1},u_{t-2}+\alpha_{1}u_{t-3}) \\
&amp;= 0
\end{align}
\]</span></p>
<p>An MA models a linear relationship between teh dependent variable and teh current and past values of a stochastic term.</p>
<ol start="2" style="list-style-type: decimal">
<li>Auto regressive process of order 1 (AR(1))</li>
</ol>
<p><span class="math display">\[
y_t = \rho y_{t-1}+ u_t, |\rho|&lt;1
\]</span></p>
<p>where <span class="math inline">\(u_t\)</span> is drawn iid over t with variance <span class="math inline">\(\sigma^2\)</span></p>
<p><span class="math display">\[
\begin{align}
Cov(y_t,y_{t-1}) &amp;= Cov(\rho y_{t-1} + u-t,y_{t-1}) \\
&amp;= \rho Var(y_{t-1}) \\
&amp;= \rho \frac{\sigma^2}{1-\rho^2}
\end{align}
\]</span></p>
<p><span class="math display">\[
\begin{align}
Cov(y_t,y_{t-h}) &amp;= \rho^h \frac{\sigma^2}{1-\rho^2}
\end{align}
\]</span></p>
<p>Stationarity:
in the continuum of t, the distribution of each t is the same</p>
<p><span class="math display">\[
E(y_t) = E(y_{t-1}) = ...= E(y_0) \\
y_1 = \rho y_0 + u_1
\]</span>
where the initial observation <span class="math inline">\(y_0=0\)</span></p>
<p>Assume <span class="math inline">\(E(y_t)=0\)</span></p>
<p><span class="math display">\[
y_t = \rho^t y_{t-t} + \rho^{t-1}u_1 + \rho^{t-2}u_2 +...+ \rho u_{t-1} + u_t \\
= \rho^t y_0 + \rho^{t-1}u_1 + \rho^{t-2}u_2 +...+ \rho u_{t-1} + u_t
\]</span></p>
<p>Hence, <span class="math inline">\(y_t\)</span> is the weighted of all of the <span class="math inline">\(u_t\)</span> time observations before.
y will be correlated with all the previous observations as well as future observations.</p>
<p><span class="math display">\[
Var(y_t) = Var(\rho y_{t-1} + u_t) \\
= \rho^2 Var(y_{t-1}) + Var(u_t) + 2\rho Cov(y_{t-1}u_t) \\
= \rho^2 Var(y_{t-1}) + \sigma^2
\]</span>
Hence,</p>
<p><span class="math display">\[
Var(y_t) = \frac{\sigma^2}{1-\rho^2}
\]</span>
to have Variance constantly over time, then <span class="math inline">\(\rho \neq 1\)</span> or <span class="math inline">\(-1\)</span>.</p>
<p><strong>Then</strong> stationarity requires <span class="math inline">\(\rho \neq 1\)</span> or -1.
weakly dependent process <span class="math inline">\(|\rho|&lt;1\)</span></p>
<p>To estimate the AR(1) process, we use <strong>Yule-Walker Equation</strong></p>
<p><span class="math display">\[
y_t = \epsilon_t + \phi y_{t-1} \\
y_t y_{t-\tau} = \epsilon_t y_{t-\tau} + \phi y_{t-1}y_{t-\tau} \\
\]</span>
For <span class="math inline">\(\tau \ge 1\)</span>, we have</p>
<p><span class="math display">\[
\gamma \tau = \phi \gamma (\tau -1) \\
\rho_t = \phi^t
\]</span>
when you generalize to pth order autoregressive process, AR(p):</p>
<p><span class="math display">\[
y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + ... + \phi_p y_{t-p} + \epsilon_t
\]</span></p>
<p>AR(p) process is <strong>covariance stationary</strong>, and decay in autocorrelations.</p>
<p>When we combine MA(q) and AR(p), we have ARMA(p,q) process, where you can see seasonality. For example, ARMA(1,1)</p>
<p><span class="math display">\[
y_t = \phi y_{t-1} + \epsilon_t + \alpha \epsilon_{t-1}
\]</span></p>
<p>Random Walk process</p>
<p><span class="math display">\[
y_t = y_0 + \sum_{s=1}^{t}u_t
\]</span></p>
<ul>
<li>not stationary : when <span class="math inline">\(y_0 = 0\)</span> then <span class="math inline">\(E(y_t)= 0\)</span>, but <span class="math inline">\(Var(y_t)=t\sigma^2\)</span>. Further along in the spectrum, the variance will be larger</li>
<li>not weakly dependent: <span class="math inline">\(Cov(\sum_{s=1}^{t}u_s,\sum_{s=1}^{t-h}u_s) = (t-h)\sigma^2\)</span>. So the covariance (fixed) is not diminishing as h increases</li>
</ul>
<p><span class="math display">\[
Assumption A5a: \{y_t,x_{t1},..,x_{tk-1} \}
\]</span>
where <span class="math inline">\(t=1,...,T\)</span> are <strong>stationary and weakly dependent processes</strong>.</p>
<p>Alternative <a href="general-math.html#weak-law">Weak Law</a>, <a href="probability-theory.html#central-limit-theorem">Central Limit Theorem</a><br />
If <span class="math inline">\(z_t\)</span> is a weakly dependent stationary process with a finite first absolute moment and <span class="math inline">\(E(z_t) = \mu\)</span>, then</p>
<p><span class="math display">\[
T^{-1}\sum_{t=1}^{T}z_t \to^p \mu
\]</span>
If additional regulatory conditions hold <span class="citation">(Greene <a href="#ref-Greene_1990" role="doc-biblioref">1990</a>)</span>, then</p>
<p><span class="math display">\[
\sqrt{T}(\bar{z}-\mu) \to^d N(0,B)
\]</span>
where <span class="math inline">\(B= Var(z_t) + 2\sum_{h=1}^{\infty}Cov(z_t,z_{t-h})\)</span></p>
<p><br></p>
</div>
</div>
<div id="a6-normal-distribution" class="section level5">
<h5><span class="header-section-number">5.1.1.1.6</span> A6 Normal Distribution</h5>
<p><span class="math display" id="eq:A6">\[
\begin{equation}
A6: \epsilon|\mathbf{x}\sim N(0,\sigma^2I_n)
\tag{5.6}
\end{equation}
\]</span>
The error term is normally distributed</p>
<p><br></p>
<p>From A1-A3, we have <strong>identification</strong> (also known as <strong>Orthogonality Condition</strong>) of the population parameter <span class="math inline">\(\beta\)</span></p>
<p><span class="math display">\[\begin{align}
y &amp;= {x}\beta + \epsilon &amp;&amp; \text{A1} \\
x&#39;y &amp;= x&#39;x\beta + x&#39;\epsilon &amp;&amp; \text{} \\
E(x&#39;y) &amp;= E(x&#39;x)\beta + E(x&#39;\epsilon)  &amp;&amp; \text{} \\
E(x&#39;y) &amp;= E(x&#39;x)\beta &amp;&amp; \text{A3} \\
[E(x&#39;x)]^{-1}E(x&#39;y) &amp;= [E(x&#39;x)]^{-1}E(x&#39;x)\beta &amp;&amp; \text{A2} \\
[E(x&#39;x)]^{-1}E(x&#39;y) &amp;= \beta
\end{align}\]</span></p>
<p>is the row vector of parameters that produces the best predictor of y
we choose the min of :
<span class="math display">\[
\underset{\gamma}{\operatorname{argmin}}E((y-x\gamma)^2)
\]</span>
First Order Condition
<span class="math display">\[
\begin{split}
\frac{\partial((y-x\gamma)^2)}{\partial\gamma}&amp;=0 \\
-2E(x&#39;(y-x\gamma))&amp;=0 \\
E(x&#39;y)-E(x&#39;x\gamma) &amp;=0 \\
E(x&#39;y) &amp;= E(x&#39;x)\gamma \\
(E(x&#39;x))^{-1}E(x&#39;y) &amp;= \gamma
\end{split}
\]</span></p>
<p>Second Order Conditon
<span class="math display">\[
\begin{split}
\frac{\partial^2E((y-x\gamma)^2)}{}&amp;=0 \\
E(\frac{\partial(y-x\partial)^2)}{\partial\gamma\partial\gamma&#39;}) &amp;= 2E(x&#39;x)
\end{split}
\]</span>
If <a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3</a> holds, then <span class="math inline">\(2E(x&#39;x)\)</span> is PSD <span class="math inline">\(\rightarrow\)</span> minimum</p>
<p><br>
<br></p>
</div>
</div>
<div id="theorems" class="section level4">
<h4><span class="header-section-number">5.1.1.2</span> Theorems</h4>
<div id="frisch-waugh-lovell-theorem" class="section level5">
<h5><span class="header-section-number">5.1.1.2.1</span> Frisch-Waugh-Lovell Theorem</h5>
<p><span class="math display">\[
\mathbf{y=X\beta + \epsilon=X_1\beta_1+X_2\beta_2 +\epsilon}
\]</span>
Equivalently,
<span class="math display">\[
\left(
\begin{array}{c}
X_1&#39;X_1 &amp; X_1&#39;X_2 \\
X_2&#39;X_1 &amp; X_2&#39;X_2
\end{array}
\right)
\left(
\begin{array}{c}
\hat{\beta_1} \\
\hat{\beta_2}
\end{array}
\right)
=
\left(
\begin{array}{c}
X_1&#39;y \\
X_2&#39;y
\end{array}
\right)
\]</span>
Hence,
<span class="math display">\[
\mathbf{\hat{\beta_1}=(X_1&#39;X_1)^{-1}X_1&#39;y - (X_1&#39;X_1)^{-1}X_1&#39;X_2\hat{\beta_2}}
\]</span></p>
<ol style="list-style-type: decimal">
<li>Betas from the multiple regression are not the same as the betas from each of the individual simple regression</li>
<li>Different set of X will affect all the coefficient estimates.</li>
<li>If <span class="math inline">\(X_1&#39;X_2 = 0\)</span> or $=0, then 1 and 2 do not hold.</li>
</ol>
<p><br></p>
</div>
<div id="gauss-markov-theorem" class="section level5">
<h5><span class="header-section-number">5.1.1.2.2</span> Gauss-Markov Theorem</h5>
<p>For a linear regression model</p>
<p><span class="math display">\[
\mathbf{y=X\beta + \epsilon}
\]</span></p>
<p>Under <a href="linear-regression.html#a1-linearity">A1</a>, <a href="linear-regression.html#a2-full-rank">A2</a>, <a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3</a>, <a href="linear-regression.html#a4-homoskedasticity">A4</a>, OLS estimator defined as</p>
<p><span class="math display">\[
\hat{\beta} = \mathbf{(X&#39;X)^{-1}X&#39;y}
\]</span>
is the minimum variance linear (in y) unbiased estimator of <span class="math inline">\(\beta\)</span></p>
<p>Let <span class="math inline">\(\tilde{\beta}=\mathbf{Cy}\)</span>, be another linear estimator where <span class="math inline">\(\mathbf{C}\)</span> is k x n and only function of X), then for it be unbiased,</p>
<p><span class="math display">\[
\begin{split}
E(\tilde{\beta}|\mathbf{X}) &amp;= E(\mathbf{Cy|X}) \\
&amp;= E(\mathbf{CX\beta + C\epsilon|X}) \\
&amp;= \mathbf{CX\beta}
\end{split}
\]</span>
which equals the true parameter <span class="math inline">\(\beta\)</span> only if <span class="math inline">\(\mathbf{CX=I}\)</span><br />
Equivalently,
<span class="math inline">\(\tilde{\beta} = \beta + \mathbf{C}\epsilon\)</span>
and the variance of the estimator is <span class="math inline">\(Var(\tilde{\beta}|\mathbf{X}) = \sigma^2\mathbf{CC&#39;}\)</span></p>
<p>To show minimum variance,
<span class="math display">\[
\begin{split}
&amp;=\sigma^2\mathbf{(C-(X&#39;X)^{-1}X&#39;)(C-(X&#39;X)^{-1}X&#39;)&#39;} \\
&amp;= \sigma^2\mathbf{(CC&#39; - CX(X&#39;X)^{-1})-(X&#39;X)^{-1}X&#39;C + (X&#39;X)^{-1}X&#39;X(X&#39;X)^{-1})} \\
&amp;= \sigma^2 (\mathbf{CC&#39; - (X&#39;X)^{-1}-(X&#39;X)^{-1} + (X&#39;X)^{-1}}) \\
&amp;= \sigma^2\mathbf{CC&#39;} - \sigma^2(\mathbf{X&#39;X})^{-1} \\
&amp;= Var(\tilde{\beta}|\mathbf{X}) - Var(\hat{\beta}|\mathbf{X})
\end{split}
\]</span></p>
<p><strong>Hierarchy of OLS Assumptions</strong></p>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th>Identification Data Description</th>
<th>Unbiasedness Consistency</th>
<th><a href="linear-regression.html#gauss-markov-theorem">Gauss-Markov</a> (BLUE) Asymptotic Inference (z and Chi-squared)</th>
<th>Classical LM (BUE) Small-sample Inference (t and F)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Variation in X</td>
<td>Variation in X</td>
<td>Variation in X</td>
<td>Variation in X</td>
</tr>
<tr class="even">
<td></td>
<td>Random Sampling</td>
<td>Random Sampling</td>
<td>Random Sampling</td>
</tr>
<tr class="odd">
<td></td>
<td>Linearity in Parameters</td>
<td>Linearity in Parameters</td>
<td>Linearity in Parameters</td>
</tr>
<tr class="even">
<td></td>
<td>Zero Conditional Mean</td>
<td>Zero Conditional Mean</td>
<td>Zero Conditional Mean</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td>Homoskedasticity</td>
<td>Homoskedasticity</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td>Normality of Errors</td>
</tr>
</tbody>
</table>
<p><br>
<br>
<br>
<br></p>
</div>
</div>
<div id="finite-sample-properties" class="section level4">
<h4><span class="header-section-number">5.1.1.3</span> Finite Sample Properties</h4>
<ul>
<li>n is fixed</li>
<li><strong>Bias</strong> On average, how close is our estimate to the true value
<ul>
<li><span class="math inline">\(Bias = E(\hat{\beta}) -\beta\)</span> where <span class="math inline">\(\beta\)</span> is the true parameter value and <span class="math inline">\(\hat{\beta}\)</span> is the estimator for <span class="math inline">\(\beta\)</span></li>
<li>An estimator is <strong>unbiased</strong> when
<ul>
<li><span class="math inline">\(Bias = E(\hat{\beta}) -\beta = 0\)</span> or <span class="math inline">\(E(\hat{\beta})=\beta\)</span></li>
<li>means that the estimator will produce estimates that are, on average, equal to the value it it trying to estimate</li>
</ul></li>
</ul></li>
<li><strong>Distribution of an estimator</strong>: An estimator is a function of random variables (data)</li>
<li><strong>Standard Deviation</strong>: the spread of the estimator.</li>
</ul>
<p><strong>OLS</strong></p>
<p>Under <a href="linear-regression.html#a1-linearity">A1</a> <a href="linear-regression.html#a2-full-rank">A2</a> <a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3</a>, OLS is unbiased</p>
<p><span class="math display">\[
\begin{align}
E(\hat{\beta}) &amp;= E(\mathbf{(X&#39;X)^{-1}X&#39;y}) &amp;&amp; \text{A2}\\
     &amp;= E(\mathbf{(X&#39;X)^{-1}X&#39;(X\beta + \epsilon)}) &amp;&amp; \text{A1}\\
     &amp;= E(\mathbf{(X&#39;X)^{-1}X&#39;X\beta + (X&#39;X)^{-1}X&#39;\epsilon})  &amp;&amp; \text{} \\
     &amp;= E(\beta + \mathbf{(X&#39;X)^{-1}X&#39;\epsilon}) \\
     &amp;= \beta + E(\mathbf{(X&#39;X^{-1}\epsilon)}) \\
     &amp;= \beta + E(E((\mathbf{X&#39;X)^{-1}X&#39;\epsilon|X})) &amp;&amp;\text{LIE} \\
     &amp;= \beta + E((\mathbf{X&#39;X)^{-1}X&#39;}E\mathbf{(\epsilon|X})) \\
     &amp;= \beta + E((\mathbf{X&#39;X)^{-1}X&#39;}0)) &amp;&amp; \text{A3} \\
     &amp;= \beta
\end{align}
\]</span></p>
<p>where LIE stands for <a href="general-math.html#law-of-iterated-expectation">Law of Iterated Expectation</a></p>
<p>If <a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3</a> does not hold, then OLS will be <strong>biased</strong></p>
<p>From <strong>Frisch-Waugh-Lovell Theorem</strong>, if we have the omitted variable <span class="math inline">\(\hat{\beta}_2 \neq 0\)</span> and <span class="math inline">\(\mathbf{X_1&#39;X_2} \neq 0\)</span>, then the omitted variable will cause OLS estimator to be biased.</p>
<p>Under <a href="linear-regression.html#a1-linearity">A1</a> <a href="linear-regression.html#a2-full-rank">A2</a> <a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3</a> <a href="linear-regression.html#a4-homoskedasticity">A4</a>, we have the conditional variance of the OLS estimator as follows]</p>
<p><span class="math display">\[
\begin{align}
Var(\hat{\beta}|\mathbf{X}) &amp;= Var(\beta + \mathbf{(X&#39;X)^{-1}X&#39;\epsilon|X}) &amp;&amp; \text{A1-A2}\\
    &amp;= Var((\mathbf{X&#39;X)^{-1}X&#39;\epsilon|X)} \\
    &amp;= \mathbf{X&#39;X^{-1}X&#39;} Var(\epsilon|\mathbf{X})\mathbf{X(X&#39;X)^{-1}} \\
    &amp;= \mathbf{X&#39;X^{-1}X&#39;} \sigma^2I \mathbf{X(X&#39;X)^{-1}} &amp;&amp; \text{A4} \\
    &amp;= \sigma^2\mathbf{X&#39;X^{-1}X&#39;} I \mathbf{X(X&#39;X)^{-1}} \\
    &amp;= \sigma^2\mathbf{(X&#39;X)^{-1}}
\end{align}
\]</span></p>
<p>Sources of variation</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\sigma^2=Var(\epsilon_i|\mathbf{X})\)</span>
<ul>
<li>The amount of unexplained variation <span class="math inline">\(\epsilon_i\)</span> is large relative to the explained <span class="math inline">\(\mathbf{x_i \beta}\)</span> variation</li>
</ul></li>
<li>“Small” <span class="math inline">\(Var(x_{i1}), Var(x_{i1}),..\)</span>
<ul>
<li>Not a lot of variation in <span class="math inline">\(\mathbf{X}\)</span> (no information)</li>
<li>small sample size</li>
</ul></li>
<li>“Strong” correlation between the explanatory variables
<ul>
<li><span class="math inline">\(x_{i1}\)</span> is highly correlated with a linear combination of 1, <span class="math inline">\(x_{i2}\)</span>, <span class="math inline">\(x_{i3}\)</span>, …</li>
<li>include many irrelevant variables will contribute to this.</li>
<li>If <span class="math inline">\(x_1\)</span> is perfectly determined in the regression <span class="math inline">\(\rightarrow\)</span> <strong>Perfect Collinearity</strong> <span class="math inline">\(\rightarrow\)</span> <a href="linear-regression.html#a2-full-rank">A2</a> is violated.</li>
<li>If <span class="math inline">\(x_1\)</span> is highly correlated with a linear combination of other variables, then we have <strong>Multicollinearity</strong></li>
</ul></li>
</ol>
<div id="check-for-multicollinearity" class="section level5">
<h5><span class="header-section-number">5.1.1.3.1</span> Check for Multicollinearity</h5>
<p><strong>Variance Inflation Factor</strong> (VIF)
Rule of thumb <span class="math inline">\(VIF \ge 10\)</span> is large</p>
<p><span class="math display">\[
VIF = \frac{1}{1-R_1^2} 
\]</span></p>
</div>
<div id="standard-errors" class="section level5">
<h5><span class="header-section-number">5.1.1.3.2</span> Standard Errors</h5>
<ul>
<li><span class="math inline">\(Var(\hat{\beta}|\mathbf{X})=\sigma^2\mathbf{(X&#39;X)^{-1}}\)</span> is the variance of the estimate <span class="math inline">\(\hat{\beta}\)</span></li>
<li><strong>Standard Errors</strong> are estimators/estimates of the standard deviation (square root of the variance) of the estimator <span class="math inline">\(\hat{\beta}\)</span></li>
<li>Under A1-A5, then we can estimate <span class="math inline">\(\sigma^2=Var(\epsilon^2|\mathbf{X})\)</span> the standard errors as</li>
</ul>
<p><span class="math display">\[
s^2 = \frac{1}{n-k}\sum_{i=1}^{n}e_i^2 \\
= \frac{1}{n-k}SSR
\]</span></p>
<ul>
<li>degrees of freedom adjustment: because <span class="math inline">\(e_i \neq \epsilon_i\)</span> and are estimated using k estimates for <span class="math inline">\(\beta\)</span>, we lose degrees of freedom in our variance estimate.</li>
<li><span class="math inline">\(s=\sqrt{s^2}\)</span> is a biased estimator for the standard deviation ([Jensen’s Inequality])</li>
</ul>
<p><strong>Standard Errors for <span class="math inline">\(\hat{\beta}\)</span></strong></p>
<p><span class="math display">\[
SE(\hat{\beta}_{j-1})=s\sqrt{[(\mathbf{X&#39;X})^{-1}]_{jj}} \\
= \frac{s}{\sqrt{SST_{j-1}(1-R_{j-1}^2)}}
\]</span>
where <span class="math inline">\(SST_{j-1}\)</span> and <span class="math inline">\(R_{j-1}^2\)</span> from the following regression</p>
<p><span class="math inline">\(x_{j-1}\)</span> on 1, <span class="math inline">\(x_1\)</span>,… <span class="math inline">\(x_{j-2}\)</span>,<span class="math inline">\(x_j\)</span>,<span class="math inline">\(x_{j+1}\)</span>, …, <span class="math inline">\(x_{k-1}\)</span></p>
<p><strong>Summary of Finite Sample Properties</strong></p>
<ul>
<li>Under A1-A3: OLS is unbiased</li>
<li>Under A1-A4: The variance of the OLS estimator is <span class="math inline">\(Var(\hat{\beta}|\mathbf{X})=\sigma^2\mathbf{(X&#39;X)^{-1}}\)</span></li>
<li>Under A1-A4, A6: OLS estimator <span class="math inline">\(\hat{\beta} \sim N(\beta,\sigma^2\mathbf{(X&#39;X)^{-1}})\)</span></li>
<li>Under A1-A4, Gauss-Markov Theorem holds <span class="math inline">\(\rightarrow\)</span> OLS is BLUE</li>
<li>Under A1-A5, the above standard errors are unbiased estimator of standard deviation for <span class="math inline">\(\hat{\beta}\)</span></li>
</ul>
</div>
</div>
<div id="large-sample-properties" class="section level4">
<h4><span class="header-section-number">5.1.1.4</span> Large Sample Properties</h4>
<ul>
<li>let <span class="math inline">\(n \rightarrow \infty\)</span></li>
<li>A perspective that allows us to evaluate the “quality” of estimators when finite sample properties are not informative, or impossible to compute</li>
<li>consistency, asymptotic distribution, asymptotic variance</li>
</ul>
<p><strong>Motivation</strong></p>
<ul>
<li><a href="linear-regression.html#finite-sample-properties">Finite Sample Properties</a> need strong assumption <a href="linear-regression.html#a1-linearity">A1</a> <a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3</a> <a href="linear-regression.html#a4-homoskedasticity">A4</a> <a href="linear-regression.html#a6-normal-distribution">A6</a></li>
<li>Other estimation such as GLS, MLE need to be analyzed using <a href="linear-regression.html#large-sample-properties">Large Sample Properties</a></li>
</ul>
<p>Let <span class="math inline">\(\mu(\mathbf{X})=E(y|\mathbf{X})\)</span> be the <strong>Conditional Expectation Function</strong></p>
<ul>
<li><span class="math inline">\(\mu(\mathbf{X})\)</span> is the minimum mean squared predictor (over all possible functions)</li>
</ul>
<p><span class="math display">\[
minE((y-f(\mathbf{X}))^2)
\]</span></p>
<p>under A1 and A3,</p>
<p><span class="math display">\[
\mu(\mathbf{X})=\mathbf{X}\beta
\]</span></p>
<p>Then the <strong>linear projection</strong></p>
<p><span class="math display">\[
L(y|1,\mathbf{X})=\gamma_0 + \mathbf{X}Var(X)^{-1}Cov(X,Y)
\]</span></p>
<p>where <span class="math inline">\(\mathbf{X}Var(X)^{-1}Cov(X,Y)=\gamma\)</span></p>
<p>is the minimum mean squared linear approximation to be conditional mean function</p>
<p><span class="math display">\[
(\gamma_0,\gamma) = arg min E((E(y|\mathbf{X})-(a+\mathbf{Xb})^2)
\]</span></p>
<ul>
<li>OLS is always <strong>consistent</strong> for the linear projection, but not necessarily unbiased.</li>
<li>Linear projection has no causal interpretation</li>
<li>Linear projection does not depend on assumption A1 and A3</li>
</ul>
<p>Evaluating an estimator using large sample properties:</p>
<ul>
<li>Consistency: measure of centrality</li>
<li>Limiting Distribution: the shape of the scaled estimator as the sample size increases</li>
<li>Asymptotic variance: spread of the estimator with regards to its limiting distribution.</li>
</ul>
<p>An estimator <span class="math inline">\(\hat{\theta}\)</span> is consistent for <span class="math inline">\(\theta\)</span> if <span class="math inline">\(\hat{\theta}_n \to^p \theta\)</span></p>
<ul>
<li>As n increases, the estimator converges to the population parameter value.</li>
<li>Unbiased does not imply consistency and consistency does not imply unbiased.</li>
</ul>
<p>Based on <a href="general-math.html#weak-law">Weak Law</a> of Large Numbers</p>
<p><span class="math display">\[
\begin{split}
\hat{\beta} &amp;= \mathbf{(X&#39;X)^{-1}X&#39;y} \\
&amp;= \mathbf{(\sum_{i=1}^{n}x_i&#39;x_i)^{-1} \sum_{i=1}^{n}x_i&#39;y_i} \\
&amp;= (n^{-1}\mathbf{\sum_{i=1}^{n}x_i&#39;x_i)^{-1}} n^{-1}\mathbf{\sum_{i=1}^{n}x_i&#39;y_i} \\
plim(\hat{\beta}) &amp;= plim((n^{-1}\mathbf{\sum_{i=1}^{n}x_i&#39;x_i)^{-1}} n^{-1}\mathbf{\sum_{i=1}^{n}x_i&#39;y_i}) \\
&amp;= plim((n^{-1}\mathbf{\sum_{i=1}^{n}x_i&#39;x_i)^{-1}})plim(n^{-1}\mathbf{\sum_{i=1}^{n}x_i&#39;y_i}) \\
&amp;= (plim(n^{-1}\mathbf{\sum_{i=1}^{n}x_i&#39;x_i)^{-1}})plim(n^{-1}\mathbf{\sum_{i=1}^{n}x_i&#39;y_i}) &amp;&amp; \text{ due to A2, A5} \\
&amp;= E(\mathbf{x_i&#39;x_i})^{-1}E(\mathbf{x_i&#39;y_i})
\end{split}
\]</span></p>
<p><span class="math display">\[
E(\mathbf{x_i&#39;x_i})^{-1}E(\mathbf{x_i&#39;y_i}) = \beta + E(\mathbf{x_i&#39;x_i})^{-1}E(\mathbf{x_i&#39;\epsilon_i})
\]</span></p>
<p>Under <a href="linear-regression.html#a1-linearity">A1</a>, <a href="linear-regression.html#a2-full-rank">A2</a>, <a href="linear-regression.html#a3a">A3a</a>, <a href="linear-regression.html#a5-data-generation-random-sampling">A5</a> OLS is consistent, but not guarantee unbiased.</p>
<p>Under <a href="linear-regression.html#a1-linearity">A1</a>, <a href="linear-regression.html#a2-full-rank">A2</a>, <a href="linear-regression.html#a3a">A3a</a>, <a href="linear-regression.html#a5-data-generation-random-sampling">A5</a>, and <span class="math inline">\(\mathbf{x_i&#39;x_i}\)</span> has finite first and second moments (<a href="probability-theory.html#central-limit-theorem">CLT</a>), <span class="math inline">\(Var(\mathbf{x_i&#39;}\epsilon_i)=\mathbf{B}\)</span></p>
<ul>
<li><span class="math inline">\((n^{-1}\sum_{i=1}^{n}\mathbf{x_i&#39;x_i})^{-1} \to^p (E(\mathbf{x&#39;_ix_i}))^{-1}\)</span></li>
<li><span class="math inline">\(\sqrt{n}(n^{-1}\sum_{i=1}^{n}\mathbf{x_i&#39;}\epsilon_i) \to^d N(0,\mathbf{B})\)</span></li>
</ul>
<p><span class="math display">\[
\sqrt{n}(\hat{\beta}-\beta) = (n^{-1}\sum_{i=1}^{n}\mathbf{x_i&#39;x_i})^{-1}\sqrt{n}(n^{-1}\sum_{i=1}^{n}\mathbf{x_i&#39;x_i}) \to^{d} N(0,\Sigma)
\]</span>
where <span class="math inline">\(\Sigma=(E(\mathbf{x_i&#39;x_i}))^{-1}\mathbf{B}(E(\mathbf{x_i&#39;x_i}))^{-1}\)</span></p>
<ul>
<li>holds under <a href="linear-regression.html#a3a">A3a</a></li>
<li>Do not need <a href="linear-regression.html#a4-homoskedasticity">A4</a> and <a href="linear-regression.html#a6-normal-distribution">A6</a> to apply CLT
<ul>
<li>If <a href="linear-regression.html#a4-homoskedasticity">A4</a> does not hold, then <span class="math inline">\(\mathbf{B}=Var(\mathbf{x_i&#39;}\epsilon_i)=\sigma^2E(x_i&#39;x_i)\)</span> which means <span class="math inline">\(\Sigma=\sigma^2(E(\mathbf{x_i&#39;x_i}))^{-1}\)</span>, use standard errors</li>
</ul></li>
</ul>
<p>Heteroskedasticity can be from</p>
<ul>
<li>Limited dependent variable</li>
<li>Dependent variables with large/skewed ranges</li>
</ul>
<p>Solving Asymptotic Variance</p>
<p><span class="math display">\[
\begin{split}
\Sigma &amp;= (E(\mathbf{x_i&#39;x_i}))^{-1}\mathbf{B}(E(\mathbf{x_i&#39;x_i}))^{-1} \\
&amp;= (E(\mathbf{x_i&#39;x_i}))^{-1}Var(\mathbf{x_i&#39;}\epsilon_i)(E(\mathbf{x_i&#39;x_i}))^{-1} \\
&amp;= (E(\mathbf{x_i&#39;x_i}))^{-1}E[(\mathbf{x_i&#39;}\epsilon_i-0)(\mathbf{x_i&#39;}\epsilon_i-0)](E(\mathbf{x_i&#39;x_i}))^{-1} &amp;&amp; \text{A3a} \\
&amp;= (E(\mathbf{x_i&#39;x_i}))^{-1}E[E(\mathbf{\epsilon_i^2|x_i)x_i&#39;x_i]}(E(\mathbf{x_i&#39;x_i}))^{-1} &amp;&amp; \text{LIE} \\
&amp;= (E(\mathbf{x_i&#39;x_i}))^{-1}\sigma^2E(\mathbf{x_i&#39;x_i})(E(\mathbf{x_i&#39;x_i}))^{-1} &amp;&amp; \text{A4} \\
&amp;= \sigma^2(E(\mathbf{x_i&#39;x_i}))
\end{split}
\]</span></p>
<p>Under <a href="linear-regression.html#a1-linearity">A1</a>, <a href="linear-regression.html#a2-full-rank">A2</a>, <a href="linear-regression.html#a3a">A3a</a>, <a href="linear-regression.html#a4-homoskedasticity">A4</a>, <a href="linear-regression.html#a5-data-generation-random-sampling">A5</a>:</p>
<p><span class="math display">\[
\sqrt{n}(\hat{\beta}-\beta) \to^d N(0,\sigma^2(E(\mathbf{x_i&#39;x_i}))^{-1})
\]</span></p>
<ul>
<li>The Asymptotic variance is approximation for the variance in the scaled random variable for <span class="math inline">\(\sqrt{n}(\hat{\beta}-\beta)\)</span> when n is large.</li>
<li>use <span class="math inline">\(Avar(\sqrt{n}(\hat{\beta}-\beta))/n\)</span> as an approximation for finite sample variance for large n:</li>
</ul>
<p><span class="math display">\[
Avar(\sqrt{n}(\hat{\beta}-\beta)) \approx Var(\sqrt{n}(\hat{\beta}-\beta)) \\
Avar(\sqrt{n}(\hat{\beta}-\beta))/n \approx Var(\sqrt{n}(\hat{\beta}-\beta))/n = Var(\hat{\beta})
\]</span></p>
<ul>
<li>Avar(.) does not behave the same way as Var(.)</li>
</ul>
<p><span class="math display">\[
Avar(\sqrt{n}(\hat{\beta}-\beta))/n \neq Avar(\sqrt{n}(\hat{\beta}-\beta)/\sqrt{n}) \\
\neq Avar(\hat{\beta})
\]</span></p>
<p>In <a href="linear-regression.html#finite-sample-properties">Finite Sample Properties</a>, we calculate standard errors as an estimate for the conditional standard deviation:</p>
<p><span class="math display">\[
SE_{fs}(\hat{\beta}_{j-1})=\sqrt{\hat{Var}}(\hat{\beta}_{j-1}|\mathbf{X}) = \sqrt{s^2[\mathbf{(X&#39;X)}^{-1}]_{jj}}
\]</span></p>
<p>In <a href="linear-regression.html#large-sample-properties">Large Sample Properties</a>, we calculate standard errors as an estimate for the square root of asymptotic variance</p>
<p><span class="math display">\[
SE_{ls}(\hat{\beta}_{j-1})=\sqrt{\hat{Avar}(\sqrt{n}\hat{\beta}_{j-1})/n} = \sqrt{s^2[\mathbf{(X&#39;X)}^{-1}]_{jj}}
\]</span></p>
<p>Hence, the standard error estimator is the same for finite sample and large sample.
* Same estimator, but conceptually estimating two different things.
* Valid under weaker assumptions: the assumptions needed to produce a consistent estimator for the finite sample conditional variance (A1-A5) are stronger than those needed to produce a consistent estimator for the asymptotic variance (A1,A2,A3a,A4,A5)</p>
</div>
<div id="application" class="section level4">
<h4><span class="header-section-number">5.1.1.5</span> Application</h4>
</div>
</div>
<div id="feasible-generalized-least-squares" class="section level3">
<h3><span class="header-section-number">5.1.2</span> Feasible Generalized Least Squares</h3>
<p>Motivation for a more efficient estimator</p>
<ul>
<li><a href="linear-regression.html#gauss-markov-theorem">Gauss-Markov Theorem</a> holds under A1-A4</li>
<li>A4: <span class="math inline">\(Var(\epsilon| \mathbf{X} )=\sigma^2I_n\)</span>
<ul>
<li>Heteroskedasticity: <span class="math inline">\(Var(\epsilon_i|\mathbf{X}) \neq \sigma^2I_n\)</span></li>
<li>Serial Correlation: <span class="math inline">\(Cov(\epsilon_i,\epsilon_j|\mathbf{X}) \neq 0\)</span></li>
</ul></li>
<li>Without A4, how can we know which unbiased estimator is the most efficient?</li>
</ul>
<p>Original (unweighted) model:</p>
<p><span class="math display">\[
\mathbf{y=X\beta+ \epsilon}
\]</span>
Suppose A1-A3 hold, but A4 does not hold,
<span class="math display">\[
\mathbf{Var(\epsilon|X)=\Omega \neq \sigma^2 I_n}
\]</span></p>
<p>We will try to use OLS to estimate the transformed (weighted) model</p>
<p><span class="math display">\[
\mathbf{wy=wX\beta + w\epsilon}
\]</span>
We need to choose <span class="math inline">\(\mathbf{w}\)</span> so that</p>
<p><span class="math display">\[
\mathbf{w&#39;w = \Omega^{-1}}
\]</span>
then <span class="math inline">\(\mathbf{w}\)</span> (full-rank matrix) is the <strong>Cholesky decomposition</strong> of <span class="math inline">\(\mathbf{\Omega^{-1}}\)</span> (full-rank matrix)</p>
<p>In other words, <span class="math inline">\(\mathbf{w}\)</span> is the squared root of <span class="math inline">\(\Omega\)</span> (squared root version in matrix)</p>
<p><span class="math display">\[
\Omega = var(\epsilon | X) \\
\Omega^{-1} = var(\epsilon | X)^{-1}
\]</span></p>
<p>Then, the transformed equation (IGLS) will have the following properties.</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\mathbf{\hat{\beta}_{IGLS}} &amp;= \mathbf{(X&#39;w&#39;wX)^{-1}X&#39;w&#39;wy} \\
&amp; = \mathbf{(X&#39;\Omega^{-1}X)^{-1}X&#39;\Omega^{-1}y} \\
&amp; = \mathbf{\beta + X&#39;\Omega^{-1}X&#39;\Omega^{-1}\epsilon}
\end{split}
\end{equation}\]</span></p>
<p>Since A1-A3 hold for the unweighted model
<span class="math display">\[\begin{equation}
\begin{split}
\mathbf{E(\hat{\beta}_{IGLS}|X)} &amp; = E(\mathbf{\beta + (X&#39;\Omega^{-1}X&#39;\Omega^{-1}\epsilon)}|X)\\
&amp; = \mathbf{\beta + E(X&#39;\Omega^{-1}X&#39;\Omega^{-1}\epsilon)|X)} \\
&amp; = \mathbf{\beta + X&#39;\Omega^{-1}X&#39;\Omega^{-1}E(\epsilon|X)}  &amp;&amp; \text{since A3: $E(\epsilon|X)=0$} \\
&amp; = \mathbf{\beta}
\end{split}
\end{equation}\]</span></p>
<p><span class="math inline">\(\rightarrow\)</span> IGLS estimator is unbiased</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\mathbf{Var(w\epsilon|X)} &amp;= \mathbf{wVar(\epsilon|X)w&#39;} \\
&amp; = \mathbf{w\Omega w&#39;} \\
&amp; = \mathbf{w(w&#39;w)^{-1}w&#39;} &amp;&amp; \text{since w is a full-rank matrix}\\
&amp; = \mathbf{ww^{-1}(w&#39;)^{-1}w&#39;} \\
&amp; = \mathbf{I_n}
\end{split}
\end{equation}\]</span></p>
<p><span class="math inline">\(\rightarrow\)</span> A4 holds for the transformed (weighted) equation</p>
<p>Then, the variance for the estimator is</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
Var(\hat{\beta}_{IGLS}|\mathbf{X}) &amp; = \mathbf{Var(\beta + (X&#39;\Omega ^{-1}X)^{-1}X&#39;\Omega^{-1}\epsilon|X)} \\
&amp;= \mathbf{Var((X&#39;\Omega ^{-1}X)^{-1}X&#39;\Omega^{-1}\epsilon|X)} \\
&amp;= \mathbf{(X&#39;\Omega ^{-1}X)^{-1}X&#39;\Omega^{-1} Var(\epsilon|X)   \Omega^{-1}X(X&#39;\Omega ^{-1}X)^{-1}} &amp;&amp; \text{because A4 holds}\\
&amp;= \mathbf{(X&#39;\Omega ^{-1}X)^{-1}X&#39;\Omega^{-1} \Omega \Omega^{-1} \Omega^{-1}X(X&#39;\Omega ^{-1}X)^{-1}} \\
&amp;= \mathbf{(X&#39;\Omega ^{-1}X)^{-1}}
\end{split}
\end{equation}\]</span></p>
<p>Let <span class="math inline">\(A = \mathbf{(X&#39;X)^{-1}X&#39;-(X&#39;\Omega ^{-1} X)X&#39; \Omega^{-1}}\)</span> then
<span class="math display">\[
Var(\hat{\beta}_{OLS}|X)- Var(\hat{\beta}_{IGLS}|X) = A\Omega A&#39;
\]</span>
And <span class="math inline">\(\Omega\)</span> is Positive Semi Definite, then <span class="math inline">\(A\Omega A&#39;\)</span> also PSD, then IGLS is more efficient</p>
<p>The name <strong>Infeasible</strong> comes from the fact that it is impossible to compute this estimator.</p>
<p><span class="math display">\[\begin{equation}
\mathbf{w} = 
\left(
\begin{array}{c}
w_{11} &amp; 0 &amp; 0 &amp; ... &amp; 0 \\
w_{21} &amp; w_{22} &amp; 0 &amp; ... &amp; 0 \\
w_{31} &amp; w_{32} &amp; w_{33} &amp; ... &amp; ... \\
w_{n1} &amp; w_{n2} &amp; w_{n3} &amp; ... &amp; w_{nn} \\
\end{array}
\right)
\end{equation}\]</span></p>
<p>With <span class="math inline">\(n(n+1)/2\)</span> number of elements and n observations <span class="math inline">\(\rightarrow\)</span> infeasible to estimate. (number of equation &gt; data)</p>
<p>Hence, we need to make assumption on <span class="math inline">\(\Omega\)</span> to make it feasible to estimate <span class="math inline">\(\mathbf{w}\)</span>:</p>
<ol style="list-style-type: decimal">
<li><a href="heteroskedasticity-2.html#heteroskedasticity-2">Heteroskedasticity</a> : multiplicative exponential model</li>
<li><a href="linear-regression.html#ar1">AR(1)</a></li>
<li><a href="linear-regression.html#cluster">Cluster</a></li>
</ol>
<div id="heteroskedasticity" class="section level4">
<h4><span class="header-section-number">5.1.2.1</span> Heteroskedasticity</h4>
<p><span class="math display" id="eq:h-var-error-term">\[\begin{equation}
\begin{split}
Var(\epsilon_i |x_i) &amp; = E(\epsilon^2|x_i) \neq \sigma^2 \\
&amp; = h(x_i) = \sigma_i^2 \text{(variance of the error term is a function of x)}
\end{split}
\tag{5.7}
\end{equation}\]</span></p>
<p>For our model,</p>
<p><span class="math display">\[
y_i = x_i\beta + \epsilon_i \\
(1/\sigma_i)y_i = (1/\sigma_i)x_i\beta + (1/\sigma_i)\epsilon_i
\]</span></p>
<p>then, from <a href="linear-regression.html#eq:h-var-error-term">(5.7)</a></p>
<p><span class="math display">\[
\begin{equation}
\begin{split}
Var((1/\sigma_i)\epsilon_i|X) &amp;= (1/\sigma_i^2) Var(\epsilon_i|X) \\
&amp;= (1/\sigma_i^2)\sigma_i^2 \\
&amp;= 1
\end{split}
\end{equation}
\]</span></p>
<p>then the weight matrix <span class="math inline">\(\mathbf{w}\)</span> in the matrix equation</p>
<p><span class="math display">\[
\mathbf{wy=wX\beta + w\epsilon}
\]</span></p>
<p><span class="math display">\[
\mathbf{w}= 
\left(
\begin{array}{c}
1/\sigma_1 &amp; 0 &amp; 0 &amp; ... &amp; 0 \\
0 &amp; 1/\sigma_2 &amp; 0 &amp; ... &amp; 0 \\
0 &amp; 0 &amp; 1/\sigma_3 &amp; ... &amp; . \\
. &amp; . &amp; . &amp; . &amp; 0 \\
0 &amp; 0 &amp; . &amp; . &amp; 1/\sigma_n
\end{array}
\right)
\]</span></p>
<p><strong>Infeasible Weighted Least Squares</strong></p>
<ol style="list-style-type: decimal">
<li>Assume we know <span class="math inline">\(\sigma_i^2\)</span> (Infeasible)</li>
<li>The IWLS estimator is obtained as the least squared estimated for the following weighted equation</li>
</ol>
<p><span class="math display">\[
(1/\sigma_i)y_i = (1/\sigma_i)\mathbf{x}_i\beta + (1/\sigma_i)\epsilon_i 
\]</span></p>
<ul>
<li>Usual standard errors for the weighted equation are valid if <span class="math inline">\(Var(\epsilon | \mathbf{X}) = \sigma_i^2\)</span></li>
<li>If <span class="math inline">\(Var(\epsilon | \mathbf{X}) \neq \sigma_i^2\)</span> then heteroskedastic robust standard errors are valid.</li>
</ul>
<p><strong>Problem</strong>: We do not know <span class="math inline">\(\sigma_i^2=Var(\epsilon_i|\mathbf{x_i})=E(\epsilon_i^2|\mathbf{x}_i)\)</span></p>
<ul>
<li>One observation <span class="math inline">\(\epsilon_i\)</span> cannot estimate a sample variance estimate <span class="math inline">\(\sigma_i^2\)</span>
<ul>
<li>Model <span class="math inline">\(\epsilon_i^2\)</span> as reasonable (strictly positive) function of <span class="math inline">\(x_i\)</span> and independent error <span class="math inline">\(v_i\)</span> (strictly positive)</li>
</ul></li>
</ul>
<p><span class="math display">\[
\epsilon_i^2=v_i exp(\mathbf{x_i\gamma})
\]</span>
Then we can apply a log transformation to recover a linear in parameters model,</p>
<p><span class="math display">\[
ln(\epsilon_i^2) = \mathbf{x_i\gamma} + ln(v_i)
\]</span>
where <span class="math inline">\(ln(v_i)\)</span> is independent <span class="math inline">\(\mathbf{x}_i\)</span></p>
<p>We do not observe <span class="math inline">\(\epsilon_i\)</span>
* OLS residual (<span class="math inline">\(e_i\)</span>) as an approximate</p>
</div>
<div id="serial-correlation" class="section level4">
<h4><span class="header-section-number">5.1.2.2</span> Serial Correlation</h4>
<p><span class="math display">\[
Cov(\epsilon_i, \epsilon_j | \mathbf{X}) \neq 0
\]</span></p>
<p>Under covariance stationary,</p>
<p><span class="math display">\[
Cov(\epsilon_i,\epsilon_j|\mathbf{X}) = Cov(\epsilon_i, \epsilon_{i+h}|\mathbf{x_i,x_{i+h}})=\gamma_h
\]</span></p>
<p>And the variance covariance matrix is</p>
<p><span class="math display">\[
Var(\epsilon|\mathbf{X}) = \Omega = 
\left(
\begin{array}{c}
\sigma^2 &amp; \gamma_1 &amp; \gamma_2 &amp; ... &amp; \gamma_{n-1} \\
\gamma_1 &amp; \sigma^2 &amp; \gamma_1 &amp; ... &amp; \gamma_{n-2} \\
\gamma_2 &amp; \gamma_1 &amp; \sigma^2 &amp; ... &amp; ... \\
. &amp; . &amp; . &amp; . &amp; \gamma_1 \\
\gamma_{n-1} &amp; \gamma_{n-2} &amp; . &amp; \gamma_1 &amp; \sigma^2
\end{array}
\right)
\]</span></p>
<p>There n parameters to estimate - need some sort fo structure to reduce number of parameters to estimate.</p>
<ul>
<li><a href="linear-regression.html#ar1">Time Series</a>
<ul>
<li>Effect of inflation and deficit on Treasury BIll interest rates</li>
</ul></li>
<li><a href="linear-regression.html#cluster">Cross-sectional</a>
<ul>
<li>Clustering</li>
</ul></li>
</ul>
<div id="ar1" class="section level5">
<h5><span class="header-section-number">5.1.2.2.1</span> AR(1)</h5>
<p><span class="math display">\[
y_t= \beta_0 + x_t\beta_1 + \epsilon_t \\
\epsilon_t = \rho \epsilon_{t-1} + u_t
\]</span></p>
<p>and the variance covariance matrix is
<span class="math display">\[
Var(\epsilon | \mathbf{X})= \frac{\sigma^2_u}{1-\rho}
\left(
\begin{array}{c}
1 &amp; \rho &amp; \rho^2 &amp; ... &amp; \rho^{n-1} \\
\rho &amp; 1 &amp; \rho &amp; ... &amp; \rho^{n-2} \\
\rho^2 &amp; \rho &amp; 1 &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; \rho \\
\rho^{n-1} &amp; \rho^{n-2} &amp; . &amp; \rho &amp; 1 \\
\end{array}
\right)
\]</span></p>
<p>Hence, there is only 1 parameter to estimate: <span class="math inline">\(\rho\)</span></p>
<ul>
<li>Under A1, A2, A3a, A5a, OLS is consistent and asymptotically normal</li>
<li>Use <a href="time-series.html#newey-west-standard-errors">Newey West Standard Errors</a> for valid inference.</li>
<li>Apply <a href="linear-regression.html#infeasible-cochrane-orcutt">Infeasible Cochrane Orcutt</a> (as if we knew <span class="math inline">\(\rho\)</span>)</li>
<li>Because</li>
</ul>
<p><span class="math display">\[
u_t = \epsilon_t - \rho \epsilon_{t-1}
\]</span>
satisfies A3, A4, A5 we’d like to to transform the above equation to one that has <span class="math inline">\(u_t\)</span> as the error.</p>
<p><span class="math display">\[
\begin{align}
y_t - \rho y_{t-1} &amp;= (\beta_0 + x\beta_1 + \epsilon_t) - \rho (\beta_0 + x_{t-1}\beta_1 + \epsilon_{t-1}) \\
&amp; = (1-\rho)\beta_0 + (x_t - \rho x_{t-1})\beta_1 + u_t
\end{align}
\]</span></p>
<div id="infeasible-cochrane-orcutt" class="section level6">
<h6><span class="header-section-number">5.1.2.2.1.1</span> Infeasible Cochrane Orcutt</h6>
<ol style="list-style-type: decimal">
<li>Assume that we know <span class="math inline">\(\rho\)</span> (Infeasible)</li>
<li>The ICO estimator is obtained as the least squared estimated for the following weighted first difference equation</li>
</ol>
<p><span class="math display">\[
y_t -\rho y_{t-1} = (1-\rho)\beta_0 + (x_t - \rho x_{t-1})\beta_1 + u_t
\]</span></p>
<ul>
<li>Usual standard errors for the weighted first difference equation are valid if the errors truly follow an AR(1) process</li>
<li>If the serial correlation is generated from a more complex dynamic process then <a href="time-series.html#newey-west-standard-errors">Newey-West HAC standard errors</a> are valid</li>
</ul>
<p><strong>Problem</strong>
We do not know <span class="math inline">\(\rho\)</span></p>
<ul>
<li><span class="math inline">\(\rho\)</span> is the correlation between <span class="math inline">\(\epsilon_t\)</span> and <span class="math inline">\(\epsilon_{t-1}\)</span>: estimate using OLS residuals (<span class="math inline">\(e_i\)</span>) as proxy</li>
</ul>
<p><span class="math display">\[
\hat{\rho} = \frac{\sum_{t=1}^{T}e_te_{t-1}}{\sum_{t=1}^{T}e_t^2}
\]</span></p>
<p>which can be obtained from the OLS regression of</p>
<p><span class="math display">\[
e_t = \rho e_{t-1} + u_t
\]</span>
where we suppress the intercept.</p>
<ul>
<li>We are losing an observation
<ul>
<li>By taking the first difference we are dropping the first observation</li>
</ul></li>
</ul>
<p><span class="math display">\[
y_1 = \beta_0 + x_1 \beta_1 + \epsilon_1
\]</span>
+ <a href="linear-regression.html#feasiable-prais-winsten">Feasiable Prais Winsten</a> Transformation applies the <a href="linear-regression.html#infeasible-cochrane-orcutt">Infeasible Cochrane Orcutt</a> but includes a weighted version of the first observation</p>
<p><span class="math display">\[
(\sqrt{1-\rho^2})y_1 = \beta_0 + (\sqrt{1-\rho^2})x_1 \beta_1 + (\sqrt{1-\rho^2}) \epsilon_1
\]</span></p>
</div>
</div>
<div id="cluster" class="section level5">
<h5><span class="header-section-number">5.1.2.2.2</span> Cluster</h5>
<p><span class="math display">\[
y_{gi} = \mathbf{x}_{gi}\beta + \epsilon_{gi}
\]</span></p>
<p><span class="math display">\[
Cov(\epsilon_{gi}, \epsilon_{hj})
\begin{cases}
= 0 &amp; \text{for $g \neq h$ and any pair (i,j)} \\
\neq 0 &amp; \text{for any (i,j) pair}\\
\end{cases}
\]</span></p>
<p><strong>Intra-group Correlation</strong><br />
Each individual in a single group may be correlated but independent across groups.</p>
<ul>
<li><a href="linear-regression.html#a4-homoskedasticity">A4</a> is violated. usual standard errors for OLS are valid.</li>
<li>Use <strong>cluster robust standard errors</strong> for OLS.</li>
</ul>
<p>Suppose there are 3 groups with different n</p>
<p><span class="math display">\[
Var(\epsilon| \mathbf{X})= \Omega =
\left(
\begin{array}{c}
\sigma^2 &amp; \delta_{12}^1 &amp; \delta_{13}^1 &amp; 0 &amp; 0 &amp; 0 \\
\delta_{12}^1 &amp; \sigma^2 &amp; \delta_{23}^1 &amp; 0 &amp; 0 &amp; 0 \\
\delta_{13}^1 &amp; \delta_{23}^1 &amp; \sigma^2 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \sigma^2 &amp; \delta_{12}^2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \delta_{12}^2 &amp; \sigma^2 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \sigma^2
\end{array}
\right)
\]</span></p>
<p>where <span class="math inline">\(Cov(\epsilon_{gi}, \epsilon_{gj}) = \delta_{ij}^g\)</span> and <span class="math inline">\(Cov(\epsilon_{gi}, \epsilon_{hj}) = 0\)</span> for any i and j</p>
<p><strong>Infeasible Generalized Least Squares (Cluster)</strong></p>
<ol style="list-style-type: decimal">
<li>Assume that <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\delta_{ij}^g\)</span> are known, plug into <span class="math inline">\(\Omega\)</span> and solve for the inverse <span class="math inline">\(\Omega^{-1}\)</span> (infeasible)</li>
<li>The Infeasible Generalized Least Squares Estimator is</li>
</ol>
<p><span class="math display">\[
\hat{\beta}_{IGLS} = \mathbf{(X&#39;\Omega^{-1}X)^{-1}X&#39;\Omega^{-1}y}
\]</span></p>
<p><strong>Problem</strong>
* We do not know <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\delta_{ij}^g\)</span>
+ Can make assumptions about data generating process that is causing the clustering behavior.
- Will give structure to <span class="math inline">\(Cov(\epsilon_{gi},\epsilon_{gj})= \delta_{ij}^g\)</span> which makes it feasible to estimate
- if the assumptions are wrong then we should use cluster robust standard errors.</p>
<p><strong>Solution</strong>
Assume <strong>group level random effects</strong> specification in the error</p>
<p><span class="math display">\[
y_{gi} = \mathbf{g}_i \beta + c_g + u_{gi} \\
Var(c_g|\mathbf{x}_i) = \sigma^2_c \\
Var(u_{gi}|\mathbf{x}_i) = \sigma^2_u
\]</span></p>
<p>where <span class="math inline">\(c_g\)</span> and <span class="math inline">\(u_{gi}\)</span> are independent of each other, and mean independent of <span class="math inline">\(\mathbf{x}_i\)</span></p>
<ul>
<li><span class="math inline">\(c_g\)</span> captures the common group shocks (independent across groups)</li>
<li><span class="math inline">\(u_{gi}\)</span> captures the individual shocks (independent across individuals and groups)</li>
</ul>
<p>Then the error variance is</p>
<p><span class="math display">\[
Var(\epsilon| \mathbf{X})= \Omega =
\left(
\begin{array}{c}
\sigma^2_c + \sigma^2_u &amp; \sigma^2_c &amp; \sigma^2_c &amp; 0 &amp; 0 &amp; 0 \\
\sigma^2_c &amp; \sigma^2 + \sigma^2_u &amp; \sigma^2_c &amp; 0 &amp; 0 &amp; 0 \\
\sigma^2_c &amp; \sigma^2_c  &amp; \sigma^2+ \sigma^2_u &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \sigma^2+ \sigma^2_u &amp; \sigma^2_c &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; \sigma^2_c &amp; \sigma^2+ \sigma^2_u &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \sigma^2+ \sigma^2_u
\end{array}
\right)
\]</span></p>
<p>Use <a href="linear-regression.html#feasible-group-level-random-effects">Feasible group level Random Effects</a></p>
</div>
</div>
</div>
<div id="weighted-least-squares" class="section level3">
<h3><span class="header-section-number">5.1.3</span> Weighted Least Squares</h3>
<ol style="list-style-type: decimal">
<li>Estimate the following equation using OLS</li>
</ol>
<p><span class="math display">\[
y_i = \mathbf{x}_i \beta + \epsilon_i
\]</span>
and obtain the residuals <span class="math inline">\(e_i=y_i -\mathbf{x}_i \hat{\beta}\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Transform the residual and estimate the following by OLS,</li>
</ol>
<p><span class="math display">\[
ln(e_i^2)= \mathbf{x}_i\gamma + ln(v_i)
\]</span></p>
<p>and obtain the predicted values <span class="math inline">\(g_i=\mathbf{x}_i \hat{\gamma}\)</span></p>
<ol start="3" style="list-style-type: decimal">
<li>The weights will be the untransformed predicted outcome,</li>
</ol>
<p><span class="math display">\[
\hat{\sigma}_i =\sqrt{exp(g_i)}
\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>The FWLS (Feasible WLS) estimator is obtained as the least squared estimated for the following weighted equation</li>
</ol>
<p><span class="math display">\[
(1/\hat{\sigma}_i)y_i = (1/\hat{\sigma}_i) \mathbf{x}_i\beta + (1/\hat{\sigma}_i)\epsilon_i
\]</span></p>
<p><strong>Properties of the FWLS</strong></p>
<ul>
<li>The infeasible WLS estimator is unbiased under A1-A3 for the unweighted equation.</li>
<li>The FWLS estimator is NOT an unbiased estimator.</li>
<li>The FWLS estimator is consistent under <a href="linear-regression.html#a1-linearity">A1</a>, <a href="linear-regression.html#a2-full-rank">A2</a>, (for the unweighted equation), <a href="linear-regression.html#a5-data-generation-random-sampling">A5</a>, and <span class="math inline">\(E(\mathbf{x}_i&#39;\epsilon_i/\sigma^2_i)=0\)</span>
<ul>
<li><a href="linear-regression.html#a3a">A3a</a> is not sufficient for the above equation</li>
<li><a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3</a> is sufficient for the above equation.</li>
</ul></li>
<li>The FWLS estimator is asymptotically more efficient than OLS if the errors have multiplicative exponential heteroskedasticity.
<ul>
<li>If the errors are truly multiplicative exponential heteroskedasticity, then usual standard errors are valid</li>
<li>If we believe that there may be some mis-specification with the <strong>multiplicative exponential model</strong>, then we should report heteroskedastic robust standard errors.</li>
</ul></li>
</ul>
</div>
<div id="feasiable-prais-winsten" class="section level3">
<h3><span class="header-section-number">5.1.4</span> Feasiable Prais Winsten</h3>
<p>Weighting Matrix</p>
<p><span class="math display">\[
\mathbf{w} = 
\left(
\begin{array}{c}
\sqrt{1- \hat{\rho}^2} &amp; 0 &amp; 0 &amp;... &amp; 0 \\
-\hat{\rho} &amp; 1 &amp; 0 &amp; ... &amp; 0 \\
0 &amp;  -\hat{\rho} &amp; 1 &amp; &amp; . \\
. &amp; . &amp; . &amp; . &amp; 0 \\
0 &amp; . &amp; 0 &amp; -\hat{\rho} &amp; 1
\end{array}
\right)
\]</span></p>
<ol style="list-style-type: decimal">
<li>Estimate the following equation using OLS</li>
</ol>
<p><span class="math display">\[
y_t = \mathbf{x}_t \beta + \epsilon_t
\]</span>
and obtain the residuals <span class="math inline">\(e_t = y_t - \mathbf{x}_t \hat{\beta}\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Estimate the correlation coefficient for the <a href="linear-regression.html#ar1">AR(1)</a> process by estimating the following by OLS (without no intercept)</li>
</ol>
<p><span class="math display">\[
e_t = \rho e_{t-1} + u_t
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Transform the outcome and independent variables <span class="math inline">\(\mathbf{wy}\)</span> and <span class="math inline">\(\mathbf{wX}\)</span> respectively (weight matrix as stated).</li>
<li>The <a href="linear-regression.html#feasiable-prais-winsten">FPW</a> estimator is obtained as the least squared estimated for the following weighted equation</li>
</ol>
<p><span class="math display">\[
\mathbf{wy = wX\beta + w\epsilon}
\]</span></p>
<p><strong>Properties of <a href="linear-regression.html#feasiable-prais-winsten">Feasiable Prais Winsten</a> Estimator</strong></p>
<ul>
<li>The Infeasible PW estimator is under A1-A3 for the unweighted equation</li>
<li>The <a href="linear-regression.html#feasiable-prais-winsten">FPW</a> estimator is biased</li>
<li>The <a href="linear-regression.html#feasiable-prais-winsten">FPW</a> is consistent under <a href="linear-regression.html#a1-linearity">A1</a> <a href="linear-regression.html#a2-full-rank">A2</a> <a href="linear-regression.html#a5-data-generation-random-sampling">A5</a> and</li>
</ul>
<p><span class="math display">\[
E((\mathbf{x_t - \rho x_{t-1}})&#39;)(\epsilon_t - \rho \epsilon_{t-1})=0
\]</span></p>
<pre><code>+ [A3a] is not sufficient for the above equaiton 
+ [A3][A3 Exogeneity of Independent Variables] is suffiicent for the above equaiton</code></pre>
<ul>
<li>The <a href="linear-regression.html#feasiable-prais-winsten">FPW</a> estimator is asymptotically more efficient than OLS if the errors are truly generated as AR(1) process
<ul>
<li>If the errors are truly generated as AR(1) process then usual standard errors are valid</li>
<li>If we are concerned that there may be a more complex dependence structure of heteroskedasticity, then we use <a href="time-series.html#newey-west-standard-errors">Newey West Standard Errors</a></li>
</ul></li>
</ul>
</div>
<div id="feasible-group-level-random-effects" class="section level3">
<h3><span class="header-section-number">5.1.5</span> Feasible group level Random Effects</h3>
<ol style="list-style-type: decimal">
<li>Estimate the following equation using OLS</li>
</ol>
<p><span class="math display">\[
y_{gi} = \mathbf{x}_{gi}\beta + \epsilon_{gi}
\]</span>
and obtain the residuals <span class="math inline">\(e_{gi} = y_{gi} - \mathbf{x}_{gi}\hat{\beta}\)</span>
2. Estimate the variance using the usual $s^2 estimator</p>
<p><span class="math display">\[
s^2 = \frac{1}{n-k}\sum_{i=1}^{n}e_i^2
\]</span>
as an estimator for <span class="math inline">\(\sigma^2_c + \sigma^2_u\)</span> and estimate the within group correlation,</p>
<p><span class="math display">\[
\hat{\sigma}^2_c = \frac{1}{G} \sum_{g=1}^{G} (\frac{1}{\sum_{i=1}^{n_g-1}i}\sum_{i\neq j}\sum_{j}^{n_g}e_{gi}e_{gj})
\]</span></p>
<p>and plug in the estimates to obtain <span class="math inline">\(\hat{\Omega}\)</span></p>
<ol start="3" style="list-style-type: decimal">
<li>The feasible group level RE estimator is obtained as</li>
</ol>
<p><span class="math display">\[
\hat{\beta}= \mathbf{(X&#39;\hat{\Omega}^{-1}X)^{-1}X&#39;\hat{\Omega}^{-1}y}
\]</span></p>
<p><strong>Properties of the <a href="linear-regression.html#feasible-group-level-random-effects">Feasible group level Random Effects</a> Estimator</strong></p>
<ul>
<li>The infeasible group RE estimator is a linear estimator and is unbiased under A1-A3 for the unweighted equation
<ul>
<li>A3 requires <span class="math inline">\(E(\epsilon_{gi}|\mathbf{x}_i) = E(c_{g}|\mathbf{x}_i)+ (u_{gi}|\mathbf{x}_i)=0\)</span> so we generally assume <span class="math inline">\(E(c_{g}|\mathbf{x}_i)+ (u_{gi}|\mathbf{x}_i)=0\)</span>. The assumption <span class="math inline">\(E(c_{g}|\mathbf{x}_i)=0\)</span> is generally called <strong>random effects assumption</strong></li>
</ul></li>
<li>The <a href="linear-regression.html#feasible-group-level-random-effects">Feasible group level Random Effects</a> is biased</li>
<li>The <a href="linear-regression.html#feasible-group-level-random-effects">Feasible group level Random Effects</a> is consistent under A1-A3a, and A5a for the unweighted equation.
<ul>
<li><a href="linear-regression.html#a3a">A3a</a> requires <span class="math inline">\(E(\mathbf{x}_i&#39;\epsilon_{gi}) = E(\mathbf{x}_i&#39;c_{g})+ (\mathbf{x}_i&#39;u_{gi})=0\)</span></li>
</ul></li>
<li>The <a href="linear-regression.html#feasible-group-level-random-effects">Feasible group level Random Effects</a> estimator is asymptotically more efficient than OLS if the errors follow the random effects specification
<ul>
<li>If the errors do follow the random effects specification than the usual standard errors are consistent</li>
<li>If there might be a more complex dependence structure or heteroskedasticity, then we need cluster robust standard errors.</li>
</ul></li>
</ul>
</div>
<div id="generalized-least-squares" class="section level3">
<h3><span class="header-section-number">5.1.6</span> Generalized Least Squares</h3>
</div>
<div id="maximum-likelihood" class="section level3">
<h3><span class="header-section-number">5.1.7</span> Maximum Likelihood</h3>
<p>Premise: find values of the parameters that maximize the probability of observing the data
In other words, we try to maximize the value of theta in the likelihood function
<span class="math display">\[
L(\theta)=\prod_{i=1}^{n}f(y_i|\theta)
\]</span>
<span class="math inline">\(f(y|\theta)\)</span> is the probability density of observing a single value of Y given some value of <span class="math inline">\(\theta\)</span>
<span class="math inline">\(f(y|\theta)\)</span> can be specify as various type of distributions. You can review back section <a href="probability-theory.html#distributions">Distributions</a>. For example
If y is a dichotomous variable, then
<span class="math display">\[
L(\theta)=\prod_{i=1}^{n}\theta^{y_i}(1-\theta)^{1-y_i}
\]</span></p>
<p><span class="math inline">\(\hat{\theta}\)</span> is the Maximum Likelihood estimate if <span class="math inline">\(L(\hat{\theta}) &gt; L(\theta_0)\)</span> for all values of <span class="math inline">\(\theta_0\)</span> in the parameter space.</p>
<div id="motivation-for-mle" class="section level4">
<h4><span class="header-section-number">5.1.7.1</span> Motivation for MLE</h4>
<p>Suppose we know the conditional distribution of y given x:</p>
<p><span class="math display">\[
f_{Y|X}(y,x;\theta)
\]</span>
where <span class="math inline">\(\theta\)</span> is the unknown parameter of distribution. Sometimes we are only concerned with the unconditional distribution <span class="math inline">\(f_{Y}(y;\theta)\)</span></p>
<p>Then given a sample of iid data, we can calculate the joint distribution of the entire sample,</p>
<p><span class="math display">\[
f_{Y_1,...,Y_n|X_1,...,X_n(y_1,...y_n,x_1,...,x_n;\theta)}= \prod_{i=1}^{n}f_{Y|X}(y_i,x_i;\theta)
\]</span>
The joint distribution evaluated at the sample is the likelihood (probability) that we observed this particular sample (depends on <span class="math inline">\(\theta\)</span>)</p>
<p>Idea for MLE: Given a sample, we choose our estimates of the parameters that gives the highest likelihood (probability) of observing our particular sample</p>
<p><span class="math display">\[
max_{\theta} \prod_{i=1}^{n}f_{Y|X}(y_i,x_i; \theta)
\]</span></p>
<p>Equivalently,</p>
<p><span class="math display">\[
max_{\theta} \prod_{i=1}^{n} ln(f_{Y|X}(y_i,x_i; \theta))
\]</span>
Solving for the Maximum Likelihood Estimator</p>
<ol style="list-style-type: decimal">
<li>Solve First Order Condition</li>
</ol>
<p><span class="math display">\[
\frac{\partial}{\partial \theta}\sum_{i=1}^{n} ln(f_{Y|X}(y_i,x_i;\hat{\theta}_{MLE})) = 0
\]</span>
where <span class="math inline">\(\hat{\theta}_{MLE}\)</span> is defined.</p>
<ol start="2" style="list-style-type: decimal">
<li>Evaluate Second Order Condition</li>
</ol>
<p><span class="math display">\[
\frac{\partial^2}{\partial \theta^2} \sum_{i=1}^{n} ln(f_{Y|X}(y_i,x_i;\hat{\theta}_{MLE})) &lt; 0
\]</span></p>
<p>where the above condition ensures we can solve for a maximum</p>
<p>Examples:<br />
Unconditional Poisson Distribution: Number of products ordered on Amazon within an hour, number of website visits a day for a political campaign.</p>
<p>Exponential Distribution: Length of time until an earthquake occurs, length of time a car battery lasts.</p>
<p><span class="math display">\[
f_{Y|X}(y,x;\theta) = exp(-y/x\theta)/x\theta \\
f_{Y_1,..Y_n|X_1,...,X_n(y_1,...,y_n,x_1,...,x_n;\theta)} = \prod_{i=1}^{n}exp(-y_i/x_i \theta)/x_i \theta
\]</span></p>
</div>
<div id="assumption" class="section level4">
<h4><span class="header-section-number">5.1.7.2</span> Assumption</h4>
<ul>
<li><strong>High Level Regulatory Assumptions</strong> is the sufficient condition used to show large sample properties
<ul>
<li>Hence, for each MLE, we will need to either assume or verify if the regulatory assumptiosn holds.</li>
</ul></li>
<li>observations are independent and have the same density function.</li>
<li>Under multivariate normal assumption, ML yields consistent estimates of the means and the covariance matrix for multivariate distribution with finite fourth moments <span class="citation">(Little and Smith <a href="#ref-Little_1987" role="doc-biblioref">1987</a>)</span></li>
</ul>
<p>To find the MLE, we usually differentiate the <strong>log-likelihood</strong> function and set it equal to 0.</p>
<p><span class="math display">\[
\frac{d}{d\theta}l(\theta) = 0 
\]</span>
This is the <strong>score</strong> equation</p>
<p>Our confidence in the MLE is quantified by the “pointedness” of the log-likelihood
<span class="math display">\[
I_O(\theta)= \frac{d^2}{d\theta^2}l(\theta) = 0 
\]</span>
called the <strong>observed information</strong></p>
<p>while
<span class="math display">\[
I(\theta)=E[I_O(\theta;Y)]
\]</span>
is the expected information. (also known as Fisher Information). which we base our variance of the estimator.</p>
<p><span class="math display">\[
V(\hat{\Theta}) \approx I(\theta)^{-1}
\]</span></p>
<p><strong>Consistency</strong> of MLE<br />
Suppose that <span class="math inline">\(y_i\)</span> and <span class="math inline">\(x_i\)</span> are iid drawn from the true conditional pdf <span class="math inline">\(f_{Y|X}(y_i,x_i;\theta_0)\)</span>. If the following regulatory assumptions hold,</p>
<p>R1: If <span class="math inline">\(\theta \neq \theta_0\)</span> then <span class="math inline">\(f_{Y|X}(y_i,x_i;\theta) \neq f_{Y|X}(y_i,x_i;\theta_0)\)</span><br />
R2: The set <span class="math inline">\(\Theta\)</span> that contains the true parameters <span class="math inline">\(\theta_0\)</span> is compact<br />
R3: The log-likelihood <span class="math inline">\(ln(f_{Y|X}(y_i,x_i;\theta_0))\)</span> is continuous at each <span class="math inline">\(\theta\)</span> with probability 1<br />
R4: <span class="math inline">\(E(sup_{\theta \in \Theta}|ln(f_{Y|X}(y_i,x_i;\theta_0))|)\)</span></p>
<p>then the MLE estimator is consistent,</p>
<p><span class="math display">\[
\hat{\theta}_{MLE} \to^p \theta_0
\]</span></p>
<p><strong>Asymptotic Normality</strong> of MLE</p>
<p>Suppose that <span class="math inline">\(y_1\)</span> and <span class="math inline">\(x_i\)</span> are iid drawn from the true conditional pdf <span class="math inline">\(f_{Y|X}(y_i,x_i;\theta)\)</span>. If R1-R4 and the following hold</p>
<p>R5: <span class="math inline">\(\theta_0\)</span> is in the interior of the set <span class="math inline">\(\Theta\)</span><br />
R6: <span class="math inline">\(f_{Y|X}(y_i,x_i;\theta)\)</span> is twice continuously differentiable in <span class="math inline">\(\theta\)</span> and <span class="math inline">\(f_{Y|X}(y_i,x_i;\theta) &gt;0\)</span> for a neighborhood <span class="math inline">\(N \in \Theta\)</span> around <span class="math inline">\(\theta_0\)</span><br />
R7: <span class="math inline">\(\int sup_{\theta \in N}||\partial f_{Y|X}(y_i,x_i;\theta)\partial\theta||d(y,x) &lt;\infty\)</span>, <span class="math inline">\(\int sup_{\theta \in N} || \partial^2 f_{Y|X}(y_i,x_i;\theta)/\partial \theta \partial \theta&#39; || d(y,x) &lt; \infty\)</span> and <span class="math inline">\(E(sup_{\theta \in N} || \partial^2ln(f_{Y|X}(y_i,x_i;\theta)) / \partial \theta \partial \theta&#39; ||) &lt; \infty\)</span><br />
R8: The information matrix <span class="math inline">\(I(\theta_0) = Var(\partial f_{Y|X}(y,x_i; \theta_0)/\partial \theta)\)</span> exists and is non-singular</p>
<p>then the MLE estimator is asymptotically normal,</p>
<p><span class="math display">\[
\sqrt{n}(\hat{\theta}_{MLE} - \theta_0) \to^d N(0,I(\theta_0)^{-1})
\]</span></p>
</div>
<div id="properties" class="section level4">
<h4><span class="header-section-number">5.1.7.3</span> Properties</h4>
<p><span class="citation">(EJD, Agresti, and Finlay <a href="#ref-EJD_1998" role="doc-biblioref">1998</a>)</span></p>
<ol style="list-style-type: decimal">
<li>Consistent: estimates are approximately unbiased in large samples</li>
<li>Asymptotically efficient: approximately smaller standard errors compared to other estimator</li>
<li>Asymptotically normal: with repeated sampling, the estimates will have an approximately normal distribution.</li>
<li>Invariance: MLE for <span class="math inline">\(g(\theta) = g(\hat{\theta})\)</span></li>
</ol>
<p><span class="math display">\[
\hat{\Theta} \approx^d (\theta,I(\hat{\theta)^{-1}}))
\]</span></p>
<p>Explicit vs Implicit MLE</p>
<ul>
<li>If we solve the score equation to get an expression of MLE, then it’s called <strong>explicit</strong></li>
<li>If there is no closed form for MLE, and we need some algorithms to derive its expression, it’s called <strong>implicit</strong></li>
</ul>
<p><strong>Large Sample Property</strong> of MLE</p>
<p>Implicit in these theorems is the assumption that we know what the conditional distribution,</p>
<p><span class="math display">\[
f_{Y|X}(y_i,x_i;\theta_0)
\]</span>
but just do now know the exact parameter value.</p>
<ul>
<li>Any Distributional mis-specification will result in inconsistent parameter estimates.<br />
</li>
<li>Quasi-MLE: Particular settings/ assumption that allow for certain types of distributional mis-specification (Ex: as long as the distribution is part of particular class or satisfies a particular assumption, then estimating with a wrong distribution will not lead to inconsistent parameter estimates).<br />
</li>
<li>non-parametric/ Semi-parametric estimation: no or very little distributional assumption are made. (hard to implement, derive properties, and interpret)</li>
</ul>
<p><br></p>
<p>The asymptotic variance of the MLE achieves the <strong>Cramer-Rao Lower Bound</strong></p>
<ul>
<li>The <strong>Cramer-Rao Lower Bound</strong> is a lower brand for the asymptotic variance of a consistent and asymptotically normally distributed estimator.<br />
</li>
<li>If an estimator achieves the lower bound then it is the most efficient estimator.</li>
</ul>
<p>The maximum Likelihood estimator (assuming the distribution is correctly specified and R1-R8 hold) is the most efficient consistent and asymptotically normal estimator.<br />
* most efficient among ALL consistent estimators (not limited to unbiased or linear estimators).</p>
<p><strong>Note</strong></p>
<ul>
<li>ML is better choice for binary, strictly positive, count, or inherent heteroskedasticity than linear model.</li>
<li>ML will assume that we know the conditional distribution of the outcome, and derive an estimator using that information.
<ul>
<li>Adds an assumption that we know the distribution (which is similar to <a href="linear-regression.html#a6-normal-distribution">A6 Normal Distribution</a> in linear model)</li>
<li>will produce a more efficient estimator.</li>
</ul></li>
</ul>
</div>
<div id="application-1" class="section level4">
<h4><span class="header-section-number">5.1.7.4</span> Application</h4>
<p>Other applications of MLE</p>
<ul>
<li>Corner Solution
<ul>
<li>Ex: hours worked, donations to charity<br />
</li>
<li>Estimate with Tobit<br />
</li>
</ul></li>
<li>Non-negative count
<ul>
<li>Ex: Numbers of arrest, Number of cigarettes smoked a day<br />
</li>
<li>Estimate with Poisson regression<br />
</li>
</ul></li>
<li>Multinomial Choice
<ul>
<li>Ex: Demand for cars, votes for primary election</li>
<li>Estimate with mutinomial probit or logit<br />
</li>
</ul></li>
<li>Ordinal Choice
<ul>
<li>Ex: Levels of Happiness, Levels of Income<br />
</li>
<li>Ordered Probit</li>
</ul></li>
</ul>
<p>Model for binary Response<br />
A binary variable will have a <a href="probability-theory.html#bernoulli">Bernoulli</a> distribution:</p>
<p><span class="math display">\[
f_Y(y_i;p) = p^{y_i}(1-p)^{(1-y_i)}
\]</span>
where p is the probability of success. The conditional distribution is:</p>
<p><span class="math display">\[
f_{Y|X}(y_i,x_i;p(.)) = p(x_i)^{y_i} (1-p(x_i))^{(1-y_i)}
\]</span></p>
<p>So choose <span class="math inline">\(p(x_i)\)</span> to be a reasonable function of <span class="math inline">\(x_i\)</span> and unknown parameters <span class="math inline">\(\theta\)</span></p>
<p>We can use <strong>latent variable model</strong> as probability functions</p>
<p><span class="math display">\[
y_i = 1\{y_i^* &gt; 0 \}  \\
y_i^* = x_i \beta-\epsilon_i
\]</span></p>
<ul>
<li><span class="math inline">\(y_i^*\)</span> is a latent variable (unobserved) that is not well-defined in terms of units/magnitudes<br />
</li>
<li><span class="math inline">\(\epsilon_i\)</span> is a mean 0 unobserved random variable.</li>
</ul>
<p>We can rewrite the mdoel without the latent variable,</p>
<p><span class="math display">\[
y_i = 1\{x_i beta &gt; \epsilon_i \}
\]</span></p>
<p>Then the probability function,</p>
<p><span class="math display">\[
p(x_i) = P(y_i = 1|x_i) \\
= P(x_i \beta &gt; \epsilon_i | x_i) \\
= F_{\epsilon|X}(x_i \beta | x_i)
\]</span></p>
<p>then we need to choose a conditional distribution for <span class="math inline">\(\epsilon_i\)</span>. Hence, we can make additional strong independence assumption</p>
<p><span class="math inline">\(\epsilon_i\)</span> is independent of <span class="math inline">\(x_i\)</span></p>
<p>Then the probability function is simply,</p>
<p><span class="math display">\[
p(x_i) = F_\epsilon(x_i \beta)
\]</span>
The probability function is also the conditional expectation function,</p>
<p><span class="math display">\[
E(y_i | x_i) = P(y_i = 1|x_i) = F_\epsilon (x_i \beta)
\]</span></p>
<p>so we allow the conditional expectation function to be non-linear.</p>
<p>Common distributional assumption</p>
<ol style="list-style-type: decimal">
<li><strong>Probit</strong>: Assume <span class="math inline">\(\epsilon_i\)</span> is standard normally distributed, then <span class="math inline">\(F_\epsilon(.) = \Phi(.)\)</span> is the standard normal CDF.<br />
</li>
<li><strong>Logit</strong>: Assume <span class="math inline">\(\epsilon_i\)</span> is standard logistically distributed, then <span class="math inline">\(F_\epsilon(.) = \Lambda(.)\)</span> is the standard normal CDF.</li>
</ol>
<p>Step to derive</p>
<ol style="list-style-type: decimal">
<li>Choose a distribution (normal or logistic) and plug into the following log likelihood,</li>
</ol>
<p><span class="math display">\[
ln(f_{Y|X} (y_i , x_i; \beta)) = y_i ln(F_\epsilon(x_i \beta)) + (1-y_i)ln(1-F_\epsilon(x_i \beta))
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Solve the MLE by finding the Maximum of</li>
</ol>
<p><span class="math display">\[
\hat{\beta}_{MLE} = argmax \sum_{i=1}^{n}ln(f_{Y|X}(y_i,x_i; \beta))
\]</span></p>
<p><br></p>
<p><strong>Properties</strong> of the Probit and Logit Estimators</p>
<ul>
<li>Probit or Logit is consistent and asymptotically normal if
<ul>
<li>[A2][] holds: <span class="math inline">\(E(x_i&#39; x_i)\)</span> exists and is non-singular<br />
</li>
<li>[A5][] (or <a href="linear-regression.html#a5a">A5a</a>) holds: {y_i,x_i} are iid (or stationary and weakly dependent).</li>
<li>Distributional assumptions on <span class="math inline">\(\epsilon_i\)</span> hold: Normal/Logistic and independent of <span class="math inline">\(x_i\)</span></li>
</ul></li>
<li>Under the same assumptions, Probit or Logit is also asymptotically efficient with asymptotic variance,</li>
</ul>
<p><span class="math display">\[
I(\beta_0)^{-1} = [E(\frac{(f_\epsilon(x_i \beta_0))^2}{F_\epsilon(x_i\beta_0)(1-F_\epsilon(x_i\beta_0))}x_i&#39; x_i)]^{-1}
\]</span></p>
<p>where <span class="math inline">\(F_\epsilon(x_i\beta_0)\)</span> is the probability density function (derivative of the CDF)</p>
<p><br></p>
<div id="interpretation" class="section level5">
<h5><span class="header-section-number">5.1.7.4.1</span> Interpretation</h5>
<p><span class="math inline">\(\beta\)</span> is the average response in the latent variable associated with a change in <span class="math inline">\(x_i\)</span></p>
<ul>
<li>Magnitudes do not have meaning</li>
<li>Direction does have meaning</li>
</ul>
<p>The <strong>partial effect</strong> for a Non-linear binary response model</p>
<p><span class="math display">\[
E(y_i |x_i) = F_\epsilon (x_i \beta) \\
PE(x_{ij}) = \frac{\partial E(y_i |x_i)}{\partial x_{ij}} = f_\epsilon (x_i \beta)\beta_j
\]</span></p>
<ul>
<li>The partial effect is the coefficient parameter <span class="math inline">\(\beta_j\)</span> multiplied by a scaling factor <span class="math inline">\(f_\epsilon (x_i \beta)\)</span><br />
</li>
<li>The scaling factor depends on <span class="math inline">\(x_i\)</span> so the partial effect changes depending on what <span class="math inline">\(x_i\)</span> is</li>
</ul>
<p>Single value for the partial effect</p>
<ul>
<li><strong>Partial Effect at the Average (PEA)</strong> is the partial effect for an average individual</li>
</ul>
<p><span class="math display">\[
f_{\epsilon}(\bar{x}\hat{\beta})\hat{\beta}_j
\]</span></p>
<ul>
<li><strong>Average Partial Effect (APE)</strong> is the average of all partial effect for each individual.</li>
</ul>
<p><span class="math display">\[
\frac{1}{n}\sum_{i=1}^{n}f_\epsilon(x_i \hat{\beta})\hat{\beta}_j
\]</span></p>
<p>In the linear model, APE = PEA.<br />
In a non-linear model (e.g., binary response), APE <span class="math inline">\(\neq\)</span> PEA</p>
</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-EJD_1998">
<p>EJD, Alan Agresti, and Barbara Finlay. 1998. “Statistical Methods for the Social Sciences.” <em>Journal of the American Statistical Association</em> 93 (442): 844. <a href="https://doi.org/10.2307/2670147">https://doi.org/10.2307/2670147</a>.</p>
</div>
<div id="ref-Greene_1990">
<p>Greene, William H. 1990. <em>Econometric Analysis</em>.</p>
</div>
<div id="ref-Little_1987">
<p>Little, Roderick J. A., and Philip J. Smith. 1987. “Editing and Imputation for Quantitative Survey Data.” <em>Journal of the American Statistical Association</em> 82 (397): 58–68. <a href="https://doi.org/10.1080/01621459.1987.10478391">https://doi.org/10.1080/01621459.1987.10478391</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="non-linear-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mikenguyen13/data_analysis/edit/main/03-regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Data Analysis.pdf", "Data Analysis.epub", "Data Analysis.mobi"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true,
"sharing": {
"facebook": true,
"github": true,
"twitter": true,
"linkedin": true
},
"info": true,
"edit": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
