<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.1 Linear Regression | A Guide on Data Analysis</title>
  <meta name="description" content="This is a guide on how to conduct a data analysis routine" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="5.1 Linear Regression | A Guide on Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a guide on how to conduct a data analysis routine" />
  <meta name="github-repo" content="mikenguyen13/data_analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.1 Linear Regression | A Guide on Data Analysis" />
  
  <meta name="twitter:description" content="This is a guide on how to conduct a data analysis routine" />
  

<meta name="author" content="Mike Nguyen" />


<meta name="date" content="2020-11-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regression-analysis.html"/>
<link rel="next" href="non-linear-regression.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>



<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Guide on Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Prerequisites</a><ul>
<li class="chapter" data-level="1.1" data-path="general-math.html"><a href="general-math.html"><i class="fa fa-check"></i><b>1.1</b> General Math</a></li>
<li class="chapter" data-level="1.2" data-path="matrix-theory.html"><a href="matrix-theory.html"><i class="fa fa-check"></i><b>1.2</b> Matrix Theory</a><ul>
<li class="chapter" data-level="1.2.1" data-path="matrix-theory.html"><a href="matrix-theory.html#rank"><i class="fa fa-check"></i><b>1.2.1</b> Rank</a></li>
<li class="chapter" data-level="1.2.2" data-path="matrix-theory.html"><a href="matrix-theory.html#inverse"><i class="fa fa-check"></i><b>1.2.2</b> Inverse</a></li>
<li class="chapter" data-level="1.2.3" data-path="matrix-theory.html"><a href="matrix-theory.html#definiteness"><i class="fa fa-check"></i><b>1.2.3</b> Definiteness</a></li>
<li class="chapter" data-level="1.2.4" data-path="matrix-theory.html"><a href="matrix-theory.html#matrix-calculus"><i class="fa fa-check"></i><b>1.2.4</b> Matrix Calculus</a></li>
<li class="chapter" data-level="1.2.5" data-path="matrix-theory.html"><a href="matrix-theory.html#optimization"><i class="fa fa-check"></i><b>1.2.5</b> Optimization</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>1.3</b> Probability Theory</a><ul>
<li class="chapter" data-level="1.3.1" data-path="probability-theory.html"><a href="probability-theory.html#axiom-and-theorems-of-probability"><i class="fa fa-check"></i><b>1.3.1</b> Axiom and Theorems of Probability</a></li>
<li class="chapter" data-level="1.3.2" data-path="probability-theory.html"><a href="probability-theory.html#random-variable"><i class="fa fa-check"></i><b>1.3.2</b> Random variable</a></li>
<li class="chapter" data-level="1.3.3" data-path="probability-theory.html"><a href="probability-theory.html#moment"><i class="fa fa-check"></i><b>1.3.3</b> Moment</a></li>
<li class="chapter" data-level="1.3.4" data-path="probability-theory.html"><a href="probability-theory.html#distributions"><i class="fa fa-check"></i><b>1.3.4</b> Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a></li>
<li class="chapter" data-level="3" data-path="descriptive-stat.html"><a href="descriptive-stat.html"><i class="fa fa-check"></i><b>3</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="3.1" data-path="numerical-measures.html"><a href="numerical-measures.html"><i class="fa fa-check"></i><b>3.1</b> Numerical Measures</a></li>
<li class="chapter" data-level="3.2" data-path="graphical-measures.html"><a href="graphical-measures.html"><i class="fa fa-check"></i><b>3.2</b> Graphical Measures</a><ul>
<li class="chapter" data-level="3.2.1" data-path="graphical-measures.html"><a href="graphical-measures.html#shape"><i class="fa fa-check"></i><b>3.2.1</b> Shape</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="normality-assessment.html"><a href="normality-assessment.html"><i class="fa fa-check"></i><b>3.3</b> Normality Assessment</a><ul>
<li class="chapter" data-level="3.3.1" data-path="normality-assessment.html"><a href="normality-assessment.html#graphical-assessment"><i class="fa fa-check"></i><b>3.3.1</b> Graphical Assessment</a></li>
<li class="chapter" data-level="3.3.2" data-path="normality-assessment.html"><a href="normality-assessment.html#summary-statistics"><i class="fa fa-check"></i><b>3.3.2</b> Summary Statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="basic-statistical-inference.html"><a href="basic-statistical-inference.html"><i class="fa fa-check"></i><b>4</b> Basic Statistical Inference</a><ul>
<li class="chapter" data-level="4.1" data-path="one-sample-inference.html"><a href="one-sample-inference.html"><i class="fa fa-check"></i><b>4.1</b> One Sample Inference</a></li>
<li class="chapter" data-level="4.2" data-path="two-sample-inference.html"><a href="two-sample-inference.html"><i class="fa fa-check"></i><b>4.2</b> Two Sample Inference</a></li>
<li class="chapter" data-level="4.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html"><i class="fa fa-check"></i><b>4.3</b> Categorical Data Analysis</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regression-analysis.html"><a href="regression-analysis.html"><i class="fa fa-check"></i><b>5</b> Regression Analysis</a><ul>
<li class="chapter" data-level="5.1" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>5.1</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1.1" data-path="linear-regression.html"><a href="linear-regression.html#ordinary-least-squares"><i class="fa fa-check"></i><b>5.1.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="5.1.2" data-path="linear-regression.html"><a href="linear-regression.html#feasible-generalized-least-squares"><i class="fa fa-check"></i><b>5.1.2</b> Feasible Generalized Least Squares</a></li>
<li class="chapter" data-level="5.1.3" data-path="linear-regression.html"><a href="linear-regression.html#weighted-least-squares"><i class="fa fa-check"></i><b>5.1.3</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="5.1.4" data-path="linear-regression.html"><a href="linear-regression.html#generalized-least-squares"><i class="fa fa-check"></i><b>5.1.4</b> Generalized Least Squares</a></li>
<li class="chapter" data-level="5.1.5" data-path="linear-regression.html"><a href="linear-regression.html#maximum-likelihood"><i class="fa fa-check"></i><b>5.1.5</b> Maximum Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="non-linear-regression.html"><a href="non-linear-regression.html"><i class="fa fa-check"></i><b>5.2</b> Non-linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="imputation-missing-data.html"><a href="imputation-missing-data.html"><i class="fa fa-check"></i><b>6</b> Imputation (Missing Data)</a><ul>
<li class="chapter" data-level="6.1" data-path="assumptions.html"><a href="assumptions.html"><i class="fa fa-check"></i><b>6.1</b> Assumptions</a><ul>
<li class="chapter" data-level="6.1.1" data-path="assumptions.html"><a href="assumptions.html#missing-completely-at-random-mcar"><i class="fa fa-check"></i><b>6.1.1</b> Missing Completely at Random (MCAR)</a></li>
<li class="chapter" data-level="6.1.2" data-path="assumptions.html"><a href="assumptions.html#missing-at-random-mar"><i class="fa fa-check"></i><b>6.1.2</b> Missing at Random (MAR)</a></li>
<li class="chapter" data-level="6.1.3" data-path="assumptions.html"><a href="assumptions.html#ignorable"><i class="fa fa-check"></i><b>6.1.3</b> Ignorable</a></li>
<li class="chapter" data-level="6.1.4" data-path="assumptions.html"><a href="assumptions.html#nonignorable"><i class="fa fa-check"></i><b>6.1.4</b> Nonignorable</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html"><i class="fa fa-check"></i><b>6.2</b> Solutions to Missing data</a><ul>
<li class="chapter" data-level="6.2.1" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#listwise-deletion"><i class="fa fa-check"></i><b>6.2.1</b> Listwise Deletion</a></li>
<li class="chapter" data-level="6.2.2" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#pairwise-deletion"><i class="fa fa-check"></i><b>6.2.2</b> Pairwise Deletion</a></li>
<li class="chapter" data-level="6.2.3" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#dummy-variable-adjustment"><i class="fa fa-check"></i><b>6.2.3</b> Dummy Variable Adjustment</a></li>
<li class="chapter" data-level="6.2.4" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#imputation"><i class="fa fa-check"></i><b>6.2.4</b> Imputation</a></li>
<li class="chapter" data-level="6.2.5" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#criteria-for-choosing-an-effective-approach"><i class="fa fa-check"></i><b>6.2.5</b> Criteria for Choosing an Effective Approach</a></li>
<li class="chapter" data-level="6.2.6" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#another-perspective"><i class="fa fa-check"></i><b>6.2.6</b> Another Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="experimental-design.html"><a href="experimental-design.html"><i class="fa fa-check"></i><b>7</b> Experimental Design</a><ul>
<li class="chapter" data-level="7.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html"><i class="fa fa-check"></i><b>7.1</b> Analysis of Variance (ANOVA)</a><ul>
<li class="chapter" data-level="7.1.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#completely-randomized-design-crd"><i class="fa fa-check"></i><b>7.1.1</b> Completely Randomized Design (CRD)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>8</b> Deep Learning</a><ul>
<li class="chapter" data-level="8.1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>8.1</b> Overview</a></li>
<li class="chapter" data-level="8.2" data-path="tensor-flow-and-r.html"><a href="tensor-flow-and-r.html"><i class="fa fa-check"></i><b>8.2</b> Tensor Flow and R</a><ul>
<li class="chapter" data-level="8.2.1" data-path="tensor-flow-and-r.html"><a href="tensor-flow-and-r.html#r-packages"><i class="fa fa-check"></i><b>8.2.1</b> R packages</a></li>
<li class="chapter" data-level="8.2.2" data-path="tensor-flow-and-r.html"><a href="tensor-flow-and-r.html#keras-layers"><i class="fa fa-check"></i><b>8.2.2</b> Keras layers</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="compiling-models.html"><a href="compiling-models.html"><i class="fa fa-check"></i><b>8.3</b> Compiling models</a></li>
<li class="chapter" data-level="8.4" data-path="losses-optimizers-and-metrics.html"><a href="losses-optimizers-and-metrics.html"><i class="fa fa-check"></i><b>8.4</b> Losses, Optimizers, and Metrics</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="research-methods.html"><a href="research-methods.html"><i class="fa fa-check"></i><b>9</b> Research Methods</a><ul>
<li class="chapter" data-level="9.1" data-path="but-for-world.html"><a href="but-for-world.html"><i class="fa fa-check"></i><b>9.1</b> But-for World</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="causality.html"><a href="causality.html"><i class="fa fa-check"></i><b>10</b> Causality</a></li>
<li class="chapter" data-level="11" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>11</b> Appendix</a><ul>
<li class="chapter" data-level="11.1" data-path="short-cut.html"><a href="short-cut.html"><i class="fa fa-check"></i><b>11.1</b> Short-cut</a></li>
<li class="chapter" data-level="11.2" data-path="function-short-cut.html"><a href="function-short-cut.html"><i class="fa fa-check"></i><b>11.2</b> Function short-cut</a></li>
<li class="chapter" data-level="11.3" data-path="citation.html"><a href="citation.html"><i class="fa fa-check"></i><b>11.3</b> Citation</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="bookdown-cheat-sheet.html"><a href="bookdown-cheat-sheet.html"><i class="fa fa-check"></i><b>12</b> Bookdown cheat sheet</a><ul>
<li class="chapter" data-level="12.1" data-path="math-expresssion-syntax.html"><a href="math-expresssion-syntax.html"><i class="fa fa-check"></i><b>12.1</b> Math Expresssion/ Syntax</a><ul>
<li class="chapter" data-level="12.1.1" data-path="math-expresssion-syntax.html"><a href="math-expresssion-syntax.html#statistics-notation"><i class="fa fa-check"></i><b>12.1.1</b> Statistics Notation</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="table.html"><a href="table.html"><i class="fa fa-check"></i><b>12.2</b> Table</a></li>
<li class="chapter" data-level="12.3" data-path="heading.html"><a href="heading.html"><i class="fa fa-check"></i><b>12.3</b> heading</a></li>
<li class="chapter" data-level="12.4" data-path="id-example.html"><a href="id-example.html"><i class="fa fa-check"></i><b>12.4</b> About labelling things</a></li>
<li class="chapter" data-level="12.5" data-path="cross-references.html"><a href="cross-references.html"><i class="fa fa-check"></i><b>12.5</b> Cross-references</a></li>
<li class="chapter" data-level="12.6" data-path="figures-tables-citations.html"><a href="figures-tables-citations.html"><i class="fa fa-check"></i><b>12.6</b> Figures, tables, citations</a></li>
<li class="chapter" data-level="12.7" data-path="how-the-square-bracket-links-work.html"><a href="how-the-square-bracket-links-work.html"><i class="fa fa-check"></i><b>12.7</b> How the square bracket links work</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Guide on Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression" class="section level2">
<h2><span class="header-section-number">5.1</span> Linear Regression</h2>
<div id="ordinary-least-squares" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Ordinary Least Squares</h3>
<p>The most fundamental model in statistics or econometric is a OLS linear regression.
OLS = Maximum likelihood when the error term is assumed to be normally distributed.</p>
<div id="ols-assumptions" class="section level4">
<h4><span class="header-section-number">5.1.1.1</span> OLS Assumptions</h4>
<div id="a1-linearity" class="section level5">
<h5><span class="header-section-number">5.1.1.1.1</span> A1 Linearity</h5>
<p><span class="math display" id="eq:A1">\[
\begin{equation}
A1: y=\mathbf{x}\beta + \epsilon
\tag{5.1}
\end{equation}
\]</span></p>
<p>Not restrictive</p>
<ul>
<li>x can be nonlinear transformation including interactions, natural log, quadratic</li>
</ul>
<p>With A3 (Exogeneity of Independent), linearity can be restrictive</p>
<div id="log-linear" class="section level6">
<h6><span class="header-section-number">5.1.1.1.1.1</span> Log-linear</h6>
</div>
<div id="linear-log" class="section level6">
<h6><span class="header-section-number">5.1.1.1.1.2</span> Linear-Log</h6>
</div>
<div id="log-log" class="section level6">
<h6><span class="header-section-number">5.1.1.1.1.3</span> Log-Log</h6>
</div>
<div id="higher-orders" class="section level6">
<h6><span class="header-section-number">5.1.1.1.1.4</span> Higher Orders</h6>
<p><span class="math inline">\(y=\beta_0 + x_1\beta_1 + x_1^2\beta_2 + \epsilon\)</span>
<span class="math display">\[
\frac{\partial y}{\partial x_1}=\beta_1 + 2x_1\beta_2
\]</span></p>
<ul>
<li>The effect of <span class="math inline">\(x_1\)</span> on y depends on the level of <span class="math inline">\(x_1\)</span></li>
<li>The partial effect at the average = <span class="math inline">\(\beta_1+2E(x_1)\beta_2\)</span></li>
<li>Average Partial Effect = <span class="math inline">\(E(\beta_1 + 2x_1\beta_2)\)</span></li>
</ul>
</div>
<div id="interactions" class="section level6">
<h6><span class="header-section-number">5.1.1.1.1.5</span> Interactions</h6>
<p><span class="math inline">\(y=\beta_0 + x_1\beta_1 + x_2\beta_2 + x_1x_2\beta_3 + \epsilon\)</span></p>
<ul>
<li><span class="math inline">\(\beta_1\)</span> is the average effect on y for a unit change in <span class="math inline">\(x_1\)</span> when <span class="math inline">\(x_2=0\)</span></li>
<li><span class="math inline">\(\beta_1 + x_2\beta_3\)</span> is the partial effect of <span class="math inline">\(x_1\)</span> on y which depends on the level of <span class="math inline">\(x_2\)</span></li>
</ul>
<p><br>
<br></p>
</div>
</div>
<div id="a2-full-rank" class="section level5">
<h5><span class="header-section-number">5.1.1.1.2</span> A2 Full rank</h5>
<p><span class="math display" id="eq:A2">\[
\begin{equation}
A2: rank(E(x&#39;x))=k
\tag{5.2}
\end{equation}
\]</span></p>
<p>also known as <strong>identification condition</strong></p>
<ul>
<li>columns of <span class="math inline">\(\mathbf{x}\)</span> cannot be written as a linear function of the other columns</li>
<li>which ensures that each parameter is unique and exists in the population regression equation</li>
</ul>
<p><br>
<br></p>
</div>
<div id="a3-exogeneity-of-independent-variables" class="section level5">
<h5><span class="header-section-number">5.1.1.1.3</span> A3 Exogeneity of Independent Variables</h5>
<p><span class="math display" id="eq:A3">\[
\begin{equation}
A3: E[\epsilon|x_1,x_2,...,x_k]=E[\epsilon|\mathbf{x}]=0
\tag{5.3}
\end{equation}
\]</span>
<strong>strict exogeneity</strong></p>
<ul>
<li>also known as <strong>mean independence</strong> check back on <a href="probability-theory.html#correlation-and-independence">Correlation and Independence</a></li>
<li>by the <a href="probability-theory.html#law-of-iterated-expectations">Law of Iterated Expectations</a> <span class="math inline">\(E(\epsilon)=0\)</span>, which can be satisfied by always including an intercept.</li>
<li>independent variables do not carry information for prediction of <span class="math inline">\(\epsilon\)</span></li>
<li>A3 implies <span class="math inline">\(E(y|x)=x\beta\)</span>, which means the conditional mean function must be a linear function of x <a href="linear-regression.html#a1-linearity">A1 Linearity</a></li>
</ul>
<p><br>
<br></p>
</div>
<div id="a4-homoskedasticity" class="section level5">
<h5><span class="header-section-number">5.1.1.1.4</span> A4 Homoskedasticity</h5>
<p><span class="math display" id="eq:A4">\[
\begin{equation}
A4: Var(\epsilon|x)=Var(\epsilon)=\sigma^2
\tag{5.4}
\end{equation}
\]</span></p>
<ul>
<li>Variation in the disturbance to be the same over the independent variables</li>
</ul>
<p><br>
<br></p>
</div>
<div id="a5-data-generation-random-sampling" class="section level5">
<h5><span class="header-section-number">5.1.1.1.5</span> A5 Data Generation (random Sampling)</h5>
<p><span class="math display" id="eq:A5">\[
\begin{equation}
A5: {y_i,x_{i1},...,x_{ik-1}: i = 1,..., n}
\tag{5.5}
\end{equation}
\]</span>
is a random sample</p>
<ul>
<li>random sample mean samples are independent and identically distributed (iid) from a joint distribution of <span class="math inline">\((y,\mathbf{x})\)</span></li>
<li>with <a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3</a> and <a href="linear-regression.html#a4-homoskedasticity">A4</a>, we have
<ul>
<li><strong>Strict Exogeneity</strong>: <span class="math inline">\(E(\epsilon_i|x_1,...,x_n)=0\)</span>. independent variables do not carry information for prediction of <span class="math inline">\(\epsilon\)</span></li>
<li><strong>Non-autocorrelation</strong>: <span class="math inline">\(E(\epsilon_i\epsilon_j|x_1,...,x_n)=0\)</span> The error term is uncorrelated across the draws conditional on the independent variables <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(A4: Var(\epsilon|\mathbf{X})=Var(\epsilon)=\sigma^2I_n\)</span></li>
</ul></li>
<li>In times series and spatial settings, A5 is less likely to hold.</li>
</ul>
<p><br></p>
</div>
<div id="a6-normal-distribution" class="section level5">
<h5><span class="header-section-number">5.1.1.1.6</span> A6 Normal Distribution</h5>
<p><span class="math display" id="eq:A6">\[
\begin{equation}
A6: \epsilon|\mathbf{x}\sim N(0,\sigma^2I_n)
\tag{5.6}
\end{equation}
\]</span>
The error term is normally distributed</p>
<p><br></p>
<p>From A1-A3, we have <strong>identification</strong> (also known as <strong>Orthogonality Condition</strong>) of the population parameter <span class="math inline">\(\beta\)</span></p>
<p><span class="math display">\[\begin{align}
y &amp;= {x}\beta + \epsilon &amp;&amp; \text{A1} \\
x&#39;y &amp;= x&#39;x\beta + x&#39;\epsilon &amp;&amp; \text{} \\
E(x&#39;y) &amp;= E(x&#39;x)\beta + E(x&#39;\epsilon)  &amp;&amp; \text{} \\
E(x&#39;y) &amp;= E(x&#39;x)\beta &amp;&amp; \text{A3} \\
[E(x&#39;x)]^{-1}E(x&#39;y) &amp;= [E(x&#39;x)]^{-1}E(x&#39;x)\beta &amp;&amp; \text{A2} \\
[E(x&#39;x)]^{-1}E(x&#39;y) &amp;= \beta
\end{align}\]</span></p>
<p>is the row vector of parameters that produces the best predictor of y
we choose the min of :
<span class="math display">\[
\underset{\gamma}{\operatorname{argmin}}E((y-x\gamma)^2)
\]</span>
First Order Condition
<span class="math display">\[
\begin{split}
\frac{\partial((y-x\gamma)^2)}{\partial\gamma}&amp;=0 \\
-2E(x&#39;(y-x\gamma))&amp;=0 \\
E(x&#39;y)-E(x&#39;x\gamma) &amp;=0 \\
E(x&#39;y) &amp;= E(x&#39;x)\gamma \\
(E(x&#39;x))^{-1}E(x&#39;y) &amp;= \gamma
\end{split}
\]</span></p>
<p>Second Order Conditon
<span class="math display">\[
\begin{split}
\frac{\partial^2E((y-x\gamma)^2)}{}&amp;=0 \\
E(\frac{\partial(y-x\partial)^2)}{\partial\gamma\partial\gamma&#39;}) &amp;= 2E(x&#39;x)
\end{split}
\]</span>
If <a href="linear-regression.html#a3-exogeneity-of-independent-variables">A3</a> holds, then <span class="math inline">\(2E(x&#39;x)\)</span> is PSD <span class="math inline">\(\rightarrow\)</span> minimum</p>
<p><br>
<br></p>
</div>
</div>
<div id="frisch-waugh-lovell-theorem" class="section level4">
<h4><span class="header-section-number">5.1.1.2</span> Frisch-Waugh-Lovell Theorem</h4>
<p><span class="math display">\[
\mathbf{y=X\beta + \epsilon=X_1\beta_1+X_2\beta_2 +\epsilon}
\]</span>
Equivalently,
<span class="math display">\[
\left(
\begin{array}{c}
X_1&#39;X_1 &amp; X_1&#39;X_2 \\
X_2&#39;X_1 &amp; X_2&#39;X_2
\end{array}
\right)
\left(
\begin{array}{c}
\hat{\beta_1} \\
\hat{\beta_2}
\end{array}
\right)
=
\left(
\begin{array}{c}
X_1&#39;y \\
X_2&#39;y
\end{array}
\right)
\]</span>
Hence,
<span class="math display">\[
\mathbf{\hat{\beta_1}=(X_1&#39;X_1)^{-1}X_1&#39;y - (X_1&#39;X_1)^{-1}X_1&#39;X_2\hat{\beta_2}}
\]</span></p>
<ol style="list-style-type: decimal">
<li>Betas from the multiple regression are not the same as the betas from each of the individual simple regression</li>
<li>Different set of X will affect all the coefficient estimates.</li>
<li>If <span class="math inline">\(X_1&#39;X_2 = 0\)</span> or $=0, then 1 and 2 do not hold.</li>
</ol>
<p><br></p>
<p><strong>Hierarchy of OLS Assumptions</strong></p>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th>Identification Data Description</th>
<th>Unbiasedness Consistency</th>
<th>Gauss-Markov(BLUE) Asymptotic Inference (z and Chi-squared)</th>
<th>Classical LM (BUE) Small-sample Inference (t and F)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Variation in X</td>
<td>Variation in X</td>
<td>Variation in X</td>
<td>Variation in X</td>
</tr>
<tr class="even">
<td></td>
<td>Random Sampling</td>
<td>Random Sampling</td>
<td>Random Sampling</td>
</tr>
<tr class="odd">
<td></td>
<td>Linearity in Parameters</td>
<td>Linearity in Parameters</td>
<td>Linearity in Parameters</td>
</tr>
<tr class="even">
<td></td>
<td>Zero Conditional Mean</td>
<td>Zero Conditional Mean</td>
<td>Zero Conditional Mean</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td>Homoskedasticity</td>
<td>Homoskedasticity</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td>Normality of Errors</td>
</tr>
</tbody>
</table>
<p><br>
<br>
<br>
<br></p>
</div>
</div>
<div id="feasible-generalized-least-squares" class="section level3">
<h3><span class="header-section-number">5.1.2</span> Feasible Generalized Least Squares</h3>
<p>Motivation for a more efficient estimator</p>
<ul>
<li>Gauss Markov Theorem holds under A1-A4</li>
<li>A4: <span class="math inline">\(Var(\epsilon| \mathbf{X} )=\sigma^2I_n\)</span>
<ul>
<li>Heteroskedasticity: <span class="math inline">\(Var(\epsilon_i|\mathbf{X}) \neq \sigma^2I_n\)</span></li>
<li>Serial Correlation: <span class="math inline">\(Cov(\epsilon_i,\epsilon_j|\mathbf{X}) \neq 0\)</span></li>
</ul></li>
<li>Without A4, how can we know which unbiased estimator is the most efficient?</li>
</ul>
<p>Original (unweighted) model:</p>
<p><span class="math display">\[
\mathbf{y=X\beta+ \epsilon}
\]</span>
Suppose A1-A3 hold, but A4 does not hold,
<span class="math display">\[
\mathbf{Var(\epsilon|X)=\Omega \neq \sigma^2 I_n}
\]</span></p>
<p>We will try to use OLS to estimate the transformed (weighted) model</p>
<p><span class="math display">\[
\mathbf{wy=wX\beta + w\epsilon}
\]</span>
We need to choose <span class="math inline">\(\mathbf{w}\)</span> so that</p>
<p><span class="math display">\[
\mathbf{w&#39;w = \Omega^{-1}}
\]</span>
then <span class="math inline">\(\mathbf{w}\)</span> (full-rank matrix) is the <strong>Cholesky decomposition</strong> of <span class="math inline">\(\mathbf{\Omega^{-1}}\)</span> (full-rank matrix)</p>
<p>In other words, <span class="math inline">\(\mathbf{w}\)</span> is the squared root of <span class="math inline">\(\Omega\)</span> (squared root version in matrix)</p>
<p><span class="math display">\[
\Omega = var(\epsilon | X) \\
\Omega^{-1} = var(\epsilon | X)^{-1}
\]</span></p>
<p>Then, the transformed equation (IGLS) will have the following properties.</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\mathbf{\hat{\beta}_{IGLS}} &amp;= \mathbf{(X&#39;w&#39;wX)^{-1}X&#39;w&#39;wy} \\
&amp; = \mathbf{(X&#39;\Omega^{-1}X)^{-1}X&#39;\Omega^{-1}y} \\
&amp; = \mathbf{\beta + X&#39;\Omega^{-1}X&#39;\Omega^{-1}\epsilon}
\end{split}
\end{equation}\]</span></p>
<p>Since A1-A3 hold for the unweighted model
<span class="math display">\[\begin{equation}
\begin{split}
\mathbf{E(\hat{\beta}_{IGLS}|X)} &amp; = E(\mathbf{\beta + (X&#39;\Omega^{-1}X&#39;\Omega^{-1}\epsilon)}|X)\\
&amp; = \mathbf{\beta + E(X&#39;\Omega^{-1}X&#39;\Omega^{-1}\epsilon)|X)} \\
&amp; = \mathbf{\beta + X&#39;\Omega^{-1}X&#39;\Omega^{-1}E(\epsilon|X)}  &amp;&amp; \text{since A3: $E(\epsilon|X)=0$} \\
&amp; = \mathbf{\beta}
\end{split}
\end{equation}\]</span></p>
<p><span class="math inline">\(\rightarrow\)</span> IGLS estimator is unbiased</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
\mathbf{Var(w\epsilon|X)} &amp;= \mathbf{wVar(\epsilon|X)w&#39;} \\
&amp; = \mathbf{w\Omega w&#39;} \\
&amp; = \mathbf{w(w&#39;w)^{-1}w&#39;} &amp;&amp; \text{since w is a full-rank matrix}\\
&amp; = \mathbf{ww^{-1}(w&#39;)^{-1}w&#39;} \\
&amp; = \mathbf{I_n}
\end{split}
\end{equation}\]</span></p>
<p><span class="math inline">\(\rightarrow\)</span> A4 holds for the transformed (weighted) equation</p>
<p>Then, the variance for the estimator is</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
Var(\hat{\beta}_{IGLS}|\mathbf{X}) &amp; = \mathbf{Var(\beta + (X&#39;\Omega ^{-1}X)^{-1}X&#39;\Omega^{-1}\epsilon|X)} \\
&amp;= \mathbf{Var((X&#39;\Omega ^{-1}X)^{-1}X&#39;\Omega^{-1}\epsilon|X)} \\
&amp;= \mathbf{(X&#39;\Omega ^{-1}X)^{-1}X&#39;\Omega^{-1} Var(\epsilon|X)   \Omega^{-1}X(X&#39;\Omega ^{-1}X)^{-1}} &amp;&amp; \text{because A4 holds}\\
&amp;= \mathbf{(X&#39;\Omega ^{-1}X)^{-1}X&#39;\Omega^{-1} \Omega \Omega^{-1} \Omega^{-1}X(X&#39;\Omega ^{-1}X)^{-1}} \\
&amp;= \mathbf{(X&#39;\Omega ^{-1}X)^{-1}}
\end{split}
\end{equation}\]</span></p>
<p>Let <span class="math inline">\(A = \mathbf{(X&#39;X)^{-1}X&#39;-(X&#39;\Omega ^{-1} X)X&#39; \Omega^{-1}}\)</span> then
<span class="math display">\[
Var(\hat{\beta}_{OLS}|X)- Var(\hat{\beta}_{IGLS}|X) = A\Omega A&#39;
\]</span>
And <span class="math inline">\(\Omega\)</span> is Positive Semi Definite, then <span class="math inline">\(A\Omega A&#39;\)</span> also PSD, then IGLS is more efficient</p>
<p>The name <strong>Infeasible</strong> comes from the fact that it is impossible to compute this estimator.</p>
<p><span class="math display">\[\begin{equation}
\mathbf{w} = 
\left(
\begin{array}{c}
w_{11} &amp; 0 &amp; 0 &amp; ... &amp; 0 \\
w_{21} &amp; w_{22} &amp; 0 &amp; ... &amp; 0 \\
w_{31} &amp; w_{32} &amp; w_{33} &amp; ... &amp; ... \\
w_{n1} &amp; w_{n2} &amp; w_{n3} &amp; ... &amp; w_{nn} \\
\end{array}
\right)
\end{equation}\]</span></p>
<p>With <span class="math inline">\(n(n+1)/2\)</span> number of elements and n observations <span class="math inline">\(\rightarrow\)</span> infeasible to estimate. (number of equation &gt; data)</p>
<p>Hence, we need to make assumption on <span class="math inline">\(\Omega\)</span> to make it feasible to estimate <span class="math inline">\(\mathbf{w}\)</span>:</p>
<ol style="list-style-type: decimal">
<li><a href="linear-regression.html#heteroskedasticity">Heteroskedasticity</a> : multiplicative exponential model</li>
<li>Serial Correlation: AR(1)</li>
<li>Serial Correlation: Cluster</li>
</ol>
<div id="heteroskedasticity" class="section level4">
<h4><span class="header-section-number">5.1.2.1</span> Heteroskedasticity</h4>
<p><span class="math display" id="eq:h-var-error-term">\[\begin{equation}
\begin{split}
Var(\epsilon_i |x_i) &amp; = E(\epsilon^2|x_i) \neq \sigma^2 \\
&amp; = h(x_i) = \sigma_i^2 \text{(variance of the error term is a function of x)}
\end{split}
\tag{5.7}
\end{equation}\]</span></p>
<p>For our model,</p>
<p><span class="math display">\[
y_i = x_i\beta + \epsilon_i \\
(1/\sigma_i)y_i = (1/\sigma_i)x_i\beta + (1/\sigma_i)\epsilon_i
\]</span></p>
<p>then, from <a href="linear-regression.html#eq:h-var-error-term">(5.7)</a></p>
<p><span class="math display">\[\begin{equation}
\begin{split}
Var((1/\sigma_i)\epsilon_i|X) &amp;= (1/\sigma_i^2) Var(\epsilon_i|X) \\
&amp;= (1/\sigma_i^2)\sigma_i^2 \\
&amp;= 1
\end{split}
\end{equation}\]</span></p>
<p>then the</p>
</div>
</div>
<div id="weighted-least-squares" class="section level3">
<h3><span class="header-section-number">5.1.3</span> Weighted Least Squares</h3>
</div>
<div id="generalized-least-squares" class="section level3">
<h3><span class="header-section-number">5.1.4</span> Generalized Least Squares</h3>
</div>
<div id="maximum-likelihood" class="section level3">
<h3><span class="header-section-number">5.1.5</span> Maximum Likelihood</h3>
<p>Premise: find values of the parameters that maximize the probability of observing the data
In other words, we try to maximize the value of theta in the likelihood function
<span class="math display">\[
L(\theta)=\prod_{i=1}^{n}f(y_i|\theta)
\]</span>
<span class="math inline">\(f(y|\theta)\)</span> is the probability density of observing a single value of Y given some value of <span class="math inline">\(\theta\)</span>
<span class="math inline">\(f(y|\theta)\)</span> can be specify as various type of distributions. You can review back section <a href="probability-theory.html#distributions">Distributions</a>. For example
If y is a dichotomous variable, then
<span class="math display">\[
L(\theta)=\prod_{i=1}^{n}\theta^{y_i}(1-\theta)^{1-y_i}
\]</span></p>
<p><strong>Assumption</strong>:
observations are independent and have the same density function.
Under multivariate normal assumption, ML yields consistent estimates of the means and the covariance matrix for multivariate distribution with finite fourth moments <span class="citation">(Little and Smith <a href="#ref-Little_1987" role="doc-biblioref">1987</a>)</span></p>
<p><strong>Properties</strong>
<span class="citation">(EJD, Agresti, and Finlay <a href="#ref-EJD_1998" role="doc-biblioref">1998</a>)</span></p>
<ol style="list-style-type: decimal">
<li>Consistent: estimates are approximately unbiased in large samples</li>
<li>Asymptotically efficient: approximately smaller standard errors compared to other estimator</li>
<li>Asymptotically normal: with repeated sampling, the estimates will have an approximately normal distribution.</li>
</ol>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-EJD_1998">
<p>EJD, Alan Agresti, and Barbara Finlay. 1998. “Statistical Methods for the Social Sciences.” <em>Journal of the American Statistical Association</em> 93 (442): 844. <a href="https://doi.org/10.2307/2670147">https://doi.org/10.2307/2670147</a>.</p>
</div>
<div id="ref-Little_1987">
<p>Little, Roderick J. A., and Philip J. Smith. 1987. “Editing and Imputation for Quantitative Survey Data.” <em>Journal of the American Statistical Association</em> 82 (397): 58–68. <a href="https://doi.org/10.1080/01621459.1987.10478391">https://doi.org/10.1080/01621459.1987.10478391</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="non-linear-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mikenguyen13/data_analysis/edit/main/03-regression-analysis.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Data Analysis.pdf", "Data Analysis.epub", "Data Analysis.mobi"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true,
"sharing": {
"facebook": true,
"github": true,
"twitter": true,
"linkedin": true
},
"info": true,
"edit": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
