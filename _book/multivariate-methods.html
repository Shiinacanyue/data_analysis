<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 21 Multivariate Methods | A Guide on Data Analysis</title>
  <meta name="description" content="This is a guide on how to conduct data analysis" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 21 Multivariate Methods | A Guide on Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a guide on how to conduct data analysis" />
  <meta name="github-repo" content="mikenguyen13/data_analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 21 Multivariate Methods | A Guide on Data Analysis" />
  
  <meta name="twitter:description" content="This is a guide on how to conduct data analysis" />
  

<meta name="author" content="Mike Nguyen" />


<meta name="date" content="2021-04-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="single-factor-covariance-model.html"/>
<link rel="next" href="manova.html"/>
<script src="libs/header-attrs-2.7/header-attrs.js"></script>
<script src="libs/jquery-3.5.0/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/proj4js-2.3.15/proj4.js"></script>
<link href="libs/highcharts-8.1.2/css/motion.css" rel="stylesheet" />
<script src="libs/highcharts-8.1.2/highcharts.js"></script>
<script src="libs/highcharts-8.1.2/highcharts-3d.js"></script>
<script src="libs/highcharts-8.1.2/highcharts-more.js"></script>
<script src="libs/highcharts-8.1.2/modules/stock.js"></script>
<script src="libs/highcharts-8.1.2/modules/map.js"></script>
<script src="libs/highcharts-8.1.2/modules/annotations.js"></script>
<script src="libs/highcharts-8.1.2/modules/data.js"></script>
<script src="libs/highcharts-8.1.2/modules/drilldown.js"></script>
<script src="libs/highcharts-8.1.2/modules/item-series.js"></script>
<script src="libs/highcharts-8.1.2/modules/offline-exporting.js"></script>
<script src="libs/highcharts-8.1.2/modules/overlapping-datalabels.js"></script>
<script src="libs/highcharts-8.1.2/modules/exporting.js"></script>
<script src="libs/highcharts-8.1.2/modules/export-data.js"></script>
<script src="libs/highcharts-8.1.2/modules/funnel.js"></script>
<script src="libs/highcharts-8.1.2/modules/heatmap.js"></script>
<script src="libs/highcharts-8.1.2/modules/treemap.js"></script>
<script src="libs/highcharts-8.1.2/modules/sankey.js"></script>
<script src="libs/highcharts-8.1.2/modules/dependency-wheel.js"></script>
<script src="libs/highcharts-8.1.2/modules/organization.js"></script>
<script src="libs/highcharts-8.1.2/modules/solid-gauge.js"></script>
<script src="libs/highcharts-8.1.2/modules/streamgraph.js"></script>
<script src="libs/highcharts-8.1.2/modules/sunburst.js"></script>
<script src="libs/highcharts-8.1.2/modules/vector.js"></script>
<script src="libs/highcharts-8.1.2/modules/wordcloud.js"></script>
<script src="libs/highcharts-8.1.2/modules/xrange.js"></script>
<script src="libs/highcharts-8.1.2/modules/tilemap.js"></script>
<script src="libs/highcharts-8.1.2/modules/venn.js"></script>
<script src="libs/highcharts-8.1.2/modules/gantt.js"></script>
<script src="libs/highcharts-8.1.2/modules/timeline.js"></script>
<script src="libs/highcharts-8.1.2/modules/parallel-coordinates.js"></script>
<script src="libs/highcharts-8.1.2/modules/bullet.js"></script>
<script src="libs/highcharts-8.1.2/modules/coloraxis.js"></script>
<script src="libs/highcharts-8.1.2/modules/dumbbell.js"></script>
<script src="libs/highcharts-8.1.2/modules/lollipop.js"></script>
<script src="libs/highcharts-8.1.2/modules/series-label.js"></script>
<script src="libs/highcharts-8.1.2/plugins/motion.js"></script>
<script src="libs/highcharts-8.1.2/custom/reset.js"></script>
<script src="libs/highcharts-8.1.2/modules/boost.js"></script>
<script src="libs/highchart-binding-0.8.2/highchart.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Guide on Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a>
<ul>
<li class="chapter" data-level="2.1" data-path="matrix-theory.html"><a href="matrix-theory.html"><i class="fa fa-check"></i><b>2.1</b> Matrix Theory</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="matrix-theory.html"><a href="matrix-theory.html#rank"><i class="fa fa-check"></i><b>2.1.1</b> Rank</a></li>
<li class="chapter" data-level="2.1.2" data-path="matrix-theory.html"><a href="matrix-theory.html#inverse"><i class="fa fa-check"></i><b>2.1.2</b> Inverse</a></li>
<li class="chapter" data-level="2.1.3" data-path="matrix-theory.html"><a href="matrix-theory.html#definiteness"><i class="fa fa-check"></i><b>2.1.3</b> Definiteness</a></li>
<li class="chapter" data-level="2.1.4" data-path="matrix-theory.html"><a href="matrix-theory.html#matrix-calculus"><i class="fa fa-check"></i><b>2.1.4</b> Matrix Calculus</a></li>
<li class="chapter" data-level="2.1.5" data-path="matrix-theory.html"><a href="matrix-theory.html#optimization"><i class="fa fa-check"></i><b>2.1.5</b> Optimization</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>2.2</b> Probability Theory</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="probability-theory.html"><a href="probability-theory.html#axiom-and-theorems-of-probability"><i class="fa fa-check"></i><b>2.2.1</b> Axiom and Theorems of Probability</a></li>
<li class="chapter" data-level="2.2.2" data-path="probability-theory.html"><a href="probability-theory.html#central-limit-theorem"><i class="fa fa-check"></i><b>2.2.2</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="2.2.3" data-path="probability-theory.html"><a href="probability-theory.html#random-variable"><i class="fa fa-check"></i><b>2.2.3</b> Random variable</a></li>
<li class="chapter" data-level="2.2.4" data-path="probability-theory.html"><a href="probability-theory.html#moment-generating-function"><i class="fa fa-check"></i><b>2.2.4</b> Moment generating function</a></li>
<li class="chapter" data-level="2.2.5" data-path="probability-theory.html"><a href="probability-theory.html#moment"><i class="fa fa-check"></i><b>2.2.5</b> Moment</a></li>
<li class="chapter" data-level="2.2.6" data-path="probability-theory.html"><a href="probability-theory.html#distributions"><i class="fa fa-check"></i><b>2.2.6</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="general-math.html"><a href="general-math.html"><i class="fa fa-check"></i><b>2.3</b> General Math</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="general-math.html"><a href="general-math.html#law-of-large-numbers"><i class="fa fa-check"></i><b>2.3.1</b> Law of large numbers</a></li>
<li class="chapter" data-level="2.3.2" data-path="general-math.html"><a href="general-math.html#law-of-iterated-expectation"><i class="fa fa-check"></i><b>2.3.2</b> Law of Iterated Expectation</a></li>
<li class="chapter" data-level="2.3.3" data-path="general-math.html"><a href="general-math.html#convergence"><i class="fa fa-check"></i><b>2.3.3</b> Convergence</a></li>
<li class="chapter" data-level="2.3.4" data-path="general-math.html"><a href="general-math.html#sufficient-statistics"><i class="fa fa-check"></i><b>2.3.4</b> Sufficient Statistics</a></li>
<li class="chapter" data-level="2.3.5" data-path="general-math.html"><a href="general-math.html#parameter-transformations"><i class="fa fa-check"></i><b>2.3.5</b> Parameter transformations</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>2.4</b> Methods</a></li>
<li class="chapter" data-level="2.5" data-path="data-importexport.html"><a href="data-importexport.html"><i class="fa fa-check"></i><b>2.5</b> Data Import/Export</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="data-importexport.html"><a href="data-importexport.html#medium-size"><i class="fa fa-check"></i><b>2.5.1</b> Medium size</a></li>
<li class="chapter" data-level="2.5.2" data-path="data-importexport.html"><a href="data-importexport.html#large-size"><i class="fa fa-check"></i><b>2.5.2</b> Large size</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="data-manipulation.html"><a href="data-manipulation.html"><i class="fa fa-check"></i><b>2.6</b> Data Manipulation</a></li>
</ul></li>
<li class="part"><span><b>I BASIC</b></span></li>
<li class="chapter" data-level="3" data-path="descriptive-stat.html"><a href="descriptive-stat.html"><i class="fa fa-check"></i><b>3</b> Descriptive Statistics</a>
<ul>
<li class="chapter" data-level="3.1" data-path="numerical-measures.html"><a href="numerical-measures.html"><i class="fa fa-check"></i><b>3.1</b> Numerical Measures</a></li>
<li class="chapter" data-level="3.2" data-path="graphical-measures.html"><a href="graphical-measures.html"><i class="fa fa-check"></i><b>3.2</b> Graphical Measures</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="graphical-measures.html"><a href="graphical-measures.html#shape"><i class="fa fa-check"></i><b>3.2.1</b> Shape</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="normality-assessment.html"><a href="normality-assessment.html"><i class="fa fa-check"></i><b>3.3</b> Normality Assessment</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="normality-assessment.html"><a href="normality-assessment.html#graphical-assessment"><i class="fa fa-check"></i><b>3.3.1</b> Graphical Assessment</a></li>
<li class="chapter" data-level="3.3.2" data-path="normality-assessment.html"><a href="normality-assessment.html#summary-statistics"><i class="fa fa-check"></i><b>3.3.2</b> Summary Statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="basic-statistical-inference.html"><a href="basic-statistical-inference.html"><i class="fa fa-check"></i><b>4</b> Basic Statistical Inference</a>
<ul>
<li class="chapter" data-level="4.1" data-path="one-sample-inference.html"><a href="one-sample-inference.html"><i class="fa fa-check"></i><b>4.1</b> One Sample Inference</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="one-sample-inference.html"><a href="one-sample-inference.html#the-mean"><i class="fa fa-check"></i><b>4.1.1</b> The Mean</a></li>
<li class="chapter" data-level="4.1.2" data-path="one-sample-inference.html"><a href="one-sample-inference.html#single-variance"><i class="fa fa-check"></i><b>4.1.2</b> Single Variance</a></li>
<li class="chapter" data-level="4.1.3" data-path="one-sample-inference.html"><a href="one-sample-inference.html#single-proportion-p"><i class="fa fa-check"></i><b>4.1.3</b> Single Proportion (p)</a></li>
<li class="chapter" data-level="4.1.4" data-path="one-sample-inference.html"><a href="one-sample-inference.html#power"><i class="fa fa-check"></i><b>4.1.4</b> Power</a></li>
<li class="chapter" data-level="4.1.5" data-path="one-sample-inference.html"><a href="one-sample-inference.html#sample-size"><i class="fa fa-check"></i><b>4.1.5</b> Sample Size</a></li>
<li class="chapter" data-level="4.1.6" data-path="one-sample-inference.html"><a href="one-sample-inference.html#note"><i class="fa fa-check"></i><b>4.1.6</b> Note</a></li>
<li class="chapter" data-level="4.1.7" data-path="one-sample-inference.html"><a href="one-sample-inference.html#one-sample-non-parametric-methods"><i class="fa fa-check"></i><b>4.1.7</b> One-sample Non-parametric Methods</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="two-sample-inference.html"><a href="two-sample-inference.html"><i class="fa fa-check"></i><b>4.2</b> Two Sample Inference</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="two-sample-inference.html"><a href="two-sample-inference.html#means"><i class="fa fa-check"></i><b>4.2.1</b> Means</a></li>
<li class="chapter" data-level="4.2.2" data-path="two-sample-inference.html"><a href="two-sample-inference.html#variances"><i class="fa fa-check"></i><b>4.2.2</b> Variances</a></li>
<li class="chapter" data-level="4.2.3" data-path="two-sample-inference.html"><a href="two-sample-inference.html#power-1"><i class="fa fa-check"></i><b>4.2.3</b> Power</a></li>
<li class="chapter" data-level="4.2.4" data-path="two-sample-inference.html"><a href="two-sample-inference.html#sample-size-1"><i class="fa fa-check"></i><b>4.2.4</b> Sample Size</a></li>
<li class="chapter" data-level="4.2.5" data-path="two-sample-inference.html"><a href="two-sample-inference.html#matched-pair-designs"><i class="fa fa-check"></i><b>4.2.5</b> Matched Pair Designs</a></li>
<li class="chapter" data-level="4.2.6" data-path="two-sample-inference.html"><a href="two-sample-inference.html#nonparametric-tests-for-two-samples"><i class="fa fa-check"></i><b>4.2.6</b> Nonparametric Tests for Two Samples</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html"><i class="fa fa-check"></i><b>4.3</b> Categorical Data Analysis</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#inferences-for-small-samples"><i class="fa fa-check"></i><b>4.3.1</b> Inferences for Small Samples</a></li>
<li class="chapter" data-level="4.3.2" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#test-of-association"><i class="fa fa-check"></i><b>4.3.2</b> Test of Association</a></li>
<li class="chapter" data-level="4.3.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#ordinal-association"><i class="fa fa-check"></i><b>4.3.3</b> Ordinal Association</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II REGRESSION</b></span></li>
<li class="chapter" data-level="5" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html"><i class="fa fa-check"></i><b>5.1</b> Ordinary Least Squares</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#simple-regression-basic-model"><i class="fa fa-check"></i><b>5.1.1</b> Simple Regression (Basic Model)</a></li>
<li class="chapter" data-level="5.1.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#multiple-linear-regression"><i class="fa fa-check"></i><b>5.1.2</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="5.1.3" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#ols-assumptions"><i class="fa fa-check"></i><b>5.1.3</b> OLS Assumptions</a></li>
<li class="chapter" data-level="5.1.4" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#theorems"><i class="fa fa-check"></i><b>5.1.4</b> Theorems</a></li>
<li class="chapter" data-level="5.1.5" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#variable-selection"><i class="fa fa-check"></i><b>5.1.5</b> Variable Selection</a></li>
<li class="chapter" data-level="5.1.6" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#diagnostics-1"><i class="fa fa-check"></i><b>5.1.6</b> Diagnostics</a></li>
<li class="chapter" data-level="5.1.7" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#model-validation"><i class="fa fa-check"></i><b>5.1.7</b> Model Validation</a></li>
<li class="chapter" data-level="5.1.8" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#finite-sample-properties"><i class="fa fa-check"></i><b>5.1.8</b> Finite Sample Properties</a></li>
<li class="chapter" data-level="5.1.9" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#large-sample-properties"><i class="fa fa-check"></i><b>5.1.9</b> Large Sample Properties</a></li>
<li class="chapter" data-level="5.1.10" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#application"><i class="fa fa-check"></i><b>5.1.10</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="feasible-generalized-least-squares.html"><a href="feasible-generalized-least-squares.html"><i class="fa fa-check"></i><b>5.2</b> Feasible Generalized Least Squares</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="feasible-generalized-least-squares.html"><a href="feasible-generalized-least-squares.html#heteroskedasticity"><i class="fa fa-check"></i><b>5.2.1</b> Heteroskedasticity</a></li>
<li class="chapter" data-level="5.2.2" data-path="feasible-generalized-least-squares.html"><a href="feasible-generalized-least-squares.html#serial-correlation"><i class="fa fa-check"></i><b>5.2.2</b> Serial Correlation</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="weighted-least-squares.html"><a href="weighted-least-squares.html"><i class="fa fa-check"></i><b>5.3</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="5.4" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html"><i class="fa fa-check"></i><b>5.4</b> Generalized Least Squares</a></li>
<li class="chapter" data-level="5.5" data-path="feasiable-prais-winsten.html"><a href="feasiable-prais-winsten.html"><i class="fa fa-check"></i><b>5.5</b> Feasiable Prais Winsten</a></li>
<li class="chapter" data-level="5.6" data-path="feasible-group-level-random-effects.html"><a href="feasible-group-level-random-effects.html"><i class="fa fa-check"></i><b>5.6</b> Feasible group level Random Effects</a></li>
<li class="chapter" data-level="5.7" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>5.7</b> Ridge Regression</a></li>
<li class="chapter" data-level="5.8" data-path="principal-component-regression.html"><a href="principal-component-regression.html"><i class="fa fa-check"></i><b>5.8</b> Principal Component Regression</a></li>
<li class="chapter" data-level="5.9" data-path="robust-regression.html"><a href="robust-regression.html"><i class="fa fa-check"></i><b>5.9</b> Robust Regression</a>
<ul>
<li class="chapter" data-level="5.9.1" data-path="robust-regression.html"><a href="robust-regression.html#least-absolute-residuals-lar-regression"><i class="fa fa-check"></i><b>5.9.1</b> Least Absolute Residuals (LAR) Regression</a></li>
<li class="chapter" data-level="5.9.2" data-path="robust-regression.html"><a href="robust-regression.html#least-median-of-squares-lms-regression"><i class="fa fa-check"></i><b>5.9.2</b> Least Median of Squares (LMS) Regression</a></li>
<li class="chapter" data-level="5.9.3" data-path="robust-regression.html"><a href="robust-regression.html#iteratively-reweighted-least-squares-irls-robust-regression"><i class="fa fa-check"></i><b>5.9.3</b> Iteratively Reweighted Least Squares (IRLS) Robust Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="maximum-likelihood-regression.html"><a href="maximum-likelihood-regression.html"><i class="fa fa-check"></i><b>5.10</b> Maximum Likelihood</a>
<ul>
<li class="chapter" data-level="5.10.1" data-path="maximum-likelihood-regression.html"><a href="maximum-likelihood-regression.html#motivation-for-mle"><i class="fa fa-check"></i><b>5.10.1</b> Motivation for MLE</a></li>
<li class="chapter" data-level="5.10.2" data-path="maximum-likelihood-regression.html"><a href="maximum-likelihood-regression.html#assumption"><i class="fa fa-check"></i><b>5.10.2</b> Assumption</a></li>
<li class="chapter" data-level="5.10.3" data-path="maximum-likelihood-regression.html"><a href="maximum-likelihood-regression.html#properties"><i class="fa fa-check"></i><b>5.10.3</b> Properties</a></li>
<li class="chapter" data-level="5.10.4" data-path="maximum-likelihood-regression.html"><a href="maximum-likelihood-regression.html#compare-to-ols"><i class="fa fa-check"></i><b>5.10.4</b> Compare to OLS</a></li>
<li class="chapter" data-level="5.10.5" data-path="maximum-likelihood-regression.html"><a href="maximum-likelihood-regression.html#application-1"><i class="fa fa-check"></i><b>5.10.5</b> Application</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="non-linear-regression.html"><a href="non-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Non-linear Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="inference-1.html"><a href="inference-1.html"><i class="fa fa-check"></i><b>6.1</b> Inference</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="inference-1.html"><a href="inference-1.html#linear-function-of-the-parameters"><i class="fa fa-check"></i><b>6.1.1</b> Linear Function of the Parameters</a></li>
<li class="chapter" data-level="6.1.2" data-path="inference-1.html"><a href="inference-1.html#nonlinear"><i class="fa fa-check"></i><b>6.1.2</b> Nonlinear</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="non-linear-least-squares.html"><a href="non-linear-least-squares.html"><i class="fa fa-check"></i><b>6.2</b> Non-linear Least Squares</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="non-linear-least-squares.html"><a href="non-linear-least-squares.html#alternative-of-gauss-newton-algorithm"><i class="fa fa-check"></i><b>6.2.1</b> Alternative of Gauss-Newton Algorithm</a></li>
<li class="chapter" data-level="6.2.2" data-path="non-linear-least-squares.html"><a href="non-linear-least-squares.html#practical-considerations"><i class="fa fa-check"></i><b>6.2.2</b> Practical Considerations</a></li>
<li class="chapter" data-level="6.2.3" data-path="non-linear-least-squares.html"><a href="non-linear-least-squares.html#modelestiamtion-adequcy"><i class="fa fa-check"></i><b>6.2.3</b> Model/Estiamtion Adequcy</a></li>
<li class="chapter" data-level="6.2.4" data-path="non-linear-least-squares.html"><a href="non-linear-least-squares.html#application-2"><i class="fa fa-check"></i><b>6.2.4</b> Application</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>7</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="7.1" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>7.1</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="logistic-regression.html"><a href="logistic-regression.html#application-3"><i class="fa fa-check"></i><b>7.1.1</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="probit-regression.html"><a href="probit-regression.html"><i class="fa fa-check"></i><b>7.2</b> Probit Regression</a></li>
<li class="chapter" data-level="7.3" data-path="binomial-regression.html"><a href="binomial-regression.html"><i class="fa fa-check"></i><b>7.3</b> Binomial Regression</a></li>
<li class="chapter" data-level="7.4" data-path="poisson-regression.html"><a href="poisson-regression.html"><i class="fa fa-check"></i><b>7.4</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="poisson-regression.html"><a href="poisson-regression.html#application-4"><i class="fa fa-check"></i><b>7.4.1</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="negative-binomial-regression.html"><a href="negative-binomial-regression.html"><i class="fa fa-check"></i><b>7.5</b> Negative Binomial Regression</a></li>
<li class="chapter" data-level="7.6" data-path="multinomial.html"><a href="multinomial.html"><i class="fa fa-check"></i><b>7.6</b> Multinomial</a></li>
<li class="chapter" data-level="7.7" data-path="generalization.html"><a href="generalization.html"><i class="fa fa-check"></i><b>7.7</b> Generalization</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="generalization.html"><a href="generalization.html#estimation-1"><i class="fa fa-check"></i><b>7.7.1</b> Estimation</a></li>
<li class="chapter" data-level="7.7.2" data-path="generalization.html"><a href="generalization.html#inference-2"><i class="fa fa-check"></i><b>7.7.2</b> Inference</a></li>
<li class="chapter" data-level="7.7.3" data-path="generalization.html"><a href="generalization.html#deviance"><i class="fa fa-check"></i><b>7.7.3</b> Deviance</a></li>
<li class="chapter" data-level="7.7.4" data-path="generalization.html"><a href="generalization.html#diagnostic-plots"><i class="fa fa-check"></i><b>7.7.4</b> Diagnostic Plots</a></li>
<li class="chapter" data-level="7.7.5" data-path="generalization.html"><a href="generalization.html#goodness-of-fit"><i class="fa fa-check"></i><b>7.7.5</b> Goodness of Fit</a></li>
<li class="chapter" data-level="7.7.6" data-path="generalization.html"><a href="generalization.html#over-dispersion"><i class="fa fa-check"></i><b>7.7.6</b> Over-Dispersion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="linear-mixed-models.html"><a href="linear-mixed-models.html"><i class="fa fa-check"></i><b>8</b> Linear Mixed Models</a>
<ul>
<li class="chapter" data-level="8.1" data-path="dependent-data.html"><a href="dependent-data.html"><i class="fa fa-check"></i><b>8.1</b> Dependent Data</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="dependent-data.html"><a href="dependent-data.html#random-intercepts-model"><i class="fa fa-check"></i><b>8.1.1</b> Random-Intercepts Model</a></li>
<li class="chapter" data-level="8.1.2" data-path="dependent-data.html"><a href="dependent-data.html#covariance-models"><i class="fa fa-check"></i><b>8.1.2</b> Covariance Models</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="estimation-2.html"><a href="estimation-2.html"><i class="fa fa-check"></i><b>8.2</b> Estimation</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="estimation-2.html"><a href="estimation-2.html#estimating-mathbfv"><i class="fa fa-check"></i><b>8.2.1</b> Estimating <span class="math inline">\(\mathbf{V}\)</span></a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="inference-3.html"><a href="inference-3.html"><i class="fa fa-check"></i><b>8.3</b> Inference</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="inference-3.html"><a href="inference-3.html#parameters-beta"><i class="fa fa-check"></i><b>8.3.1</b> Parameters <span class="math inline">\(\beta\)</span></a></li>
<li class="chapter" data-level="8.3.2" data-path="inference-3.html"><a href="inference-3.html#variance-components"><i class="fa fa-check"></i><b>8.3.2</b> Variance Components</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="information-criteria.html"><a href="information-criteria.html"><i class="fa fa-check"></i><b>8.4</b> Information Criteria</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="information-criteria.html"><a href="information-criteria.html#akaikes-information-criteria-aic"><i class="fa fa-check"></i><b>8.4.1</b> Akaike’s Information Criteria (AIC)</a></li>
<li class="chapter" data-level="8.4.2" data-path="information-criteria.html"><a href="information-criteria.html#corrected-aic-aicc"><i class="fa fa-check"></i><b>8.4.2</b> Corrected AIC (AICC)</a></li>
<li class="chapter" data-level="8.4.3" data-path="information-criteria.html"><a href="information-criteria.html#bayesian-information-criteria-bic"><i class="fa fa-check"></i><b>8.4.3</b> Bayesian Information Criteria (BIC)</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="split-plot-designs.html"><a href="split-plot-designs.html"><i class="fa fa-check"></i><b>8.5</b> Split-Plot Designs</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="split-plot-designs.html"><a href="split-plot-designs.html#application-5"><i class="fa fa-check"></i><b>8.5.1</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="repeated-measures-in-mixed-models.html"><a href="repeated-measures-in-mixed-models.html"><i class="fa fa-check"></i><b>8.6</b> Repeated Measures in Mixed Models</a></li>
<li class="chapter" data-level="8.7" data-path="unbalanced-or-unequally-spaced-data.html"><a href="unbalanced-or-unequally-spaced-data.html"><i class="fa fa-check"></i><b>8.7</b> Unbalanced or Unequally Spaced Data</a></li>
<li class="chapter" data-level="8.8" data-path="application-6.html"><a href="application-6.html"><i class="fa fa-check"></i><b>8.8</b> Application</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="application-6.html"><a href="application-6.html#example-1-pulps"><i class="fa fa-check"></i><b>8.8.1</b> Example 1 (Pulps)</a></li>
<li class="chapter" data-level="8.8.2" data-path="application-6.html"><a href="application-6.html#example-2-rats"><i class="fa fa-check"></i><b>8.8.2</b> Example 2 (Rats)</a></li>
<li class="chapter" data-level="8.8.3" data-path="application-6.html"><a href="application-6.html#example-3-agridat"><i class="fa fa-check"></i><b>8.8.3</b> Example 3 (Agridat)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="nonlinear-and-generalized-linear-mixed-models.html"><a href="nonlinear-and-generalized-linear-mixed-models.html"><i class="fa fa-check"></i><b>9</b> Nonlinear and Generalized Linear Mixed Models</a>
<ul>
<li class="chapter" data-level="9.1" data-path="estimation-3.html"><a href="estimation-3.html"><i class="fa fa-check"></i><b>9.1</b> Estimation</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="estimation-3.html"><a href="estimation-3.html#estimation-by-numerical-integration"><i class="fa fa-check"></i><b>9.1.1</b> Estimation by Numerical Integration</a></li>
<li class="chapter" data-level="9.1.2" data-path="estimation-3.html"><a href="estimation-3.html#estimation-by-linearization"><i class="fa fa-check"></i><b>9.1.2</b> Estimation by Linearization</a></li>
<li class="chapter" data-level="9.1.3" data-path="estimation-3.html"><a href="estimation-3.html#estimation-by-bayesian-hierarchical-models"><i class="fa fa-check"></i><b>9.1.3</b> Estimation by Bayesian Hierarchical Models</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="application-7.html"><a href="application-7.html"><i class="fa fa-check"></i><b>9.2</b> Application</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="application-7.html"><a href="application-7.html#binomial-cbpp-data"><i class="fa fa-check"></i><b>9.2.1</b> Binomial (CBPP Data)</a></li>
<li class="chapter" data-level="9.2.2" data-path="application-7.html"><a href="application-7.html#count-owl-data"><i class="fa fa-check"></i><b>9.2.2</b> Count (Owl Data)</a></li>
<li class="chapter" data-level="9.2.3" data-path="application-7.html"><a href="application-7.html#binomial-1"><i class="fa fa-check"></i><b>9.2.3</b> Binomial</a></li>
<li class="chapter" data-level="9.2.4" data-path="application-7.html"><a href="application-7.html#example-from-schabenberger_2001-section-8.4.1"><i class="fa fa-check"></i><b>9.2.4</b> Example from <span class="citation">(<span>Schabenberger and Pierce 2001</span>)</span> section 8.4.1</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>9.3</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="generalized-method-of-moments.html"><a href="generalized-method-of-moments.html"><i class="fa fa-check"></i><b>10</b> Generalized Method of Moments</a></li>
<li class="chapter" data-level="11" data-path="minimum-distance.html"><a href="minimum-distance.html"><i class="fa fa-check"></i><b>11</b> Minimum Distance</a></li>
<li class="chapter" data-level="12" data-path="spline-regression.html"><a href="spline-regression.html"><i class="fa fa-check"></i><b>12</b> Spline Regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="regression-splines.html"><a href="regression-splines.html"><i class="fa fa-check"></i><b>12.1</b> Regression Splines</a></li>
<li class="chapter" data-level="12.2" data-path="natural-splines.html"><a href="natural-splines.html"><i class="fa fa-check"></i><b>12.2</b> Natural splines</a></li>
<li class="chapter" data-level="12.3" data-path="smoothing-splines.html"><a href="smoothing-splines.html"><i class="fa fa-check"></i><b>12.3</b> Smoothing splines</a></li>
<li class="chapter" data-level="12.4" data-path="application-8.html"><a href="application-8.html"><i class="fa fa-check"></i><b>12.4</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="generalized-additive-models.html"><a href="generalized-additive-models.html"><i class="fa fa-check"></i><b>13</b> Generalized Additive Models</a></li>
<li class="chapter" data-level="14" data-path="quantile-regression.html"><a href="quantile-regression.html"><i class="fa fa-check"></i><b>14</b> Quantile Regression</a>
<ul>
<li class="chapter" data-level="14.1" data-path="application-9.html"><a href="application-9.html"><i class="fa fa-check"></i><b>14.1</b> Application</a></li>
</ul></li>
<li class="part"><span><b>III RAMIFICATIONS</b></span></li>
<li class="chapter" data-level="15" data-path="model-specification.html"><a href="model-specification.html"><i class="fa fa-check"></i><b>15</b> Model Specification</a>
<ul>
<li class="chapter" data-level="15.1" data-path="nested-model.html"><a href="nested-model.html"><i class="fa fa-check"></i><b>15.1</b> Nested Model</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="nested-model.html"><a href="nested-model.html#chow-test"><i class="fa fa-check"></i><b>15.1.1</b> Chow test</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="non-nested-model.html"><a href="non-nested-model.html"><i class="fa fa-check"></i><b>15.2</b> Non-Nested Model</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="non-nested-model.html"><a href="non-nested-model.html#davidson-mackinnon-test"><i class="fa fa-check"></i><b>15.2.1</b> Davidson-Mackinnon test</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="heteroskedasticity-1.html"><a href="heteroskedasticity-1.html"><i class="fa fa-check"></i><b>15.3</b> Heteroskedasticity</a>
<ul>
<li class="chapter" data-level="15.3.1" data-path="heteroskedasticity-1.html"><a href="heteroskedasticity-1.html#breusch-pagan-test"><i class="fa fa-check"></i><b>15.3.1</b> Breusch-Pagan test</a></li>
<li class="chapter" data-level="15.3.2" data-path="heteroskedasticity-1.html"><a href="heteroskedasticity-1.html#white-test"><i class="fa fa-check"></i><b>15.3.2</b> White test</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="endogeneity.html"><a href="endogeneity.html"><i class="fa fa-check"></i><b>16</b> Endogeneity</a>
<ul>
<li class="chapter" data-level="16.1" data-path="endogenous-treatment.html"><a href="endogenous-treatment.html"><i class="fa fa-check"></i><b>16.1</b> Endogenous Treatment</a>
<ul>
<li class="chapter" data-level="16.1.1" data-path="endogenous-treatment.html"><a href="endogenous-treatment.html#instrumental-variable"><i class="fa fa-check"></i><b>16.1.1</b> Instrumental Variable</a></li>
<li class="chapter" data-level="16.1.2" data-path="endogenous-treatment.html"><a href="endogenous-treatment.html#internal-instrumental-variable"><i class="fa fa-check"></i><b>16.1.2</b> Internal instrumental variable</a></li>
<li class="chapter" data-level="16.1.3" data-path="endogenous-treatment.html"><a href="endogenous-treatment.html#proxy-variables"><i class="fa fa-check"></i><b>16.1.3</b> Proxy Variables</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="endogenous-sample-selection.html"><a href="endogenous-sample-selection.html"><i class="fa fa-check"></i><b>16.2</b> Endogenous Sample Selection</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="endogenous-sample-selection.html"><a href="endogenous-sample-selection.html#tobit-2"><i class="fa fa-check"></i><b>16.2.1</b> Tobit-2</a></li>
<li class="chapter" data-level="16.2.2" data-path="endogenous-sample-selection.html"><a href="endogenous-sample-selection.html#tobit-5"><i class="fa fa-check"></i><b>16.2.2</b> Tobit-5</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="imputation-missing-data.html"><a href="imputation-missing-data.html"><i class="fa fa-check"></i><b>17</b> Imputation (Missing Data)</a>
<ul>
<li class="chapter" data-level="17.1" data-path="assumptions-1.html"><a href="assumptions-1.html"><i class="fa fa-check"></i><b>17.1</b> Assumptions</a>
<ul>
<li class="chapter" data-level="17.1.1" data-path="assumptions-1.html"><a href="assumptions-1.html#missing-completely-at-random-mcar"><i class="fa fa-check"></i><b>17.1.1</b> Missing Completely at Random (MCAR)</a></li>
<li class="chapter" data-level="17.1.2" data-path="assumptions-1.html"><a href="assumptions-1.html#missing-at-random-mar"><i class="fa fa-check"></i><b>17.1.2</b> Missing at Random (MAR)</a></li>
<li class="chapter" data-level="17.1.3" data-path="assumptions-1.html"><a href="assumptions-1.html#ignorable"><i class="fa fa-check"></i><b>17.1.3</b> Ignorable</a></li>
<li class="chapter" data-level="17.1.4" data-path="assumptions-1.html"><a href="assumptions-1.html#nonignorable"><i class="fa fa-check"></i><b>17.1.4</b> Nonignorable</a></li>
</ul></li>
<li class="chapter" data-level="17.2" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html"><i class="fa fa-check"></i><b>17.2</b> Solutions to Missing data</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#listwise-deletion"><i class="fa fa-check"></i><b>17.2.1</b> Listwise Deletion</a></li>
<li class="chapter" data-level="17.2.2" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#pairwise-deletion"><i class="fa fa-check"></i><b>17.2.2</b> Pairwise Deletion</a></li>
<li class="chapter" data-level="17.2.3" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#dummy-variable-adjustment"><i class="fa fa-check"></i><b>17.2.3</b> Dummy Variable Adjustment</a></li>
<li class="chapter" data-level="17.2.4" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#imputation"><i class="fa fa-check"></i><b>17.2.4</b> Imputation</a></li>
<li class="chapter" data-level="17.2.5" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#other-methods"><i class="fa fa-check"></i><b>17.2.5</b> Other methods</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="criteria-for-choosing-an-effective-approach.html"><a href="criteria-for-choosing-an-effective-approach.html"><i class="fa fa-check"></i><b>17.3</b> Criteria for Choosing an Effective Approach</a></li>
<li class="chapter" data-level="17.4" data-path="another-perspective.html"><a href="another-perspective.html"><i class="fa fa-check"></i><b>17.4</b> Another Perspective</a></li>
<li class="chapter" data-level="17.5" data-path="diagnosing-the-mechanism.html"><a href="diagnosing-the-mechanism.html"><i class="fa fa-check"></i><b>17.5</b> Diagnosing the Mechanism</a>
<ul>
<li class="chapter" data-level="17.5.1" data-path="diagnosing-the-mechanism.html"><a href="diagnosing-the-mechanism.html#mar-vs.-mnar"><i class="fa fa-check"></i><b>17.5.1</b> MAR vs. MNAR</a></li>
<li class="chapter" data-level="17.5.2" data-path="diagnosing-the-mechanism.html"><a href="diagnosing-the-mechanism.html#mcar-vs.-mar"><i class="fa fa-check"></i><b>17.5.2</b> MCAR vs. MAR</a></li>
</ul></li>
<li class="chapter" data-level="17.6" data-path="application-10.html"><a href="application-10.html"><i class="fa fa-check"></i><b>17.6</b> Application</a>
<ul>
<li class="chapter" data-level="17.6.1" data-path="application-10.html"><a href="application-10.html#imputation-with-mean-median-mode"><i class="fa fa-check"></i><b>17.6.1</b> Imputation with mean / median / mode</a></li>
<li class="chapter" data-level="17.6.2" data-path="application-10.html"><a href="application-10.html#knn"><i class="fa fa-check"></i><b>17.6.2</b> KNN</a></li>
<li class="chapter" data-level="17.6.3" data-path="application-10.html"><a href="application-10.html#rpart"><i class="fa fa-check"></i><b>17.6.3</b> rpart</a></li>
<li class="chapter" data-level="17.6.4" data-path="application-10.html"><a href="application-10.html#mice-multivariate-imputation-via-chained-equations"><i class="fa fa-check"></i><b>17.6.4</b> MICE (Multivariate Imputation via Chained Equations)</a></li>
<li class="chapter" data-level="17.6.5" data-path="application-10.html"><a href="application-10.html#amelia"><i class="fa fa-check"></i><b>17.6.5</b> Amelia</a></li>
<li class="chapter" data-level="17.6.6" data-path="application-10.html"><a href="application-10.html#missforest"><i class="fa fa-check"></i><b>17.6.6</b> missForest</a></li>
<li class="chapter" data-level="17.6.7" data-path="application-10.html"><a href="application-10.html#hmisc"><i class="fa fa-check"></i><b>17.6.7</b> Hmisc</a></li>
<li class="chapter" data-level="17.6.8" data-path="application-10.html"><a href="application-10.html#mi"><i class="fa fa-check"></i><b>17.6.8</b> mi</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>18</b> Data</a>
<ul>
<li class="chapter" data-level="18.1" data-path="cross-sectional.html"><a href="cross-sectional.html"><i class="fa fa-check"></i><b>18.1</b> Cross-Sectional</a></li>
<li class="chapter" data-level="18.2" data-path="time-series-1.html"><a href="time-series-1.html"><i class="fa fa-check"></i><b>18.2</b> Time Series</a>
<ul>
<li class="chapter" data-level="18.2.1" data-path="time-series-1.html"><a href="time-series-1.html#deterministic-time-trend"><i class="fa fa-check"></i><b>18.2.1</b> Deterministic Time trend</a></li>
<li class="chapter" data-level="18.2.2" data-path="time-series-1.html"><a href="time-series-1.html#feedback-effect"><i class="fa fa-check"></i><b>18.2.2</b> Feedback Effect</a></li>
<li class="chapter" data-level="18.2.3" data-path="time-series-1.html"><a href="time-series-1.html#dynamic-specification"><i class="fa fa-check"></i><b>18.2.3</b> Dynamic Specification</a></li>
<li class="chapter" data-level="18.2.4" data-path="time-series-1.html"><a href="time-series-1.html#dynamically-complete"><i class="fa fa-check"></i><b>18.2.4</b> Dynamically Complete</a></li>
<li class="chapter" data-level="18.2.5" data-path="time-series-1.html"><a href="time-series-1.html#highly-persistent-data"><i class="fa fa-check"></i><b>18.2.5</b> Highly Persistent Data</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="repeated-cross-sections.html"><a href="repeated-cross-sections.html"><i class="fa fa-check"></i><b>18.3</b> Repeated Cross Sections</a>
<ul>
<li class="chapter" data-level="18.3.1" data-path="repeated-cross-sections.html"><a href="repeated-cross-sections.html#pooled-cross-section"><i class="fa fa-check"></i><b>18.3.1</b> Pooled Cross Section</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="panel-data.html"><a href="panel-data.html"><i class="fa fa-check"></i><b>18.4</b> Panel Data</a>
<ul>
<li class="chapter" data-level="18.4.1" data-path="panel-data.html"><a href="panel-data.html#pooled-ols-estimator"><i class="fa fa-check"></i><b>18.4.1</b> Pooled OLS Estimator</a></li>
<li class="chapter" data-level="18.4.2" data-path="panel-data.html"><a href="panel-data.html#individual-specific-effects-model"><i class="fa fa-check"></i><b>18.4.2</b> Individual-specific effects model</a></li>
<li class="chapter" data-level="18.4.3" data-path="panel-data.html"><a href="panel-data.html#tests-for-assumptions"><i class="fa fa-check"></i><b>18.4.3</b> Tests for Assumptions</a></li>
<li class="chapter" data-level="18.4.4" data-path="panel-data.html"><a href="panel-data.html#model-selection"><i class="fa fa-check"></i><b>18.4.4</b> Model Selection</a></li>
<li class="chapter" data-level="18.4.5" data-path="panel-data.html"><a href="panel-data.html#summary-2"><i class="fa fa-check"></i><b>18.4.5</b> Summary</a></li>
<li class="chapter" data-level="18.4.6" data-path="panel-data.html"><a href="panel-data.html#application-11"><i class="fa fa-check"></i><b>18.4.6</b> Application</a></li>
<li class="chapter" data-level="18.4.7" data-path="panel-data.html"><a href="panel-data.html#other-estimators"><i class="fa fa-check"></i><b>18.4.7</b> Other Estimators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>19</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="19.1" data-path="types-of-hypothesis-testing.html"><a href="types-of-hypothesis-testing.html"><i class="fa fa-check"></i><b>19.1</b> Types of hypothesis testing</a></li>
<li class="chapter" data-level="19.2" data-path="wald-test.html"><a href="wald-test.html"><i class="fa fa-check"></i><b>19.2</b> Wald test</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="wald-test.html"><a href="wald-test.html#multiple-hypothesis"><i class="fa fa-check"></i><b>19.2.1</b> Multiple Hypothesis</a></li>
<li class="chapter" data-level="19.2.2" data-path="wald-test.html"><a href="wald-test.html#linear-combination"><i class="fa fa-check"></i><b>19.2.2</b> Linear Combination</a></li>
<li class="chapter" data-level="19.2.3" data-path="wald-test.html"><a href="wald-test.html#application-12"><i class="fa fa-check"></i><b>19.2.3</b> Application</a></li>
<li class="chapter" data-level="19.2.4" data-path="wald-test.html"><a href="wald-test.html#nonlinear-1"><i class="fa fa-check"></i><b>19.2.4</b> Nonlinear</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="the-likelihood-ratio-test.html"><a href="the-likelihood-ratio-test.html"><i class="fa fa-check"></i><b>19.3</b> The likelihood ratio test</a></li>
<li class="chapter" data-level="19.4" data-path="lagrange-multiplier-score.html"><a href="lagrange-multiplier-score.html"><i class="fa fa-check"></i><b>19.4</b> Lagrange Multiplier (Score)</a></li>
</ul></li>
<li class="part"><span><b>IV EXPERIMENTAL DESIGN</b></span></li>
<li class="chapter" data-level="20" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html"><i class="fa fa-check"></i><b>20</b> Analysis of Variance (ANOVA)</a>
<ul>
<li class="chapter" data-level="20.1" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html"><i class="fa fa-check"></i><b>20.1</b> Completely Randomized Design (CRD)</a>
<ul>
<li class="chapter" data-level="20.1.1" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#single-factor-fixed-effects-model"><i class="fa fa-check"></i><b>20.1.1</b> Single Factor Fixed Effects Model</a></li>
<li class="chapter" data-level="20.1.2" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#single-factor-random-effects-model"><i class="fa fa-check"></i><b>20.1.2</b> Single Factor Random Effects Model</a></li>
<li class="chapter" data-level="20.1.3" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#two-factor-fixed-effect-anova"><i class="fa fa-check"></i><b>20.1.3</b> Two Factor Fixed Effect ANOVA</a></li>
<li class="chapter" data-level="20.1.4" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#two-way-random-effects-anova"><i class="fa fa-check"></i><b>20.1.4</b> Two-Way Random Effects ANOVA</a></li>
<li class="chapter" data-level="20.1.5" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#two-way-mixed-effects-anova"><i class="fa fa-check"></i><b>20.1.5</b> Two-Way Mixed Effects ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="nonparametric-anova.html"><a href="nonparametric-anova.html"><i class="fa fa-check"></i><b>20.2</b> Nonparametric ANOVA</a>
<ul>
<li class="chapter" data-level="20.2.1" data-path="nonparametric-anova.html"><a href="nonparametric-anova.html#kruskal-wallis"><i class="fa fa-check"></i><b>20.2.1</b> Kruskal-Wallis</a></li>
<li class="chapter" data-level="20.2.2" data-path="nonparametric-anova.html"><a href="nonparametric-anova.html#friedman-test"><i class="fa fa-check"></i><b>20.2.2</b> Friedman Test</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="sample-size-planning-for-anova.html"><a href="sample-size-planning-for-anova.html"><i class="fa fa-check"></i><b>20.3</b> Sample Size Planning for ANOVA</a>
<ul>
<li class="chapter" data-level="20.3.1" data-path="sample-size-planning-for-anova.html"><a href="sample-size-planning-for-anova.html#balanced-designs"><i class="fa fa-check"></i><b>20.3.1</b> Balanced Designs</a></li>
<li class="chapter" data-level="20.3.2" data-path="sample-size-planning-for-anova.html"><a href="sample-size-planning-for-anova.html#randomized-block-experiments"><i class="fa fa-check"></i><b>20.3.2</b> Randomized Block Experiments</a></li>
</ul></li>
<li class="chapter" data-level="20.4" data-path="randomized-block-designs.html"><a href="randomized-block-designs.html"><i class="fa fa-check"></i><b>20.4</b> Randomized Block Designs</a>
<ul>
<li class="chapter" data-level="20.4.1" data-path="randomized-block-designs.html"><a href="randomized-block-designs.html#tukey-test-of-additivity"><i class="fa fa-check"></i><b>20.4.1</b> Tukey Test of Additivity</a></li>
</ul></li>
<li class="chapter" data-level="20.5" data-path="nested-designs.html"><a href="nested-designs.html"><i class="fa fa-check"></i><b>20.5</b> Nested Designs</a>
<ul>
<li class="chapter" data-level="20.5.1" data-path="nested-designs.html"><a href="nested-designs.html#two-factor-nested-designs"><i class="fa fa-check"></i><b>20.5.1</b> Two-Factor Nested Designs</a></li>
</ul></li>
<li class="chapter" data-level="20.6" data-path="single-factor-covariance-model.html"><a href="single-factor-covariance-model.html"><i class="fa fa-check"></i><b>20.6</b> Single Factor Covariance Model</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="multivariate-methods.html"><a href="multivariate-methods.html"><i class="fa fa-check"></i><b>21</b> Multivariate Methods</a>
<ul>
<li class="chapter" data-level="21.0.1" data-path="multivariate-methods.html"><a href="multivariate-methods.html#properties-of-mvn"><i class="fa fa-check"></i><b>21.0.1</b> <strong>Properties of MVN</strong></a></li>
<li class="chapter" data-level="21.0.2" data-path="multivariate-methods.html"><a href="multivariate-methods.html#mean-vector-inference"><i class="fa fa-check"></i><b>21.0.2</b> Mean Vector Inference</a></li>
<li class="chapter" data-level="21.0.3" data-path="multivariate-methods.html"><a href="multivariate-methods.html#general-hypothesis-testing"><i class="fa fa-check"></i><b>21.0.3</b> General Hypothesis Testing</a></li>
<li class="chapter" data-level="21.1" data-path="manova.html"><a href="manova.html"><i class="fa fa-check"></i><b>21.1</b> MANOVA</a></li>
<li class="chapter" data-level="21.2" data-path="principal-components.html"><a href="principal-components.html"><i class="fa fa-check"></i><b>21.2</b> Principal Components</a></li>
<li class="chapter" data-level="21.3" data-path="factor-analysis.html"><a href="factor-analysis.html"><i class="fa fa-check"></i><b>21.3</b> Factor Analysis</a></li>
<li class="chapter" data-level="21.4" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html"><i class="fa fa-check"></i><b>21.4</b> Discriminant Analysis</a></li>
<li class="chapter" data-level="21.5" data-path="cluster-analysis.html"><a href="cluster-analysis.html"><i class="fa fa-check"></i><b>21.5</b> Cluster Analysis</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="causality.html"><a href="causality.html"><i class="fa fa-check"></i><b>22</b> Causality</a></li>
<li class="chapter" data-level="23" data-path="report.html"><a href="report.html"><i class="fa fa-check"></i><b>23</b> Report</a>
<ul>
<li class="chapter" data-level="23.1" data-path="one-summary-table.html"><a href="one-summary-table.html"><i class="fa fa-check"></i><b>23.1</b> One summary table</a></li>
<li class="chapter" data-level="23.2" data-path="model-comparison.html"><a href="model-comparison.html"><i class="fa fa-check"></i><b>23.2</b> Model Comparison</a></li>
<li class="chapter" data-level="23.3" data-path="changes-in-an-estimate.html"><a href="changes-in-an-estimate.html"><i class="fa fa-check"></i><b>23.3</b> Changes in an estimate</a></li>
</ul></li>
<li class="appendix"><span><b>APPENDIX</b></span></li>
<li class="chapter" data-level="A" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>A</b> Appendix</a>
<ul>
<li class="chapter" data-level="A.1" data-path="git.html"><a href="git.html"><i class="fa fa-check"></i><b>A.1</b> Git</a></li>
<li class="chapter" data-level="A.2" data-path="short-cut.html"><a href="short-cut.html"><i class="fa fa-check"></i><b>A.2</b> Short-cut</a></li>
<li class="chapter" data-level="A.3" data-path="function-short-cut.html"><a href="function-short-cut.html"><i class="fa fa-check"></i><b>A.3</b> Function short-cut</a></li>
<li class="chapter" data-level="A.4" data-path="citation.html"><a href="citation.html"><i class="fa fa-check"></i><b>A.4</b> Citation</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="bookdown-cheat-sheet.html"><a href="bookdown-cheat-sheet.html"><i class="fa fa-check"></i><b>B</b> Bookdown cheat sheet</a>
<ul>
<li class="chapter" data-level="B.1" data-path="operation.html"><a href="operation.html"><i class="fa fa-check"></i><b>B.1</b> Operation</a></li>
<li class="chapter" data-level="B.2" data-path="math-expresssion-syntax.html"><a href="math-expresssion-syntax.html"><i class="fa fa-check"></i><b>B.2</b> Math Expresssion/ Syntax</a>
<ul>
<li class="chapter" data-level="B.2.1" data-path="math-expresssion-syntax.html"><a href="math-expresssion-syntax.html#statistics-notation"><i class="fa fa-check"></i><b>B.2.1</b> Statistics Notation</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="table.html"><a href="table.html"><i class="fa fa-check"></i><b>B.3</b> Table</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Guide on Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multivariate-methods" class="section level1" number="21">
<h1><span class="header-section-number">Chapter 21</span> Multivariate Methods</h1>
<p><span class="math inline">\(y_1,...,y_p\)</span> are possibly correlated random variables with means <span class="math inline">\(\mu_1,...,\mu_p\)</span></p>
<p><span class="math display">\[
\mathbf{y} = 
\left(
\begin{array}
{c}
y_1 \\
. \\
y_p \\
\end{array}
\right)
\]</span></p>
<p><span class="math display">\[
E(\mathbf{y}) = 
\left(
\begin{array}
{c}
\mu_1 \\
. \\
\mu_p \\
\end{array}
\right)
\]</span></p>
<p>Let <span class="math inline">\(\sigma_{ij} = cov(y_i, y_j)\)</span> for <span class="math inline">\(i,j = 1,…,p\)</span></p>
<p><span class="math display">\[
\mathbf{\Sigma} = (\sigma_{ij}) = 
\left(
\begin{array}
{cccc}
\sigma_{11} &amp; \sigma_{22} &amp; ... &amp;  \sigma_{1p} \\
\sigma_{21} &amp; \sigma_{22} &amp; ... &amp; \sigma_{2p} \\
. &amp; . &amp; . &amp; . \\
\sigma_{p1} &amp; \sigma_{p2} &amp; ... &amp; \sigma_{pp}
\end{array}
\right)
\]</span></p>
<p>where <span class="math inline">\(\mathbf{\Sigma}\)</span> (symmetric) is the variance-covariance or dispersion matrix</p>
<p>Let <span class="math inline">\(\mathbf{u}_{p \times 1}\)</span> and <span class="math inline">\(\mathbf{v}_{q \times 1}\)</span> be random vectors with means <span class="math inline">\(\mu_u\)</span> and <span class="math inline">\(\mu_v\)</span> . Then</p>
<p><span class="math display">\[
\mathbf{\Sigma}_{uv} = cov(\mathbf{u,v}) = E[(\mathbf{u} - \mu_u)(\mathbf{v} - \mu_v)&#39;]
\]</span></p>
<p>in which <span class="math inline">\(\mathbf{\Sigma}_{uv} \neq \mathbf{\Sigma}_{vu}\)</span> and <span class="math inline">\(\mathbf{\Sigma}_{uv} = \mathbf{\Sigma}_{vu}&#39;\)</span></p>
<p><br />
<strong>Properties of Covariance Matrices</strong></p>
<ol style="list-style-type: decimal">
<li>Symmetric <span class="math inline">\(\mathbf{\Sigma}&#39; = \mathbf{\Sigma}\)</span></li>
<li>Non-negative definite <span class="math inline">\(\mathbf{a&#39;\Sigma a} \ge 0\)</span> for any <span class="math inline">\(\mathbf{a} \in R^p\)</span>, which is equivalent to eigenvalues of <span class="math inline">\(\mathbf{\Sigma}\)</span>, <span class="math inline">\(\lambda_1 \ge \lambda_2 \ge ... \ge \lambda_p \ge 0\)</span></li>
<li><span class="math inline">\(|\mathbf{\Sigma}| = \lambda_1 \lambda_2 ... \lambda_p \ge 0\)</span> (<strong>generalized variance</strong>) (the bigger this number is, the more variation there is</li>
<li><span class="math inline">\(trace(\mathbf{\Sigma}) = tr(\mathbf{\Sigma}) = \lambda_1 + ... + \lambda_p = \sigma_{11} + ... + \sigma_{pp} =\)</span> sum of variance (<strong>total variance</strong>)</li>
</ol>
<p>Note:</p>
<ul>
<li><span class="math inline">\(\mathbf{\Sigma}\)</span> is typically required to be positive definite, which means all eigenvalues are positive, and <span class="math inline">\(\mathbf{\Sigma}\)</span> has an inverse <span class="math inline">\(\mathbf{\Sigma}^{-1}\)</span> such that <span class="math inline">\(\mathbf{\Sigma}^{-1}\mathbf{\Sigma} = \mathbf{I}_{p \times p} = \mathbf{\Sigma \Sigma}^{-1}\)</span></li>
</ul>
<p><br></p>
<p><strong>Correlation Matrices</strong></p>
<p><span class="math display">\[
\rho_{ij} = \frac{\sigma_{ij}}{\sqrt{\sigma_{ii} \sigma_{jj}}}
\]</span></p>
<p><span class="math display">\[
\mathbf{R} = 
\left(
\begin{array}
{cccc}
\rho_{11} &amp; \rho_{12} &amp; ... &amp; \rho_{1p} \\
\rho_{21} &amp; \rho_{22} &amp; ... &amp; \rho_{2p} \\
. &amp; . &amp; . &amp;. \\
\rho_{p1} &amp; \rho_{p2} &amp; ... &amp; \rho_{pp} \\
\end{array}
\right)
\]</span></p>
<p>where <span class="math inline">\(\rho_{ij}\)</span> is the correlation, and <span class="math inline">\(\rho_{ii} = 1\)</span> for all i</p>
<p>Alternatively,</p>
<p><span class="math display">\[
\mathbf{R} = [diag(\mathbf{\Sigma})]^{-1/2}\mathbf{\Sigma}[diag(\mathbf{\Sigma})]^{-1/2}
\]</span></p>
<p>where <span class="math inline">\(diag(\mathbf{\Sigma})\)</span> is the matrix which has the <span class="math inline">\(\sigma_{ii}\)</span>’s on the diagonal and 0’s elsewhere</p>
<p>and <span class="math inline">\(\mathbf{A}^{1/2}\)</span> (the square root of a symmetric matrix) is a symmetric matrix such as <span class="math inline">\(\mathbf{A} = \mathbf{A}^{1/2}\mathbf{A}^{1/2}\)</span></p>
<p><strong>Equalities</strong></p>
<p>Let</p>
<ul>
<li><p><span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> be random vectors with means <span class="math inline">\(\mu_x\)</span> and <span class="math inline">\(\mu_y\)</span> and variance -variance matrices <span class="math inline">\(\mathbf{\Sigma}_x\)</span> and <span class="math inline">\(\mathbf{\Sigma}_y\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span> be matrices of constants and <span class="math inline">\(\mathbf{c}\)</span> and <span class="math inline">\(\mathbf{d}\)</span> be vectors of constants</p></li>
</ul>
<p>Then</p>
<ul>
<li><p><span class="math inline">\(E(\mathbf{Ay + c} ) = \mathbf{A} \mu_y + c\)</span></p></li>
<li><p><span class="math inline">\(var(\mathbf{Ay + c}) = \mathbf{A} var(\mathbf{y})\mathbf{A}&#39; = \mathbf{A \Sigma_y A}&#39;\)</span></p></li>
<li><p><span class="math inline">\(cov(\mathbf{Ay + c, By+ d}) = \mathbf{A\Sigma_y B}&#39;\)</span></p></li>
<li><p><span class="math inline">\(E(\mathbf{Ay + Bx + c}) = \mathbf{A \mu_y + B \mu_x + c}\)</span></p></li>
<li><p><span class="math inline">\(var(\mathbf{Ay + Bx + c}) = \mathbf{A \Sigma_y A&#39; + B \Sigma_x B&#39; + A \Sigma_{yx}B&#39; + B\Sigma&#39;_{yx}A&#39;}\)</span></p></li>
</ul>
<p><strong>Multivariate Normal Distribution</strong></p>
<p>Let <span class="math inline">\(\mathbf{y}\)</span> be a multivariate normal (MVN) random variable with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\mathbf{\Sigma}\)</span>. Then the density of <span class="math inline">\(\mathbf{y}\)</span> is</p>
<p><span class="math display">\[
f(\mathbf{y}) = \frac{1}{(2\pi)^{p/2}|\mathbf{\Sigma}|^{1/2}} \exp(-\frac{1}{2} \mathbf{(y-\mu)&#39;\Sigma^{-1}(y-\mu)} )
\]</span></p>
<p><span class="math inline">\(\mathbf{y} \sim N_p(\mu, \mathbf{\Sigma})\)</span></p>
<div id="properties-of-mvn" class="section level3" number="21.0.1">
<h3><span class="header-section-number">21.0.1</span> <strong>Properties of MVN</strong></h3>
<ul>
<li><p>Let <span class="math inline">\(\mathbf{A}_{r \times p}\)</span> be a fixed matrix. Then <span class="math inline">\(\mathbf{Ay} \sim N_r (\mathbf{A \mu, A \Sigma A&#39;})\)</span> . <span class="math inline">\(r \le p\)</span> and all rows of <span class="math inline">\(\mathbf{A}\)</span> must be linearly independent to guarantee that <span class="math inline">\(\mathbf{A \Sigma A}&#39;\)</span> is non-singular.</p></li>
<li><p>Let <span class="math inline">\(\mathbf{G}\)</span> be a matrix such that <span class="math inline">\(\mathbf{\Sigma}^{-1} = \mathbf{GG}&#39;\)</span>. Then <span class="math inline">\(\mathbf{G&#39;y} \sim N_p(\mathbf{G&#39; \mu, I})\)</span> and <span class="math inline">\(\mathbf{G&#39;(y-\mu)} \sim N_p (0,\mathbf{I})\)</span></p></li>
<li><p>Any fixed linear combination of <span class="math inline">\(y_1,...,y_p\)</span> (say <span class="math inline">\(\mathbf{c&#39;y}\)</span>) follows <span class="math inline">\(\mathbf{c&#39;y} \sim N_1 (\mathbf{c&#39; \mu, c&#39; \Sigma c})\)</span></p></li>
<li><p>Define a partition, <span class="math inline">\([\mathbf{y}&#39;_1,\mathbf{y}_2&#39;]&#39;\)</span> where <span class="math inline">\(\mathbf{y}\)</span>_1 is <span class="math inline">\(p_1 \times 1\)</span> ,<span class="math inline">\(\mathbf{y}_2\)</span> is <span class="math inline">\(p_2 \times 1\)</span>, <span class="math inline">\(p_1 + p_2 = p\)</span> and <span class="math inline">\(p_1,p_2 \ge 1\)</span> Then</p></li>
</ul>
<p><span class="math display">\[
\left(
\begin{array}
{c}
\mathbf{y}_1 \\
\mathbf{y}_2 \\
\end{array}
\right)
\sim
N
\left(
\left(
\begin{array}
{c}
\mu_1 \\
\mu_2 \\
\end{array}
\right),
\left(
\begin{array}
{cc}
\mathbf{\Sigma}_{11} &amp; \mathbf{\Sigma}_{12} \\
\mathbf{\Sigma}_{21} &amp; \mathbf{\Sigma}_{22}\\
\end{array}
\right)
\right)
\]</span></p>
<ul>
<li><p>The marginal distributions of <span class="math inline">\(\mathbf{y}_1\)</span> and <span class="math inline">\(\mathbf{y}_2\)</span> are <span class="math inline">\(\mathbf{y}_1 \sim N_{p1}(\mathbf{\mu_1, \Sigma_{11}})\)</span> and <span class="math inline">\(\mathbf{y}_2 \sim N_{p2}(\mathbf{\mu_2, \Sigma_{22}})\)</span></p></li>
<li><p>Individual components <span class="math inline">\(y_1,...,y_p\)</span> are all normally distributed <span class="math inline">\(y_i \sim N_1(\mu_i, \sigma_{ii})\)</span></p></li>
<li><p>The conditional distribution of <span class="math inline">\(\mathbf{y}_1\)</span> and <span class="math inline">\(\mathbf{y}_2\)</span> is normal</p>
<ul>
<li><p><span class="math inline">\(\mathbf{y}_1 | \mathbf{y}_2 \sim N_{p1}(\mathbf{\mu_1 + \Sigma_{12} \Sigma_{22}^{-1}(y_2 - \mu_2),\Sigma_{11} - \Sigma_{12} \Sigma_{22}^{-1} \sigma_{21}})\)</span></p>
<ul>
<li>In this formula, we see if we know (have info about) <span class="math inline">\(\mathbf{y}_2\)</span>, we can re-weight <span class="math inline">\(\mathbf{y}_1\)</span> ’s mean, and the variance is reduced because we know more about <span class="math inline">\(\mathbf{y}_1\)</span> because we know <span class="math inline">\(\mathbf{y}_2\)</span></li>
</ul></li>
<li><p>which is analogous to <span class="math inline">\(\mathbf{y}_2 | \mathbf{y}_1\)</span>. And <span class="math inline">\(\mathbf{y}_1\)</span> and <span class="math inline">\(\mathbf{y}_2\)</span> are independently distrusted only if <span class="math inline">\(\mathbf{\Sigma}_{12} = 0\)</span></p></li>
</ul></li>
<li><p>If <span class="math inline">\(\mathbf{y} \sim N(\mathbf{\mu, \Sigma})\)</span> and <span class="math inline">\(\mathbf{\Sigma}\)</span> is positive definite, then <span class="math inline">\(\mathbf{(y-\mu)&#39; \Sigma^{-1} (y - \mu)} \sim \chi^2_{(p)}\)</span></p></li>
<li><p>If <span class="math inline">\(\mathbf{y}_i\)</span> are independent <span class="math inline">\(N_p (\mathbf{\mu}_i , \mathbf{\Sigma}_i)\)</span> random variables, then for fixed matrices <span class="math inline">\(\mathbf{A}_{i(m \times p)}\)</span>, <span class="math inline">\(\sum_{i=1}^k \mathbf{A}_i \mathbf{y}_i \sim N_m (\sum_{i=1}^{k} \mathbf{A}_i \mathbf{\mu}_i, \sum_{i=1}^k \mathbf{A}_i \mathbf{\Sigma}_i \mathbf{A}_i)\)</span></p></li>
</ul>
<p><strong>Multiple Regression</strong></p>
<p><span class="math display">\[
\left(
\begin{array}
{c}
Y \\
\mathbf{x}
\end{array}
\right)
\sim 
N_{p+1}
\left(
\left[
\begin{array}
{c}
\mu_y \\
\mathbf{\mu}_x
\end{array}
\right]
,
\left[
\begin{array}
{cc}
\sigma^2_Y &amp; \mathbf{\Sigma}_{yx} \\
\mathbf{\Sigma}_{yx} &amp; \mathbf{\Sigma}_{xx}
\end{array}
\right]
\right)
\]</span></p>
<p>The conditional distribution of Y given x follows a univariate normal distribution with</p>
<p><span class="math display">\[
\begin{aligned}
E(Y| \mathbf{x}) &amp;= \mu_y + \mathbf{\Sigma}_{yx} \Sigma_{xx}^{-1} (\mathbf{x}- \mu_x) \\
&amp;= \mu_y - \Sigma_{yx} \Sigma_{xx}^{-1}\mu_x + \Sigma_{yx} \Sigma_{xx}^{-1}\mathbf{x} \\
&amp;= \beta_0 + \mathbf{\beta&#39;x}
\end{aligned} 
\]</span></p>
<p>where <span class="math inline">\(\beta = (\beta_1,...,\beta_p)&#39; = \mathbf{\Sigma}_{xx}^{-1} \mathbf{\Sigma}_{yx}&#39;\)</span> (e.g., analogous to <span class="math inline">\(\mathbf{(x&#39;x)^{-1}x&#39;y}\)</span> but not the same if we consider <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(\mathbf{x}_i\)</span>, <span class="math inline">\(i = 1,..,n\)</span> and use the empirical covariance formula: <span class="math inline">\(var(Y|\mathbf{x}) = \sigma^2_Y - \mathbf{\Sigma_{yx}\Sigma^{-1}_{xx} \Sigma&#39;_{yx}}\)</span>)</p>
<p><br></p>
<p><strong>Samples from Multivariate Normal Populations</strong></p>
<p>A random sample of size n, <span class="math inline">\(\mathbf{y}_1,.., \mathbf{y}_n\)</span> from <span class="math inline">\(N_p (\mathbf{\mu}, \mathbf{\Sigma})\)</span>. Then</p>
<ul>
<li><p>Since <span class="math inline">\(\mathbf{y}_1,..., \mathbf{y}_n\)</span> are iid, their sample mean, <span class="math inline">\(\bar{\mathbf{y}} = \sum_{i=1}^n \mathbf{y}_i/n \sim N_p (\mathbf{\mu}, \mathbf{\Sigma}/n)\)</span>. that is, <span class="math inline">\(\bar{\mathbf{y}}\)</span> is an unbiased estimator of <span class="math inline">\(\mathbf{\mu}\)</span></p></li>
<li><p>The <span class="math inline">\(p \times p\)</span> sample variance-covariance matrix, <span class="math inline">\(\mathbf{S}\)</span> is <span class="math inline">\(\mathbf{S} = \frac{1}{n-1}\sum_{i=1}^n (\mathbf{y}_i - \bar{\mathbf{y}})(\mathbf{y}_i - \bar{\mathbf{y}})&#39; = \frac{1}{n-1} (\sum_{i=1}^n \mathbf{y}_i \mathbf{y}_i&#39; - n \bar{\mathbf{y}}\bar{\mathbf{y}}&#39;)\)</span></p>
<ul>
<li>where <span class="math inline">\(\mathbf{S}\)</span> is symmetric, unbiased estimator of <span class="math inline">\(\mathbf{\Sigma}\)</span> and has <span class="math inline">\(p(p+1)/2\)</span> random variables.</li>
</ul></li>
<li><p><span class="math inline">\((n-1)\mathbf{S} \sim W_p (n-1, \mathbf{\Sigma})\)</span> is a Wishart distribution with n-1 degrees of freedom and expectation <span class="math inline">\((n-1) \mathbf{\Sigma}\)</span>. The Wishart distribution is a multivariate extension of the Chi-squared distribution.</p></li>
<li><p><span class="math inline">\(\bar{\mathbf{y}}\)</span> and <span class="math inline">\(\mathbf{S}\)</span> are independent</p></li>
<li><p><span class="math inline">\(\bar{\mathbf{y}}\)</span> and <span class="math inline">\(\mathbf{S}\)</span> are sufficient statistics. (All of the info in the data about <span class="math inline">\(\mathbf{\mu}\)</span> and <span class="math inline">\(\mathbf{\Sigma}\)</span> is contained in <span class="math inline">\(\bar{\mathbf{y}}\)</span> and <span class="math inline">\(\mathbf{S}\)</span> , regardless of sample size).</p></li>
</ul>
<p><br></p>
<p><strong>Large Sample Properties</strong></p>
<p><span class="math inline">\(\mathbf{y}_1,..., \mathbf{y}_n\)</span> are a random sample from some population with mean <span class="math inline">\(\mathbf{\mu}\)</span> and variance-covariance matrix <span class="math inline">\(\mathbf{\Sigma}\)</span></p>
<ul>
<li><p><span class="math inline">\(\bar{\mathbf{y}}\)</span> is a consistent estimator for <span class="math inline">\(\mu\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{S}\)</span> is a consistent estimator for <span class="math inline">\(\mathbf{\Sigma}\)</span></p></li>
<li><p><strong>Multivariate Central Limit Theorem</strong>: Similar to the univariate case, <span class="math inline">\(\sqrt{n}(\bar{\mathbf{y}} - \mu) \dot{\sim} N_p (\mathbf{0,\Sigma})\)</span> where n is large relative to p (<span class="math inline">\(n \ge 25p\)</span>), which is equivalent to <span class="math inline">\(\bar{\mathbf{y}} \dot{\sim} N_p (\mu, \mathbf{\Sigma}/n)\)</span></p></li>
<li><p><strong>Wald’s Theorem</strong>: <span class="math inline">\(n(\bar{\mathbf{y}} - \mu)&#39; \mathbf{S}^{-1} (\bar{\mathbf{y}} - \mu)\)</span> when n is large relative to p.</p></li>
</ul>
<p><br></p>
<p>Maximum Likelihood Estimation for MVN</p>
<p>Suppose iid <span class="math inline">\(\mathbf{y}_1 ,... \mathbf{y}_n \sim N_p (\mu, \mathbf{\Sigma})\)</span>, the likelihood function for the data is</p>
<p><span class="math display">\[
\begin{aligned}
L(\mu, \mathbf{\Sigma}) &amp;= \prod_{j=1}^n (\frac{1}{(2\pi)^{p/2}|\mathbf{\Sigma}|^{1/2}} \exp(-\frac{1}{2}(\mathbf{y}_j -\mu)&#39;\mathbf{\Sigma}^{-1})(\mathbf{y}_j -\mu)) \\
&amp;= \frac{1}{(2\pi)^{np/2}|\mathbf{\Sigma}|^{n/2}} \exp(-\frac{1}{2} \sum_{j=1}^n(\mathbf{y}_j -\mu)&#39;\mathbf{\Sigma}^{-1})(\mathbf{y}_j -\mu)
\end{aligned}
\]</span></p>
<p>Then, the MLEs are</p>
<p><span class="math display">\[
\hat{\mu} = \bar{\mathbf{y}}
\]</span></p>
<p><span class="math display">\[
\hat{\mathbf{\Sigma}} = \frac{n-1}{n} \mathbf{S}
\]</span></p>
<p>using derivatives of the log of the likelihood function with respect to <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\mathbf{\Sigma}\)</span></p>
<p><br></p>
<p><strong>Properties of MLEs</strong></p>
<ul>
<li><p>Invariance: If <span class="math inline">\(\hat{\theta}\)</span> is the MLE of <span class="math inline">\(\theta\)</span>, then the MLE of <span class="math inline">\(h(\theta)\)</span> is <span class="math inline">\(h(\hat{\theta})\)</span> for any function h(.)</p></li>
<li><p>Consistency: MLEs are consistent estimators, but they are usually biased</p></li>
<li><p>Efficiency: MLEs are efficient estimators (no other estimator has a smaller variance for large samples)</p></li>
<li><p>Asymptotic normality: Suppose that <span class="math inline">\(\hat{\theta}_n\)</span> is the MLE for <span class="math inline">\(\theta\)</span> based upon n independent observations. Then <span class="math inline">\(\hat{\theta}_n \dot{\sim} N(\theta, \mathbf{H}^{-1})\)</span></p>
<ul>
<li><p><span class="math inline">\(\mathbf{H}\)</span> is the Fisher Information Matrix, which contains the expected values of the second partial derivatives fo the log-likelihood function. the (i,j)th element of <span class="math inline">\(\mathbf{H}\)</span> is <span class="math inline">\(-E(\frac{\partial^2 l(\mathbf{\theta})}{\partial \theta_i \partial \theta_j})\)</span></p></li>
<li><p>we can estimate <span class="math inline">\(\mathbf{H}\)</span> by finding the form determined above, and evaluate it at <span class="math inline">\(\theta = \hat{\theta}_n\)</span></p></li>
</ul></li>
<li><p>Likelihood ratio testing: for some null hypothesis, <span class="math inline">\(H_0\)</span> we can form a likelihood ratio test</p>
<ul>
<li><p>The statistic is: <span class="math inline">\(\Lambda = \frac{\max_{H_0}l(\mathbf{\mu}, \mathbf{\Sigma|Y})}{\max l(\mu, \mathbf{\Sigma | Y})}\)</span></p></li>
<li><p>For large n, <span class="math inline">\(-2 \log \Lambda \sim \chi^2_{(v)}\)</span> where v is the number of parameters in the unrestricted space minus the number of parameters under <span class="math inline">\(H_0\)</span></p></li>
</ul></li>
</ul>
<p><br></p>
<p><strong>Test of Multivariate Normality</strong></p>
<ul>
<li><p>Check univariate normality for each trait (X) separately</p>
<ul>
<li><p>Can check <a href="normality-assessment.html#normality-assessment">Normality Assessment</a></p></li>
<li><p>The good thing is that if any of the univariate trait is not normal, then the joint distribution is not normal (see again [Properties of MVN]). If a joint multivariate distribution is normal, then the marginal distribution has to be normal.</p></li>
<li><p>However, marginal normality of all traits does not imply joint MVN</p></li>
<li><p>Easily rule out multivariate normality, but not easy to prove it</p></li>
</ul></li>
<li><p>Mardia’s tests for multivariate normality</p>
<ul>
<li><p>Multivariate skewness is<span class="math display">\[
\beta_{1,p} = E[(\mathbf{y}- \mathbf{\mu})&#39; \mathbf{\Sigma}^{-1} (\mathbf{x} - \mathbf{\mu})]^3
\]</span></p></li>
<li><p>where <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are independent, but have the same distribution (note: <span class="math inline">\(\beta\)</span> here is not regression coefficient)</p></li>
<li><p>Multivariate kurtosis is defined as</p></li>
<li><p><span class="math display">\[
\beta_{2,p} - E[(\mathbf{y}- \mathbf{\mu})&#39; \mathbf{\Sigma}^{-1} (\mathbf{x} - \mathbf{\mu})]^2
\]</span></p></li>
<li><p>For the MVN distribution, we have <span class="math inline">\(\beta_{1,p} = 0\)</span> and <span class="math inline">\(\beta_{2,p} = p(p+2)\)</span></p></li>
<li><p>For a sample of size n, we can estimate</p>
<p><span class="math display">\[
\hat{\beta}_{1,p} = \frac{1}{n^2}\sum_{i=1}^n \sum_{j=1}^n g^2_{ij}
\]</span></p>
<p><span class="math display">\[
\hat{\beta}_{2,p} = \frac{1}{n} \sum_{i=1}^n g^2_{ii}
\]</span></p>
<ul>
<li>where <span class="math inline">\(g_{ij} = (\mathbf{y}_i - \bar{\mathbf{y}})&#39; \mathbf{S}^{-1} (\mathbf{y}_j - \bar{\mathbf{y}})\)</span>. Note: <span class="math inline">\(g_{ii} = d^2_i\)</span> where <span class="math inline">\(d^2_i\)</span> is the Mahalanobis distance</li>
</ul></li>
<li><p><span class="citation">(<a href="references.html#ref-MARDIA_1970" role="doc-biblioref">MARDIA 1970</a>)</span> shows for large n</p>
<p><span class="math display">\[
\kappa_1 = \frac{n \hat{\beta}_{1,p}}{6} \dot{\sim} \chi^2_{p(p+1)(p+2)/6}
\]</span></p>
<p><span class="math display">\[
\kappa_2 = \frac{\hat{\beta}_{2,p} - p(p+2)}{\sqrt{8p(p+2)/n}} \sim N(0,1)
\]</span></p>
<ul>
<li><p>Hence, we can use <span class="math inline">\(\kappa_1\)</span> and <span class="math inline">\(\kappa_2\)</span> to test the null hypothesis of MVN.</p></li>
<li><p>When the data are non-normal, normal theory tests on the mean are sensitive to <span class="math inline">\(\beta_{1,p}\)</span> , while tests on the covariance are sensitive to <span class="math inline">\(\beta_{2,p}\)</span></p></li>
</ul></li>
</ul></li>
<li><p>Chi-square Q-Q plot</p>
<ul>
<li><p>Let <span class="math inline">\(\mathbf{y}_i, i = 1,...,n\)</span> be a random sample sample from <span class="math inline">\(N_p(\mathbf{\mu}, \mathbf{\Sigma})\)</span></p></li>
<li><p>Then <span class="math inline">\(\mathbf{z}_i = \mathbf{\Sigma}^{-1/2}(\mathbf{y}_i - \mathbf{\mu}), i = 1,...,n\)</span> are iid <span class="math inline">\(N_p (\mathbf{0}, \mathbf{I})\)</span>. Thus, <span class="math inline">\(d_i^2 = \mathbf{z}_i&#39; \mathbf{z}_i \sim \chi^2_p , i = 1,...,n\)</span></p></li>
<li><p>plot the ordered <span class="math inline">\(d_i^2\)</span> values against the qualities of the <span class="math inline">\(\chi^2_p\)</span> distribution. When normality holds, the plot should approximately resemble a straight lien passing through the origin at a 45 degree</p></li>
<li><p>it requires large sample size (i.e., sensitive to sample size). Even if we generate data from a MVN, the tail of the Chi-square Q-Q plot can still be out of line.</p></li>
</ul></li>
<li><p>If the data are not normal, we can</p>
<ul>
<li><p>ignore it</p></li>
<li><p>use nonparametric methods</p></li>
<li><p>use models based upon an approximate distirubiton (e.g., GLMM)</p></li>
<li><p>try performing a transformation</p></li>
</ul></li>
</ul>
<p><br></p>
</div>
<div id="mean-vector-inference" class="section level3" number="21.0.2">
<h3><span class="header-section-number">21.0.2</span> Mean Vector Inference</h3>
<p>In the univariate normal distribution, we test <span class="math inline">\(H_0: \mu =\mu_0\)</span> by using</p>
<p><span class="math display">\[
T = \frac{\bar{y}- \mu_0}{s/\sqrt{n}} \sim t_{n-1}
\]</span></p>
<p>under the null hypothesis. And reject the null if <span class="math inline">\(|T|\)</span> is large relative to <span class="math inline">\(t_{(1-\alpha/2,n-1)}\)</span> because it means that seeing a value as large as what we observed is rare if the null is true</p>
<p>Equivalently,</p>
<p><span class="math display">\[
T^2 = \frac{(\bar{y}- \mu_0)^2}{s^2/n} = n(\bar{y}- \mu_0)(s^2)^{-1}(\bar{y}- \mu_0) \sim f_{(1,n-1)}
\]</span></p>
<div id="natural-multivariate-generalization" class="section level4" number="21.0.2.1">
<h4><span class="header-section-number">21.0.2.1</span> <strong>Natural Multivariate Generalization</strong></h4>
<p><span class="math display">\[
H_0: \mathbf{\mu} = \mathbf{\mu}_0 \\
H_a: \mathbf{\mu} \neq \mathbf{\mu}_0
\]</span></p>
<p>Define <strong>Hotelling’s</strong> <span class="math inline">\(T^2\)</span> by</p>
<p><span class="math display">\[
T^2 = n(\bar{\mathbf{y}} - \mathbf{\mu}_0)&#39;\mathbf{S}^{-1}(\bar{\mathbf{y}} - \mathbf{\mu}_0)
\]</span></p>
<p>which can be viewed as a generalized distance between <span class="math inline">\(\bar{\mathbf{y}}\)</span> and <span class="math inline">\(\mathbf{\mu}_0\)</span></p>
<p>Under the assumption of normality,</p>
<p><span class="math display">\[
F = \frac{n-p}{(n-1)p} T^2 \sim f_{(p,n-p)}
\]</span></p>
<p>and reject the null hypothesis when <span class="math inline">\(F &gt; f_{(1-\alpha, p, n-p)}\)</span></p>
<ul>
<li><p>The <span class="math inline">\(T^2\)</span> test is invariant to changes in measurement units.</p>
<ul>
<li>If <span class="math inline">\(\mathbf{z = Cy + d}\)</span> where <span class="math inline">\(\mathbf{C}\)</span> and <span class="math inline">\(\mathbf{d}\)</span> do not depend on <span class="math inline">\(\mathbf{y}\)</span>, then <span class="math inline">\(T^2(\mathbf{z}) - T^2(\mathbf{y})\)</span></li>
</ul></li>
<li><p>The <span class="math inline">\(T^2\)</span> test can be derived as a <strong>likelihood ratio</strong> test of <span class="math inline">\(H_0: \mu = \mu_0\)</span></p></li>
</ul>
<p><br></p>
</div>
<div id="confidence-intervals" class="section level4" number="21.0.2.2">
<h4><span class="header-section-number">21.0.2.2</span> Confidence Intervals</h4>
<div id="confidence-region" class="section level5" number="21.0.2.2.1">
<h5><span class="header-section-number">21.0.2.2.1</span> Confidence Region</h5>
<p>An “exact” <span class="math inline">\(100(1-\alpha)\%\)</span> confidence region for <span class="math inline">\(\mathbf{\mu}\)</span> is the set of all vectors, <span class="math inline">\(\mathbf{v}\)</span>, which are “close enough” to the observed mean vector, <span class="math inline">\(\bar{\mathbf{y}}\)</span> to satisfy</p>
<p><span class="math display">\[
n(\bar{\mathbf{y}} - \mathbf{\mu}_0)&#39;\mathbf{S}^{-1}(\bar{\mathbf{y}} - \mathbf{\mu}_0) \le \frac{(n-1)p}{n-p} f_{(1-\alpha, p, n-p)}
\]</span></p>
<ul>
<li><span class="math inline">\(\mathbf{v}\)</span> are just the mean vectors that are not rejected by the <span class="math inline">\(T^2\)</span> test when <span class="math inline">\(\mathbf{\bar{y}}\)</span> is observed.</li>
</ul>
<p>In case that you have 2 parameters, the confidence region is a “hyper-ellipsoid.”</p>
<p>In this region, it consists of all <span class="math inline">\(\mathbf{\mu}_0\)</span> vectors for which the <span class="math inline">\(T^2\)</span> test would not reject <span class="math inline">\(H_0\)</span> at significance level <span class="math inline">\(\alpha\)</span></p>
<p>Even though the confidence region better assesses the joint knowledge concerning plausible values of <span class="math inline">\(\mathbf{\mu}\)</span> , people typically include confidence statement about the individual component means. We’d like all of the separate confidence statements to hold <strong>simultaneously</strong> with a specified high probability. Simultaneous confidence intervals: intervals <strong>against</strong> any statement being incorrect</p>
<p><br></p>
<div id="simultaneous-confidence-statements" class="section level6" number="21.0.2.2.1.1">
<h6><span class="header-section-number">21.0.2.2.1.1</span> Simultaneous Confidence Statements</h6>
<ul>
<li>Intervals based on a rectangular confidence region by projecting the previous region onto the coordinate axes:</li>
</ul>
<p><span class="math display">\[
\bar{y}_{i} \pm \sqrt{\frac{(n-1)p}{n-p}f_{(1-\alpha, p,n-p)}\frac{s_{ii}}{n}}
\]</span></p>
<p>for all <span class="math inline">\(i = 1,..,p\)</span></p>
<p>which implied confidence region is conservative; it has at least <span class="math inline">\(100(1- \alpha)\%\)</span></p>
<p>Generally, simultaneous <span class="math inline">\(100(1-\alpha) \%\)</span> confidence intervals for all linear combinations , <span class="math inline">\(\mathbf{a}\)</span> of the elements of the mean vector are given by</p>
<p><span class="math display">\[
\mathbf{a&#39;\bar{y}} \pm \sqrt{\frac{(n-1)p}{n-p}f_{(1-\alpha, p,n-p)}\frac{\mathbf{a&#39;Sa}}{n}}
\]</span></p>
<ul>
<li><p>works for any arbitrary linear combination <span class="math inline">\(\mathbf{a&#39;\mu} = a_1 \mu_1 + ... + a_p \mu_p\)</span>, which is a projection onto the axis in the direction of <span class="math inline">\(\mathbf{a}\)</span></p></li>
<li><p>These intervals have the property that the probability that at least one such interval does not contain the appropriate <span class="math inline">\(\mathbf{a&#39; \mu}\)</span> is no more than <span class="math inline">\(\alpha\)</span></p></li>
<li><p>These types of intervals can be used for “data snooping” (like <a href="completely-randomized-design-crd.html#scheffe">Scheffe</a>)</p></li>
</ul>
<p><br></p>
</div>
<div id="one-mu-at-a-time" class="section level6" number="21.0.2.2.1.2">
<h6><span class="header-section-number">21.0.2.2.1.2</span> One <span class="math inline">\(\mu\)</span> at a time</h6>
<ul>
<li>One at a time confidence intervals:</li>
</ul>
<p><span class="math display">\[
\bar{y}_i \pm t_{(1 - \alpha/2, n-1} \sqrt{\frac{s_{ii}}{n}}
\]</span></p>
<ul>
<li><p>Each of these intervals has a probability of <span class="math inline">\(1-\alpha\)</span> of covering the appropriate <span class="math inline">\(\mu_i\)</span></p></li>
<li><p>But they ignore the covariance structure of the <span class="math inline">\(p\)</span> variables</p></li>
<li><p>If we only care about <span class="math inline">\(k\)</span> simultaneous intervals, we can use “one at a time” method with the <a href="completely-randomized-design-crd.html#bonferroni">Bonferroni</a> correction.</p></li>
<li><p>This method gets more conservative as the number of intervals <span class="math inline">\(k\)</span> increases.</p></li>
</ul>
<p><br></p>
</div>
</div>
</div>
</div>
<div id="general-hypothesis-testing" class="section level3" number="21.0.3">
<h3><span class="header-section-number">21.0.3</span> General Hypothesis Testing</h3>
<div id="one-sample-tests" class="section level4" number="21.0.3.1">
<h4><span class="header-section-number">21.0.3.1</span> One-sample Tests</h4>
<p><span class="math display">\[
H_0: \mathbf{C \mu= 0} 
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\mathbf{C}\)</span> is a <span class="math inline">\(c \times p\)</span> matrix of rank c where <span class="math inline">\(c \le p\)</span></li>
</ul>
<p>We can test this hypothesis using the following statistic</p>
<p><span class="math display">\[
F = \frac{n - c}{(n-1)c} T^2
\]</span></p>
<p>where <span class="math inline">\(T^2 = n(\mathbf{C\bar{y}})&#39; (\mathbf{CSC&#39;})^{-1} (\mathbf{C\bar{y}})\)</span></p>
<p>Example:</p>
<p><span class="math display">\[
H_0: \mu_1 = \mu_2 = ... = \mu_p
\]</span></p>
<p>Equivalently,</p>
<p><span class="math display">\[
\mu_1 - \mu_2 = 0 \\
\vdots \\
\mu_{p-1} - \mu_p = 0
\]</span></p>
<p>a total of <span class="math inline">\(p-1\)</span> tests. Hence, we have <span class="math inline">\(\mathbf{C}\)</span> as the <span class="math inline">\(p - 1 \times p\)</span> matrix</p>
<p><span class="math display">\[
\mathbf{C} = 
\left(
\begin{array}
{ccccc}
1 &amp; -1 &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; 1 &amp; -1 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \ldots &amp; 1 &amp; -1 
\end{array}
\right)
\]</span></p>
<p>number of rows = <span class="math inline">\(c = p -1\)</span></p>
<p>Equivalently, we can also compare all of the other means to the first mean. Then, we test <span class="math inline">\(\mu_1 - \mu_2 = 0, \mu_1 - \mu_3 = 0,..., \mu_1 - \mu_p = 0\)</span>, the <span class="math inline">\((p-1) \times p\)</span> matrix <span class="math inline">\(\mathbf{C}\)</span> is</p>
<p><span class="math display">\[
\mathbf{C} = 
\left(
\begin{array}
{ccccc}
-1 &amp; 1 &amp; 0 &amp; \ldots &amp; 0 \\
-1 &amp; 0 &amp; 1 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
-1 &amp; 0 &amp; \ldots &amp; 0 &amp; 1 
\end{array}
\right)
\]</span></p>
<p>The value of <span class="math inline">\(T^2\)</span> is invariant to these equivalent choices of <span class="math inline">\(\mathbf{C}\)</span></p>
<p>This is often used for <strong>repeated measures designs</strong>, where each subject receives each treatment once over successive periods of time (all treatments are administered to each unit).</p>
<p><br></p>
<p>Example:</p>
<p>Let <span class="math inline">\(y_{ij}\)</span> be the response from subject i at time j for <span class="math inline">\(i = 1,..,n, j = 1,...,T\)</span>. In this case, <span class="math inline">\(\mathbf{y}_i = (y_{i1}, ..., y_{iT})&#39;, i = 1,...,n\)</span> are a random sample from <span class="math inline">\(N_T (\mathbf{\mu}, \mathbf{\Sigma})\)</span></p>
<p>Let <span class="math inline">\(n=8\)</span> subjects, <span class="math inline">\(T = 6\)</span>. We are interested in <span class="math inline">\(\mu_1, .., \mu_6\)</span></p>
<p><span class="math display">\[
H_0: \mu_1 = \mu_2 = ... = \mu_6
\]</span></p>
<p>Equivalently,</p>
<p><span class="math display">\[
\mu_1 - \mu_2 = 0 \\
\mu_2 - \mu_3 = 0 \\
... \\
\mu_5  - \mu_6 = 0
\]</span></p>
<p>We can test orthogonal polynomials for 4 equally spaced time points. To test for example the null hypothesis that quadratic and cubic effects are jointly equal to 0, we would define <span class="math inline">\(\mathbf{C}\)</span></p>
<p><span class="math display">\[
\mathbf{C} = 
\left(
\begin{array}
{cccc}
1 &amp; -1 &amp; -1 &amp; 1 \\
-1 &amp; 3 &amp; -3 &amp; 1
\end{array}
\right)
\]</span></p>
</div>
<div id="two-sample-tests" class="section level4" number="21.0.3.2">
<h4><span class="header-section-number">21.0.3.2</span> Two-Sample Tests</h4>
<p>Consider the analogous two sample multivariate tests.</p>
<p>Example: we have data on two independent random samples, one sample from each of two populations</p>
<p><span class="math display">\[
\mathbf{y}_{1i} \sim N_p (\mathbf{\mu_1, \Sigma}) \\
\mathbf{y}_{2j} \sim N_p (\mathbf{\mu_2, \Sigma})
\]</span></p>
<p>We <strong>assume</strong></p>
<ul>
<li><p>normality</p></li>
<li><p>equal variance-covariance matrices</p></li>
<li><p>independent random samples</p></li>
</ul>
<p>We can summarize our data using the <strong>sufficient statistics</strong> <span class="math inline">\(\mathbf{\bar{y}}_1, \mathbf{S}_1, \mathbf{\bar{y}}_2, \mathbf{S}_2\)</span> with respective sample sizes, <span class="math inline">\(n_1,n_2\)</span></p>
<p>Since we assume that <span class="math inline">\(\mathbf{\Sigma}_1 = \mathbf{\Sigma}_2 = \mathbf{\Sigma}\)</span>, compute a pooled estimate of the variance-covariance matrix on <span class="math inline">\(n_1 + n_2 - 2\)</span> df</p>
<p><span class="math display">\[
\mathbf{S} = \frac{(n_1 - 1)\mathbf{S}_1 + (n_2-1) \mathbf{S}_2}{(n_1 -1) + (n_2 - 1)}
\]</span></p>
<p><span class="math display">\[
H_0: \mathbf{\mu}_1 = \mathbf{\mu}_2 \\
H_a: \mathbf{\mu}_1 \neq \mathbf{\mu}_2
\]</span></p>
<p>At least one element of the mean vectors is different</p>
<p>We use</p>
<ul>
<li><p><span class="math inline">\(\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2\)</span> to estimate <span class="math inline">\(\mu_1 - \mu_2\)</span></p></li>
<li><p><span class="math inline">\(\mathbf{S}\)</span> to estimate <span class="math inline">\(\mathbf{\Sigma}\)</span></p>
<p>Note: because we assume the two populations are independent, there is no covariance</p>
<p><span class="math inline">\(cov(\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2) = var(\mathbf{\bar{y}}_1) + var(\mathbf{\bar{y}}_2) = \frac{\mathbf{\Sigma_1}}{n_1} + \frac{\mathbf{\Sigma_2}}{n_2} = \mathbf{\Sigma}(\frac{1}{n_1} + \frac{1}{n_2})\)</span></p></li>
</ul>
<p>Reject <span class="math inline">\(H_0\)</span> if</p>
<p><span class="math display">\[
\begin{aligned}
T^2 &amp;= (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)&#39;\{ \mathbf{S} (\frac{1}{n_1} + \frac{1}{n_2})\}^{-1} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)\\
&amp;= \frac{n_1 n_2}{n_1 +n_2} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)&#39;\{ \mathbf{S} \}^{-1} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)\\
&amp; \ge \frac{(n_1 + n_2 -2)p}{n_1 + n_2 - p - 1} f_{(1- \alpha,n_1 + n_2 - p -1)}
\end{aligned}
\]</span></p>
<p>or equivalently, if</p>
<p><span class="math display">\[
F = \frac{n_1 + n_2 - p -1}{(n_1 + n_2 -2)p} T^2 \ge f_{(1- \alpha, p , n_1 + n_2 -p -1)}
\]</span></p>
<p>A <span class="math inline">\(100(1-\alpha) \%\)</span> confidence region for <span class="math inline">\(\mu_1 - \mu_2\)</span> consists of all vector <span class="math inline">\(\delta\)</span> which satisfy</p>
<p><span class="math display">\[
\frac{n_1 n_2}{n_1 + n_2} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2 - \mathbf{\delta})&#39; \mathbf{S}^{-1}(\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2 - \mathbf{\delta}) \le \frac{(n_1 + n_2 - 2)p}{n_1 + n_2 -p - 1}f_{(1-\alpha, p , n_1 + n_2 - p -1)}
\]</span></p>
<p>The simultaneous confidence intervals for all linear combinations of <span class="math inline">\(\mu_1 - \mu_2\)</span> have the form</p>
<p><span class="math display">\[
\mathbf{a&#39;}(\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2) \pm \sqrt{\frac{(n_1 + n_2 -2)p}{n_1 + n_2 - p -1}}f_{(1-\alpha, p, n_1 + n_2 -p -1)} \times \sqrt{\mathbf{a&#39;Sa}(\frac{1}{n_1} + \frac{1}{n_2})}
\]</span></p>
<p>Bonferroni intervals, for k combinations</p>
<p><span class="math display">\[
(\bar{y}_{1i} - \bar{y}_{2i}) \pm t_{(1-\alpha/2k, n_1 + n_2 - 2)}\sqrt{(\frac{1}{n_1}  + \frac{1}{n_2})s_{ii}}
\]</span></p>
</div>
<div id="model-assumptions" class="section level4" number="21.0.3.3">
<h4><span class="header-section-number">21.0.3.3</span> Model Assumptions</h4>
<p>If model assumption are not met</p>
<ul>
<li><p>Unequal Covariance Matrices</p>
<ul>
<li><p>If <span class="math inline">\(n_1 = n_2\)</span> (large samples) there is little effect on the Type I error rate and power fo the two sample test</p></li>
<li><p>If <span class="math inline">\(n_1 &gt; n_2\)</span> and the eigenvalues of <span class="math inline">\(\mathbf{\Sigma}_1 \mathbf{\Sigma}^{-1}_2\)</span> are less than 1, the Type I error level is inflated</p></li>
<li><p>If <span class="math inline">\(n_1 &gt; n_2\)</span> and some eigenvalues of <span class="math inline">\(\mathbf{\Sigma}_1 \mathbf{\Sigma}_2^{-1}\)</span> are greater than 1, the Type I error rate is too small, leading to a reduction in power</p></li>
</ul></li>
<li><p>Sample Not Normal</p>
<ul>
<li><p>Type I error level of the two sample <span class="math inline">\(T^2\)</span> test isn’t much affect by moderate departures from normality if the two populations being sampled have similar distributions</p></li>
<li><p>One sample <span class="math inline">\(T^2\)</span> test is much more sensitive to lack of normality, especially when the distribution is skewed.</p></li>
<li><p>Intuitively, you can think that in one sample your distribution will be sensitive, but the distribution of the difference between two similar distributions will not be as sensitive.</p></li>
<li><p>Solutions:</p>
<ul>
<li><p>Transform to make the data more normal</p></li>
<li><p>Large large samples, use the <span class="math inline">\(\chi^2\)</span> (Wald) test, in which populations don’t need to be normal, or equal sample sizes, or equal variance-covariance matrices</p>
<ul>
<li><span class="math inline">\(H_0: \mu_1 - \mu_2 =0\)</span> use <span class="math inline">\((\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)&#39;( \frac{1}{n_1} \mathbf{S}_1 + \frac{1}{n_2}\mathbf{S}_2)^{-1}(\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2) \dot{\sim} \chi^2_{(p)}\)</span></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<p><br></p>
<div id="equal-covariance-matrices-tests" class="section level5" number="21.0.3.3.1">
<h5><span class="header-section-number">21.0.3.3.1</span> Equal Covariance Matrices Tests</h5>
<p>With independent random samples from k populations of p-dimensional vectors. We compute the sample covariance matrix for each, <span class="math inline">\(\mathbf{S}_i\)</span>, where <span class="math inline">\(i = 1,...,k\)</span></p>
<p><span class="math display">\[
H_0: \mathbf{\Sigma}_1 = \mathbf{\Sigma}_2 = \ldots = \mathbf{\Sigma}_k = \mathbf{\Sigma} \\
H_a: \text{at least 2 are different}
\]</span></p>
<p>Assume <span class="math inline">\(H_0\)</span> is true, we would use a pooled estimate of the common covariance matrix, <span class="math inline">\(\mathbf{\Sigma}\)</span></p>
<p><span class="math display">\[
\mathbf{S} = \frac{\sum_{i=1}^k (n_i -1)\mathbf{S}_i}{\sum_{i=1}^k (n_i - 1)}
\]</span></p>
<p>with <span class="math inline">\(\sum_{i=1}^k (n_i -1)\)</span></p>
<p><br></p>
<div id="bartletts-test" class="section level6" number="21.0.3.3.1.1">
<h6><span class="header-section-number">21.0.3.3.1.1</span> Bartlett’s Test</h6>
<p>(a modification of the likelihood ratio test). Define</p>
<p><span class="math display">\[
N = \sum_{i=1}^k n_i
\]</span></p>
<p>and (note: <span class="math inline">\(| |\)</span> are determinants here, not absolute value)</p>
<p><span class="math display">\[
M = (N - k) \log|\mathbf{S}| - \sum_{i=1}^k (n_i - 1)  \log|\mathbf{S}_i|
\]</span></p>
<p><span class="math display">\[
C^{-1} = 1 - \frac{2p^2 + 3p - 1}{6(p+1)(k-1)} \{\sum_{i=1}^k (\frac{1}{n_i - 1}) - \frac{1}{N-k} \}
\]</span></p>
<ul>
<li><p>Reject <span class="math inline">\(H_0\)</span> when <span class="math inline">\(MC^{-1} &gt; \chi^2_{1- \alpha, (k-1)p(p+1)/2}\)</span></p></li>
<li><p>If not all samples are from normal populations, <span class="math inline">\(MC^{-1}\)</span> has a distribution which is often shifted to the right of the nominal <span class="math inline">\(\chi^2\)</span> distribution, which means <span class="math inline">\(H_0\)</span> is often rejected even when it is true (the Type I error level is inflated). Hence, it is better to test individual normality first, or then multivariate normality before you do Bartlett’s test.</p></li>
</ul>
<p><br></p>
</div>
</div>
</div>
<div id="two-sample-repeated-measurements" class="section level4" number="21.0.3.4">
<h4><span class="header-section-number">21.0.3.4</span> Two-Sample Repeated Measurements</h4>
<ul>
<li><p>Define <span class="math inline">\(\mathbf{y}_{hi} = (y_{hi1}, ..., y_{hit})&#39;\)</span> to be the observations from the i-th subject in the h-th group for times 1 through T</p></li>
<li><p>Assume that <span class="math inline">\(\mathbf{y}_{11}, ..., \mathbf{y}_{1n_1}\)</span> are iid <span class="math inline">\(N_t(\mathbf{\mu}_1, \mathbf{\Sigma})\)</span> and that <span class="math inline">\(\mathbf{y}_{21},...,\mathbf{y}_{2n_2}\)</span> are iid <span class="math inline">\(N_t(\mathbf{\mu}_2, \mathbf{\Sigma})\)</span></p></li>
<li><p><span class="math inline">\(H_0: \mathbf{C}(\mathbf{\mu}_1 - \mathbf{\mu}_2) = \mathbf{0}_c\)</span> where <span class="math inline">\(\mathbf{C}\)</span> is a <span class="math inline">\(c \times t\)</span> matrix of rank <span class="math inline">\(c\)</span> where <span class="math inline">\(c \le t\)</span></p></li>
<li><p>The test statistic has the form</p></li>
</ul>
<p><span class="math display">\[
T^2 = \frac{n_1 n_2}{n_1 + n_2} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)&#39; \mathbf{C}&#39;(\mathbf{CSC}&#39;)^{-1}\mathbf{C} (\mathbf{\bar{y}}_1 - \mathbf{\bar{y}}_2)
\]</span></p>
<p>where <span class="math inline">\(\mathbf{S}\)</span> is the pooled covariance estimate. Then,</p>
<p><span class="math display">\[
F = \frac{n_1 + n_2 - c -1}{(n_1 + n_2-2)c} T^2 \sim f_{(c, n_1 + n_2 - c-1)}
\]</span></p>
<p>when <span class="math inline">\(H_0\)</span> is true</p>
<p>If the null hypothesis <span class="math inline">\(H_0: \mu_1 = \mu_2\)</span> is rejected. A weaker hypothesis is that the profiles for the two groups are parallel.</p>
<p><span class="math display">\[
\mu_{11} - \mu_{21} = \mu_{12} - \mu_{22} \\
\vdots \\
\mu_{1t-1} - \mu_{2t-1} = \mu_{1t} - \mu_{2t}
\]</span></p>
<p>The null hypothesis matrix term is then</p>
<p><span class="math inline">\(H_0: \mathbf{C}(\mu_1 - \mu_2) = \mathbf{0}_c\)</span> , where <span class="math inline">\(c = t - 1\)</span> and</p>
<p><span class="math display">\[
\mathbf{C} = 
\left(
\begin{array}
{ccccc}
1 &amp; -1 &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; 1 &amp; -1 &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \ldots &amp; -1 
\end{array}
\right)_{(t-1) \times t}
\]</span></p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="single-factor-covariance-model.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="manova.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mikenguyen13/data_analysis/edit/main/13-multivariate.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Data Analysis.pdf", "Data Analysis.epub", "Data Analysis.mobi"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true,
"sharing": {
"facebook": true,
"github": true,
"twitter": true,
"linkedin": true
},
"info": true,
"edit": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
