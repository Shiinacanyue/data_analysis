<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6.2 Non-linear Least Squares | A Guide on Data Analysis</title>
  <meta name="description" content="This is a guide on how to conduct a data analysis routine" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="6.2 Non-linear Least Squares | A Guide on Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a guide on how to conduct a data analysis routine" />
  <meta name="github-repo" content="mikenguyen13/data_analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6.2 Non-linear Least Squares | A Guide on Data Analysis" />
  
  <meta name="twitter:description" content="This is a guide on how to conduct a data analysis routine" />
  

<meta name="author" content="Mike Nguyen" />


<meta name="date" content="2021-02-05" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="inference-1.html"/>
<link rel="next" href="genelized-method-of-moments.html"/>
<script src="libs/jquery-3.5.0/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/proj4js-2.3.15/proj4.js"></script>
<link href="libs/highcharts-8.1.2/css/motion.css" rel="stylesheet" />
<script src="libs/highcharts-8.1.2/highcharts.js"></script>
<script src="libs/highcharts-8.1.2/highcharts-3d.js"></script>
<script src="libs/highcharts-8.1.2/highcharts-more.js"></script>
<script src="libs/highcharts-8.1.2/modules/stock.js"></script>
<script src="libs/highcharts-8.1.2/modules/map.js"></script>
<script src="libs/highcharts-8.1.2/modules/annotations.js"></script>
<script src="libs/highcharts-8.1.2/modules/data.js"></script>
<script src="libs/highcharts-8.1.2/modules/drilldown.js"></script>
<script src="libs/highcharts-8.1.2/modules/item-series.js"></script>
<script src="libs/highcharts-8.1.2/modules/offline-exporting.js"></script>
<script src="libs/highcharts-8.1.2/modules/overlapping-datalabels.js"></script>
<script src="libs/highcharts-8.1.2/modules/exporting.js"></script>
<script src="libs/highcharts-8.1.2/modules/export-data.js"></script>
<script src="libs/highcharts-8.1.2/modules/funnel.js"></script>
<script src="libs/highcharts-8.1.2/modules/heatmap.js"></script>
<script src="libs/highcharts-8.1.2/modules/treemap.js"></script>
<script src="libs/highcharts-8.1.2/modules/sankey.js"></script>
<script src="libs/highcharts-8.1.2/modules/dependency-wheel.js"></script>
<script src="libs/highcharts-8.1.2/modules/organization.js"></script>
<script src="libs/highcharts-8.1.2/modules/solid-gauge.js"></script>
<script src="libs/highcharts-8.1.2/modules/streamgraph.js"></script>
<script src="libs/highcharts-8.1.2/modules/sunburst.js"></script>
<script src="libs/highcharts-8.1.2/modules/vector.js"></script>
<script src="libs/highcharts-8.1.2/modules/wordcloud.js"></script>
<script src="libs/highcharts-8.1.2/modules/xrange.js"></script>
<script src="libs/highcharts-8.1.2/modules/tilemap.js"></script>
<script src="libs/highcharts-8.1.2/modules/venn.js"></script>
<script src="libs/highcharts-8.1.2/modules/gantt.js"></script>
<script src="libs/highcharts-8.1.2/modules/timeline.js"></script>
<script src="libs/highcharts-8.1.2/modules/parallel-coordinates.js"></script>
<script src="libs/highcharts-8.1.2/modules/bullet.js"></script>
<script src="libs/highcharts-8.1.2/modules/coloraxis.js"></script>
<script src="libs/highcharts-8.1.2/modules/dumbbell.js"></script>
<script src="libs/highcharts-8.1.2/modules/lollipop.js"></script>
<script src="libs/highcharts-8.1.2/modules/series-label.js"></script>
<script src="libs/highcharts-8.1.2/plugins/motion.js"></script>
<script src="libs/highcharts-8.1.2/custom/reset.js"></script>
<script src="libs/highcharts-8.1.2/modules/boost.js"></script>
<script src="libs/highchart-binding-0.8.2/highchart.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Guide on Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a><ul>
<li class="chapter" data-level="2.1" data-path="matrix-theory.html"><a href="matrix-theory.html"><i class="fa fa-check"></i><b>2.1</b> Matrix Theory</a><ul>
<li class="chapter" data-level="2.1.1" data-path="matrix-theory.html"><a href="matrix-theory.html#rank"><i class="fa fa-check"></i><b>2.1.1</b> Rank</a></li>
<li class="chapter" data-level="2.1.2" data-path="matrix-theory.html"><a href="matrix-theory.html#inverse"><i class="fa fa-check"></i><b>2.1.2</b> Inverse</a></li>
<li class="chapter" data-level="2.1.3" data-path="matrix-theory.html"><a href="matrix-theory.html#definiteness"><i class="fa fa-check"></i><b>2.1.3</b> Definiteness</a></li>
<li class="chapter" data-level="2.1.4" data-path="matrix-theory.html"><a href="matrix-theory.html#matrix-calculus"><i class="fa fa-check"></i><b>2.1.4</b> Matrix Calculus</a></li>
<li class="chapter" data-level="2.1.5" data-path="matrix-theory.html"><a href="matrix-theory.html#optimization"><i class="fa fa-check"></i><b>2.1.5</b> Optimization</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>2.2</b> Probability Theory</a><ul>
<li class="chapter" data-level="2.2.1" data-path="probability-theory.html"><a href="probability-theory.html#axiom-and-theorems-of-probability"><i class="fa fa-check"></i><b>2.2.1</b> Axiom and Theorems of Probability</a></li>
<li class="chapter" data-level="2.2.2" data-path="probability-theory.html"><a href="probability-theory.html#central-limit-theorem"><i class="fa fa-check"></i><b>2.2.2</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="2.2.3" data-path="probability-theory.html"><a href="probability-theory.html#random-variable"><i class="fa fa-check"></i><b>2.2.3</b> Random variable</a></li>
<li class="chapter" data-level="2.2.4" data-path="probability-theory.html"><a href="probability-theory.html#moment-generating-function"><i class="fa fa-check"></i><b>2.2.4</b> Moment generating function</a></li>
<li class="chapter" data-level="2.2.5" data-path="probability-theory.html"><a href="probability-theory.html#moment"><i class="fa fa-check"></i><b>2.2.5</b> Moment</a></li>
<li class="chapter" data-level="2.2.6" data-path="probability-theory.html"><a href="probability-theory.html#distributions"><i class="fa fa-check"></i><b>2.2.6</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="general-math.html"><a href="general-math.html"><i class="fa fa-check"></i><b>2.3</b> General Math</a><ul>
<li class="chapter" data-level="2.3.1" data-path="general-math.html"><a href="general-math.html#law-of-large-numbers"><i class="fa fa-check"></i><b>2.3.1</b> Law of large numbers</a></li>
<li class="chapter" data-level="2.3.2" data-path="general-math.html"><a href="general-math.html#law-of-iterated-expectation"><i class="fa fa-check"></i><b>2.3.2</b> Law of Iterated Expectation</a></li>
<li class="chapter" data-level="2.3.3" data-path="general-math.html"><a href="general-math.html#convergence"><i class="fa fa-check"></i><b>2.3.3</b> Convergence</a></li>
<li class="chapter" data-level="2.3.4" data-path="general-math.html"><a href="general-math.html#sufficient-statistics"><i class="fa fa-check"></i><b>2.3.4</b> Sufficient Statistics</a></li>
<li class="chapter" data-level="2.3.5" data-path="general-math.html"><a href="general-math.html#parameter-transformations"><i class="fa fa-check"></i><b>2.3.5</b> Parameter transformations</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>2.4</b> Methods</a></li>
<li class="chapter" data-level="2.5" data-path="data-manipulation.html"><a href="data-manipulation.html"><i class="fa fa-check"></i><b>2.5</b> Data Manipulation</a></li>
</ul></li>
<li class="part"><span><b>I BASIC</b></span></li>
<li class="chapter" data-level="3" data-path="descriptive-stat.html"><a href="descriptive-stat.html"><i class="fa fa-check"></i><b>3</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="3.1" data-path="numerical-measures.html"><a href="numerical-measures.html"><i class="fa fa-check"></i><b>3.1</b> Numerical Measures</a></li>
<li class="chapter" data-level="3.2" data-path="graphical-measures.html"><a href="graphical-measures.html"><i class="fa fa-check"></i><b>3.2</b> Graphical Measures</a><ul>
<li class="chapter" data-level="3.2.1" data-path="graphical-measures.html"><a href="graphical-measures.html#shape"><i class="fa fa-check"></i><b>3.2.1</b> Shape</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="normality-assessment.html"><a href="normality-assessment.html"><i class="fa fa-check"></i><b>3.3</b> Normality Assessment</a><ul>
<li class="chapter" data-level="3.3.1" data-path="normality-assessment.html"><a href="normality-assessment.html#graphical-assessment"><i class="fa fa-check"></i><b>3.3.1</b> Graphical Assessment</a></li>
<li class="chapter" data-level="3.3.2" data-path="normality-assessment.html"><a href="normality-assessment.html#summary-statistics"><i class="fa fa-check"></i><b>3.3.2</b> Summary Statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="basic-statistical-inference.html"><a href="basic-statistical-inference.html"><i class="fa fa-check"></i><b>4</b> Basic Statistical Inference</a><ul>
<li class="chapter" data-level="4.1" data-path="one-sample-inference.html"><a href="one-sample-inference.html"><i class="fa fa-check"></i><b>4.1</b> One Sample Inference</a><ul>
<li class="chapter" data-level="4.1.1" data-path="one-sample-inference.html"><a href="one-sample-inference.html#the-mean"><i class="fa fa-check"></i><b>4.1.1</b> The Mean</a></li>
<li class="chapter" data-level="4.1.2" data-path="one-sample-inference.html"><a href="one-sample-inference.html#single-variance"><i class="fa fa-check"></i><b>4.1.2</b> Single Variance</a></li>
<li class="chapter" data-level="4.1.3" data-path="one-sample-inference.html"><a href="one-sample-inference.html#single-proportion-p"><i class="fa fa-check"></i><b>4.1.3</b> Single Proportion (p)</a></li>
<li class="chapter" data-level="4.1.4" data-path="one-sample-inference.html"><a href="one-sample-inference.html#power"><i class="fa fa-check"></i><b>4.1.4</b> Power</a></li>
<li class="chapter" data-level="4.1.5" data-path="one-sample-inference.html"><a href="one-sample-inference.html#sample-size"><i class="fa fa-check"></i><b>4.1.5</b> Sample Size</a></li>
<li class="chapter" data-level="4.1.6" data-path="one-sample-inference.html"><a href="one-sample-inference.html#note"><i class="fa fa-check"></i><b>4.1.6</b> Note</a></li>
<li class="chapter" data-level="4.1.7" data-path="one-sample-inference.html"><a href="one-sample-inference.html#one-sample-non-parametric-methods"><i class="fa fa-check"></i><b>4.1.7</b> One-sample Non-parametric Methods</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="two-sample-inference.html"><a href="two-sample-inference.html"><i class="fa fa-check"></i><b>4.2</b> Two Sample Inference</a><ul>
<li class="chapter" data-level="4.2.1" data-path="two-sample-inference.html"><a href="two-sample-inference.html#means"><i class="fa fa-check"></i><b>4.2.1</b> Means</a></li>
<li class="chapter" data-level="4.2.2" data-path="two-sample-inference.html"><a href="two-sample-inference.html#variances"><i class="fa fa-check"></i><b>4.2.2</b> Variances</a></li>
<li class="chapter" data-level="4.2.3" data-path="two-sample-inference.html"><a href="two-sample-inference.html#power-1"><i class="fa fa-check"></i><b>4.2.3</b> Power</a></li>
<li class="chapter" data-level="4.2.4" data-path="two-sample-inference.html"><a href="two-sample-inference.html#sample-size-1"><i class="fa fa-check"></i><b>4.2.4</b> Sample Size</a></li>
<li class="chapter" data-level="4.2.5" data-path="two-sample-inference.html"><a href="two-sample-inference.html#matched-pair-designs"><i class="fa fa-check"></i><b>4.2.5</b> Matched Pair Designs</a></li>
<li class="chapter" data-level="4.2.6" data-path="two-sample-inference.html"><a href="two-sample-inference.html#nonparametric-tests-for-two-samples"><i class="fa fa-check"></i><b>4.2.6</b> Nonparametric Tests for Two Samples</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html"><i class="fa fa-check"></i><b>4.3</b> Categorical Data Analysis</a><ul>
<li class="chapter" data-level="4.3.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#inferences-for-small-samples"><i class="fa fa-check"></i><b>4.3.1</b> Inferences for Small Samples</a></li>
<li class="chapter" data-level="4.3.2" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#test-of-association"><i class="fa fa-check"></i><b>4.3.2</b> Test of Association</a></li>
<li class="chapter" data-level="4.3.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#ordinal-association"><i class="fa fa-check"></i><b>4.3.3</b> Ordinal Association</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II REGRESSION</b></span></li>
<li class="chapter" data-level="5" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>5</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html"><i class="fa fa-check"></i><b>5.1</b> Ordinary Least Squares</a><ul>
<li class="chapter" data-level="5.1.1" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#simple-regression-basic-model"><i class="fa fa-check"></i><b>5.1.1</b> Simple Regression (Basic Model)</a></li>
<li class="chapter" data-level="5.1.2" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#multiple-linear-regression"><i class="fa fa-check"></i><b>5.1.2</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="5.1.3" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#ols-assumptions"><i class="fa fa-check"></i><b>5.1.3</b> OLS Assumptions</a></li>
<li class="chapter" data-level="5.1.4" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#theorems"><i class="fa fa-check"></i><b>5.1.4</b> Theorems</a></li>
<li class="chapter" data-level="5.1.5" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#variable-selection"><i class="fa fa-check"></i><b>5.1.5</b> Variable Selection</a></li>
<li class="chapter" data-level="5.1.6" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#diagnostics-1"><i class="fa fa-check"></i><b>5.1.6</b> Diagnostics</a></li>
<li class="chapter" data-level="5.1.7" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#model-validation"><i class="fa fa-check"></i><b>5.1.7</b> Model Validation</a></li>
<li class="chapter" data-level="5.1.8" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#finite-sample-properties"><i class="fa fa-check"></i><b>5.1.8</b> Finite Sample Properties</a></li>
<li class="chapter" data-level="5.1.9" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#large-sample-properties"><i class="fa fa-check"></i><b>5.1.9</b> Large Sample Properties</a></li>
<li class="chapter" data-level="5.1.10" data-path="ordinary-least-squares.html"><a href="ordinary-least-squares.html#application"><i class="fa fa-check"></i><b>5.1.10</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="feasible-generalized-least-squares.html"><a href="feasible-generalized-least-squares.html"><i class="fa fa-check"></i><b>5.2</b> Feasible Generalized Least Squares</a><ul>
<li class="chapter" data-level="5.2.1" data-path="feasible-generalized-least-squares.html"><a href="feasible-generalized-least-squares.html#heteroskedasticity"><i class="fa fa-check"></i><b>5.2.1</b> Heteroskedasticity</a></li>
<li class="chapter" data-level="5.2.2" data-path="feasible-generalized-least-squares.html"><a href="feasible-generalized-least-squares.html#serial-correlation"><i class="fa fa-check"></i><b>5.2.2</b> Serial Correlation</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="weighted-least-squares.html"><a href="weighted-least-squares.html"><i class="fa fa-check"></i><b>5.3</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="5.4" data-path="generalized-least-squares.html"><a href="generalized-least-squares.html"><i class="fa fa-check"></i><b>5.4</b> Generalized Least Squares</a></li>
<li class="chapter" data-level="5.5" data-path="feasiable-prais-winsten.html"><a href="feasiable-prais-winsten.html"><i class="fa fa-check"></i><b>5.5</b> Feasiable Prais Winsten</a></li>
<li class="chapter" data-level="5.6" data-path="feasible-group-level-random-effects.html"><a href="feasible-group-level-random-effects.html"><i class="fa fa-check"></i><b>5.6</b> Feasible group level Random Effects</a></li>
<li class="chapter" data-level="5.7" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>5.7</b> Ridge Regression</a></li>
<li class="chapter" data-level="5.8" data-path="principal-component-regression.html"><a href="principal-component-regression.html"><i class="fa fa-check"></i><b>5.8</b> Principal Component Regression</a></li>
<li class="chapter" data-level="5.9" data-path="robust-regression.html"><a href="robust-regression.html"><i class="fa fa-check"></i><b>5.9</b> Robust Regression</a><ul>
<li class="chapter" data-level="5.9.1" data-path="robust-regression.html"><a href="robust-regression.html#least-absolute-residuals-lar-regression"><i class="fa fa-check"></i><b>5.9.1</b> Least Absolute Residuals (LAR) Regression</a></li>
<li class="chapter" data-level="5.9.2" data-path="robust-regression.html"><a href="robust-regression.html#least-median-of-squares-lms-regression"><i class="fa fa-check"></i><b>5.9.2</b> Least Median of Squares (LMS) Regression</a></li>
<li class="chapter" data-level="5.9.3" data-path="robust-regression.html"><a href="robust-regression.html#iteratively-reweighted-least-squares-irls-robust-regression"><i class="fa fa-check"></i><b>5.9.3</b> Iteratively Reweighted Least Squares (IRLS) Robust Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html"><i class="fa fa-check"></i><b>5.10</b> Maximum Likelihood</a><ul>
<li class="chapter" data-level="5.10.1" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#motivation-for-mle"><i class="fa fa-check"></i><b>5.10.1</b> Motivation for MLE</a></li>
<li class="chapter" data-level="5.10.2" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#assumption"><i class="fa fa-check"></i><b>5.10.2</b> Assumption</a></li>
<li class="chapter" data-level="5.10.3" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#properties"><i class="fa fa-check"></i><b>5.10.3</b> Properties</a></li>
<li class="chapter" data-level="5.10.4" data-path="maximum-likelihood.html"><a href="maximum-likelihood.html#application-1"><i class="fa fa-check"></i><b>5.10.4</b> Application</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="non-linear-regression.html"><a href="non-linear-regression.html"><i class="fa fa-check"></i><b>6</b> Non-linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="inference-1.html"><a href="inference-1.html"><i class="fa fa-check"></i><b>6.1</b> Inference</a><ul>
<li class="chapter" data-level="6.1.1" data-path="inference-1.html"><a href="inference-1.html#linear-function-of-the-parameters"><i class="fa fa-check"></i><b>6.1.1</b> Linear Function of the Parameters</a></li>
<li class="chapter" data-level="6.1.2" data-path="inference-1.html"><a href="inference-1.html#nonlinear"><i class="fa fa-check"></i><b>6.1.2</b> Nonlinear</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="non-linear-least-squares.html"><a href="non-linear-least-squares.html"><i class="fa fa-check"></i><b>6.2</b> Non-linear Least Squares</a><ul>
<li class="chapter" data-level="6.2.1" data-path="non-linear-least-squares.html"><a href="non-linear-least-squares.html#alternative-of-gauss-newton-algorithm"><i class="fa fa-check"></i><b>6.2.1</b> Alternative of Gauss-Newton Algorithm</a></li>
<li class="chapter" data-level="6.2.2" data-path="non-linear-least-squares.html"><a href="non-linear-least-squares.html#practical-considerations"><i class="fa fa-check"></i><b>6.2.2</b> Practical Considerations</a></li>
<li class="chapter" data-level="6.2.3" data-path="non-linear-least-squares.html"><a href="non-linear-least-squares.html#modelestiamtion-adequcy"><i class="fa fa-check"></i><b>6.2.3</b> Model/Estiamtion Adequcy</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="genelized-method-of-moments.html"><a href="genelized-method-of-moments.html"><i class="fa fa-check"></i><b>6.3</b> Genelized Method of Moments</a></li>
<li class="chapter" data-level="6.4" data-path="minimum-distance.html"><a href="minimum-distance.html"><i class="fa fa-check"></i><b>6.4</b> Minimum Distance</a></li>
<li class="chapter" data-level="6.5" data-path="spline-regression.html"><a href="spline-regression.html"><i class="fa fa-check"></i><b>6.5</b> Spline Regression</a><ul>
<li class="chapter" data-level="6.5.1" data-path="spline-regression.html"><a href="spline-regression.html#regression-splines"><i class="fa fa-check"></i><b>6.5.1</b> Regression Splines</a></li>
<li class="chapter" data-level="6.5.2" data-path="spline-regression.html"><a href="spline-regression.html#natural-splines"><i class="fa fa-check"></i><b>6.5.2</b> Natural splines</a></li>
<li class="chapter" data-level="6.5.3" data-path="spline-regression.html"><a href="spline-regression.html#smoothing-spliness"><i class="fa fa-check"></i><b>6.5.3</b> Smoothing spliness</a></li>
<li class="chapter" data-level="6.5.4" data-path="spline-regression.html"><a href="spline-regression.html#application-2"><i class="fa fa-check"></i><b>6.5.4</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="generalized-additive-models.html"><a href="generalized-additive-models.html"><i class="fa fa-check"></i><b>6.6</b> Generalized Additive Models</a></li>
<li class="chapter" data-level="6.7" data-path="quantile-regression.html"><a href="quantile-regression.html"><i class="fa fa-check"></i><b>6.7</b> Quantile Regression</a><ul>
<li class="chapter" data-level="6.7.1" data-path="quantile-regression.html"><a href="quantile-regression.html#application-3"><i class="fa fa-check"></i><b>6.7.1</b> Application</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="model-specification.html"><a href="model-specification.html"><i class="fa fa-check"></i><b>7</b> Model Specification</a><ul>
<li class="chapter" data-level="7.1" data-path="nested-model.html"><a href="nested-model.html"><i class="fa fa-check"></i><b>7.1</b> Nested Model</a><ul>
<li class="chapter" data-level="7.1.1" data-path="nested-model.html"><a href="nested-model.html#chow-test"><i class="fa fa-check"></i><b>7.1.1</b> Chow test</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="non-nested-model.html"><a href="non-nested-model.html"><i class="fa fa-check"></i><b>7.2</b> Non-Nested Model</a><ul>
<li class="chapter" data-level="7.2.1" data-path="non-nested-model.html"><a href="non-nested-model.html#davidson-mackinnon-test"><i class="fa fa-check"></i><b>7.2.1</b> Davidson-Mackinnon test</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="heteroskedasticity-1.html"><a href="heteroskedasticity-1.html"><i class="fa fa-check"></i><b>7.3</b> Heteroskedasticity</a><ul>
<li class="chapter" data-level="7.3.1" data-path="heteroskedasticity-1.html"><a href="heteroskedasticity-1.html#breusch-pagan-test"><i class="fa fa-check"></i><b>7.3.1</b> Breusch-Pagan test</a></li>
<li class="chapter" data-level="7.3.2" data-path="heteroskedasticity-1.html"><a href="heteroskedasticity-1.html#white-test"><i class="fa fa-check"></i><b>7.3.2</b> White test</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="endogeneity.html"><a href="endogeneity.html"><i class="fa fa-check"></i><b>8</b> Endogeneity</a><ul>
<li class="chapter" data-level="8.1" data-path="testing-assumption.html"><a href="testing-assumption.html"><i class="fa fa-check"></i><b>8.1</b> Testing Assumption</a><ul>
<li class="chapter" data-level="8.1.1" data-path="testing-assumption.html"><a href="testing-assumption.html#test-of-endogeneity"><i class="fa fa-check"></i><b>8.1.1</b> Test of Endogeneity</a></li>
<li class="chapter" data-level="8.1.2" data-path="testing-assumption.html"><a href="testing-assumption.html#testing-instruments-assumptions"><i class="fa fa-check"></i><b>8.1.2</b> Testing Instrument’s assumptions</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="omitted-variables-bias.html"><a href="omitted-variables-bias.html"><i class="fa fa-check"></i><b>8.2</b> Omitted Variables Bias</a></li>
<li class="chapter" data-level="8.3" data-path="feedback-effect-simultaneity.html"><a href="feedback-effect-simultaneity.html"><i class="fa fa-check"></i><b>8.3</b> Feedback Effect (Simultaneity)</a></li>
<li class="chapter" data-level="8.4" data-path="endogenous-sample-design-sample-selection.html"><a href="endogenous-sample-design-sample-selection.html"><i class="fa fa-check"></i><b>8.4</b> Endogenous sample design (sample selection)</a></li>
<li class="chapter" data-level="8.5" data-path="measurement-error.html"><a href="measurement-error.html"><i class="fa fa-check"></i><b>8.5</b> Measurement Error</a></li>
<li class="chapter" data-level="8.6" data-path="proxy-variables.html"><a href="proxy-variables.html"><i class="fa fa-check"></i><b>8.6</b> Proxy Variables</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>9</b> Data</a><ul>
<li class="chapter" data-level="9.1" data-path="cross-sectional.html"><a href="cross-sectional.html"><i class="fa fa-check"></i><b>9.1</b> Cross-Sectional</a></li>
<li class="chapter" data-level="9.2" data-path="time-series-1.html"><a href="time-series-1.html"><i class="fa fa-check"></i><b>9.2</b> Time Series</a><ul>
<li class="chapter" data-level="9.2.1" data-path="time-series-1.html"><a href="time-series-1.html#deterministic-time-trend"><i class="fa fa-check"></i><b>9.2.1</b> Deterministic Time trend</a></li>
<li class="chapter" data-level="9.2.2" data-path="time-series-1.html"><a href="time-series-1.html#feedback-effect"><i class="fa fa-check"></i><b>9.2.2</b> Feedback Effect</a></li>
<li class="chapter" data-level="9.2.3" data-path="time-series-1.html"><a href="time-series-1.html#dynamic-specification"><i class="fa fa-check"></i><b>9.2.3</b> Dynamic Specification</a></li>
<li class="chapter" data-level="9.2.4" data-path="time-series-1.html"><a href="time-series-1.html#dynamically-complete"><i class="fa fa-check"></i><b>9.2.4</b> Dynamically Complete</a></li>
<li class="chapter" data-level="9.2.5" data-path="time-series-1.html"><a href="time-series-1.html#highly-persistent-data"><i class="fa fa-check"></i><b>9.2.5</b> Highly Persistent Data</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="repeated-cross-sections.html"><a href="repeated-cross-sections.html"><i class="fa fa-check"></i><b>9.3</b> Repeated Cross Sections</a><ul>
<li class="chapter" data-level="9.3.1" data-path="repeated-cross-sections.html"><a href="repeated-cross-sections.html#pooled-cross-section"><i class="fa fa-check"></i><b>9.3.1</b> Pooled Cross Section</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="panel-data.html"><a href="panel-data.html"><i class="fa fa-check"></i><b>9.4</b> Panel Data</a><ul>
<li class="chapter" data-level="9.4.1" data-path="panel-data.html"><a href="panel-data.html#pooled-ols-esimator"><i class="fa fa-check"></i><b>9.4.1</b> Pooled OLS Esimator</a></li>
<li class="chapter" data-level="9.4.2" data-path="panel-data.html"><a href="panel-data.html#individual-specific-effects-model"><i class="fa fa-check"></i><b>9.4.2</b> Individual-specific effects model</a></li>
<li class="chapter" data-level="9.4.3" data-path="panel-data.html"><a href="panel-data.html#tests-for-assumptions"><i class="fa fa-check"></i><b>9.4.3</b> Tests for Assumptions</a></li>
<li class="chapter" data-level="9.4.4" data-path="panel-data.html"><a href="panel-data.html#model-selection"><i class="fa fa-check"></i><b>9.4.4</b> Model Selection</a></li>
<li class="chapter" data-level="9.4.5" data-path="panel-data.html"><a href="panel-data.html#summary-1"><i class="fa fa-check"></i><b>9.4.5</b> Summary</a></li>
<li class="chapter" data-level="9.4.6" data-path="panel-data.html"><a href="panel-data.html#application-4"><i class="fa fa-check"></i><b>9.4.6</b> Application</a></li>
<li class="chapter" data-level="9.4.7" data-path="panel-data.html"><a href="panel-data.html#other-estimators"><i class="fa fa-check"></i><b>9.4.7</b> Other Estimators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>10</b> Hypothesis Testing</a><ul>
<li class="chapter" data-level="10.1" data-path="types-of-hypothesis-testing.html"><a href="types-of-hypothesis-testing.html"><i class="fa fa-check"></i><b>10.1</b> Types of hypothesis testing</a></li>
<li class="chapter" data-level="10.2" data-path="wald-test.html"><a href="wald-test.html"><i class="fa fa-check"></i><b>10.2</b> Wald test</a><ul>
<li class="chapter" data-level="10.2.1" data-path="wald-test.html"><a href="wald-test.html#multiple-hypothesis"><i class="fa fa-check"></i><b>10.2.1</b> Multiple Hypothesis</a></li>
<li class="chapter" data-level="10.2.2" data-path="wald-test.html"><a href="wald-test.html#linear-combination"><i class="fa fa-check"></i><b>10.2.2</b> Linear Combination</a></li>
<li class="chapter" data-level="10.2.3" data-path="wald-test.html"><a href="wald-test.html#application-5"><i class="fa fa-check"></i><b>10.2.3</b> Application</a></li>
<li class="chapter" data-level="10.2.4" data-path="wald-test.html"><a href="wald-test.html#nonlinear-1"><i class="fa fa-check"></i><b>10.2.4</b> Nonlinear</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="the-likelihood-ratio-test.html"><a href="the-likelihood-ratio-test.html"><i class="fa fa-check"></i><b>10.3</b> The likelihood ratio test</a></li>
<li class="chapter" data-level="10.4" data-path="lagrange-multiplier-score.html"><a href="lagrange-multiplier-score.html"><i class="fa fa-check"></i><b>10.4</b> Lagrange Multiplier (Score)</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="imputation-missing-data.html"><a href="imputation-missing-data.html"><i class="fa fa-check"></i><b>11</b> Imputation (Missing Data)</a><ul>
<li class="chapter" data-level="11.1" data-path="assumptions-1.html"><a href="assumptions-1.html"><i class="fa fa-check"></i><b>11.1</b> Assumptions</a><ul>
<li class="chapter" data-level="11.1.1" data-path="assumptions-1.html"><a href="assumptions-1.html#missing-completely-at-random-mcar"><i class="fa fa-check"></i><b>11.1.1</b> Missing Completely at Random (MCAR)</a></li>
<li class="chapter" data-level="11.1.2" data-path="assumptions-1.html"><a href="assumptions-1.html#missing-at-random-mar"><i class="fa fa-check"></i><b>11.1.2</b> Missing at Random (MAR)</a></li>
<li class="chapter" data-level="11.1.3" data-path="assumptions-1.html"><a href="assumptions-1.html#ignorable"><i class="fa fa-check"></i><b>11.1.3</b> Ignorable</a></li>
<li class="chapter" data-level="11.1.4" data-path="assumptions-1.html"><a href="assumptions-1.html#nonignorable"><i class="fa fa-check"></i><b>11.1.4</b> Nonignorable</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html"><i class="fa fa-check"></i><b>11.2</b> Solutions to Missing data</a><ul>
<li class="chapter" data-level="11.2.1" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#listwise-deletion"><i class="fa fa-check"></i><b>11.2.1</b> Listwise Deletion</a></li>
<li class="chapter" data-level="11.2.2" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#pairwise-deletion"><i class="fa fa-check"></i><b>11.2.2</b> Pairwise Deletion</a></li>
<li class="chapter" data-level="11.2.3" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#dummy-variable-adjustment"><i class="fa fa-check"></i><b>11.2.3</b> Dummy Variable Adjustment</a></li>
<li class="chapter" data-level="11.2.4" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#imputation"><i class="fa fa-check"></i><b>11.2.4</b> Imputation</a></li>
<li class="chapter" data-level="11.2.5" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#heckmans-sample-selection-model"><i class="fa fa-check"></i><b>11.2.5</b> Heckman’s Sample Selection Model</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="criteria-for-choosing-an-effective-approach.html"><a href="criteria-for-choosing-an-effective-approach.html"><i class="fa fa-check"></i><b>11.3</b> Criteria for Choosing an Effective Approach</a></li>
<li class="chapter" data-level="11.4" data-path="another-perspective.html"><a href="another-perspective.html"><i class="fa fa-check"></i><b>11.4</b> Another Perspective</a></li>
<li class="chapter" data-level="11.5" data-path="diagnosing-the-mechanism.html"><a href="diagnosing-the-mechanism.html"><i class="fa fa-check"></i><b>11.5</b> Diagnosing the Mechanism</a><ul>
<li class="chapter" data-level="11.5.1" data-path="diagnosing-the-mechanism.html"><a href="diagnosing-the-mechanism.html#mar-vs.-mnar"><i class="fa fa-check"></i><b>11.5.1</b> MAR vs. MNAR</a></li>
<li class="chapter" data-level="11.5.2" data-path="diagnosing-the-mechanism.html"><a href="diagnosing-the-mechanism.html#mcar-vs.-mar"><i class="fa fa-check"></i><b>11.5.2</b> MCAR vs. MAR</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="application-6.html"><a href="application-6.html"><i class="fa fa-check"></i><b>11.6</b> Application</a><ul>
<li class="chapter" data-level="11.6.1" data-path="application-6.html"><a href="application-6.html#imputation-with-mean-median-mode"><i class="fa fa-check"></i><b>11.6.1</b> Imputation with mean / median / mode</a></li>
<li class="chapter" data-level="11.6.2" data-path="application-6.html"><a href="application-6.html#knn"><i class="fa fa-check"></i><b>11.6.2</b> KNN</a></li>
<li class="chapter" data-level="11.6.3" data-path="application-6.html"><a href="application-6.html#rpart"><i class="fa fa-check"></i><b>11.6.3</b> rpart</a></li>
<li class="chapter" data-level="11.6.4" data-path="application-6.html"><a href="application-6.html#mice-multivariate-imputation-via-chained-equations"><i class="fa fa-check"></i><b>11.6.4</b> MICE (Multivariate Imputation via Chained Equations)</a></li>
<li class="chapter" data-level="11.6.5" data-path="application-6.html"><a href="application-6.html#amelia"><i class="fa fa-check"></i><b>11.6.5</b> Amelia</a></li>
<li class="chapter" data-level="11.6.6" data-path="application-6.html"><a href="application-6.html#missforest"><i class="fa fa-check"></i><b>11.6.6</b> missForest</a></li>
<li class="chapter" data-level="11.6.7" data-path="application-6.html"><a href="application-6.html#hmisc"><i class="fa fa-check"></i><b>11.6.7</b> Hmisc</a></li>
<li class="chapter" data-level="11.6.8" data-path="application-6.html"><a href="application-6.html#mi"><i class="fa fa-check"></i><b>11.6.8</b> mi</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III EXPERIMENTAL DESIGN</b></span></li>
<li class="chapter" data-level="12" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html"><i class="fa fa-check"></i><b>12</b> Analysis of Variance (ANOVA)</a><ul>
<li class="chapter" data-level="12.1" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html"><i class="fa fa-check"></i><b>12.1</b> Completely Randomized Design (CRD)</a><ul>
<li class="chapter" data-level="12.1.1" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#single-factor-fixed-effects-model"><i class="fa fa-check"></i><b>12.1.1</b> Single Factor Fixed Effects Model</a></li>
<li class="chapter" data-level="12.1.2" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#single-factor-random-effects-model"><i class="fa fa-check"></i><b>12.1.2</b> Single Factor Random Effects Model</a></li>
<li class="chapter" data-level="12.1.3" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#two-factor-fixed-effect-anova"><i class="fa fa-check"></i><b>12.1.3</b> Two Factor Fixed Effect ANOVA</a></li>
<li class="chapter" data-level="12.1.4" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#two-way-random-effects-anova"><i class="fa fa-check"></i><b>12.1.4</b> Two-Way Random Effects ANOVA</a></li>
<li class="chapter" data-level="12.1.5" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#two-way-mixed-effects-anova"><i class="fa fa-check"></i><b>12.1.5</b> Two-Way Mixed Effects ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="randomized-block-designs.html"><a href="randomized-block-designs.html"><i class="fa fa-check"></i><b>12.2</b> Randomized Block Designs</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="causality.html"><a href="causality.html"><i class="fa fa-check"></i><b>13</b> Causality</a></li>
<li class="chapter" data-level="14" data-path="report.html"><a href="report.html"><i class="fa fa-check"></i><b>14</b> Report</a><ul>
<li class="chapter" data-level="14.1" data-path="one-summary-table.html"><a href="one-summary-table.html"><i class="fa fa-check"></i><b>14.1</b> One summary table</a></li>
<li class="chapter" data-level="14.2" data-path="model-comparison.html"><a href="model-comparison.html"><i class="fa fa-check"></i><b>14.2</b> Model Comparison</a></li>
<li class="chapter" data-level="14.3" data-path="changes-in-an-estimate.html"><a href="changes-in-an-estimate.html"><i class="fa fa-check"></i><b>14.3</b> Changes in an estimate</a></li>
</ul></li>
<li class="appendix"><span><b>APPENDIX</b></span></li>
<li class="chapter" data-level="A" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>A</b> Appendix</a><ul>
<li class="chapter" data-level="A.1" data-path="short-cut.html"><a href="short-cut.html"><i class="fa fa-check"></i><b>A.1</b> Short-cut</a></li>
<li class="chapter" data-level="A.2" data-path="function-short-cut.html"><a href="function-short-cut.html"><i class="fa fa-check"></i><b>A.2</b> Function short-cut</a></li>
<li class="chapter" data-level="A.3" data-path="citation.html"><a href="citation.html"><i class="fa fa-check"></i><b>A.3</b> Citation</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="bookdown-cheat-sheet.html"><a href="bookdown-cheat-sheet.html"><i class="fa fa-check"></i><b>B</b> Bookdown cheat sheet</a><ul>
<li class="chapter" data-level="B.1" data-path="operation.html"><a href="operation.html"><i class="fa fa-check"></i><b>B.1</b> Operation</a></li>
<li class="chapter" data-level="B.2" data-path="math-expresssion-syntax.html"><a href="math-expresssion-syntax.html"><i class="fa fa-check"></i><b>B.2</b> Math Expresssion/ Syntax</a><ul>
<li class="chapter" data-level="B.2.1" data-path="math-expresssion-syntax.html"><a href="math-expresssion-syntax.html#statistics-notation"><i class="fa fa-check"></i><b>B.2.1</b> Statistics Notation</a></li>
</ul></li>
<li class="chapter" data-level="B.3" data-path="table.html"><a href="table.html"><i class="fa fa-check"></i><b>B.3</b> Table</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Guide on Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="non-linear-least-squares" class="section level2">
<h2><span class="header-section-number">6.2</span> Non-linear Least Squares</h2>
<ul>
<li>The LS estimate of <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\hat{\theta}\)</span> is the set of parameters that minimizes the residual sum of squares:<br />
<span class="math display">\[
S(\hat{\theta}) = SSE(\hat{\theta}) = \sum_{i=1}^{n}\{Y_i - f(\mathbf{x_i};\hat{\theta})\}^2
\]</span></li>
<li>to obtain the solution, we can consider the partial derivatives of <span class="math inline">\(S(\theta)\)</span> with respect to each <span class="math inline">\(\theta_j\)</span> and set them to 0, which gives a system of p equations. Each normal equation is
<span class="math display">\[
\frac{\partial S(\theta)}{\partial \theta_j} = -2\sum_{i=1}^{n}\{Y_i -f(\mathbf{x}_i;\theta)\}[\frac{\partial(\mathbf{x}_i;\theta)}{\partial \theta_j}] = 0
\]</span></li>
<li>but we can’t obtain a solution directly/analytically for this equation.</li>
</ul>
<p><strong>Numerical Solutions</strong></p>
<ul>
<li>Grid search
<ul>
<li>A “grid” of possible parameter values and see which one minimize the residual sum of squares.<br />
</li>
<li>finer grid = greater accuracy<br />
</li>
<li>could be inefficient, and hard when p is large.<br />
</li>
</ul></li>
<li>Gauss-Newton Algorithm
<ul>
<li>we have an initial estimate of <span class="math inline">\(\theta\)</span> denoted as <span class="math inline">\(\hat{\theta}^{(0)}\)</span><br />
</li>
<li>use a Taylor expansions of <span class="math inline">\(f(\mathbf{x}_i;\theta)\)</span> as a function of <span class="math inline">\(\theta\)</span> about the point <span class="math inline">\(\hat{\theta}^{(0)}\)</span></li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{align} 
Y_i &amp;= f(x_i;\theta) + \epsilon_i \\
&amp;= f(x_i;\theta) + \sum_{j=1}^{p}\{\frac{\partial f(x_i;\theta)}{\partial \theta_j}\}_{\theta = \hat{\theta}^{(0)}} (\theta_j - \hat{\theta}^{(0)}) + \text{remainder} + \epsilon_i
\end{align}
\]</span></p>
<p>Equivalently,</p>
<p>In matrix notation,</p>
<p><span class="math display">\[
\mathbf{Y} = 
\left[ \begin{array}
{ccc}
Y_1 \\
. \\
Y_n
\end{array} \right]
\]</span></p>
<p><span class="math display">\[
\mathbf{f}(\hat{\theta}^{(0)}) =
\left[ \begin{array}
{ccc}
f(\mathbf{x_1,\hat{\theta}}^{(0)}) \\
. \\
f(\mathbf{x_n,\hat{\theta}}^{(0)})
\end{array} \right]
\]</span></p>
<p><span class="math display">\[
\mathbf{\epsilon} = 
\left[ \begin{array}
{ccc}
\epsilon_1 \\
. \\
\epsilon_n
\end{array} \right]
\]</span></p>
<p><span class="math display">\[
\mathbf{F}(\hat{\theta}^{(0)}) = 
\left[ \begin{array}
{ccc}
\frac{\partial f(x_1,\mathbf{\theta})}{\partial \theta_1} &amp;&amp; ... &amp;&amp; \frac{\partial f(x_1,\mathbf{\theta})}{\partial \theta_p}\\
. &amp;&amp; . &amp;&amp; . \\
\frac{\partial f(x_n,\mathbf{\theta})}{\partial \theta_1} &amp;&amp; ... &amp;&amp; \frac{\partial f(x_n,\mathbf{\theta})}{\partial \theta_p}
\end{array} \right]_{\theta = \hat{\theta}^{(0)}}
\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[
\mathbf{Y} = \mathbf{f}(\hat{\theta}^{(0)}) + \mathbf{F}(\hat{\theta}^{(0)})(\theta - \hat{\theta}^{(0)}) + \epsilon + \text{remainder}
\]</span>
where we assume that the remainder is small and the error term is only assumed to be iid with mean 0 and variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>We can rewrite the above equation as</p>
<p><span class="math display">\[
\mathbf{Y} - \mathbf{f}(\hat{\theta}^{(0)}) \approx \mathbf{F}(\hat{\theta}^{(0)})(\theta - \hat{\theta}^{(0)}) + \epsilon
\]</span>
where it is in the form of linear model. After we solve for <span class="math inline">\((\theta - \hat{\theta}^{(0)})\)</span> and let it equal to <span class="math inline">\(\hat{\delta}^{(1)}\)</span><br />
Then we new estimate is given by adding the Gauss increment adjustment to the initial estimate <span class="math inline">\(\hat{\theta}^{(1)} = \hat{\theta}^{(0)} + \hat{\delta}^{(1)}\)</span><br />
We can repeat this process.</p>
<p>Gauss-Newton Algorithm Steps:</p>
<ol style="list-style-type: decimal">
<li>initial estimate <span class="math inline">\(\hat{\theta}^{(0)}\)</span>, set j = 0<br />
</li>
<li>Taylor series expansion and calculate <span class="math inline">\(\mathbf{f}(\hat{\theta}^{(j)})\)</span> and <span class="math inline">\(\mathbf{F}(\hat{\theta}^{(j)})\)</span></li>
<li>Use OLS to get <span class="math inline">\(\hat{\delta}^{(j+1)}\)</span><br />
</li>
<li>get the new estimate <span class="math inline">\(\hat{\theta}^{(j+1)}\)</span>, return to step 2<br />
</li>
<li>continue until “convergence”<br />
</li>
<li>With the final parameter estimate <span class="math inline">\(\hat{\theta}\)</span>, we can estimate <span class="math inline">\(\sigma^2\)</span> if <span class="math inline">\(\epsilon \sim (\mathbf{0}, \sigma^2 \mathbf{I})\)</span> by</li>
</ol>
<p><span class="math display">\[
\hat{\sigma}^2= \frac{1}{n-p}(\mathbf{Y}-\mathbf{f}(x;\hat{\theta}))&#39;(\mathbf{Y}-\mathbf{f}(x;\hat{\theta}))
\]</span></p>
<p><br></p>
<p><strong>Criteria for convergence</strong></p>
<ol style="list-style-type: decimal">
<li>Minor change in the objective function (SSE = residual sum of squares)<br />
<span class="math display">\[
\frac{|SSE(\hat{\theta}^{(j+1)})-SSE(\hat{\theta}^{(j)})|}{SSE(\hat{\theta}^{(j)})} &lt; \gamma_1
\]</span></li>
<li>Minor change in the parameter estimates<br />
<span class="math display">\[
|\hat{\theta}^{(j+1)}-\hat{\theta}^{(j)}| &lt; \gamma_2
\]</span></li>
<li>“residual projection” criterion of <span class="citation">(Bates and Watts <a href="#ref-Bates_1981" role="doc-biblioref">1981</a>)</span></li>
</ol>
<div id="alternative-of-gauss-newton-algorithm" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Alternative of Gauss-Newton Algorithm</h3>
<div id="gauss-newton-algorithm" class="section level4">
<h4><span class="header-section-number">6.2.1.1</span> Gauss-Newton Algorithm</h4>
<p>Normal equations:</p>
<p><span class="math display">\[
\frac{\partial SSE(\theta)}{\partial \theta} = 2\mathbf{F}(\theta)&#39;[\mathbf{Y}-\mathbf{f}(\theta)]
\]</span></p>
<p><span class="math display">\[
\begin{align}
\hat{\theta}^{(j+1)} &amp;= \hat{\theta}^{(j)} + \hat{\delta}^{(j+1)} \\
&amp;= \hat{\theta}^{(j)} + [\mathbf{F}((\hat{\theta})^{(j)})&#39;\mathbf{F}(\hat{\theta}^{(j)})]^{-1}\mathbf{F}(\hat{\theta})^{(j)} \\
&amp;= \hat{\theta}^{(j)} - \frac{1}{2}[\mathbf{F}(\hat{\theta}^{(j)})&#39;\mathbf{F}(\hat{\theta}^{(j)})]^{-1}\frac{\partial SSE(\hat{\theta}^{(j)})}{\partial \theta}
\end{align}
\]</span>
where</p>
<ul>
<li><span class="math inline">\(\frac{\partial SSE(\hat{\theta}^{(j)})}{\partial \theta}\)</span> is a gradient vecotr (points in the direction in which the SSE increases most rapidly). This path is known as steepest ascent.<br />
</li>
<li><span class="math inline">\([\mathbf{F}(\hat{\theta}^{(j)})&#39;\mathbf{F}(\hat{\theta}^{(j)})]^{-1}\)</span> indicates how far to move<br />
</li>
<li><span class="math inline">\(-1/2\)</span>: indicator of the direction of steepest descent.</li>
</ul>
</div>
<div id="modified-gauss-newton-algorithm" class="section level4">
<h4><span class="header-section-number">6.2.1.2</span> Modified Gauss-Newton Algorithm</h4>
<p>To avoid overstepping (the local min), we can use the modified Gauss-Newton Algorithm. We define a new proposal for <span class="math inline">\(\theta\)</span></p>
<p><span class="math display">\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} + \alpha_j \hat{\delta}^{(j+1)}, 0 &lt; \alpha_j &lt; 1
\]</span>
where</p>
<ul>
<li><span class="math inline">\(\alpha_j\)</span> (called the “learning rate”): is used to modify the step length.</li>
</ul>
<p>We could also have <span class="math inline">\(\alpha *1/2\)</span>, but typically it is assumed to be absorbed into the learning rate.</p>
<p>A way to choose <span class="math inline">\(\alpha_j\)</span>, we can use <strong>step halving</strong></p>
<p><span class="math display">\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} + \frac{1}{2^k}\hat{\delta}^{(j+1)}
\]</span>
where</p>
<ul>
<li>k is the smallest non-negative integer such that<br />
<span class="math display">\[
SSE(\hat{\theta}^{(j)}+\frac{1}{2^k}\hat{\delta}^{(j+1)}) &lt; SSE(\hat{\theta}^{(j)})
\]</span>
which means we try <span class="math inline">\(\hat{\delta}^{(j+1)}\)</span>, then <span class="math inline">\(\hat{\delta}^{(j+1)}/2\)</span>, <span class="math inline">\(\hat{\delta}^{(j+1)}/4\)</span>, etc.</li>
</ul>
<p>The most general form of the convergence algorithm is</p>
<p><span class="math display">\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j \mathbf{A}_j \frac{\partial Q(\hat{\theta}^{(j)})}{\partial \theta} 
\]</span>
where</p>
<ul>
<li><span class="math inline">\(\mathbf{A}_j\)</span> is a positive definite matrix<br />
</li>
<li><span class="math inline">\(\alpha_j\)</span> is the learning rate<br />
</li>
<li><span class="math inline">\(\frac{\partial Q(\hat{\theta}^{(j)})}{\partial \theta}\)</span>is the gradient based on some objective function Q (a function of <span class="math inline">\(\theta\)</span>), which is typically the SSE in nonlinear regression applications (e.g., cross-entropy for classification).</li>
</ul>
<p>Refer back to the <strong>Modified Gauss-Newton Algorithm</strong>, we can see it is in this form</p>
<p><span class="math display">\[
\hat{\theta}^{(j+1)} =\hat{\theta}^{(j)} - \alpha_j[\mathbf{F}(\hat{\theta}^{(j)})&#39;\mathbf{F}(\hat{\theta}^{(j)})]^{-1}\frac{\partial SSE(\hat{\theta}^{(j)})}{\partial \theta}
\]</span>
where Q = SSE, <span class="math inline">\([\mathbf{F}(\hat{\theta}^{(j)})&#39;\mathbf{F}(\hat{\theta}^{(j)})]^{-1} = \mathbf{A}\)</span></p>
</div>
<div id="steepest-descent" class="section level4">
<h4><span class="header-section-number">6.2.1.3</span> Steepest Descent</h4>
<p>(also known just “gradient descent”)</p>
<p><span class="math display">\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j \mathbf{I}_{p \times p}\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}
\]</span></p>
<ul>
<li>slow to converge, moves rapidly initially.<br />
</li>
<li>could be use for starting values</li>
</ul>
</div>
<div id="levenberg--marquardt" class="section level4">
<h4><span class="header-section-number">6.2.1.4</span> Levenberg -Marquardt</h4>
<p><span class="math display">\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j [\mathbf{F}(\hat{\theta}^{(j)})&#39;\mathbf{F}(\hat{\theta}^{(j)})+ \tau \mathbf{I}_{p \times p}]\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}
\]</span></p>
<p>which is a compromise between the <a href="non-linear-least-squares.html#gauss-newton-algorithm">Gauss-Newton Algorithm</a> and the <a href="non-linear-least-squares.html#steepest-descent">Steepest Descent</a>.</p>
<ul>
<li>best when <span class="math inline">\(\mathbf{F}(\hat{\theta}^{(j)})&#39;\mathbf{F}(\hat{\theta}^{(j)})\)</span> is nearly singular (<span class="math inline">\(\mathbf{F}(\hat{\theta}^{(j)})\)</span> isn’t of full rank)<br />
</li>
<li>similar to ridge regression<br />
</li>
<li>If <span class="math inline">\(SSE(\hat{\theta}^{(j+1)}) &lt; SSE(\hat{\theta}^{(j)})\)</span>, then <span class="math inline">\(\tau= \tau/10\)</span> for the next iteration. Otherwise, <span class="math inline">\(\tau = 10 \tau\)</span></li>
</ul>
</div>
<div id="newton-raphson" class="section level4">
<h4><span class="header-section-number">6.2.1.5</span> Newton-Raphson</h4>
<p><span class="math display">\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j [\frac{\partial^2Q(\hat{\theta}^{(j)})}{\partial \theta \partial \theta&#39;}]^{-1}\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}
\]</span></p>
<p>The <strong>Hessian matrix</strong> can be rewritten as:</p>
<p><span class="math display">\[
\frac{\partial^2Q(\hat{\theta}^{(j)})}{\partial \theta \partial \theta&#39;} = 2 \mathbf{F}((\hat{\theta})^{(j)})&#39;\mathbf{F}(\hat{\theta}^{(j)}) - 2\sum_{i=1}^{n}[Y_i - f(x_i;\theta)]\frac{\partial^2f(x_i;\theta)}{\partial \theta \partial \theta&#39;}
\]</span>
which contains the same term that <a href="non-linear-least-squares.html#gauss-newton-algorithm">Gauss-Newton Algorithm</a>, combined with one containing the second partial derivatives of f(). (methods that require the second derivatives of the objective function are known as “second-order methods”.)<br />
However, the last term  can sometimes be nonsingular.</p>
</div>
<div id="quasi-newton" class="section level4">
<h4><span class="header-section-number">6.2.1.6</span> Quasi-Newton</h4>
<p>update <span class="math inline">\(\theta\)</span> according to</p>
<p><span class="math display">\[
\hat{\theta}^{(j+1)} = \hat{\theta}^{(j)} - \alpha_j \mathbf{H}_j^{-1}\frac{\partial \mathbf{Q}(\hat{\theta}^{(j)})}{\partial \theta}
\]</span>
where <span class="math inline">\(H_j\)</span> is a symmetric positive definite approximation to the Hessian, which gets closer as <span class="math inline">\(j \to \infty\)</span>.</p>
<ul>
<li><span class="math inline">\(\mathbf{H}_j\)</span> is computed iteratively<br />
</li>
<li>AMong first-order methods(where only first derivatives are required), this method performs best.</li>
</ul>
</div>
<div id="derivative-free-methods" class="section level4">
<h4><span class="header-section-number">6.2.1.7</span> Derivative Free Methods</h4>
<ul>
<li><strong>secant Method</strong>: like <a href="non-linear-least-squares.html#gauss-newton-algorithm">Gauss-Newton Algorithm</a>, but calculates the derivatives numerically from past iterations.<br />
</li>
<li><strong>Simplex Methods</strong><br />
</li>
<li><strong>Genetic Algorithm</strong><br />
</li>
<li><strong>Differential Evolution Algorithms</strong><br />
</li>
<li><strong>Particle Swarm Optimization</strong><br />
</li>
<li><strong>Ant Colony Optimization</strong></li>
</ul>
</div>
</div>
<div id="practical-considerations" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Practical Considerations</h3>
<p>To converge, algorithm need good initial estimates.</p>
<ul>
<li>Starting values:
<ul>
<li>Prior or theoretical info<br />
</li>
<li>A grid search or a graph of <span class="math inline">\(SSE(\theta)\)</span><br />
</li>
<li>could also use OLS to get starting values.<br />
</li>
<li>Model interpretation: if you have some idea regarding the form of the objective function, then you can try to guess the initial value.<br />
</li>
<li>Expected Value Parameterization<br />
</li>
</ul></li>
<li>Constrained Parameters: (constraints on parameters like <span class="math inline">\(\theta_i&gt;a,a&lt; \theta_i &lt;b\)</span>)
<ul>
<li>fit the model first to see if the converged parameter estimates satisfy the constraints.</li>
<li>if they dont’ satisfy, then try re-parameterizing</li>
</ul></li>
</ul>
<div id="failure-to-converge" class="section level4">
<h4><span class="header-section-number">6.2.2.1</span> Failure to converge</h4>
<ul>
<li><span class="math inline">\(SSE(\theta)\)</span> may be “flat” in a neighborhood of the minimum.<br />
</li>
<li>You can try different or “better” starting values.<br />
</li>
<li>Might suggest the model is too complex for the data, might consider simpler model.</li>
</ul>
</div>
<div id="convergence-to-a-local-minimum" class="section level4">
<h4><span class="header-section-number">6.2.2.2</span> Convergence to a Local Minimum</h4>
<ul>
<li>Linear least squares has the property that <span class="math inline">\(SSE(\theta) = \mathbf{(Y-X\beta)&#39;(Y-X\beta)}\)</span>, which is quadratic and has a unique minimum (or maximum).<br />
</li>
<li>Nonlinear east squares need not have a unique minimum<br />
</li>
<li>Using different starting values can help<br />
</li>
<li>If the dimension of <span class="math inline">\(\theta\)</span> is low, graph <span class="math inline">\(SSE(\theta)\)</span> as a function of <span class="math inline">\(\theta_i\)</span><br />
</li>
<li>Different algorithm can help (e.g., genetic algorithm, particle swarm)</li>
</ul>
<p>To converge, algorithms need good initial estimates.</p>
<ul>
<li>Starting values:
<ul>
<li>prior or theoretical info<br />
</li>
<li>A grid search or a graph</li>
<li>OLS estimates as starting values<br />
</li>
<li>Model interpretation<br />
</li>
<li>Expected Value Parameterization<br />
</li>
</ul></li>
<li>Constrained Parameters:
<ul>
<li>try the model without the constraints first.<br />
</li>
<li>If the resulted parameter estimates does not satisfy the constraint, try re-parameterizing</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="non-linear-least-squares.html#cb154-1"></a><span class="co"># Grid search</span></span>
<span id="cb154-2"><a href="non-linear-least-squares.html#cb154-2"></a><span class="co">#choose grid of a and b values</span></span>
<span id="cb154-3"><a href="non-linear-least-squares.html#cb154-3"></a>aseq =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">10</span>,<span class="dv">18</span>,.<span class="dv">2</span>)</span>
<span id="cb154-4"><a href="non-linear-least-squares.html#cb154-4"></a>bseq =<span class="st"> </span><span class="kw">seq</span>(.<span class="dv">001</span>,.<span class="dv">075</span>,.<span class="dv">001</span>)</span>
<span id="cb154-5"><a href="non-linear-least-squares.html#cb154-5"></a></span>
<span id="cb154-6"><a href="non-linear-least-squares.html#cb154-6"></a>na =<span class="st"> </span><span class="kw">length</span>(aseq)</span>
<span id="cb154-7"><a href="non-linear-least-squares.html#cb154-7"></a>nb =<span class="st"> </span><span class="kw">length</span>(bseq)</span>
<span id="cb154-8"><a href="non-linear-least-squares.html#cb154-8"></a>SSout =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>,na<span class="op">*</span>nb,<span class="dv">3</span>) <span class="co">#matrix to save output</span></span>
<span id="cb154-9"><a href="non-linear-least-squares.html#cb154-9"></a>cnt =<span class="st"> </span><span class="dv">0</span></span>
<span id="cb154-10"><a href="non-linear-least-squares.html#cb154-10"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>na){</span>
<span id="cb154-11"><a href="non-linear-least-squares.html#cb154-11"></a>   <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>nb){</span>
<span id="cb154-12"><a href="non-linear-least-squares.html#cb154-12"></a>      cnt =<span class="st"> </span>cnt<span class="op">+</span><span class="dv">1</span></span>
<span id="cb154-13"><a href="non-linear-least-squares.html#cb154-13"></a>      ypred =<span class="st"> </span><span class="kw">mod</span>(aseq[k],bseq[j],x) <span class="co">#evaluate model w/ these parms</span></span>
<span id="cb154-14"><a href="non-linear-least-squares.html#cb154-14"></a>      ss =<span class="st"> </span><span class="kw">sum</span>((y<span class="op">-</span>ypred)<span class="op">^</span><span class="dv">2</span>)  <span class="co">#this is our SSE objective function</span></span>
<span id="cb154-15"><a href="non-linear-least-squares.html#cb154-15"></a>      <span class="co">#save values of a, b, and SSE</span></span>
<span id="cb154-16"><a href="non-linear-least-squares.html#cb154-16"></a>      SSout[cnt,<span class="dv">1</span>]=aseq[k]</span>
<span id="cb154-17"><a href="non-linear-least-squares.html#cb154-17"></a>      SSout[cnt,<span class="dv">2</span>]=bseq[j]</span>
<span id="cb154-18"><a href="non-linear-least-squares.html#cb154-18"></a>      SSout[cnt,<span class="dv">3</span>]=ss</span>
<span id="cb154-19"><a href="non-linear-least-squares.html#cb154-19"></a>   }</span>
<span id="cb154-20"><a href="non-linear-least-squares.html#cb154-20"></a>}</span>
<span id="cb154-21"><a href="non-linear-least-squares.html#cb154-21"></a><span class="co">#find minimum SSE and associated a,b values</span></span>
<span id="cb154-22"><a href="non-linear-least-squares.html#cb154-22"></a>mn_indx =<span class="st"> </span><span class="kw">which.min</span>(SSout[,<span class="dv">3</span>])</span>
<span id="cb154-23"><a href="non-linear-least-squares.html#cb154-23"></a>astrt =<span class="st"> </span>SSout[mn_indx,<span class="dv">1</span>]</span>
<span id="cb154-24"><a href="non-linear-least-squares.html#cb154-24"></a>bstrt =<span class="st"> </span>SSout[mn_indx,<span class="dv">2</span>]</span>
<span id="cb154-25"><a href="non-linear-least-squares.html#cb154-25"></a><span class="co">#now, run nls function with these starting values</span></span>
<span id="cb154-26"><a href="non-linear-least-squares.html#cb154-26"></a>nlin_modG=<span class="kw">nls</span>(y<span class="op">~</span><span class="kw">mod</span>(a,b,x),<span class="dt">start=</span><span class="kw">list</span>(<span class="dt">a=</span>astrt,<span class="dt">b=</span>bstrt)) </span>
<span id="cb154-27"><a href="non-linear-least-squares.html#cb154-27"></a></span>
<span id="cb154-28"><a href="non-linear-least-squares.html#cb154-28"></a>nlin_modG</span></code></pre></div>
<pre><code>## Nonlinear regression model
##   model: y ~ mod(a, b, x)
##    data: parent.frame()
##        a        b 
## 13.60391  0.01911 
##  residual sum-of-squares: 235.5
## 
## Number of iterations to convergence: 3 
## Achieved convergence tolerance: 2.293e-07</code></pre>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="non-linear-least-squares.html#cb156-1"></a><span class="co"># Note, the package `nls_multstart` will allow you to do a grid search without programming your own loop</span></span></code></pre></div>
<p>For prediction interval</p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="non-linear-least-squares.html#cb157-1"></a><span class="kw">plotFit</span>(nlin_modG, <span class="dt">interval =</span> <span class="st">&quot;both&quot;</span>, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">shade =</span> <span class="ot">TRUE</span>, </span>
<span id="cb157-2"><a href="non-linear-least-squares.html#cb157-2"></a>        <span class="dt">col.conf =</span> <span class="st">&quot;skyblue4&quot;</span>, <span class="dt">col.pred =</span> <span class="st">&quot;lightskyblue2&quot;</span>,<span class="dt">data=</span>datf)  </span></code></pre></div>
<p><img src="Data-Analysis_files/figure-html/unnamed-chunk-50-1.png" width="672" /></p>
<p>Based on the forms of your function, you can also have programmed starting values from <code>nls</code> function (e.e.g, logistic growth, asymptotic regression, etc).</p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="non-linear-least-squares.html#cb158-1"></a><span class="kw">apropos</span>(<span class="st">&quot;^SS&quot;</span>)</span></code></pre></div>
<pre><code>##  [1] &quot;ss&quot;             &quot;SS_nonlinModel&quot; &quot;SSasymp&quot;        &quot;SSasympOff&quot;    
##  [5] &quot;SSasympOrig&quot;    &quot;SSbiexp&quot;        &quot;SSD&quot;            &quot;SSfol&quot;         
##  [9] &quot;SSfpl&quot;          &quot;SSgompertz&quot;     &quot;SSlogis&quot;        &quot;SSmicmen&quot;      
## [13] &quot;SSout&quot;          &quot;SSweibull&quot;</code></pre>
<p>For example, a logistic growth model:</p>
<p><span class="math display">\[
P = \frac{K}{1+ exp(P_0+ rt)} + \epsilon
\]</span>
where</p>
<ul>
<li>P = population at time t<br />
</li>
<li>K = carrying capacity<br />
</li>
<li>r = population growth rate</li>
</ul>
<p>but in <code>R</code> you have slight different parameterization:</p>
<p><span class="math display">\[
P = \frac{asym}{1 + exp(\frac{xmid - t}{scal})}
\]</span>
where</p>
<ul>
<li>asym = carrying capacity<br />
</li>
<li>xmid = the x value at the inflection point of the curve<br />
</li>
<li>scal = scaling parameter.</li>
</ul>
<p>Hence, you have</p>
<ul>
<li>K = asym<br />
</li>
<li>r = -1/scal<br />
</li>
<li><span class="math inline">\(P_0 = -rxmid\)</span></li>
</ul>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="non-linear-least-squares.html#cb160-1"></a><span class="co"># simulated data</span></span>
<span id="cb160-2"><a href="non-linear-least-squares.html#cb160-2"></a>time &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">15</span>,<span class="dv">20</span>,<span class="dv">25</span>,<span class="dv">30</span>,<span class="dv">35</span>)</span>
<span id="cb160-3"><a href="non-linear-least-squares.html#cb160-3"></a>population &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">2.8</span>,<span class="fl">4.2</span>,<span class="fl">3.5</span>,<span class="fl">6.3</span>,<span class="fl">15.7</span>,<span class="fl">21.3</span>,<span class="fl">23.7</span>,<span class="fl">25.1</span>,<span class="fl">25.8</span>,<span class="fl">25.9</span>)</span>
<span id="cb160-4"><a href="non-linear-least-squares.html#cb160-4"></a><span class="kw">plot</span>(time, population, <span class="dt">las=</span><span class="dv">1</span>, <span class="dt">pch=</span><span class="dv">16</span>)</span></code></pre></div>
<p><img src="Data-Analysis_files/figure-html/unnamed-chunk-52-1.png" width="672" /></p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb161-1"><a href="non-linear-least-squares.html#cb161-1"></a><span class="co"># model fitting</span></span>
<span id="cb161-2"><a href="non-linear-least-squares.html#cb161-2"></a>logisticModelSS &lt;-<span class="st"> </span><span class="kw">nls</span>(population<span class="op">~</span><span class="kw">SSlogis</span>(time, Asym, xmid, scal))</span>
<span id="cb161-3"><a href="non-linear-least-squares.html#cb161-3"></a><span class="kw">summary</span>(logisticModelSS)</span></code></pre></div>
<pre><code>## 
## Formula: population ~ SSlogis(time, Asym, xmid, scal)
## 
## Parameters:
##      Estimate Std. Error t value Pr(&gt;|t|)    
## Asym  25.5029     0.3666   69.56 3.34e-11 ***
## xmid   8.7347     0.3007   29.05 1.48e-08 ***
## scal   3.6353     0.2186   16.63 6.96e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6528 on 7 degrees of freedom
## 
## Number of iterations to convergence: 1 
## Achieved convergence tolerance: 1.908e-06</code></pre>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb163-1"><a href="non-linear-least-squares.html#cb163-1"></a><span class="kw">coef</span>(logisticModelSS)</span></code></pre></div>
<pre><code>##      Asym      xmid      scal 
## 25.502890  8.734698  3.635333</code></pre>
<p>Other parameterization</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="non-linear-least-squares.html#cb165-1"></a><span class="co">#convert to other parameterization</span></span>
<span id="cb165-2"><a href="non-linear-least-squares.html#cb165-2"></a>Ks =<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">coef</span>(logisticModelSS)[<span class="dv">1</span>])</span>
<span id="cb165-3"><a href="non-linear-least-squares.html#cb165-3"></a>rs =<span class="st"> </span><span class="dv">-1</span><span class="op">/</span><span class="kw">as.numeric</span>(<span class="kw">coef</span>(logisticModelSS)[<span class="dv">3</span>])</span>
<span id="cb165-4"><a href="non-linear-least-squares.html#cb165-4"></a>Pos =<span class="st"> </span><span class="op">-</span><span class="st"> </span>rs <span class="op">*</span><span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">coef</span>(logisticModelSS)[<span class="dv">2</span>])</span>
<span id="cb165-5"><a href="non-linear-least-squares.html#cb165-5"></a><span class="co">#let&#39;s refit with these parameters</span></span>
<span id="cb165-6"><a href="non-linear-least-squares.html#cb165-6"></a>logisticModel &lt;-<span class="st"> </span><span class="kw">nls</span>(population <span class="op">~</span><span class="st"> </span>K <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(Po <span class="op">+</span><span class="st"> </span>r <span class="op">*</span><span class="st"> </span>time)),<span class="dt">start=</span><span class="kw">list</span>(<span class="dt">Po=</span>Pos,<span class="dt">r=</span>rs,<span class="dt">K=</span>Ks))</span>
<span id="cb165-7"><a href="non-linear-least-squares.html#cb165-7"></a><span class="kw">summary</span>(logisticModel)</span></code></pre></div>
<pre><code>## 
## Formula: population ~ K/(1 + exp(Po + r * time))
## 
## Parameters:
##    Estimate Std. Error t value Pr(&gt;|t|)    
## Po  2.40272    0.12702   18.92 2.87e-07 ***
## r  -0.27508    0.01654  -16.63 6.96e-07 ***
## K  25.50289    0.36665   69.56 3.34e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6528 on 7 degrees of freedom
## 
## Number of iterations to convergence: 0 
## Achieved convergence tolerance: 1.924e-06</code></pre>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="non-linear-least-squares.html#cb167-1"></a><span class="co">#note: initial values =  solution (highly unusual, but ok)</span></span>
<span id="cb167-2"><a href="non-linear-least-squares.html#cb167-2"></a><span class="kw">plot</span>(time, population, <span class="dt">las=</span><span class="dv">1</span>, <span class="dt">pch=</span><span class="dv">16</span>)</span>
<span id="cb167-3"><a href="non-linear-least-squares.html#cb167-3"></a><span class="kw">lines</span>(time,<span class="kw">predict</span>(logisticModel),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="Data-Analysis_files/figure-html/unnamed-chunk-54-1.png" width="672" /></p>
<p>If can also define your own self-starting fucntion if your models are uncommon (built in <code>nls</code>)</p>
<p>Example is based on <span class="citation">(Schabenberger and Pierce <a href="#ref-Schabenberger_2001" role="doc-biblioref">2001</a>)</span></p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="non-linear-least-squares.html#cb168-1"></a><span class="co">#Load data</span></span>
<span id="cb168-2"><a href="non-linear-least-squares.html#cb168-2"></a>dat &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;images/dat.txt&quot;</span>, <span class="dt">header =</span> T)</span>
<span id="cb168-3"><a href="non-linear-least-squares.html#cb168-3"></a><span class="co"># plot</span></span>
<span id="cb168-4"><a href="non-linear-least-squares.html#cb168-4"></a>dat.plot &lt;-<span class="st"> </span><span class="kw">ggplot</span>(dat)<span class="op">+</span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">x=</span>no3,<span class="dt">y=</span>ryp, <span class="dt">color=</span><span class="kw">as.factor</span>(depth))) <span class="op">+</span></span>
<span id="cb168-5"><a href="non-linear-least-squares.html#cb168-5"></a><span class="kw">labs</span>(<span class="dt">color=</span><span class="st">&#39;Depth (cm)&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">xlab</span>(<span class="st">&#39;Soil NO3&#39;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">ylab</span>(<span class="st">&#39;relative yield percent&#39;</span>)</span>
<span id="cb168-6"><a href="non-linear-least-squares.html#cb168-6"></a>dat.plot</span></code></pre></div>
<p><img src="Data-Analysis_files/figure-html/unnamed-chunk-55-1.png" width="672" /></p>
<p>The suggested model (known as plateau model) is</p>
<p><span class="math display">\[
E(Y_{ij}) = (\beta_{0j} + \beta_{1j}N_{ij})I_{N_{ij}\le \alpha_j} + (\beta_{0j} + \beta_{1j}\alpha_j)I_{N_{ij} &gt; \alpha_j}
\]</span>
where</p>
<ul>
<li>N is an observation<br />
</li>
<li>i is a particular observation<br />
</li>
<li>j = 1,2 corresponding to depths (30,60)</li>
</ul>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="non-linear-least-squares.html#cb169-1"></a><span class="co">#First define model as a function</span></span>
<span id="cb169-2"><a href="non-linear-least-squares.html#cb169-2"></a>nonlinModel &lt;-<span class="st"> </span><span class="cf">function</span>(predictor,b0,b1,alpha){</span>
<span id="cb169-3"><a href="non-linear-least-squares.html#cb169-3"></a>  <span class="kw">ifelse</span>(predictor<span class="op">&lt;=</span>alpha, </span>
<span id="cb169-4"><a href="non-linear-least-squares.html#cb169-4"></a>         b0<span class="op">+</span>b1<span class="op">*</span>predictor, <span class="co">#if observation less than cutoff simple linear model</span></span>
<span id="cb169-5"><a href="non-linear-least-squares.html#cb169-5"></a>         b0<span class="op">+</span>b1<span class="op">*</span>alpha) <span class="co">#otherwise flat line</span></span>
<span id="cb169-6"><a href="non-linear-least-squares.html#cb169-6"></a>}</span></code></pre></div>
<p>define <code>selfStart</code> function. Because we defined our model to be linear in the first part and then plateau (remain constant) we can use the first half of our predictors (sorted by increasing value) to get an initial estimate for the slope and intercept of the model, and the last predictor value (alpha) can be the starting value for the plateau parameter.</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="non-linear-least-squares.html#cb170-1"></a>nonlinModelInit &lt;-<span class="st"> </span><span class="cf">function</span>(mCall,LHS,data){</span>
<span id="cb170-2"><a href="non-linear-least-squares.html#cb170-2"></a>  <span class="co">#sort data by increasing predictor value - </span></span>
<span id="cb170-3"><a href="non-linear-least-squares.html#cb170-3"></a>  <span class="co">#done so we can just use the low level no3 conc to fit a simple model</span></span>
<span id="cb170-4"><a href="non-linear-least-squares.html#cb170-4"></a>  xy &lt;-<span class="st"> </span><span class="kw">sortedXyData</span>(mCall[[<span class="st">&#39;predictor&#39;</span>]],LHS,data)</span>
<span id="cb170-5"><a href="non-linear-least-squares.html#cb170-5"></a>  n &lt;-<span class="st"> </span><span class="kw">nrow</span>(xy)</span>
<span id="cb170-6"><a href="non-linear-least-squares.html#cb170-6"></a>  <span class="co">#For the first half of the data a simple linear model is fit</span></span>
<span id="cb170-7"><a href="non-linear-least-squares.html#cb170-7"></a>  lmFit &lt;-<span class="st"> </span><span class="kw">lm</span>(xy[<span class="dv">1</span><span class="op">:</span>(n<span class="op">/</span><span class="dv">2</span>),<span class="st">&#39;y&#39;</span>]<span class="op">~</span>xy[<span class="dv">1</span><span class="op">:</span>(n<span class="op">/</span><span class="dv">2</span>),<span class="st">&#39;x&#39;</span>])</span>
<span id="cb170-8"><a href="non-linear-least-squares.html#cb170-8"></a>  b0 &lt;-<span class="st"> </span><span class="kw">coef</span>(lmFit)[<span class="dv">1</span>]</span>
<span id="cb170-9"><a href="non-linear-least-squares.html#cb170-9"></a>  b1 &lt;-<span class="st"> </span><span class="kw">coef</span>(lmFit)[<span class="dv">2</span>]</span>
<span id="cb170-10"><a href="non-linear-least-squares.html#cb170-10"></a>  <span class="co">#for the cut off to the flat part select the last x value used in creating linear model</span></span>
<span id="cb170-11"><a href="non-linear-least-squares.html#cb170-11"></a>  alpha &lt;-<span class="st"> </span>xy[(n<span class="op">/</span><span class="dv">2</span>),<span class="st">&#39;x&#39;</span>]</span>
<span id="cb170-12"><a href="non-linear-least-squares.html#cb170-12"></a>  value &lt;-<span class="st"> </span><span class="kw">c</span>(b0,b1,alpha)</span>
<span id="cb170-13"><a href="non-linear-least-squares.html#cb170-13"></a>  <span class="kw">names</span>(value) &lt;-<span class="st"> </span>mCall[<span class="kw">c</span>(<span class="st">&#39;b0&#39;</span>,<span class="st">&#39;b1&#39;</span>,<span class="st">&#39;alpha&#39;</span>)]</span>
<span id="cb170-14"><a href="non-linear-least-squares.html#cb170-14"></a>  value</span>
<span id="cb170-15"><a href="non-linear-least-squares.html#cb170-15"></a>}</span></code></pre></div>
<p>combine model and custom function to calculate starting values.</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="non-linear-least-squares.html#cb171-1"></a>SS_nonlinModel &lt;-<span class="st"> </span><span class="kw">selfStart</span>(nonlinModel,nonlinModelInit,<span class="kw">c</span>(<span class="st">&#39;b0&#39;</span>,<span class="st">&#39;b1&#39;</span>,<span class="st">&#39;alpha&#39;</span>))</span></code></pre></div>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="non-linear-least-squares.html#cb172-1"></a><span class="co">#Above code defined model and selfStart now just need to call it for each of the depths</span></span>
<span id="cb172-2"><a href="non-linear-least-squares.html#cb172-2"></a>sep30_nls &lt;-<span class="st"> </span><span class="kw">nls</span>(ryp<span class="op">~</span><span class="kw">SS_nonlinModel</span>(<span class="dt">predictor=</span>no3,b0,b1,alpha),<span class="dt">data=</span>dat[dat<span class="op">$</span>depth<span class="op">==</span><span class="dv">30</span>,])</span>
<span id="cb172-3"><a href="non-linear-least-squares.html#cb172-3"></a></span>
<span id="cb172-4"><a href="non-linear-least-squares.html#cb172-4"></a>sep60_nls &lt;-<span class="st"> </span><span class="kw">nls</span>(ryp<span class="op">~</span><span class="kw">SS_nonlinModel</span>(<span class="dt">predictor=</span>no3,b0,b1,alpha),<span class="dt">data=</span>dat[dat<span class="op">$</span>depth<span class="op">==</span><span class="dv">60</span>,])</span>
<span id="cb172-5"><a href="non-linear-least-squares.html#cb172-5"></a></span>
<span id="cb172-6"><a href="non-linear-least-squares.html#cb172-6"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb172-7"><a href="non-linear-least-squares.html#cb172-7"></a><span class="kw">plotFit</span>(sep30_nls, <span class="dt">interval =</span> <span class="st">&quot;both&quot;</span>, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">shade =</span> <span class="ot">TRUE</span>,</span>
<span id="cb172-8"><a href="non-linear-least-squares.html#cb172-8"></a><span class="dt">col.conf =</span> <span class="st">&quot;skyblue4&quot;</span>, <span class="dt">col.pred =</span> <span class="st">&quot;lightskyblue2&quot;</span>,</span>
<span id="cb172-9"><a href="non-linear-least-squares.html#cb172-9"></a><span class="dt">data=</span>dat[dat<span class="op">$</span>depth<span class="op">==</span><span class="dv">30</span>,],<span class="dt">main=</span><span class="st">&#39;Results 30 cm depth&#39;</span>,</span>
<span id="cb172-10"><a href="non-linear-least-squares.html#cb172-10"></a><span class="dt">ylab =</span> <span class="st">&#39;relative yield percent&#39;</span>,<span class="dt">xlab =</span> <span class="st">&#39;Soil NO3 concentration&#39;</span>,</span>
<span id="cb172-11"><a href="non-linear-least-squares.html#cb172-11"></a><span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">120</span>))</span>
<span id="cb172-12"><a href="non-linear-least-squares.html#cb172-12"></a><span class="kw">plotFit</span>(sep60_nls, <span class="dt">interval =</span> <span class="st">&quot;both&quot;</span>, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">shade =</span> <span class="ot">TRUE</span>,</span>
<span id="cb172-13"><a href="non-linear-least-squares.html#cb172-13"></a><span class="dt">col.conf =</span> <span class="st">&quot;lightpink4&quot;</span>, <span class="dt">col.pred =</span> <span class="st">&quot;lightpink2&quot;</span>,</span>
<span id="cb172-14"><a href="non-linear-least-squares.html#cb172-14"></a><span class="dt">data=</span>dat[dat<span class="op">$</span>depth<span class="op">==</span><span class="dv">60</span>,],<span class="dt">main=</span><span class="st">&#39;Results 60 cm depth&#39;</span>,</span>
<span id="cb172-15"><a href="non-linear-least-squares.html#cb172-15"></a><span class="dt">ylab =</span> <span class="st">&#39;relative yield percent&#39;</span>,<span class="dt">xlab =</span> <span class="st">&#39;Soil NO3 concentration&#39;</span>,</span>
<span id="cb172-16"><a href="non-linear-least-squares.html#cb172-16"></a><span class="dt">xlim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">120</span>))</span></code></pre></div>
<p><img src="Data-Analysis_files/figure-html/unnamed-chunk-59-1.png" width="672" /></p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="non-linear-least-squares.html#cb173-1"></a><span class="kw">summary</span>(sep30_nls)</span></code></pre></div>
<pre><code>## 
## Formula: ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha)
## 
## Parameters:
##       Estimate Std. Error t value Pr(&gt;|t|)    
## b0     15.1943     2.9781   5.102 6.89e-07 ***
## b1      3.5760     0.1853  19.297  &lt; 2e-16 ***
## alpha  23.1324     0.5098  45.373  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.258 on 237 degrees of freedom
## 
## Number of iterations to convergence: 6 
## Achieved convergence tolerance: 3.608e-09</code></pre>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="non-linear-least-squares.html#cb175-1"></a><span class="kw">summary</span>(sep60_nls)</span></code></pre></div>
<pre><code>## 
## Formula: ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha)
## 
## Parameters:
##       Estimate Std. Error t value Pr(&gt;|t|)    
## b0      5.4519     2.9785    1.83   0.0684 .  
## b1      5.6820     0.2529   22.46   &lt;2e-16 ***
## alpha  16.2863     0.2818   57.80   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.427 on 237 degrees of freedom
## 
## Number of iterations to convergence: 5 
## Achieved convergence tolerance: 8.571e-09</code></pre>
<p>Instead of modeling the depths model separately we model them together - so there is a common slope, intercept, and plateau.</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="non-linear-least-squares.html#cb177-1"></a>red_nls &lt;-<span class="st"> </span><span class="kw">nls</span>(ryp<span class="op">~</span><span class="kw">SS_nonlinModel</span>(<span class="dt">predictor=</span>no3,b0,b1,alpha),<span class="dt">data=</span>dat)</span>
<span id="cb177-2"><a href="non-linear-least-squares.html#cb177-2"></a></span>
<span id="cb177-3"><a href="non-linear-least-squares.html#cb177-3"></a><span class="kw">summary</span>(red_nls)</span></code></pre></div>
<pre><code>## 
## Formula: ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha)
## 
## Parameters:
##       Estimate Std. Error t value Pr(&gt;|t|)    
## b0      8.7901     2.7688   3.175   0.0016 ** 
## b1      4.8995     0.2207  22.203   &lt;2e-16 ***
## alpha  18.0333     0.3242  55.630   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 9.13 on 477 degrees of freedom
## 
## Number of iterations to convergence: 7 
## Achieved convergence tolerance: 7.126e-09</code></pre>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="non-linear-least-squares.html#cb179-1"></a><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span>
<span id="cb179-2"><a href="non-linear-least-squares.html#cb179-2"></a><span class="kw">plotFit</span>(red_nls, <span class="dt">interval =</span> <span class="st">&quot;both&quot;</span>, <span class="dt">pch =</span> <span class="dv">19</span>, <span class="dt">shade =</span> <span class="ot">TRUE</span>,</span>
<span id="cb179-3"><a href="non-linear-least-squares.html#cb179-3"></a><span class="dt">col.conf =</span> <span class="st">&quot;lightblue4&quot;</span>, <span class="dt">col.pred =</span> <span class="st">&quot;lightblue2&quot;</span>,</span>
<span id="cb179-4"><a href="non-linear-least-squares.html#cb179-4"></a><span class="dt">data=</span>dat,<span class="dt">main=</span><span class="st">&#39;Results combined&#39;</span>,</span>
<span id="cb179-5"><a href="non-linear-least-squares.html#cb179-5"></a><span class="dt">ylab =</span> <span class="st">&#39;relative yield percent&#39;</span>,<span class="dt">xlab =</span> <span class="st">&#39;Soil NO3 concentration&#39;</span>)</span></code></pre></div>
<p><img src="Data-Analysis_files/figure-html/reduce-model-1.png" width="672" /></p>
<p>Examine residual values for the combined model.</p>
<div class="sourceCode" id="cb180"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb180-1"><a href="non-linear-least-squares.html#cb180-1"></a><span class="kw">library</span>(nlstools)</span>
<span id="cb180-2"><a href="non-linear-least-squares.html#cb180-2"></a><span class="co">#using nlstools nlsResiduals function to get some quick residual plots </span></span>
<span id="cb180-3"><a href="non-linear-least-squares.html#cb180-3"></a><span class="co">#can also use test.nlsResiduals(resid)</span></span>
<span id="cb180-4"><a href="non-linear-least-squares.html#cb180-4"></a><span class="co"># https://www.rdocumentation.org/packages/nlstools/versions/1.0-2</span></span>
<span id="cb180-5"><a href="non-linear-least-squares.html#cb180-5"></a>resid &lt;-<span class="st"> </span><span class="kw">nlsResiduals</span>(red_nls)</span>
<span id="cb180-6"><a href="non-linear-least-squares.html#cb180-6"></a><span class="kw">plot</span>(resid)</span></code></pre></div>
<p><img src="Data-Analysis_files/figure-html/reduce-model-resid-1.png" width="672" /></p>
<p>can we test whether the parameters for the two soil depth fits are significantly different? To know if the combined model is appropriate, we consider a parameterization where we let the parameters for the 60cm model be equal to the parameters from the 30cm model plus some increment:</p>
<p><span class="math display">\[
\beta_{02} = \beta_{01} + d_0 \\
\beta_{12} = \beta_{11} + d_1 \\
\alpha_{2} = \alpha_{1} + d_a
\]</span></p>
<p>We can implement this in the following function:</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="non-linear-least-squares.html#cb181-1"></a>nonlinModelF &lt;-<span class="st"> </span><span class="cf">function</span>(predictor,soildep,b01,b11,a1,d0,d1,da){</span>
<span id="cb181-2"><a href="non-linear-least-squares.html#cb181-2"></a>   b02 =<span class="st"> </span>b01 <span class="op">+</span><span class="st"> </span>d0 <span class="co">#make 60cm parms = 30cm parms + increment</span></span>
<span id="cb181-3"><a href="non-linear-least-squares.html#cb181-3"></a>   b12 =<span class="st"> </span>b11 <span class="op">+</span><span class="st"> </span>d1</span>
<span id="cb181-4"><a href="non-linear-least-squares.html#cb181-4"></a>   a2 =<span class="st"> </span>a1 <span class="op">+</span><span class="st"> </span>da</span>
<span id="cb181-5"><a href="non-linear-least-squares.html#cb181-5"></a>   </span>
<span id="cb181-6"><a href="non-linear-least-squares.html#cb181-6"></a>   y1 =<span class="st"> </span><span class="kw">ifelse</span>(predictor<span class="op">&lt;=</span>a1, </span>
<span id="cb181-7"><a href="non-linear-least-squares.html#cb181-7"></a>         b01<span class="op">+</span>b11<span class="op">*</span>predictor, <span class="co">#if observation less than cutoff simple linear model</span></span>
<span id="cb181-8"><a href="non-linear-least-squares.html#cb181-8"></a>         b01<span class="op">+</span>b11<span class="op">*</span>a1) <span class="co">#otherwise flat line</span></span>
<span id="cb181-9"><a href="non-linear-least-squares.html#cb181-9"></a>   y2 =<span class="st"> </span><span class="kw">ifelse</span>(predictor<span class="op">&lt;=</span>a2, </span>
<span id="cb181-10"><a href="non-linear-least-squares.html#cb181-10"></a>               b02<span class="op">+</span>b12<span class="op">*</span>predictor, </span>
<span id="cb181-11"><a href="non-linear-least-squares.html#cb181-11"></a>               b02<span class="op">+</span>b12<span class="op">*</span>a2) </span>
<span id="cb181-12"><a href="non-linear-least-squares.html#cb181-12"></a>   y =<span class="st">  </span>y1<span class="op">*</span>(soildep <span class="op">==</span><span class="st"> </span><span class="dv">30</span>) <span class="op">+</span><span class="st"> </span>y2<span class="op">*</span>(soildep <span class="op">==</span><span class="st"> </span><span class="dv">60</span>)  <span class="co">#combine models</span></span>
<span id="cb181-13"><a href="non-linear-least-squares.html#cb181-13"></a>   <span class="kw">return</span>(y)</span>
<span id="cb181-14"><a href="non-linear-least-squares.html#cb181-14"></a>}</span></code></pre></div>
<p>Starting values are easy now because we fit each model individually.</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="non-linear-least-squares.html#cb182-1"></a>Soil_full=<span class="kw">nls</span>(ryp<span class="op">~</span><span class="kw">nonlinModelF</span>(<span class="dt">predictor=</span>no3,<span class="dt">soildep=</span>depth,b01,b11,a1,d0,d1,da),</span>
<span id="cb182-2"><a href="non-linear-least-squares.html#cb182-2"></a>              <span class="dt">data=</span>dat,</span>
<span id="cb182-3"><a href="non-linear-least-squares.html#cb182-3"></a>              <span class="dt">start=</span><span class="kw">list</span>(<span class="dt">b01=</span><span class="fl">15.2</span>,<span class="dt">b11=</span><span class="fl">3.58</span>,<span class="dt">a1=</span><span class="fl">23.13</span>,<span class="dt">d0=</span><span class="op">-</span><span class="fl">9.74</span>,<span class="dt">d1=</span><span class="fl">2.11</span>,<span class="dt">da=</span><span class="op">-</span><span class="fl">6.85</span>)) </span>
<span id="cb182-4"><a href="non-linear-least-squares.html#cb182-4"></a></span>
<span id="cb182-5"><a href="non-linear-least-squares.html#cb182-5"></a><span class="kw">summary</span>(Soil_full)</span></code></pre></div>
<pre><code>## 
## Formula: ryp ~ nonlinModelF(predictor = no3, soildep = depth, b01, b11, 
##     a1, d0, d1, da)
## 
## Parameters:
##     Estimate Std. Error t value Pr(&gt;|t|)    
## b01  15.1943     2.8322   5.365 1.27e-07 ***
## b11   3.5760     0.1762  20.291  &lt; 2e-16 ***
## a1   23.1324     0.4848  47.711  &lt; 2e-16 ***
## d0   -9.7424     4.2357  -2.300   0.0219 *  
## d1    2.1060     0.3203   6.575 1.29e-10 ***
## da   -6.8461     0.5691 -12.030  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.854 on 474 degrees of freedom
## 
## Number of iterations to convergence: 1 
## Achieved convergence tolerance: 3.742e-06</code></pre>
<p>So, the increment parameters, <span class="math inline">\(d_1\)</span>,<span class="math inline">\(d_2\)</span>,<span class="math inline">\(d_a\)</span> are all significantly different from 0, suggesting that we should have two models here.</p>
</div>
</div>
<div id="modelestiamtion-adequcy" class="section level3">
<h3><span class="header-section-number">6.2.3</span> Model/Estiamtion Adequcy</h3>
<p><span class="citation">(Bates and Watts <a href="#ref-Bates_1980" role="doc-biblioref">1980</a>)</span> assess nonlinearity in terms of 2 components of curvature:</p>
<ul>
<li><strong>Intrinsic nonlinearity</strong>: the degree of bending and twisting in <span class="math inline">\(f(\theta)\)</span>; our estimation approach assumes that hte true function is relatively flat (planar) in the neighborhood fo <span class="math inline">\(\hat{\theta}\)</span>, which would not be true if <span class="math inline">\(f()\)</span> has a lot of “bending” int he neighborhood of <span class="math inline">\(\hat{\theta}\)</span> (independent of parameterizaiton)
<ul>
<li>If bad, the distribution of residuals will be seriously distorted<br />
</li>
<li>slow to converge<br />
</li>
<li>difficult to identify ( could use this function <code>rms.curve</code>)<br />
</li>
<li>Solution:
<ul>
<li>could use higher order Taylor expansions estimation<br />
</li>
<li>Bayesian method</li>
</ul></li>
</ul></li>
<li><strong>Parameter effects nonlinearity</strong>: degree to which curvature (nonlinearity) is affected by choice of <span class="math inline">\(\theta\)</span> (data dependent; dependent on parameterization)
<ul>
<li>leads to problems with inferecne on <span class="math inline">\(\hat{\theta}\)</span><br />
</li>
<li><code>rms.curve</code> in <code>MASS</code> can identify<br />
</li>
<li>bootstrap-based inference can also be used<br />
</li>
<li>Solution: try to reparaemterize.</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="non-linear-least-squares.html#cb184-1"></a><span class="co">#check parameter effects and intrinsic curvature</span></span>
<span id="cb184-2"><a href="non-linear-least-squares.html#cb184-2"></a></span>
<span id="cb184-3"><a href="non-linear-least-squares.html#cb184-3"></a>modD =<span class="st"> </span><span class="kw">deriv3</span>(<span class="op">~</span><span class="st"> </span>a<span class="op">*</span><span class="kw">exp</span>(b<span class="op">*</span>x), <span class="kw">c</span>(<span class="st">&quot;a&quot;</span>,<span class="st">&quot;b&quot;</span>),<span class="cf">function</span>(a,b,x) <span class="ot">NULL</span>)</span>
<span id="cb184-4"><a href="non-linear-least-squares.html#cb184-4"></a></span>
<span id="cb184-5"><a href="non-linear-least-squares.html#cb184-5"></a>nlin_modD=<span class="kw">nls</span>(y<span class="op">~</span><span class="kw">modD</span>(a,b,x),<span class="dt">start=</span><span class="kw">list</span>(<span class="dt">a=</span>astrt,<span class="dt">b=</span>bstrt),<span class="dt">data=</span>datf)</span>
<span id="cb184-6"><a href="non-linear-least-squares.html#cb184-6"></a></span>
<span id="cb184-7"><a href="non-linear-least-squares.html#cb184-7"></a><span class="kw">rms.curv</span>(nlin_modD)</span></code></pre></div>
<pre><code>## Parameter effects: c^theta x sqrt(F) = 0.0626 
##         Intrinsic: c^iota  x sqrt(F) = 0.0062</code></pre>
<p>In linear model, we have <a href="linear-regression.html#linear-regression">Linear Regression</a>, we have goodness of fit measure as <span class="math inline">\(R^2\)</span>:</p>
<p><span class="math display">\[
R^2 = \frac{SSR}{SSTO} = 1- \frac{SSE}{SSTO} \\
= \frac{\sum_{i=1}^n (\hat{Y}_i- \bar{Y})^2}{\sum_{i=1}^n (Y_i- \bar{Y})^2} = 1- \frac{\sum_{i=1}^n ({Y}_i- \hat{Y})^2}{\sum_{i=1}^n (Y_i- \bar{Y})^2}
\]</span>
but not valid in the nonlinear case because the error sum of squares and model sum of squares do not add to the total corrected sum of squares</p>
<p><span class="math display">\[
SSR + SSE \neq SST
\]</span></p>
<p>but we can use pseudo-<span class="math inline">\(R^2\)</span>:</p>
<p><span class="math display">\[
R^2_{pseudo} = 1 - \frac{\sum_{i=1}^n ({Y}_i- \hat{Y})^2}{\sum_{i=1}^n (Y_i- \bar{Y})^2}
\]</span>
But we can’t interpret this as the proportion of variability explained by the model. We should use as a relative comparison of different models.</p>
<p><strong>Residual Plots</strong>: standardize, similar to OLS. useful when the intrinsic curvature is small:</p>
<p>The studentized residuals</p>
<p><span class="math display">\[
r_i = \frac{e_i}{s\sqrt{1-\hat{c}_i}}
\]</span></p>
<p>where <span class="math inline">\(\hat{c}_i\)</span>is the i-th diagonal of <span class="math inline">\(\mathbf{\hat{H}= F(\hat{\theta})[F(\hat{\theta})&#39;F(\hat{\theta})]^{-1}F(\hat{\theta})&#39;}\)</span></p>
<p>We could have problems of</p>
<ul>
<li>Collinearity: the condition number of <span class="math inline">\(\mathbf{[F(\hat{\theta})&#39;F(\hat{\theta})]^{-1}}\)</span> should be less than 30. Follow <span class="citation">(Magel and Hertsgaard <a href="#ref-Magel_1987" role="doc-biblioref">1987</a>)</span>; reparameterize if possible<br />
</li>
<li>Leverage: Like <a href="ordinary-least-squares.html#ordinary-least-squares">OLS</a>, but consider <span class="math inline">\(\mathbf{\hat{H}= F(\hat{\theta})[F(\hat{\theta})&#39;F(\hat{\theta})]^{-1}F(\hat{\theta})&#39;}\)</span> (also known as “tangent plant hat matrix”) <span class="citation">(Laurent and Cook <a href="#ref-Laurent_1992" role="doc-biblioref">1992</a>)</span><br />
</li>
<li>Heterogeneous Errors: weighted Non-linear Least Squares<br />
</li>
<li>Correlated Errors:
<ul>
<li>Generalized Nonlinear Least Squares<br />
</li>
<li>Nonlinear Mixed Models<br />
</li>
<li>Bayesian methods</li>
</ul></li>
</ul>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Bates_1980">
<p>Bates, Douglas M., and Donald G. Watts. 1980. “Relative Curvature Measures of Nonlinearity.” <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 42 (1): 1–16. <a href="https://doi.org/10.1111/j.2517-6161.1980.tb01094.x">https://doi.org/10.1111/j.2517-6161.1980.tb01094.x</a>.</p>
</div>
<div id="ref-Bates_1981">
<p>Bates, Douglas M., and Donald G. Watts. 1980. “Relative Curvature Measures of Nonlinearity.” <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 42 (1): 1–16. <a href="https://doi.org/10.1111/j.2517-6161.1980.tb01094.x">https://doi.org/10.1111/j.2517-6161.1980.tb01094.x</a>.</p> 1981. “A Relative Offset Orthogonality Convergence Criterion for Nonlinear Least Squares.” <em>Technometrics</em> 23 (2): 179. <a href="https://doi.org/10.2307/1268035">https://doi.org/10.2307/1268035</a>.</p>
</div>
<div id="ref-Laurent_1992">
<p>Laurent, Roy T. St., and R. Dennis Cook. 1992. “Leverage and Superleverage in Nonlinear Regression.” <em>Journal of the American Statistical Association</em> 87 (420): 985. <a href="https://doi.org/10.2307/2290635">https://doi.org/10.2307/2290635</a>.</p>
</div>
<div id="ref-Magel_1987">
<p>Magel, Rhonda C., and Doris Hertsgaard. 1987. “A Collinearity Diagnostic for Nonlinear Regression.” <em>Communications in Statistics - Simulation and Computation</em> 16 (1): 85–97. <a href="https://doi.org/10.1080/03610918708812579">https://doi.org/10.1080/03610918708812579</a>.</p>
</div>
<div id="ref-Schabenberger_2001">
<p>Schabenberger, Oliver, and Francis J. Pierce. 2001. <em>Contemporary Statistical Models for the Plant and Soil Sciences</em>. CRC Press. <a href="https://doi.org/10.1201/9781420040197">https://doi.org/10.1201/9781420040197</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="inference-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="genelized-method-of-moments.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mikenguyen13/data_analysis/edit/main/06-2-nonlinear_regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Data Analysis.pdf", "Data Analysis.epub", "Data Analysis.mobi"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true,
"sharing": {
"facebook": true,
"github": true,
"twitter": true,
"linkedin": true
},
"info": true,
"edit": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
