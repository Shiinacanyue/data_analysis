[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"books author can found :Advanced Data Analysis: second book data analysis series, covers machine learning models (focus prediction)Marketing ResearchCommunication Theory","code":""},{"path":"introduction.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"Since beginning century, bombarded amazing advancements inventions, especially field statistics, information technology, computer science, new emerging filed - data science. However, believe downside introduction use big trendy words often (.e., big data, machine learning, deep learning).’s fun exciting learned new tools. admit hardly retain new ideas. However, writing beginning till end data analysis process solution came . Accordingly, let’s dive right .general recommendation:practice/habituate/condition, line codes write, function memorize, think like journey.practice/habituate/condition, line codes write, function memorize, think like journey.Readers can follow book several ways:\ninterested particular methods/tools, can jump section clicking section name.\nwant follow traditional path data analysis, read Linear Regression section.\nwant create experiment test hypothesis, read Analysis Variance (ANOVA) section.\nReaders can follow book several ways:interested particular methods/tools, can jump section clicking section name.want follow traditional path data analysis, read Linear Regression section.want create experiment test hypothesis, read Analysis Variance (ANOVA) section.Alternatively, rather see application models, disregard theory underlying mechanisms, can skip summary application portion section.Alternatively, rather see application models, disregard theory underlying mechanisms, can skip summary application portion section.don’t understand part, search title part part Google, read subject. just general guide.don’t understand part, search title part part Google, read subject. just general guide.want customize code beyond ones provided book, run console help(code) ?code. example, want information hist function, ’ll type console ?hist help(hist).want customize code beyond ones provided book, run console help(code) ?code. example, want information hist function, ’ll type console ?hist help(hist).Another way can search Google. Different people use different packages achieve result R. Accordingly, want create histogram, search Google histogram R, able find multiple ways create histogram R.Another way can search Google. Different people use different packages achieve result R. Accordingly, want create histogram, search Google histogram R, able find multiple ways create histogram R.Information book various sources, content based several courses taken formally. ’d like give professors credit accordingly.Tools statisticsProbability TheoryMathematical AnalysisComputer ScienceNumerical AnalysisDatabase ManagementSetup Working Environment","code":"\nif (!require(\"pacman\"))\n    install.packages(\"pacman\")\nif (!require(\"devtools\"))\n    install.packages(\"devtools\")\nlibrary(\"pacman\")\nlibrary(\"devtools\")"},{"path":"prerequisites.html","id":"prerequisites","chapter":"2 Prerequisites","heading":"2 Prerequisites","text":"chapter just quick review Matrix Theory Probability TheoryIf feel need brush theories, can jump right Descriptive Statistics","code":""},{"path":"prerequisites.html","id":"matrix-theory","chapter":"2 Prerequisites","heading":"2.1 Matrix Theory","text":"\\[\\begin{equation}\n\\begin{split}\n=\n\\left[\\begin{array}\n{cc}\na_{11} & a_{12} \\\\\na_{21} & a_{22} \\\\\n\\end{array}\n\\right]\n\\end{split}\n\\end{equation}\\]\\[\\begin{equation}\n\\begin{split}\n' =\n\\left[\\begin{array}\n{cc}\na_{11} & a_{21} \\\\\na_{12} & a_{22} \\\\\n\\end{array}\n\\right]\n\\end{split}\n\\end{equation}\\]\\[\n\\mathbf{(ABC)'=C'B''} \\\\\n\\mathbf{(B+C)= AB + AC} \\\\\n\\mathbf{AB \\neq BA} \\\\\n\\mathbf{(')'=} \\\\\n\\mathbf{(+B)' = ' + B'} \\\\\n\\mathbf{(AB)' = B''} \\\\\n\\mathbf{(AB)^{-1}= B^{-1}^{-1}} \\\\\n\\mathbf{+B = B +} \\\\\n\\mathbf{AA^{-1} = }\n\\]inverse, called invertible. invertible called singular.\\[\\begin{equation}\n\\begin{split}\n\\mathbf{} &= \n\\left(\\begin{array}\n{ccc} \na_{11} & a_{12} & a_{13} \\\\ \na_{21} & a_{22} & a_{23} \\\\ \n\\end{array}\\right)\n\\left(\\begin{array}\n{ccc}\nb_{11} & b_{12} & b_{13} \\\\\nb_{21} & b_{22} & b_{23} \\\\\nb_{31} & b_{32} & b_{33} \\\\\n\\end{array}\\right) \\\\\n&= \n\\left(\\begin{array}\n{ccc}\na_{11}b_{11}+a_{12}b_{21}+a_{13}b_{31} & \\sum_{=1}^{3}a_{1i}b_{i2} &  \\sum_{=1}^{3}a_{1i}b_{i3} \\\\\n\\sum_{=1}^{3}a_{2i}b_{i1} & \\sum_{=1}^{3}a_{2i}b_{i2} & \\sum_{=1}^{3}a_{2i}b_{i3} \\\\\n\\end{array}\\right) \n\\end{split}\n\\end{equation}\\]Let \\(\\mathbf{}\\) 3 x 1 vector, quadratic form \\[\n\\mathbf{'Ba} = \\sum_{=1}^{3}\\sum_{=1}^{3}a_i b_{ij} a_{j}\n\\]Length vector\nLet \\(\\mathbf{}\\) vector, \\(||\\mathbf{}||\\) (2-norm vector) length vector \\(\\mathbf{}\\), square root inner product vector :\\[\n||\\mathbf{}|| = \\sqrt{\\mathbf{'}}\n\\]","code":""},{"path":"prerequisites.html","id":"rank","chapter":"2 Prerequisites","heading":"2.1.1 Rank","text":"Dimension space spanned columns (rows).Number linearly independent columns/rowsFor n x k matrix k x k matrix B\\(rank()\\leq min(n,k)\\)\\(rank() = rank(') = rank(')=rank(AA')\\)\\(rank(AB)=min(rank(),rank(B))\\)B invertible rank(B) = k (non-singular)","code":""},{"path":"prerequisites.html","id":"inverse","chapter":"2 Prerequisites","heading":"2.1.2 Inverse","text":"scalar, = 0 1/exist. matrix, matrix invertible ’s non-zero matrix.non-singular square matrix invertible exists non-singular square matrix B , \\[AB=\\] \\(^{-1}=B\\). 2x2 matrix,\\[\n=\n\\left(\\begin{array}{cc}\n& b \\\\\nc & d \\\\\n\\end{array}\n\\right)\n\\]\\[\n^{-1}=\n\\frac{1}{ad-bc}\n\\left(\\begin{array}{cc}\nd & -b \\\\\n-c & \\\\\n\\end{array}\n\\right)\n\\]partition matrix,\\[\\begin{equation}\n\\begin{split}\n\\left[\\begin{array}\n{cc}\n& B \\\\\nC & D \\\\\n\\end{array}\n\\right]^{-1}\n =\n\\left[\\begin{array}\n{cc}\n\\mathbf{(-BD^{-1}C)^{-1}} & \\mathbf{-(-BD^{-1}C)^{-1}BD^-1}\\\\\n\\mathbf{-DC(-BD^{-1}C)^{-1}} & \\mathbf{D^{-1}+D^{-1}C(-BD^{-1}C)^{-1}BD^{-1}}\\ \\\\\n\\end{array}\n\\right]\n\\end{split}\n\\end{equation}\\]Properties non-singular square matrix\\(\\mathbf{^{-1}}=\\)non-zero scalar b, \\(\\mathbf{(bA)^{-1}=b^{-1}^{-1}}\\)matrix B, \\(\\mathbf(BA)^{-1}=B^{-1}^{-1}\\) B non-singular\\(\\mathbf{(^{-1})'=(')^{-1}}\\)Never notate \\(\\mathbf{1/}\\)","code":""},{"path":"prerequisites.html","id":"definiteness","chapter":"2 Prerequisites","heading":"2.1.3 Definiteness","text":"symmetric square k x k matrix, \\(\\mathbf{}\\), Positive Semi-Definite non-zero k x 1 vector \\(\\mathbf{x}\\), \\[\\mathbf{x'Ax \\geq 0 }\\]symmetric square k x k matrix, \\(\\mathbf{}\\), Negative Semi-Definite non-zero k x 1 vector \\(\\mathbf{x}\\) \\[\\mathbf{x'Ax \\leq 0 }\\]\\(\\mathbf{}\\) indefinite neither positive semi-definite negative semi-definite.identity matrix positive definiteExample Let \\(\\mathbf{x} =(x_1 x_2)'\\), 2 x 2 identity matrix,\\[\\begin{equation}\n\\begin{split}\n\\mathbf{x'Ix} \n&= (x_1 x_2) \n\\left(\\begin{array}\n{cc}\n1 & 0 \\\\\n0 & 1 \\\\\n\\end{array}\n\\right)\n\\left(\\begin{array}{c}\nx_1 \\\\\nx_2 \\\\\n\\end{array}\n\\right) \\\\\n&=\n(x_1 x_2)\n\\left(\\begin{array}\n{c}\nx_1 \\\\\nx_2 \\\\\n\\end{array}\n\\right) \\\\\n&=\nx_1^2 + x_2^2 >0\n\\end{split}\n\\end{equation}\\]Definiteness gives us ability compare matrices \\(\\mathbf{-B}\\) PSD property also helps us show efficiency (variance covariance matrix one estimator smaller another)Propertiesany variance matrix PSDa matrix \\(\\mathbf{}\\) PSD exists matrix \\(\\mathbf{B}\\) \\(\\mathbf{=B'B}\\)\\(\\mathbf{}\\) PSD, \\(\\mathbf{B'AB}\\) PSDif C non-singular, -C PSD \\(\\mathbf{C^{-1}-^{-1}}\\)PD (ND) \\(^{-1}\\) PD (ND)NoteIndefinite neither PSD NSD. comparable concept scalar.square matrix PSD invertible PDExample:Invertible / Indefinite\\[\n\\left[\n\\begin{array}\n{cc}\n-1 & 0 \\\\\n0 & 10 \\\\\n\\end{array}\n\\right]\n\\]Non-invertible/ Indefinite\\[\n\\left[\n\\begin{array}\n{cc}\n0 & 1 \\\\\n0 & 0 \\\\\n\\end{array}\n\\right]\n\\]Invertible / PSD\\[\n\\left[\n\\begin{array}\n{cc}\n1 & 0 \\\\\n0 & 1 \\\\\n\\end{array}\n\\right]\n\\]Non-Invertible / PSD\\[\n\\left[\n\\begin{array}\n{cc}\n0 & 0 \\\\\n0 & 1 \\\\\n\\end{array}\n\\right]\n\\]","code":""},{"path":"prerequisites.html","id":"matrix-calculus","chapter":"2 Prerequisites","heading":"2.1.4 Matrix Calculus","text":"\\(y=f(x_1,x_2,...,x_k)=f(x)\\) x 1 x k row vector. Gradient (first order derivative respect vector) ,\\[\n\\frac{\\partial{f(x)}}{\\partial{x}}=\n\\left(\\begin{array}{c}\n\\frac{\\partial{f(x)}}{\\partial{x_1}} \\\\\n\\frac{\\partial{f(x)}}{\\partial{x_2}} \\\\\n... \\\\\n\\frac{\\partial{f(x)}}{\\partial{x_k}}\n\\end{array}\n\\right)\n\\]Hessian (second order derivative respect vector) ,\\[\n\\frac{\\partial^2{f(x)}}{\\partial{x}\\partial{x'}}=\n\\left(\\begin{array}\n{cccc}\n\\frac{\\partial^2{f(x)}}{\\partial{x_1}\\partial{x_1}} & \\frac{\\partial^2{f(x)}}{\\partial{x_1}\\partial{x_2}} & ... & \\frac{\\partial^2{f(x)}}{\\partial{x_1}\\partial{x_k}} \\\\\n\\frac{\\partial^2{f(x)}}{\\partial{x_1}\\partial{x_2}} & \\frac{\\partial^2{f(x)}}{\\partial{x_2}\\partial{x_2}} & ... & \\frac{\\partial^2{f(x)}}{\\partial{x_2}\\partial{x_k}} \\\\\n... & ...& & ...\\\\\n\\frac{\\partial^2{f(x)}}{\\partial{x_k}\\partial{x_1}} & \\frac{\\partial^2{f(x)}}{\\partial{x_k}\\partial{x_2}} & ... & \\frac{\\partial^2{f(x)}}{\\partial{x_k}\\partial{x_k}}\n\\end{array}\n\\right)\n\\]Define derivative \\(f(\\mathbf{X})\\) respect \\(\\mathbf{X}_{(n \\times p)}\\) matrix\\[\n\\frac{\\partial f(\\mathbf{X})}{\\partial \\mathbf{X}} = (\\frac{\\partial f(\\mathbf{X})}{\\partial x_{ij}})\n\\]Define \\(\\mathbf{}\\) vector \\(\\mathbf{}\\) matrix depend upon \\(\\mathbf{y}\\). \\[\n\\frac{\\partial \\mathbf{'y}}{\\partial \\mathbf{y}} = \\mathbf{}\n\\]\\[\n\\frac{\\partial \\mathbf{y'y}}{\\partial \\mathbf{y}} = 2\\mathbf{y}\n\\]\\[\n\\frac{\\partial \\mathbf{y'Ay}}{\\partial \\mathbf{y}} = \\mathbf{(+ ')y}\n\\]\\(\\mathbf{X}\\) symmetric matrix \\[\n\\frac{\\partial |\\mathbf{X}|}{\\partial x_{ij}} = \n\\begin{cases}\nX_{ii}, = j \\\\\nX_ij, \\neq j\n\\end{cases}\n\\]\n\\(X_{ij}\\) (,j)th cofactor \\(\\mathbf{X}\\)\\(\\mathbf{X}\\) symmetric \\(\\mathbf{}\\) matrix depend upon \\(\\mathbf{X}\\) \\[\n\\frac{\\partial tr \\mathbf{XA}}{\\partial \\mathbf{X}} = \\mathbf{} + \\mathbf{}' - diag(\\mathbf{})\n\\]\\(\\mathbf{X}\\) symmetric let \\(\\mathbf{J}_{ij}\\) matrix 1 (,j)th position 0s elsewhere, \\[\n\\frac{\\partial \\mathbf{X}6{-1}}{\\partial x_{ij}} = \n\\begin{cases}\n- \\mathbf{X}^{-1}\\mathbf{J}_{ii} \\mathbf{X}^{-1} , = j \\\\\n- \\mathbf{X}^{-1}(\\mathbf{J}_{ij} + \\mathbf{J}_{ji}) \\mathbf{X}^{-1} , \\neq j\n\\end{cases}\n\\]","code":""},{"path":"prerequisites.html","id":"optimization","chapter":"2 Prerequisites","heading":"2.1.5 Optimization","text":"Second Order Condition  Convex \\(\\rightarrow\\) Min","code":""},{"path":"prerequisites.html","id":"probability-theory","chapter":"2 Prerequisites","heading":"2.2 Probability Theory","text":"","code":""},{"path":"prerequisites.html","id":"axiom-and-theorems-of-probability","chapter":"2 Prerequisites","heading":"2.2.1 Axiom and Theorems of Probability","text":"Let S denote sample space experiment P[S]=1\\(P[] \\ge 0\\) every event ALet \\(A_1,A_2,A_3,...\\) finite infinite collection mutually exclusive events. \\(P[A_1\\cup A_2 \\cup A_3 ...]=P[A_1]+P[A_2]+P[A_3]+...\\)\\(P[\\emptyset]=0\\)\\(P[']=1-P[]\\)\\(P[A_1 \\cup A_2] = P[A_1] + P[A_2] - P[A_1 \\cap A_2]\\)Conditional Probability\\[\nP[|B]=\\frac{\\cap B}{P[B]}\n\\]Independent Events Two events B independent :\\(P[\\cap B]=P[]P[B]\\)\\(P[|B]=P[]\\)\\(P[B|]=P[B]\\)finite collection events \\(A_1, A_2, ..., A_n\\) independent subcollection independent.Multiplication Rule \\(P[\\cap B] = P[|B]P[B] = P[B|]P[]\\)Bayes’ Theorem Let \\(A_1, A_2, ..., A_n\\) collection mutually exclusive events whose union S.\nLet b event \\(P[B]\\neq0\\)\nevents \\(A_j\\), j = 1,2,…,n\\[\nP[A_|B]=\\frac{P[B|A_j]P[A_j]}{\\sum_{=1}^{n}P[B|A_j]P[A_i]}\n\\]Jensen’s InequalityIf g(x) convex \\(E(g(X)) \\ge g(E(X))\\)g(x) concave \\(E(g(X)) \\le g(E(X))\\)","code":""},{"path":"prerequisites.html","id":"law-of-iterated-expectations","chapter":"2 Prerequisites","heading":"2.2.1.1 Law of Iterated Expectations","text":"\\(E(Y)=E(E(Y|X))\\)","code":""},{"path":"prerequisites.html","id":"correlation-and-independence","chapter":"2 Prerequisites","heading":"2.2.1.2 Correlation and Independence","text":"Independence\\(f(x,y)=f_X(x)f_Y(y)\\)\\(f_{Y|X}(y|x)=f_Y(y)\\) \\(f_{X|Y}(x|y)=f_X(x)\\)\\(E(g_1(X)g_2(Y))=E(g_1(X))E(g_2(Y))\\)Mean Independence (implied independence)Y mean independent X \\(E(Y|X)=E(Y)\\)\\(E(Xg(Y))=E(X)E(g(Y))\\)Uncorrelated (implied independence mean independence)\\(Cov(X,Y)=0\\)\\(Var(X+Y)=Var(X) + Var(Y)\\)\\(E(XY)=E(X)E(Y)\\)\\[\nStrongest \\\\\n\\downarrow \\\\\nIndependence \\\\\n\\downarrow \\\\\nMean Independence \\\\\n\\downarrow \\\\\nUncorrelated \\\\ \n\\downarrow \\\\\nWeakest\\\\\n\\]","code":""},{"path":"prerequisites.html","id":"central-limit-theorem","chapter":"2 Prerequisites","heading":"2.2.2 Central Limit Theorem","text":"Let \\(X_1, X_2,...,X_n\\) random sample size n distribution (necessarily normal) X mean \\(\\mu\\) variance \\(\\sigma^2\\). large n (\\(n \\ge 25\\)),\\(\\bar{X}\\) approximately normal mean \\(\\mu_{\\bar{X}}=\\mu\\) variance \\(\\sigma^2_{\\bar{X}} = Var(\\bar{X})= \\frac{\\sigma^2}{n}\\)\\(\\bar{X}\\) approximately normal mean \\(\\mu_{\\bar{X}}=\\mu\\) variance \\(\\sigma^2_{\\bar{X}} = Var(\\bar{X})= \\frac{\\sigma^2}{n}\\)\\(\\hat{p}\\)approximately normal \\(\\mu_{\\hat{p}} = p, \\sigma^2_{\\hat{p}} = \\frac{p(1-p)}{n}\\)\\(\\hat{p}\\)approximately normal \\(\\mu_{\\hat{p}} = p, \\sigma^2_{\\hat{p}} = \\frac{p(1-p)}{n}\\)\\(\\hat{p_1} - \\hat{p_2}\\) approximately normal \\(\\mu_{\\hat{p_1} - \\hat{p_2}} = p_1 - p_2, \\sigma^2_{\\hat{p_1} - \\hat{p_2}}=\\frac{p_1(1-p)}{n_1} + \\frac{p_2(1-p)}{n_2}\\)\\(\\hat{p_1} - \\hat{p_2}\\) approximately normal \\(\\mu_{\\hat{p_1} - \\hat{p_2}} = p_1 - p_2, \\sigma^2_{\\hat{p_1} - \\hat{p_2}}=\\frac{p_1(1-p)}{n_1} + \\frac{p_2(1-p)}{n_2}\\)\\(\\bar{X_1} - \\bar{X_2}\\) approximately normal \\(\\mu_{\\bar{X_1} - \\bar{X_2}} = \\mu_1 - \\mu_2, \\sigma^2_{\\bar{X_1} - \\bar{X_2}} = \\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}\\)\\(\\bar{X_1} - \\bar{X_2}\\) approximately normal \\(\\mu_{\\bar{X_1} - \\bar{X_2}} = \\mu_1 - \\mu_2, \\sigma^2_{\\bar{X_1} - \\bar{X_2}} = \\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}\\)following random variables approximately standard normal:\n\\(\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\)\n\\(\\frac{\\hat{p}-p}{\\sqrt{\\frac{p(1-p)}{n}}}\\)\n\\(\\frac{(\\hat{p_1}-\\hat{p_2})-(p_1-p_2)}{\\sqrt{\\frac{p_1(1-p_1)}{n_1}-\\frac{p_2(1-p_2)}{n_2}}}\\)\n\\(\\frac{(\\bar{X_1}-\\bar{X_2})-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\sigma^2_1}{n_1}-\\frac{\\sigma^2_2}{n_2}}}\\)\nfollowing random variables approximately standard normal:\\(\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\)\\(\\frac{\\hat{p}-p}{\\sqrt{\\frac{p(1-p)}{n}}}\\)\\(\\frac{(\\hat{p_1}-\\hat{p_2})-(p_1-p_2)}{\\sqrt{\\frac{p_1(1-p_1)}{n_1}-\\frac{p_2(1-p_2)}{n_2}}}\\)\\(\\frac{(\\bar{X_1}-\\bar{X_2})-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\sigma^2_1}{n_1}-\\frac{\\sigma^2_2}{n_2}}}\\)\\(\\{x_i\\}_{=1}^{n}\\) iid random sample probability distribution finite mean \\(\\mu\\) finite variance \\(\\sigma^2\\) sample mean \\(\\bar{x}=n^{-1}\\sum_{=1}^{n}x_i\\) scaled \\(\\sqrt{n}\\) following limiting distribution\\[\n\\sqrt{n}(\\bar{x}-\\mu) \\^d N(0,\\sigma^2)\n\\]standardize sample mean,\\[\n\\frac{\\sqrt{n}(\\bar{x}-\\mu)}{\\sigma} \\^d N(0,1)\n\\]holds random sample distribution (continuous, discrete, unknown).extends multivariate case: random sample random vector converges multivariate normal.Variance limiting distribution asymptotic variance (Avar)\\[\nAvar(\\sqrt{n}(\\bar{x}-\\mu)) = \\sigma^2 \\\\\n\\lim_{n \\\\infty} Var(\\sqrt{n}(\\bar{x}-\\mu)) = \\sigma^2 \\\\\nAvar(.) \\neq lim_{n \\\\infty} Var(.)\n\\]","code":""},{"path":"prerequisites.html","id":"random-variable","chapter":"2 Prerequisites","heading":"2.2.3 Random variable","text":"Expected Value Properties:E[c] = c constant cE[cX] = cE[X] constant cE[X+Y] = E[X] = E[Y]E[XY] = E[X].E[Y] (X Y independent)Expected Variance Properties:\\(Var(c) = 0\\) constant c\\(Var(cX) = c^2Var(X)\\) constant c\\(Var(X) \\ge 0\\)\\(Var(X) = E(X^2) - (E(X))^2\\)\\(Var(X+c)=Var(X)\\)\\(Var (X+Y) = Var(X) + Var(Y)\\) (X Y independent)Standard deviation \\(\\sigma=\\sqrt(\\sigma^2)=\\sqrt(Var X)\\)Suppose \\(y_1,...,y_p\\) possibly correlated random variables means \\(\\mu_1,...,\\mu_p\\). \\[\n\\mathbf{y} = (y_1,...,y_p)'  \\\\\nE(\\mathbf{y}) = (\\mu_1,...,\\mu_p)' = \\mathbf{\\mu}\n\\]Let \\(\\sigma_{ij} = cov(y_i,y_j)\\) \\(,j = 1,..,p\\).Define\\[\n\\mathbf{\\Sigma} = (\\sigma_{ij}) = \n\\left(\\begin{array}\n{rrrr}\n\\sigma_{11} & \\sigma_{12} & ... & \\sigma_{1p} \\\\\n\\sigma_{21} & \\sigma_{22} & ... & \\sigma_{2p} \\\\\n. & . & . & . \\\\\n\\sigma_{p1} & \\sigma_{p2} & ... & \\sigma_{pp}\\\\\n\\end{array}\\right)\n\\]Hence, \\(\\mathbf{\\Sigma}\\) variance-covariance dispersion matrix. \\(\\mathbf{\\Sigma}\\) symmetric \\((p+1)p/2\\) unique parameters.Alternatively, let \\(u_{p \\times 1}\\) \\(v_{v \\times 1}\\) random vectors means \\(\\mathbf{\\mu_u}\\) \\(\\mathbf{\\mu_v}\\). \\[\n\\mathbf{\\Sigma_{uv}} = cov(\\mathbf{u,v}) = E[\\mathbf{(u-\\mu_u)(v-\\mu_v)'}]\n\\]\\(\\Sigma_{uv} \\neq \\Sigma_{vu}\\) (\\(\\Sigma_{uv} = \\Sigma_{vu}'\\))Properties Covariance MatricesSymmetric: \\(\\mathbf{\\Sigma' = \\Sigma}\\)Eigendecomposition (spectral decomposition,symmetric decomposition): \\(\\mathbf{\\Sigma = \\Phi \\Lambda \\Phi}\\), \\(\\mathbf{\\Phi}\\) matrix eigenvectors \\(\\mathbf{\\Phi \\Phi' = }\\) (orthonormal), \\(\\mathbf{\\Lambda}\\) diagonal matrix eigenvalues \\((\\lambda_1,...,\\lambda_p)\\) diagonal.Non-negative definite, \\(\\mathbf{\\Sigma } \\ge 0\\) \\(\\mathbf{} \\R^p\\). Equivalently, eigenvalues \\(\\mathbf{\\Sigma}\\), \\(\\lambda_1 \\ge ... \\ge \\lambda_p \\ge 0\\)\\(|\\mathbf{\\Sigma}| = \\lambda_1,...\\lambda_p \\ge 0\\) (generalized variance)\\(trace(\\mathbf{\\Sigma})= tr(\\mathbf{\\Sigma}) = \\lambda_1 +... + \\lambda_p = \\sigma_{11}+...+ \\sigma_{pp}\\)= sum variances (total variance)Note: \\(\\mathbf{\\Sigma}\\) usually required positive definite. implies eigenvalues positive, \\(\\mathbf{\\Sigma}\\) inverse \\(\\mathbf{\\Sigma}^{-1}\\), \\(\\mathbf{\\Sigma}^{-1}\\mathbf{\\Sigma}= \\mathbf{}_{p \\times p} = \\mathbf{\\Sigma}\\mathbf{\\Sigma}^{-1}\\)Correlation MatricesDefine correlation \\(\\rho_{ij}\\) correlation matrix \\[\n\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii} \\sigma_{jj}}}\n\\]\\[\n\\mathbf{R} = \\left(\n\\begin{array}\n{cccc}\n\\rho_{11} & \\rho_{12} & ... & \\rho_{1p} \\\\\n\\rho_{21} & \\rho_{22} & ... & \\rho_{2p} \\\\\n. & . & . & . \\\\\n\\rho_{p1} & \\rho_{p2} & ... & \\rho_{pp}\\\\\n\\end{array}\n\\right)\n\\]\\(\\rho_{ii}=1\\) .Let x y random vectors means \\(\\mu_x\\) \\(\\mu_y\\) variance-covariance matrices \\(\\Sigma_x\\) \\(\\Sigma_y\\). Let \\(\\mathbf{}\\) \\(\\mathbf{B}\\) matrices constants c d vectors constants. ,\\(E(\\mathbf{Ay+c}) = \\mathbf{\\mu_y} +c\\)\\(var(\\mathbf{Ay +c}) = \\mathbf{}var(\\mathbf{y}) \\mathbf{}' = \\mathbf{\\Sigma_y '}\\)\\(cov(\\mathbf{Ay + c,+d} = \\mathbf{\\Sigma_y B'}\\)","code":""},{"path":"prerequisites.html","id":"moment-generating-function","chapter":"2 Prerequisites","heading":"2.2.4 Moment generating function","text":"Moment generating function properties:\\(\\frac{d^k(m_X(t))}{dt^k}|_{t=0}=E[X^k]\\)\\(\\mu=E[X]=m_X'(0)\\)\\(E[X^2]=m_X''(0)\\)mgf TheoremsLet \\(X_1,X_2,...X_n,Y\\) random variables moment-generating functions \\(m_{X_1}(t),m_{X_2}(t),...,m_{X_n}(t),m_{Y}(t)\\)\\(m_{X_1}(t)=m_{X_2}(t)\\) t open interval 0, \\(X_1\\) \\(X_2\\) distributionIf \\(Y = \\alpha + \\beta X_1\\), \\(m_{Y}(t)= e^{\\alpha t}m_{X_1}(\\beta t)\\)\\(X_1,X_2,...X_n\\) independent \\(Y = \\alpha_0 + \\alpha_1 X_1 + \\alpha_2 X_2 + ... + \\alpha_n X_n\\) (\\(\\alpha_0, ... ,\\alpha_n\\) real numbers), \\(m_{Y}(t)=e^{\\alpha_0 t}m_{X_1}(\\alpha_1t)m_{X_2}(\\alpha_2 t)...m_{X_n}(\\alpha_nt)\\)Suppose \\(X_1,X_2,...X_n\\) independent normal random variables means \\(\\mu_1,\\mu_2,...\\mu_n\\) variances \\(\\sigma^2_1,\\sigma^2_2,...,\\sigma^2_n\\). \\(Y = \\alpha_0 + \\alpha_1 X_1 + \\alpha_2 X_2 + ... + \\alpha_n X_n\\) (\\(\\alpha_0, ... ,\\alpha_n\\) real numbers), Y normally distributed mean \\(\\mu_Y = \\alpha_0 + \\alpha_1 \\mu_1 +\\alpha_2 \\mu_2 + ... + \\alpha_n \\mu_n\\) variance \\(\\sigma^2_Y = \\alpha_1^2 \\sigma_1^2 + \\alpha_2^2 \\sigma_2^2 + ... + \\alpha_n^2 \\sigma_n^2\\)","code":""},{"path":"prerequisites.html","id":"moment","chapter":"2 Prerequisites","heading":"2.2.5 Moment","text":"Skewness(X) = \\(E((X-\\mu)^3)/\\sigma^3\\)Kurtosis(X) = \\(E((X-\\mu)^4)/\\sigma^4\\)Conditional Moments\\[\nE(Y|X=x)=\n\\begin{cases}\n\\sum_yyf_Y(y|x) & \\text{discrete RV}\\\\\n\\int_yyf_Y(y|x)dy & \\text{continous RV}\\\\\n\\end{cases}\n\\]\\[\nVar(Y|X=x)=\n\\begin{cases}\n\\sum_y(y-E(Y|x))^2f_Y(y|x) & \\text{discrete RV}\\\\\n\\int_y(y-E(Y|x))^2f_Y(y|x)dy & \\text{continous RV}\\\\\n\\end{cases}\n\\]","code":""},{"path":"prerequisites.html","id":"multivariate-moments","chapter":"2 Prerequisites","heading":"2.2.5.1 Multivariate Moments","text":"\\[\\begin{equation}\nE=\n\\left(\n\\begin{array}{c}\nX \\\\\nY \\\\\n\\end{array}\n\\right)\n=\n\\left(\n\\begin{array}{c}\nE(X) \\\\\nE(Y) \\\\\n\\end{array}\n\\right)\n=\n\\left(\n\\begin{array}{c}\n\\mu_X \\\\\n\\mu_Y \\\\\n\\end{array}\n\\right)\n\\end{equation}\\]\\[\\begin{equation}\n\\begin{split}\nVar\n\\left(\n\\begin{array}{c}\nX \\\\\nY \\\\\n\\end{array}\n\\right)\n&=\n\\left(\n\\begin{array}\n{cc}\nVar(X) & Cov(X,Y) \\\\\nCov(X,Y) & Var(Y) \\\\\n\\end{array}\n\\right) \\\\\n&=\n\\left(\n\\begin{array}\n{cc}\nE((X-\\mu_X)^2) & E((X-\\mu_X)(Y-\\mu_Y)) \\\\\nE((X-\\mu_X)(Y-\\mu_Y)) & E((Y-\\mu_Y)^2) \\\\\n\\end{array}\n\\right)\n\\end{split}\n\\end{equation}\\]Properties\\(E(aX + + c)=aE(X) +(Y) + c\\)\\(Var(aX + + c) = ^2Var(X) + b^2Var(Y) + 2abCov(X,Y)\\)\\(Cov(aX + , cX + ) =acVar(X)+bdVar(Y) + (ad+bc)Cov(X,Y)\\)Correlation: \\(\\rho_{XY} = \\frac{Cov(X,Y)}{\\sigma_X\\sigma_Y}\\)","code":""},{"path":"prerequisites.html","id":"distributions","chapter":"2 Prerequisites","heading":"2.2.6 Distributions","text":"Conditional Distributions \\[\nf_{X|Y}(X|Y=y)=\\frac{f(X,Y)}{f_Y(y)}\n\\] \\(f_{X|Y}(X|Y=y)=f_X(X)\\) X Y independent","code":""},{"path":"prerequisites.html","id":"discrete","chapter":"2 Prerequisites","heading":"2.2.6.1 Discrete","text":"CDF: Cumulative Density Function\nMGF: Moment Generating Function","code":""},{"path":"prerequisites.html","id":"bernoulli","chapter":"2 Prerequisites","heading":"2.2.6.1.1 Bernoulli","text":"\\(Bernoulli(p)\\)PDF","code":"\nhist(mc2d::rbern(100000, prob=.5))"},{"path":"prerequisites.html","id":"binomial","chapter":"2 Prerequisites","heading":"2.2.6.1.2 Binomial","text":"\\(B(n,p)\\)experiment consists fixed number (n) Bernoulli trials, results success (s) failure (f)trials identical independent, probability success (p) probability failure (q = 1- p) remains trials.random variable X denotes number successes obtained n trials.Density\\[\nf(x)={{n}\\choose{x}}p^xq^{n-x}\n\\]CDF\nuse tablePDFMGF\\[\nm_X(t) =(q+pe^t)^n\n\\]Mean\\[\n\\mu = E(x) = np\n\\]Variance\\[\n\\sigma^2 =Var(X) = npq\n\\]","code":"\n# Histogram of 100000 random values from a sample of 100 with probability of 0.5\nhist(rbinom(100000, size = 100, prob = 0.5))"},{"path":"prerequisites.html","id":"poisson","chapter":"2 Prerequisites","heading":"2.2.6.1.3 Poisson","text":"\\(Pois(\\lambda)\\)Arises Poisson process, involves observing discrete events continuous “interval” time, length, space.random variable X number occurrences event within interval s unitsThe parameter \\(\\lambda\\) average number occurrences event question per measurement unit. distribution, use parameter \\(k=\\lambda s\\)Density\\[\nf(x) = \\frac{e^{-k}k^x}{x!}\n\\],k> 0, x =0,1,…CDF\nUse tablePDFMGF\\[\nm_X(t)=e^{k(e^t-1)}\n\\]Mean\\[\n\\mu = E(X) = k\n\\]Variance\\[\n\\sigma^2 = Var(X) = k\n\\]","code":"\n# Poisson dist with mean of 5 or Poisson(5)\nhist(rpois(10000, lambda = 5))"},{"path":"prerequisites.html","id":"geometric","chapter":"2 Prerequisites","heading":"2.2.6.1.4 Geometric","text":"experiment consists series trails. outcome trial can classed either “success” (s) “failure” (f). (called Bernoulli trial).trials identical independent sense outcome one trial effect outcome . probability success (p) probability failure (q=1-p) remains trial trial.lack memoryX: number trials needed obtain first success.Density\\[\nf(x)=pq^{x-1}\n\\]CDF\\[\nF(x) = 1- q^x\n\\]PDFMGF\\[\nm_X(t) = \\frac{pe^t}{1-qe^t}\n\\]\\(t < -ln(q)\\)Mean\\[\n\\mu = \\frac{1}{p}\n\\]Variance\\[\n\\sigma^2 = Var(X) = \\frac{q}{p^2}\n\\]","code":"\n# hist of Geometric distribution with probability of success = 0.5\nhist(rgeom(n = 10000, prob = 0.5))"},{"path":"prerequisites.html","id":"hypergeometric","chapter":"2 Prerequisites","heading":"2.2.6.1.5 Hypergeometric","text":"experiment consists drawing random sample size n without replacement without regard order collection N objects.N objects, r trait interest; N-r traitX number objects sample trait.Density\\[\nf(x)=\\frac{{{r}\\choose{x}}{{N-r}\\choose{n-x}}}{{{N}\\choose{n}}}\n\\]\\(max[0,n-(N-r)] \\le x \\le min(n,r)\\)PDFMean\\[\n\\mu = E(x)= \\frac{nr}{N}\n\\]Variance\\[\n\\sigma^2 = var(X) = n (\\frac{r}{N})(\\frac{N-r}{N})(\\frac{N-n}{N-1})\n\\]Note large N (\\(\\frac{n}{N} \\le 0.05\\)), distribution can approximated using Binomial distribution \\(p = \\frac{r}{N}\\)","code":"\n# hist of hypergeometric distribution with the number of white balls = 50, and the number of black balls = 20, and number of balls drawn = 30. \nhist(rhyper(nn = 10000 , m=50, n=20, k=30))"},{"path":"prerequisites.html","id":"section","chapter":"2 Prerequisites","heading":"2.2.6.1.6 ","text":"","code":""},{"path":"prerequisites.html","id":"continuous","chapter":"2 Prerequisites","heading":"2.2.6.2 Continuous","text":"","code":""},{"path":"prerequisites.html","id":"uniform","chapter":"2 Prerequisites","heading":"2.2.6.2.1 Uniform","text":"Defined interval (,b) probabilities “equally likely” subintervals equal length.Density\\[\nf(x)=\\frac{1}{b-}\n\\]< x < bCDF\\[\n\\begin{cases}\n0 & \\text{x <} \\\\\n\\frac{x-}{b-} & \\text{$\\le x \\le b$ }\\\\\n1 & \\text{x >b}\\\\\n\\end{cases}\n\\]PDFMGF\\[\n\\begin{cases}\n\\frac{e^{tb} - e^{ta}}{t(b-)}&\\text{ $t \\neq 0$}\\\\\n1&\\text{$ t \\neq 0$}\\\\\n\\end{cases}\n\\]Mean\\[\n\\mu = E(X) = \\frac{+b}{2}\n\\]Variance\\[\n\\sigma^2 = Var(X) = \\frac{(b-)^2}{12}\n\\]","code":"\nhist(runif(100000, min = 0, max = 1))"},{"path":"prerequisites.html","id":"gamma","chapter":"2 Prerequisites","heading":"2.2.6.2.2 Gamma","text":"used define exponential chi-squared distributionsThe gamma function defined :\\[\n\\Gamma(\\alpha) = \\int_0^{\\infty} z^{\\alpha-1}e^{-z}dz\n\\]\\(\\alpha > 0\\)Properties Gamma function:\n\\(\\Gamma(1) = 1\\) + \\(\\alpha >1\\), \\(\\Gamma(\\alpha)=(\\alpha-1)\\Gamma(\\alpha-1)\\) + n integer \\(n>1\\), \\(\\Gamma(n) = (n-1)!\\)\nProperties Gamma function:\\(\\Gamma(1) = 1\\) + \\(\\alpha >1\\), \\(\\Gamma(\\alpha)=(\\alpha-1)\\Gamma(\\alpha-1)\\) + n integer \\(n>1\\), \\(\\Gamma(n) = (n-1)!\\)Density\\[\nf(x) = \\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}}x^{\\alpha-1}e^{-x/\\beta}\n\\]CDF\\[\nF(x,n,\\beta) = 1 -\\sum_{k=0}^{n-1} \\frac{(\\frac{x}{\\beta})^k e^{-x/\\beta}}{k!}\n\\]x>0, \\(\\alpha = n\\) (positive integer)PDFMGF\\[\nm_X(t) = (1-\\beta t)^{-\\alpha}\n\\]\\(t < \\frac{1}{\\beta}\\)Mean\\[\n\\mu = E(X) = \\alpha \\beta\n\\]Variance\\[\n\\sigma^2 = Var(X) = \\alpha \\beta^2\n\\]","code":"\nhist(rgamma(n = 10000, shape = 5, rate = 1))"},{"path":"prerequisites.html","id":"normal","chapter":"2 Prerequisites","heading":"2.2.6.2.3 Normal","text":"\\(N(\\mu,\\sigma^2)\\)symmetric, bell-shaped curve parameters \\(\\mu\\) \\(\\sigma^2\\)also known Gaussian.Density\\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi }}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2}\n\\]\\(-\\infty < x, \\mu< \\infty, \\sigma > 0\\)CDFUse tablePDFMGF\\[\nm_X(t) = e^{\\mu t + \\frac{\\sigma^2 t^2}{2}}\n\\]Mean\\[\n\\mu = E(X)\n\\]Variance\\[\n\\sigma^2 = Var(X)\n\\]Standard Normal Random VariableThe normal random variable Z mean \\(\\mu = 0\\) standard deviation \\(\\sigma =1\\) called standard normalAny normal random variable X mean \\(\\mu\\) standard deviation \\(\\sigma\\) can converted standard normal random variable \\(Z = \\frac{X-\\mu}{\\sigma}\\)Normal Approximation Binomial DistributionLet X binomial parameters n p. large n (\\(()p \\le .5\\) \\(np > 5\\) (B) \\(p>.5\\) \\(nq>5\\)), X approximately normally distributed mean \\(\\mu = np\\) standard deviation \\(\\sigma = \\sqrt{npq}\\)using normal approximation, add subtract 0.5 needed continuity correctionNormal Probability RuleIf X normally distributed parameters \\(\\mu\\) \\(\\sigma\\), \\(P(-\\sigma < X - \\mu < \\sigma) \\approx .68\\) * \\(P(-2\\sigma < X - \\mu < 2\\sigma) \\approx .95\\) * \\(P(-3\\sigma < X - \\mu < 3\\sigma) \\approx .997\\)","code":"\nhist(rnorm(1000000, mean = 0, sd = 1))"},{"path":"prerequisites.html","id":"logistic","chapter":"2 Prerequisites","heading":"2.2.6.2.4 Logistic","text":"\\(Logistic(\\mu,s)\\)PDF","code":"\nhist(rlogis(n = 100000, location = 0, scale = 1))"},{"path":"prerequisites.html","id":"lognomral","chapter":"2 Prerequisites","heading":"2.2.6.2.5 Lognomral","text":"\\(lognormal(\\mu,\\sigma^2)\\)PDF","code":"\nhist(rlnorm(n = 10000, meanlog = 0, sdlog = 1))"},{"path":"prerequisites.html","id":"exponential","chapter":"2 Prerequisites","heading":"2.2.6.2.6 Exponential","text":"\\(Exp(\\lambda)\\)special case gamma distribution \\(\\alpha = 1\\)Lack memory\\(\\lambda\\) = rate Within Poisson process parameter \\(\\lambda\\), W waiting tine occurrence first event, W exponential distribution \\(\\beta = 1/\\alpha\\)Density\\[\nf(x) = \\frac{1}{\\beta} e^{-x/\\beta}\n\\]\\(x,\\beta > 0\\)CDF\\[\\begin{equation}\nF(x) = \n\\begin{cases}\n0 & \\text{ $x \\le 0$}\\\\\n1 - e^{-x/\\beta} & \\text{$x > 0$}\\\\\n\\end{cases}\n\\end{equation}\\]PDFMGF\\[\nm_X(t) = (1-\\beta t)^{-1}\n\\]\\(t < 1/\\beta\\)Mean\\[\n\\mu = E(X) = \\beta\n\\]Variance\\[\n\\sigma^2 = Var(X) =\\beta^2\n\\]","code":"\nhist(rexp(n = 100000, rate = 1))"},{"path":"prerequisites.html","id":"chi-squared","chapter":"2 Prerequisites","heading":"2.2.6.2.7 Chi-squared","text":"\\(\\chi^2=\\chi^2(k)\\)special case gamma distribution \\(\\beta =2\\), \\(\\alpha = \\gamma /2\\) positive integer \\(\\gamma\\)random variable X denoted \\(\\chi_{\\gamma}^2\\) said chi-squared distribution \\(\\gamma\\) degrees freedom.Density Use density Gamma Distribution \\(\\beta = 2\\) \\(\\alpha = \\gamma/2\\)CDF Use tablePDFMGF\\[\nm_X(t) = (1-2t)^{-\\gamma/2}\n\\]Mean\\[\n\\mu = E(X) = \\gamma\n\\]Variance\\[\n\\sigma^2 = Var(X) = 2\\gamma\n\\]","code":"\nhist(rchisq(n = 10000, df=2, ncp = 0))"},{"path":"prerequisites.html","id":"student-t","chapter":"2 Prerequisites","heading":"2.2.6.2.8 Student T","text":"\\(T(v)\\)\\(T=\\frac{Z}{\\sqrt{\\chi_{\\gamma}^2/\\gamma}}\\), Z standard normal follows student-t distribution \\(\\gamma\\) dofThe distribution symmetric, bell-shaped , mean \\(\\mu=0\\)","code":"\nhist(rt(n = 100000, df=2, ncp =1))"},{"path":"prerequisites.html","id":"f-distribution","chapter":"2 Prerequisites","heading":"2.2.6.2.9 F-Distribution","text":"\\(F(d_1,d_2)\\)F distribution strictly positive\\(F=\\frac{\\chi_{\\gamma_1}^2/\\gamma_1}{\\chi_{\\gamma_2^2}/\\gamma_2}\\) follows F distribution dof \\(\\gamma_1\\) \\(\\gamma_2\\), \\(\\chi_{\\gamma_1}^2\\) \\(\\chi_{\\gamma_2}^2\\) independent chi-squared random variables.distribution asymmetric never negative.PDF","code":"\nhist(rf(n = 100000, df1=2, df2=3, ncp=1))"},{"path":"prerequisites.html","id":"cauchy","chapter":"2 Prerequisites","heading":"2.2.6.2.10 Cauchy","text":"Central Limit Theorem Weak Law apply Cauchy finite mean finite variancePDF","code":"\nhist(rcauchy(n = 100000, location = 0, scale = 1))"},{"path":"prerequisites.html","id":"multivariate-normal-distribution","chapter":"2 Prerequisites","heading":"2.2.6.2.11 Multivariate Normal Distribution","text":"Let y p-dimensional multivariate normal (MVN) rv mean \\(\\mu\\) variance \\(\\Sigma\\). , density y \\[\nf(\\mathbf{y}) = \\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}|^{1/2}}exp(-\\frac{1}{2}\\mathbf{(y-\\mu)'\\Sigma^{-1}(y-\\mu)})\n\\]\\(\\mathbf{y} \\sim N_p(\\mathbf{\\mu,\\Sigma})\\)Properties:Let \\(\\mathbf{}_{r \\times p}\\) fixed matrix. \\(\\mathbf{Ay} \\sim N_r(\\mathbf{\\mu, \\Sigma ')}\\). Note \\(r \\le p\\) rows must linearly independent guarantee \\(\\mathbf{\\Sigma '}\\) non-singular.Let \\(\\mathbf{G}\\) matrix \\(\\mathbf{\\Sigma^{-1}= GG'}\\). , \\(\\mathbf{G'y} \\sim N_p (\\mathbf{G'\\mu,})\\) \\(\\mathbf{G'(y-\\mu)} \\sim N_p (\\mathbf{0,})\\).fixed linear combination \\(y_1,...,y_p\\) say \\(\\mathbf{c'y}\\), follows \\(\\mathbf{c'y} \\sim N_1(\\mathbf{c'\\mu,c'\\Sigma c})\\)Large Sample PropertiesSuppose \\(y_1,...,y_n\\) random sample population mean \\(\\mu\\) variance-variance matrix \\(\\Sigma\\)\\[\n\\mathbf{Y} \\sim MVN(\\mathbf{\\mu,\\Sigma})\n\\]\\(\\bar{\\mathbf{y}} = \\frac{1}{n}\\sum_{=1}^n \\mathbf{y}_i\\) consistent estimator \\(\\mathbf{\\mu}\\)\\(\\mathbf{S} = \\frac{1}{n-1}\\sum_{=1}^n \\mathbf{(y_i - \\bar{y})(y_i - \\bar{y})'}\\) consistent estimator \\(\\mathbf{\\Sigma}\\)Multivariate Central Limit Theorem: Similar univariate case, \\(\\sqrt{n}(\\mathbf{\\bar{y}- \\mu}) \\sim N_p(\\mathbf{0, \\Sigma})\\) n large relative p (e.g., \\(n \\ge 25p\\)), equivalent \\(\\bar{y} \\sim N_p(\\mathbf{\\mu,\\Sigma/n})\\)Wald’s Theorem: \\(n(\\mathbf{\\bar{y}- \\mu)'S^{-1}(\\bar{y}- \\mu)} \\sim \\chi^2_{(p)}\\) n large relative p.","code":""},{"path":"prerequisites.html","id":"section-1","chapter":"2 Prerequisites","heading":"2.2.6.2.12 ","text":"","code":""},{"path":"prerequisites.html","id":"general-math","chapter":"2 Prerequisites","heading":"2.3 General Math","text":"Chebyshev’s Inequality Let X random variable mean \\(\\mu\\) standard deviation \\(\\sigma\\). positive number k:\\[\nP(|X-\\mu| < k\\sigma) \\ge 1 - \\frac{1}{k^2}\n\\]Chebyshev’s Inequality require X normally distributedMaclaurin series expansion \\[\ne^z = 1 + z + \\frac{z^2}{2!} + \\frac{z^3}{3!} + ...\n\\]Geometric series:\\[\ns_n=\\sum_{k=1}^{n}ar^{n-1}=\\frac{(1-r^n)}{1-r}\n\\]|r| < 1\\[\ns=\\sum_{k=1}^{\\infty}ar^{n-1}=\\frac{}{1-r}\n\\]","code":""},{"path":"prerequisites.html","id":"law-of-large-numbers","chapter":"2 Prerequisites","heading":"2.3.1 Law of large numbers","text":"Let \\(X_1,X_2,...\\) infinite sequence independent identically distributed (..d) , sample average \\[\n\\bar{X}_n =\\frac{1}{n} (X_1 + ... + X_n)\n\\]converges expected value (\\(\\bar{X}_n \\rightarrow \\mu\\)) \\(n \\rightarrow \\infty\\)\\[\nVar(X_i) = Var(\\frac{1}{n}(X_1 + ... + X_n)) = \\frac{1}{n^2}Var(X_1 + ... + X_n)= \\frac{n\\sigma^2}{n^2}=\\frac{\\sigma^2}{n}\n\\]difference Weak Law Strong Law regards mode convergence","code":""},{"path":"prerequisites.html","id":"weak-law","chapter":"2 Prerequisites","heading":"2.3.1.1 Weak Law","text":"sample average converges probability towards expected value\\[\n\\bar{X}_n \\rightarrow^{p} \\mu\n\\]\\(n \\rightarrow \\infty\\)\\[\n\\lim_{n\\\\infty}P(|\\bar{X}_n - \\mu| > \\epsilon) = 0\n\\]sample mean iid random sample (\\(\\{ x_i \\}_{=1}^n\\)) population finite mean finite variance \\(\\sigma^2\\) ca consistent estimation population mean \\(\\mu\\)\\[\nplim(\\bar{x})=plim(n^{-1}\\sum_{=1}^{n}x_i) =\\mu\n\\]","code":""},{"path":"prerequisites.html","id":"strong-law","chapter":"2 Prerequisites","heading":"2.3.1.2 Strong Law","text":"sample average converges almost surely expected value\\[\n\\bar{X}_n \\rightarrow^{.s} \\mu \n\\]\\(n \\rightarrow \\infty\\)Equivalently,\\[\nP(\\lim_{n\\\\infty}\\bar{X}_n =\\mu) =1\n\\]","code":""},{"path":"prerequisites.html","id":"law-of-iterated-expectation","chapter":"2 Prerequisites","heading":"2.3.2 Law of Iterated Expectation","text":"Let X, Y random variables. ,\\[\nE(X) = E(E(X|Y))\n\\]means expected value X can calculated probability distribution X|Y Y","code":""},{"path":"prerequisites.html","id":"convergence","chapter":"2 Prerequisites","heading":"2.3.3 Convergence","text":"","code":""},{"path":"prerequisites.html","id":"convergence-in-probability","chapter":"2 Prerequisites","heading":"2.3.3.1 Convergence in Probability","text":"\\(n \\rightarrow \\infty\\), estimator (random variable) close true value.random variable \\(\\theta_n\\) converges probability constant c \\[\n\\lim_{n\\\\infty}P(|\\theta_n - c| \\ge \\epsilon) = 0\n\\]positive \\(\\epsilon\\)Notation\\[\nplim(\\theta_n)=c \n\\]Equivalently,\\[\n\\theta_n \\rightarrow^p c\n\\]Properties Convergence ProbabilitySlutsky’s Theorem: continuous function g(.), \\(plim(\\theta_n)= \\theta\\) \\(plim(g(\\theta_n)) = g(\\theta)\\)Slutsky’s Theorem: continuous function g(.), \\(plim(\\theta_n)= \\theta\\) \\(plim(g(\\theta_n)) = g(\\theta)\\)\\(\\gamma_n \\rightarrow^p \\gamma\\) \n\\(plim(\\theta_n + \\gamma_n)=\\theta + \\gamma\\) + \\(plim(\\theta_n \\gamma_n) = \\theta \\gamma\\) + \\(plim(\\theta_n/\\gamma_n) = \\theta/\\gamma\\) \\(\\gamma \\neq 0\\)\n\\(\\gamma_n \\rightarrow^p \\gamma\\) \\(plim(\\theta_n + \\gamma_n)=\\theta + \\gamma\\) + \\(plim(\\theta_n \\gamma_n) = \\theta \\gamma\\) + \\(plim(\\theta_n/\\gamma_n) = \\theta/\\gamma\\) \\(\\gamma \\neq 0\\)Also hold random vectors/ matricesAlso hold random vectors/ matrices","code":""},{"path":"prerequisites.html","id":"convergence-in-distribution","chapter":"2 Prerequisites","heading":"2.3.3.2 Convergence in Distribution","text":"\\(n \\rightarrow \\infty\\), distribution random variable may converge towards another (“fixed”) distribution.random variable \\(X_n\\) CDF \\(F_n(x)\\) converges distribution random variable X CDF \\(F(X)\\) \\[\n\\lim_{n\\\\infty}|F_n(x) - F(x)| = 0\n\\]points continuity \\(F(X)\\)Notation F(x) limiting distribution \\(X_n\\) \\(X_n \\rightarrow^d X\\)E(X) limiting mean (asymptotic mean)Var(X) limiting variance (asymptotic variance)Note\\[\nE(X) \\neq \\lim_{n\\\\infty}E(X_n) \\\\\nAvar(X_n) \\neq \\lim_{n\\\\infty}Var(X_n)\n\\]Properties Convergence DistributionContinuous Mapping Theorem: continuous function g(.), \\(X_n \\^{d} g(X)\\) \\(g(X_n) \\^{d} g(X)\\)Continuous Mapping Theorem: continuous function g(.), \\(X_n \\^{d} g(X)\\) \\(g(X_n) \\^{d} g(X)\\)\\(Y_n\\^{d} c\\), \n\\(X_n + Y_n \\^{d} X + c\\)\n\\(Y_nX_n \\^{d} cX\\)\n\\(X_nY_n \\^{d} X/c\\) \\(c \\neq 0\\)\n\\(Y_n\\^{d} c\\), \\(X_n + Y_n \\^{d} X + c\\)\\(X_n + Y_n \\^{d} X + c\\)\\(Y_nX_n \\^{d} cX\\)\\(Y_nX_n \\^{d} cX\\)\\(X_nY_n \\^{d} X/c\\) \\(c \\neq 0\\)\\(X_nY_n \\^{d} X/c\\) \\(c \\neq 0\\)also hold random vectors/matricesalso hold random vectors/matrices","code":""},{"path":"prerequisites.html","id":"summary","chapter":"2 Prerequisites","heading":"2.3.3.3 Summary","text":"Properties ConvergenceConvergence Probability stronger Convergence Distribution. However, Convergence Distribution guarantee Convergence Probability","code":""},{"path":"prerequisites.html","id":"sufficient-statistics","chapter":"2 Prerequisites","heading":"2.3.4 Sufficient Statistics","text":"Likelihooddescribes extent sample provides support particular parameter value.Higher support corresponds higher value likelihoodThe exact value likelihood meaningless,relative value, (.e., comparing two values \\(\\theta\\)), informative.\\[\nL(\\theta_0; y) = P(Y = y | \\theta = \\theta_0) = f_Y(y;\\theta_0)\n\\]Likelihood Ratio\\[\n\\frac{L(\\theta_0;y)}{L(\\theta_1;y)}\n\\]Likelihood FunctionFor given sample, can create likelihoods possible values \\(\\theta\\), called likelihood function\\[\nL(\\theta) = L(\\theta; y) = f_Y(y;\\theta)\n\\]sample size n, likelihood function takes form product\\[\nL(\\theta) = \\prod_{=1}^{n}f_i (y_i;\\theta)\n\\]Equivalently, log likelihood function\\[\nl(\\theta) = \\sum_{=1}^{n} logf_i(y_i;\\theta)\n\\]Sufficient statisticsA statistic, T(y), quantity can calculated purely sample (independent \\(\\theta\\))statistic sufficient conveys available information parameter.\\[\nL(\\theta; y) = c(y)L^*(\\theta;T(y))\n\\]Nuisance parameters interested parameter (e.g., mean). parameters requiring estimation (e.g., standard deviation) nuisance parameters. can replace nuisance parameters likelihood function estimates create **profile likelihood*.","code":""},{"path":"prerequisites.html","id":"parameter-transformations","chapter":"2 Prerequisites","heading":"2.3.5 Parameter transformations","text":"log-odds transformation\\[\nLog odds = g(\\theta)= ln[\\frac{\\theta}{1-\\theta}]\n\\]log transformation","code":""},{"path":"prerequisites.html","id":"data-importexport","chapter":"2 Prerequisites","heading":"2.4 Data Import/Export","text":"Extended Manual RTable Rio VignetteR limitations:default, R use 1 core CPUBy default, R use 1 core CPUR puts data memory (limit around 2-4 GB), SAS uses data files demandR puts data memory (limit around 2-4 GB), SAS uses data files demandCategorization\nMedium-size file: within RAM limit, around 1-2 GB\nLarge file: 2-10 GB, might workaround solution\nlarge file > 10 GB, use distributed parallel computing\nCategorizationMedium-size file: within RAM limit, around 1-2 GBMedium-size file: within RAM limit, around 1-2 GBLarge file: 2-10 GB, might workaround solutionLarge file: 2-10 GB, might workaround solutionVery large file > 10 GB, use distributed parallel computingVery large file > 10 GB, use distributed parallel computingSolutions:buy RAMbuy RAMHPC packages\nExplicit Parallelism\nImplicit Parallelism\nLarge Memory\nMap/Reduce\nHPC packagesExplicit ParallelismExplicit ParallelismImplicit ParallelismImplicit ParallelismLarge MemoryLarge MemoryMap/ReduceMap/Reducespecify number rows columns, typically including command nrow =specify number rows columns, typically including command nrow =Use packages store data differently\nbigmemory, biganalytics, bigtabulate , synchronicity, bigalgebra, bigvideo use C++ store matrices, also support one class type\nmultiple class types, use ff package\nUse packages store data differentlybigmemory, biganalytics, bigtabulate , synchronicity, bigalgebra, bigvideo use C++ store matrices, also support one class typebigmemory, biganalytics, bigtabulate , synchronicity, bigalgebra, bigvideo use C++ store matrices, also support one class typeFor multiple class types, use ff packageFor multiple class types, use ff packageVery Large datasets use\nRHaddop package\nHadoopStreaming\nRhipe\nLarge datasets useRHaddop packageHadoopStreamingRhipe","code":""},{"path":"prerequisites.html","id":"medium-size","chapter":"2 Prerequisites","heading":"2.4.1 Medium size","text":"import multiple files directoryTo export single data fileTo export multiple data filesTo convert data file types","code":"\nlibrary(\"rio\")## Warning: package 'rio' was built under R version 4.0.5\nstr(import_list(dir()), which = 1)\nexport(data, \"data.csv\")\nexport(data,\"data.dta\")\nexport(data,\"data.txt\")\nexport(data,\"data_cyl.rds\")\nexport(data,\"data.rdata\")\nexport(data,\"data.R\")\nexport(data,\"data.csv.zip\")\nexport(data,\"list.json\")\nexport(list(mtcars = mtcars, iris = iris), \"data_file_type\") # where data_file_type should substituted with the extension listed above\n# convert Stata to SPSS\nconvert(\"data.dta\", \"data.sav\")"},{"path":"prerequisites.html","id":"large-size","chapter":"2 Prerequisites","heading":"2.4.2 Large size","text":"Use R clusterAmazon Web Service (AWS): $1/hrImport files chunksdata.table methodff package: method allow pass connectionsbigmemory packagesqldf packageRQLite packageDownload SQLite, pick “bundle command-line tools managing SQLite database files” Window 10Unzip file, open sqlite3.exe.Type prompt\nsqlite> .cd 'C:\\Users\\data' specify path desired directory\nsqlite> .open database_name.db open database\nimport CSV file database\nsqlite> .mode csv specify SQLite next file .csv file\nsqlite> .import file_name.csv datbase_name import csv file database\n\nsqlite> .exit ’re done, exit sqlite program\nsqlite> .cd 'C:\\Users\\data' specify path desired directorysqlite> .open database_name.db open databaseTo import CSV file database\nsqlite> .mode csv specify SQLite next file .csv file\nsqlite> .import file_name.csv datbase_name import csv file database\nsqlite> .mode csv specify SQLite next file .csv filesqlite> .import file_name.csv datbase_name import csv file databasesqlite> .exit ’re done, exit sqlite programarrow packagevroom packagedata.table packageComparisons regarding storage spaceTo work big data, can convert csv.gz , since typically, R require load whole data export . data greater 10 GB, sequentially. Even though read.csv much slower readr::read_csv , still use can pass connection, allows loop sequentially. , currently readr::read_csv skip function, even can use skip, still read skip lines previous loop.example, say read_csv(, n_max = 100, skip =0) read_csv(, n_max = 200, skip = 100) actually read first 100 rows. However, read.csv without specifying anything, continue 100 mark.Notice, sometimes might error looking like “Error (function (con, , n = 1L, size = NA_integer_, signed = TRUE, : can read binary connection”can change instead r connection rb . Even though author package suggested file able recognize appropriate form, far prevail.","code":"\nfile_in    <- file(\"in.csv\",\"r\")\nchunk_size <- 100000 # choose the best size for you\nx          <- readLines(file_in, n=chunk_size)\nrequire(data.table)\nmydata = fread(\"in.csv\", header = T)\nlibrary(\"ff\")\nx <- read.csv.ffdf(\n    file = \"file.csv\",\n    nrow = 10,\n    header = TRUE,\n    VERBOSE = TRUE,\n    first.rows = 10000,\n    next.rows = 50000,\n    colClasses = NA\n)\nmy_data <- read.big.matrix('in.csv', header = T)\nlibrary(sqldf)\nmy_data <- read.csv.sql('in.csv')\n\niris2 <- read.csv.sql(\"iris.csv\", \n    sql = \"select * from file where Species = 'setosa' \")\nlibrary(RMySQL)## Warning: package 'RMySQL' was built under R version 4.0.5## Loading required package: DBI\nlibrary(DBI)\nlibrary(dplyr)\nlibrary(\"RSQLite\")\nsetwd(\"\")\ncon <- dbConnect(RSQLite::SQLite(), \"data_base.db\")\ntbl <- tbl(con, \"data_table\")\ntbl %>% \n    filter() %>%\n    select() %>%\n    collect() # to actually pull the data into the workspace\ndbDisconnect(con)\nlibrary(\"arrow\")\nread_csv_arrow()\nlibrary(vroom)\nspec(vroom(file_path))\ncompressed <- vroom_example(\"mtcars.csv.zip\")\nvroom(compressed)\ns = fread(\"sample.csv\")test = ff::read.csv.ffdf(file = \"\")\nobject.size(test) # worst\n\ntest1 = data.table::fread(file = \"\")\nobject.size(test1) # best\n\ntest2 = readr::read_csv(\"\"))\nobject.size(test2) # 2nd\n\ntest3 = vroom(file = \"\")\nobject.size(test3) # equal to read_csv"},{"path":"prerequisites.html","id":"data-manipulation","chapter":"2 Prerequisites","heading":"2.5 Data Manipulation","text":"verbs data manipulationselect: selecting (selecting) columns based names (eg: select columns Q1 Q25)slice: selecting (selecting) rows based position (eg: select rows 1:10)mutate: add derive new columns (variables) based existing columns (eg: create new column expresses measurement cm based existing measure inches)rename: rename variables change column names (eg: change “GraduationRate100” “grad100”)filter: selecting rows based condition (eg: rows gender = Male)arrange: ordering rows based variable(s) numeric alphabetical order (eg: sort descending order Income)sample: take random samples data (eg: sample 80% data create “training” set)summarize: condense aggregate multiple values single summary values (eg: calculate median income age group)group_by: convert tbl grouped tbl operations performed “group”; allows us summarize data apply verbs data groups (eg, gender treatment)pipe: %>%Use Ctrl + Shift + M (Win) Cmd + Shift + M (Mac) enter RStudioThe pipe takes output function “pipes” first argument next function.","code":"\n# load packages\nlibrary(tidyverse)## Warning: package 'tidyverse' was built under R version 4.0.5## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --## v ggplot2 3.3.5     v purrr   0.3.4\n## v tibble  3.1.5     v dplyr   1.0.7\n## v tidyr   1.1.3     v stringr 1.4.0\n## v readr   2.0.1     v forcats 0.5.1## Warning: package 'ggplot2' was built under R version 4.0.5## Warning: package 'tibble' was built under R version 4.0.5## Warning: package 'readr' was built under R version 4.0.5## Warning: package 'dplyr' was built under R version 4.0.5## -- Conflicts ------------------------------------------ tidyverse_conflicts() --\n## x dplyr::filter() masks stats::filter()\n## x dplyr::lag()    masks stats::lag()\nlibrary(lubridate)## Warning: package 'lubridate' was built under R version 4.0.5## \n## Attaching package: 'lubridate'## The following objects are masked from 'package:base':\n## \n##     date, intersect, setdiff, union\nx <- c(1, 4, 23, 4, 45)\nn <- c(1, 3, 5)\ng <- c(\"M\", \"M\", \"F\")\ndf <- data.frame(n, g)\ndf##   n g\n## 1 1 M\n## 2 3 M\n## 3 5 F\nstr(df)## 'data.frame':    3 obs. of  2 variables:\n##  $ n: num  1 3 5\n##  $ g: chr  \"M\" \"M\" \"F\"\n#Similarly\ndf <- tibble(n, g)\ndf## # A tibble: 3 x 2\n##       n g    \n##   <dbl> <chr>\n## 1     1 M    \n## 2     3 M    \n## 3     5 F\nstr(df)## tibble [3 x 2] (S3: tbl_df/tbl/data.frame)\n##  $ n: num [1:3] 1 3 5\n##  $ g: chr [1:3] \"M\" \"M\" \"F\"\n# list form\nlst <- list(x, n, g, df)\nlst## [[1]]\n## [1]  1  4 23  4 45\n## \n## [[2]]\n## [1] 1 3 5\n## \n## [[3]]\n## [1] \"M\" \"M\" \"F\"\n## \n## [[4]]\n## # A tibble: 3 x 2\n##       n g    \n##   <dbl> <chr>\n## 1     1 M    \n## 2     3 M    \n## 3     5 F\n# Or\nlst2 <- list(num = x, size = n, sex = g, data = df)\nlst2## $num\n## [1]  1  4 23  4 45\n## \n## $size\n## [1] 1 3 5\n## \n## $sex\n## [1] \"M\" \"M\" \"F\"\n## \n## $data\n## # A tibble: 3 x 2\n##       n g    \n##   <dbl> <chr>\n## 1     1 M    \n## 2     3 M    \n## 3     5 F\n# Or\nlst3 <- list(x = c(1, 3, 5, 7),\n             y = c(2, 2, 2, 4, 5, 5, 5, 6),\n             z = c(22, 3, 3, 3, 5, 10))\nlst3## $x\n## [1] 1 3 5 7\n## \n## $y\n## [1] 2 2 2 4 5 5 5 6\n## \n## $z\n## [1] 22  3  3  3  5 10\n# find the means of x, y, z.\n\n# can do one at a time\nmean(lst3$x)## [1] 4\nmean(lst3$y)## [1] 3.875\nmean(lst3$z)## [1] 7.666667\n# list apply\nlapply(lst3, mean)## $x\n## [1] 4\n## \n## $y\n## [1] 3.875\n## \n## $z\n## [1] 7.666667\n# OR\nsapply(lst3, mean)##        x        y        z \n## 4.000000 3.875000 7.666667\n# Or, tidyverse function map() \nmap(lst3, mean)## $x\n## [1] 4\n## \n## $y\n## [1] 3.875\n## \n## $z\n## [1] 7.666667\n# The tidyverse requires a modified map function called map_dbl()\nmap_dbl(lst3, mean)##        x        y        z \n## 4.000000 3.875000 7.666667\n# Binding \ndat01 <- tibble(x = 1:5, y = 5:1)\ndat01## # A tibble: 5 x 2\n##       x     y\n##   <int> <int>\n## 1     1     5\n## 2     2     4\n## 3     3     3\n## 4     4     2\n## 5     5     1\ndat02 <- tibble(x = 10:16, y = x/2)\ndat02## # A tibble: 7 x 2\n##       x     y\n##   <int> <dbl>\n## 1    10   5  \n## 2    11   5.5\n## 3    12   6  \n## 4    13   6.5\n## 5    14   7  \n## 6    15   7.5\n## 7    16   8\ndat03 <- tibble(z = runif(5)) # 5 random numbers from interval (0,1)\ndat03## # A tibble: 5 x 1\n##        z\n##    <dbl>\n## 1 0.0734\n## 2 0.407 \n## 3 0.197 \n## 4 0.764 \n## 5 0.398\n# row binding\nbind_rows(dat01, dat02, dat01)## # A tibble: 17 x 2\n##        x     y\n##    <int> <dbl>\n##  1     1   5  \n##  2     2   4  \n##  3     3   3  \n##  4     4   2  \n##  5     5   1  \n##  6    10   5  \n##  7    11   5.5\n##  8    12   6  \n##  9    13   6.5\n## 10    14   7  \n## 11    15   7.5\n## 12    16   8  \n## 13     1   5  \n## 14     2   4  \n## 15     3   3  \n## 16     4   2  \n## 17     5   1\n# use \".id\" argument to create a new column that contains an identifier for the original data.\nbind_rows(dat01, dat02, .id = \"id\")## # A tibble: 12 x 3\n##    id        x     y\n##    <chr> <int> <dbl>\n##  1 1         1   5  \n##  2 1         2   4  \n##  3 1         3   3  \n##  4 1         4   2  \n##  5 1         5   1  \n##  6 2        10   5  \n##  7 2        11   5.5\n##  8 2        12   6  \n##  9 2        13   6.5\n## 10 2        14   7  \n## 11 2        15   7.5\n## 12 2        16   8\n# with name\nbind_rows(\"dat01\" = dat01, \"dat02\" = dat02, .id = \"id\")## # A tibble: 12 x 3\n##    id        x     y\n##    <chr> <int> <dbl>\n##  1 dat01     1   5  \n##  2 dat01     2   4  \n##  3 dat01     3   3  \n##  4 dat01     4   2  \n##  5 dat01     5   1  \n##  6 dat02    10   5  \n##  7 dat02    11   5.5\n##  8 dat02    12   6  \n##  9 dat02    13   6.5\n## 10 dat02    14   7  \n## 11 dat02    15   7.5\n## 12 dat02    16   8\n# bind_rows() also works on lists of data frames\nlist01 <- list(\"dat01\" = dat01, \"dat02\" = dat02)\nlist01## $dat01\n## # A tibble: 5 x 2\n##       x     y\n##   <int> <int>\n## 1     1     5\n## 2     2     4\n## 3     3     3\n## 4     4     2\n## 5     5     1\n## \n## $dat02\n## # A tibble: 7 x 2\n##       x     y\n##   <int> <dbl>\n## 1    10   5  \n## 2    11   5.5\n## 3    12   6  \n## 4    13   6.5\n## 5    14   7  \n## 6    15   7.5\n## 7    16   8\nbind_rows(list01)## # A tibble: 12 x 2\n##        x     y\n##    <int> <dbl>\n##  1     1   5  \n##  2     2   4  \n##  3     3   3  \n##  4     4   2  \n##  5     5   1  \n##  6    10   5  \n##  7    11   5.5\n##  8    12   6  \n##  9    13   6.5\n## 10    14   7  \n## 11    15   7.5\n## 12    16   8\nbind_rows(list01, .id = \"source\")## # A tibble: 12 x 3\n##    source     x     y\n##    <chr>  <int> <dbl>\n##  1 dat01      1   5  \n##  2 dat01      2   4  \n##  3 dat01      3   3  \n##  4 dat01      4   2  \n##  5 dat01      5   1  \n##  6 dat02     10   5  \n##  7 dat02     11   5.5\n##  8 dat02     12   6  \n##  9 dat02     13   6.5\n## 10 dat02     14   7  \n## 11 dat02     15   7.5\n## 12 dat02     16   8\n# The extended example below demonstrates how this can be very handy.\n\n# column binding\nbind_cols(dat01, dat03)## # A tibble: 5 x 3\n##       x     y      z\n##   <int> <int>  <dbl>\n## 1     1     5 0.0734\n## 2     2     4 0.407 \n## 3     3     3 0.197 \n## 4     4     2 0.764 \n## 5     5     1 0.398\n# Regular expressions -----------------------------------------------------\nnames <- c(\"Ford, MS\", \"Jones, PhD\", \"Martin, Phd\", \"Huck, MA, MLS\")\n\n# pattern: first comma and everything after it\nstr_remove(names, pattern = \", [[:print:]]+\")## [1] \"Ford\"   \"Jones\"  \"Martin\" \"Huck\"\n# [[:print:]]+ = one or more printable characters\n\n\n# Reshaping ---------------------------------------------------------------\n\n# Example of a wide data frame. Notice each person has multiple test scores\n# that span columns.\nwide <- data.frame(name=c(\"Clay\",\"Garrett\",\"Addison\"), \n                   test1=c(78, 93, 90), \n                   test2=c(87, 91, 97),\n                   test3=c(88, 99, 91))\nwide##      name test1 test2 test3\n## 1    Clay    78    87    88\n## 2 Garrett    93    91    99\n## 3 Addison    90    97    91\n# Example of a long data frame. This is the same data as above, but in long\n# format. We have one row per person per test.\nlong <- data.frame(name=rep(c(\"Clay\",\"Garrett\",\"Addison\"),each=3),\n                   test=rep(1:3, 3),\n                   score=c(78, 87, 88, 93, 91, 99, 90, 97, 91))\nlong##      name test score\n## 1    Clay    1    78\n## 2    Clay    2    87\n## 3    Clay    3    88\n## 4 Garrett    1    93\n## 5 Garrett    2    91\n## 6 Garrett    3    99\n## 7 Addison    1    90\n## 8 Addison    2    97\n## 9 Addison    3    91\n# mean score per student\naggregate(score ~ name, data = long, mean)##      name    score\n## 1 Addison 92.66667\n## 2    Clay 84.33333\n## 3 Garrett 94.33333\naggregate(score ~ test, data = long, mean)##   test    score\n## 1    1 87.00000\n## 2    2 91.66667\n## 3    3 92.66667\n# line plot of scores over test, grouped by name\nggplot(long, aes(x = factor(test), y = score, color = name, group = name)) +\n  geom_point() +\n  geom_line() +\n  xlab(\"Test\")\n#### reshape wide to long\npivot_longer(wide, test1:test3, names_to = \"test\", values_to = \"score\")## # A tibble: 9 x 3\n##   name    test  score\n##   <chr>   <chr> <dbl>\n## 1 Clay    test1    78\n## 2 Clay    test2    87\n## 3 Clay    test3    88\n## 4 Garrett test1    93\n## 5 Garrett test2    91\n## 6 Garrett test3    99\n## 7 Addison test1    90\n## 8 Addison test2    97\n## 9 Addison test3    91\n# Or\npivot_longer(wide, -name, names_to = \"test\", values_to = \"score\")## # A tibble: 9 x 3\n##   name    test  score\n##   <chr>   <chr> <dbl>\n## 1 Clay    test1    78\n## 2 Clay    test2    87\n## 3 Clay    test3    88\n## 4 Garrett test1    93\n## 5 Garrett test2    91\n## 6 Garrett test3    99\n## 7 Addison test1    90\n## 8 Addison test2    97\n## 9 Addison test3    91\n# drop \"test\" from the test column with names_prefix argument\npivot_longer(wide, -name, names_to = \"test\", values_to = \"score\", \n             names_prefix = \"test\")## # A tibble: 9 x 3\n##   name    test  score\n##   <chr>   <chr> <dbl>\n## 1 Clay    1        78\n## 2 Clay    2        87\n## 3 Clay    3        88\n## 4 Garrett 1        93\n## 5 Garrett 2        91\n## 6 Garrett 3        99\n## 7 Addison 1        90\n## 8 Addison 2        97\n## 9 Addison 3        91\n#### reshape long to wide \npivot_wider(long, name, names_from = test, values_from = score)## # A tibble: 3 x 4\n##   name      `1`   `2`   `3`\n##   <chr>   <dbl> <dbl> <dbl>\n## 1 Clay       78    87    88\n## 2 Garrett    93    91    99\n## 3 Addison    90    97    91\n# using the names_prefix argument lets us prepend text to the column names.\npivot_wider(long, name, names_from = test, values_from = score,\n            names_prefix = \"test\")## # A tibble: 3 x 4\n##   name    test1 test2 test3\n##   <chr>   <dbl> <dbl> <dbl>\n## 1 Clay       78    87    88\n## 2 Garrett    93    91    99\n## 3 Addison    90    97    91"},{"path":"descriptive-stat.html","id":"descriptive-stat","chapter":"3 Descriptive Statistics","heading":"3 Descriptive Statistics","text":"area interest want research, problem want solve, relationship want investigate, theoretical empirical processes help .Estimand defined “quantity scientific interest can calculated population change value depending data collection design used measure (.e., vary sample size survey design, number nonrespondents, follow-efforts).” (Rubin 1996)Estimands include:population meansPopulation variancescorrelationsfactor loadingregression coefficients","code":""},{"path":"descriptive-stat.html","id":"numerical-measures","chapter":"3 Descriptive Statistics","heading":"3.1 Numerical Measures","text":"differences population sampleNote:Order Statistics: \\(y_{(1)},y_{(2)},...,y_{(n)}\\) \\(y_{(1)}<y_{(2)}<...<y_{(n)}\\)Order Statistics: \\(y_{(1)},y_{(2)},...,y_{(n)}\\) \\(y_{(1)}<y_{(2)}<...<y_{(n)}\\)Coefficient variation: standard deviation mean. metric stable, dimensionless statistic comparison.Coefficient variation: standard deviation mean. metric stable, dimensionless statistic comparison.Symmetric: mean = median, skewness = 0Symmetric: mean = median, skewness = 0Skewed right: mean > median, skewness > 0Skewed right: mean > median, skewness > 0Skewed left: mean < median, skewness < 0Skewed left: mean < median, skewness < 0Central moments: \\(\\mu=E(Y)\\) , \\(\\mu_2 = \\sigma^2=E(Y-\\mu)^2\\) , \\(\\mu_3 = E(Y-\\mu)^3\\), \\(\\mu_4 = E(Y-\\mu)^4\\)Central moments: \\(\\mu=E(Y)\\) , \\(\\mu_2 = \\sigma^2=E(Y-\\mu)^2\\) , \\(\\mu_3 = E(Y-\\mu)^3\\), \\(\\mu_4 = E(Y-\\mu)^4\\)normal distributions, \\(\\mu_3=0\\), \\(g_1=0\\)normal distributions, \\(\\mu_3=0\\), \\(g_1=0\\)\\(\\hat{g_1}\\) distributed approximately N(0,6/n) sample normal population. (valid n > 150)\nlarge samples, inference skewness can based normal tables 95% confidence interval \\(g_1\\) \\(\\hat{g_1}\\pm1.96\\sqrt{6/n}\\)\nsmall samples, special tables Snedecor Cochran 1989, Table 19() Monte Carlo test\n\\(\\hat{g_1}\\) distributed approximately N(0,6/n) sample normal population. (valid n > 150)large samples, inference skewness can based normal tables 95% confidence interval \\(g_1\\) \\(\\hat{g_1}\\pm1.96\\sqrt{6/n}\\)small samples, special tables Snedecor Cochran 1989, Table 19() Monte Carlo testFor normal distribution, \\(g_2^*=3\\). Kurtosis often redefined : \\(g_2=\\frac{E(Y-\\mu)^4}{\\sigma^4}-3\\) 4th central moment estimated \\(m_4=\\sum_{=1}^{n}(y_i-\\overline{y})^4/n\\)\nasymptotic sampling distribution \\(\\hat{g_2}\\) approximately N(0,24/n) (n > 1000)\nlarge sample kurtosis uses standard normal tables\nsmall sample uses tables Snedecor Cochran, 1989, Table 19(ii) Geary 1936\nnormal distribution, \\(g_2^*=3\\). Kurtosis often redefined : \\(g_2=\\frac{E(Y-\\mu)^4}{\\sigma^4}-3\\) 4th central moment estimated \\(m_4=\\sum_{=1}^{n}(y_i-\\overline{y})^4/n\\)asymptotic sampling distribution \\(\\hat{g_2}\\) approximately N(0,24/n) (n > 1000)large sample kurtosis uses standard normal tablessmall sample uses tables Snedecor Cochran, 1989, Table 19(ii) Geary 1936","code":"\ndata = rnorm(100)\nlibrary(e1071)## Warning: package 'e1071' was built under R version 4.0.5\nskewness(data,type = 1)## [1] -0.05512945\nkurtosis(data, type = 1)## [1] -0.5439923"},{"path":"descriptive-stat.html","id":"graphical-measures","chapter":"3 Descriptive Statistics","heading":"3.2 Graphical Measures","text":"","code":""},{"path":"descriptive-stat.html","id":"shape","chapter":"3 Descriptive Statistics","heading":"3.2.1 Shape","text":"’s good habit label graph, others can easily follow.Others advanced plots","code":"\ndata = rnorm(100)\n\n# Histogram\nhist(data,labels = T,col=\"grey\",breaks = 12) \n\n# Interactive histogram  \npacman::p_load(\"highcharter\")\nhchart(data) \n# Box-and-Whisker plot\nboxplot(count ~ spray, data = InsectSprays,col = \"lightgray\",main=\"boxplot\")\n# Notched Boxplot\nboxplot(len~supp*dose, data=ToothGrowth, notch=TRUE,\n  col=(c(\"gold\",\"darkgreen\")),\n  main=\"Tooth Growth\", xlab=\"Suppliment and Dose\")## Warning in bxp(list(stats = structure(c(8.2, 9.7, 12.25, 16.5, 21.5, 4.2, : some\n## notches went outside hinges ('box'): maybe set notch=FALSE\n# If notches differ -> medians differ\n\n# Stem-and-Leaf Plots\nstem(data)## \n##   The decimal point is at the |\n## \n##   -3 | 5\n##   -2 | 51\n##   -1 | 6644332210\n##   -0 | 99998888888776666655554444333222211111100\n##    0 | 011112223333334445566666778899\n##    1 | 011233456789\n##    2 | 0027\n# Bagplot - A 2D Boxplot Extension\npacman::p_load(aplpack)\nattach(mtcars)\nbagplot(wt,mpg, xlab=\"Car Weight\", ylab=\"Miles Per Gallon\",\n  main=\"Bagplot Example\")\n# boxplot.matrix()  #library(\"sfsmisc\")\n# boxplot.n()       #library(\"gplots\")\n# vioplot()         #library(\"vioplot\")"},{"path":"descriptive-stat.html","id":"scatterplot","chapter":"3 Descriptive Statistics","heading":"3.2.2 Scatterplot","text":"","code":"\n# pairs(mtcars)"},{"path":"descriptive-stat.html","id":"normality-assessment","chapter":"3 Descriptive Statistics","heading":"3.3 Normality Assessment","text":"Since Normal (Gaussian) distribution many applications, typically want/ wish data variable normal. Hence, assess normality based Numerical Measures also Graphical Measures","code":""},{"path":"descriptive-stat.html","id":"graphical-assessment","chapter":"3 Descriptive Statistics","heading":"3.3.1 Graphical Assessment","text":"straight line represents theoretical line normally distributed data. dots represent real empirical data checking. dots fall straight line, can confident data follow normal distribution. data wiggle deviate line, concerned normality assumption.","code":"\npacman::p_load(\"car\")\nqqnorm(precip, ylab = \"Precipitation [in/yr] for 70 US cities\")\nqqline(precip)"},{"path":"descriptive-stat.html","id":"summary-statistics","chapter":"3 Descriptive Statistics","heading":"3.3.2 Summary Statistics","text":"Sometimes ’s hard tell whether data follow normal distribution just looking graph. Hence, often conduct statistical test aid decision. Common tests areMethods based normal probability plot\nCorrelation Coefficient Normal Probability Plots\nShapiro-Wilk Test\nMethods based normal probability plotCorrelation Coefficient Normal Probability PlotsShapiro-Wilk TestMethods based empirical cumulative distribution function\nAnderson-Darling Test\nKolmogorov-Smirnov Test\nCramer-von Mises Test\nJarque–Bera Test\nMethods based empirical cumulative distribution functionAnderson-Darling TestKolmogorov-Smirnov TestCramer-von Mises TestJarque–Bera Test","code":""},{"path":"descriptive-stat.html","id":"methods-based-on-normal-probability-plot","chapter":"3 Descriptive Statistics","heading":"3.3.2.1 Methods based on normal probability plot","text":"","code":""},{"path":"descriptive-stat.html","id":"correlation-coefficient-with-normal-probability-plots","chapter":"3 Descriptive Statistics","heading":"3.3.2.1.1 Correlation Coefficient with Normal Probability Plots","text":"(Looney Gulledge 1985) (Shapiro Francia 1972) correlation coefficient \\(y_{()}\\) \\(m_i^*\\) given normal probability plot:\\[W^*=\\frac{\\sum_{=1}^{n}(y_{()}-\\bar{y})(m_i^*-0)}{(\\sum_{=1}^{n}(y_{()}-\\bar{y})^2\\sum_{=1}^{n}(m_i^*-0)^2)^.5}\\]\\(\\bar{m^*}=0\\)Pearson product moment formula correlation:\\[\\hat{p}=\\frac{\\sum_{-1}^{n}(y_i-\\bar{y})(x_i-\\bar{x})}{(\\sum_{=1}^{n}(y_{}-\\bar{y})^2\\sum_{=1}^{n}(x_i-\\bar{x})^2)^.5}\\]correlation 1, plot exactly linear normality assumed.closer correlation zero, confident reject normalityInference W* needs based special tables (Looney Gulledge 1985)","code":"\nlibrary(\"EnvStats\")## \n## Attaching package: 'EnvStats'## The following object is masked from 'package:car':\n## \n##     qqPlot## The following objects are masked from 'package:e1071':\n## \n##     kurtosis, skewness## The following objects are masked from 'package:stats':\n## \n##     predict, predict.lm## The following object is masked from 'package:base':\n## \n##     print.default\ngofTest(data,test=\"ppcc\")$p.value #Probability Plot Correlation Coefficient ## [1] 0.1334752"},{"path":"descriptive-stat.html","id":"shapiro-wilk-test","chapter":"3 Descriptive Statistics","heading":"3.3.2.1.2 Shapiro-Wilk Test","text":"(Shapiro Wilk 1965)\\[W=(\\frac{\\sum_{=1}^{n}a_i(y_{()}-\\bar{y})(m_i^*-0)}{(\\sum_{=1}^{n}a_i^2(y_{()}-\\bar{y})^2\\sum_{=1}^{n}(m_i^*-0)^2)^.5})^2\\]\\(a_1,..,a_n\\) weights computed covariance matrix order statistics.Researchers typically use test assess normality. (n < 2000) normality, W close 1, just like \\(W^*\\). Notice difference W W* “weights.”","code":"\ngofTest(data,test=\"sw\")$p.value #Shapiro-Wilk is the default.## [1] 0.2959665"},{"path":"descriptive-stat.html","id":"methods-based-on-empirical-cumulative-distribution-function","chapter":"3 Descriptive Statistics","heading":"3.3.2.2 Methods based on empirical cumulative distribution function","text":"formula empirical cumulative distribution function (CDF) :\\(F_n(t)\\) = estimate probability observation \\(\\le\\) t = (number observation \\(\\le\\) t)/nThis method requires large sample sizes. However, can apply distributions normal (Gaussian) one.","code":"\n# Empirical CDF hand-code\nplot.ecdf(data,verticals = T, do.points=F)"},{"path":"descriptive-stat.html","id":"anderson-darling-test","chapter":"3 Descriptive Statistics","heading":"3.3.2.2.1 Anderson-Darling Test","text":"(Anderson Darling 1952)Anderson-Darling statistic:\\[^2=\\int_{-\\infty}^{\\infty}(F_n(t)=F(t))^2\\frac{dF(t)}{F(t)(1-F(t))}\\]weight average squared deviations (weights small large values t )normal distribution,\\(^2 = - (\\sum_{=1}^{n}(2i-1)(ln(p_i) +ln(1-p_{n+1-}))/n-n\\)\\(p_i=\\Phi(\\frac{y_{()}-\\bar{y}}{s})\\), probability standard normal variable less \\(\\frac{y_{()}-\\bar{y}}{s}\\)Reject normal assumption \\(^2\\) largeReject normal assumption \\(^2\\) largeEvaluate null hypothesis observations randomly selected normal population based critical value provided (Marsaglia Marsaglia 2004) (Stephens 1974)Evaluate null hypothesis observations randomly selected normal population based critical value provided (Marsaglia Marsaglia 2004) (Stephens 1974)test can applied distributions:\nExponential\nLogistic\nGumbel\nExtreme-value\nWeibull: log(Weibull) = Gumbel\nGamma\nLogistic\nCauchy\nvon Mises\nLog-normal (two-parameter)\ntest can applied distributions:ExponentialLogisticGumbelExtreme-valueWeibull: log(Weibull) = GumbelGammaLogisticCauchyvon MisesLog-normal (two-parameter)Consult (Stephens 1974) detailed transformation critical values.","code":"\ngofTest(data,test=\"ad\")$p.value #Anderson-Darling## [1] 0.2855979"},{"path":"descriptive-stat.html","id":"kolmogorov-smirnov-test","chapter":"3 Descriptive Statistics","heading":"3.3.2.2.2 Kolmogorov-Smirnov Test","text":"Based largest absolute difference empirical expected cumulative distributionAnother deviation K-S test Kuiper’s test","code":"\ngofTest(data,test=\"ks\")$p.value #Komogorov-Smirnov ## Warning in ksGofTest(x = c(2.69123191738218, -0.125406276131149, -1.3438246693365, : The standard Kolmogorov-Smirnov test is very conservative (Type I error smaller than assumed; high Type II error) for testing departures from the Normal distribution when you have to estimate the distribution parameters.## [1] 0.7974042"},{"path":"descriptive-stat.html","id":"cramer-von-mises-test","chapter":"3 Descriptive Statistics","heading":"3.3.2.2.3 Cramer-von Mises Test","text":"Based average squared discrepancy empirical distribution given theoretical distribution. discrepancy weighted equally (unlike Anderson-Darling test weights end points heavily)","code":"\ngofTest(data,test=\"cvm\")$p.value #Cramer-von Mises## [1] 0.35756"},{"path":"descriptive-stat.html","id":"jarquebera-test","chapter":"3 Descriptive Statistics","heading":"3.3.2.2.4 Jarque–Bera Test","text":"(Bera Jarque 1981)Based skewness kurtosis test normality.\\(JB = \\frac{n}{6}(S^2+(K-3)^2/4)\\) S sample skewness K sample kurtosis\\(S=\\frac{\\hat{\\mu_3}}{\\hat{\\sigma}^3}=\\frac{\\sum_{=1}^{n}(x_i-\\bar{x})^3/n}{(\\sum_{=1}^{n}(x_i-\\bar{x})^2/n)^\\frac{3}{2}}\\)\\(K=\\frac{\\hat{\\mu_4}}{\\hat{\\sigma}^4}=\\frac{\\sum_{=1}^{n}(x_i-\\bar{x})^4/n}{(\\sum_{=1}^{n}(x_i-\\bar{x})^2/n)^2}\\)recall \\(\\hat{\\sigma^2}\\) estimate second central moment (variance) \\(\\hat{\\mu_3}\\) \\(\\hat{\\mu_4}\\) estimates third fourth central moments.data comes normal distribution, JB statistic asymptotically chi-squared distribution two degrees freedom.null hypothesis joint hypothesis skewness zero excess kurtosis zero.","code":""},{"path":"basic-statistical-inference.html","id":"basic-statistical-inference","chapter":"4 Basic Statistical Inference","heading":"4 Basic Statistical Inference","text":"One Sample InferenceTwo Sample InferenceCategorical Data AnalysisMake inferences (interpretation) true parameter value \\(\\beta\\) based estimator/estimateTest whether underlying assumptions (true population parameters, random variables, model specification) hold true.Testing notConfirm 100% hypothesis trueConfirm 100% hypothesis falseTell interpret estimate value (Economic vs. Practical vs. Statistical Significance)Hypothesis: Translate objective better understanding results terms specifying value (sets values) population parameters /lie.Null hypothesis (\\(H_0\\)): statement population parameter take true need data provide substantial evidence .\nCan either single value (ex: \\(H_0: \\beta=0\\)) set values (ex: \\(H_0: \\beta_1 \\ge 0\\))\ngenerally value like population parameter (subjective)\n\\(H_0: \\beta_1=0\\) means like see non-zero coefficient\n\\(H_0: \\beta_1 \\ge 0\\) means like see negative effect\n\n“Test Significance” refers two-sided test: \\(H_0: \\beta_j=0\\)\nCan either single value (ex: \\(H_0: \\beta=0\\)) set values (ex: \\(H_0: \\beta_1 \\ge 0\\))generally value like population parameter (subjective)\n\\(H_0: \\beta_1=0\\) means like see non-zero coefficient\n\\(H_0: \\beta_1 \\ge 0\\) means like see negative effect\n\\(H_0: \\beta_1=0\\) means like see non-zero coefficient\\(H_0: \\beta_1 \\ge 0\\) means like see negative effect“Test Significance” refers two-sided test: \\(H_0: \\beta_j=0\\)Alternative hypothesis (\\(H_a\\) \\(H_1\\)) (Research Hypothesis): possible values population parameter may null hypothesis hold.Type ErrorError made \\(H_0\\) rejected , fact, \\(H_0\\) true.\nprobability committing Type error \\(\\alpha\\) (known level significance test)Type error (\\(\\alpha\\)): probability rejecting \\(H_0\\) true.Legal analogy: U.S. law, defendant presumed “innocent proven guilty.”\nnull hypothesis person innocent, Type error probability conclude person guilty innocent.Type II ErrorType II error level (\\(\\beta\\)): probability fail reject null hypothesis false.legal analogy, probability fail find person guilty guilty.Error made \\(H_0\\) rejected , fact, \\(H_1\\) true\nprobability committing Type II error \\(\\beta\\) (known power test)Random sample size n: collection n independent random variables taken distribution X, distribution X.Sample mean\\[\n\\bar{X}= (\\sum_{=1}^{n}X_i)/n\n\\]Sample Median\\(\\tilde{x}\\) = middle observation sample observation order smallest largest (vice versa).n odd, \\(\\tilde{x}\\) middle observation,\nn even, \\(\\tilde{x}\\) average two middle observations.Sample variance\n\\[\nS^2 = \\frac{\\sum_{=1}^{n}(X_i = \\bar{X})^2}{n-1}= \\frac{n\\sum_{=1}^{n}X_i^2 -(\\sum_{=1}^{n}X_i)^2}{n(n-1)}\n\\]Sample standard deviation\n\\[\nS = \\sqrt{S^2}\n\\]Sample proportions\n\\[\n\\hat{p} = \\frac{X}{n} = \\frac{\\text{number sample trait}}{\\text{sample size}}\n\\]\\[\n\\widehat{p_1-p_2} = \\hat{p_1}-\\hat{p_2} = \\frac{X_1}{n_1} - \\frac{X_2}{n_2} = \\frac{n_2X_1 = n_1X_2}{n_1n_2}\n\\]EstimatorsPoint Estimator\\(\\hat{\\theta}\\) statistic used approximate population parameter \\(\\theta\\)Point estimate\nnumerical value assumed \\(\\hat{\\theta}\\) evaluated given sampleUnbiased estimator\n\\(E(\\hat{\\theta}) = \\theta\\), \\(\\hat{\\theta}\\) unbiased estimator \\(\\theta\\)\\(\\bar{X}\\) unbiased estimator \\(\\mu\\)\\(S^2\\) unbiased estimator \\(\\sigma^2\\)\\(\\hat{p}\\) unbiased estimator p\\(\\widehat{p_1-P_2}\\) unbiased estimator \\(p_1- p_2\\)\\(\\bar{X_1} - \\bar{X_2}\\) unbiased estimator \\(\\mu_1 - \\mu_2\\)Note: \\(S\\) biased estimator \\(\\sigma\\)Distribution sample meanIf \\(\\bar{X}\\) sample mean based random sample size n drawn normal distribution X mean \\(\\mu\\) standard deviation \\(\\sigma\\), \\(\\bar{X}\\) normally distributed, mean \\(\\mu_{\\bar{X}} = \\mu\\) variance \\(\\sigma_{\\bar{X}}^2 = Var(\\bar{X}) = \\frac{\\sigma^2}{n}\\). standard error mean : \\(\\sigma_{\\bar{X}}= \\frac{\\sigma}{\\sqrt{n}}\\)","code":""},{"path":"basic-statistical-inference.html","id":"one-sample-inference","chapter":"4 Basic Statistical Inference","heading":"4.1 One Sample Inference","text":"\\(Y_i \\sim ..d. N(\\mu, \\sigma^2)\\)..d. standards “independent identically distributed”Hence, following model:\\(Y_i=\\mu +\\epsilon_i\\) \\(\\epsilon_i \\sim^{iid} N(0,\\sigma^2)\\)\\(E(Y_i)=\\mu\\)\\(Var(Y_i)=\\sigma^2\\)\\(\\bar{y} \\sim N(\\mu,\\sigma^2/n)\\)","code":""},{"path":"basic-statistical-inference.html","id":"the-mean","chapter":"4 Basic Statistical Inference","heading":"4.1.1 The Mean","text":"\\(\\sigma^2\\) estimated \\(s^2\\), \\[\n\\frac{\\bar{y}-\\mu}{s/\\sqrt{n}} \\sim t_{n-1}\n\\], \\(100(1-\\alpha) \\%\\) confidence interval \\(\\mu\\) obtained :\\[\n1 - \\alpha = P(-t_{\\alpha/2;n-1} \\le \\frac{\\bar{y}-\\mu}{s/\\sqrt{n}} \\le t_{\\alpha/2;n-1}) \\\\\n= P(\\bar{y} - (t_{\\alpha/2;n-1})s/\\sqrt{n} \\le \\mu \\le \\bar{y} + (t_{\\alpha/2;n-1})s/\\sqrt{n})\n\\]interval \\[\n\\bar{y} \\pm (t_{\\alpha/2;n-1})s/\\sqrt{n}\n\\]\\(s/\\sqrt{n}\\) standard error \\(\\bar{y}\\)experiment repeated many times, \\(100(1-\\alpha) \\%\\) intervals contain \\(\\mu\\)","code":""},{"path":"basic-statistical-inference.html","id":"for-difference-of-means-mu_1-mu_2-independent-samples","chapter":"4 Basic Statistical Inference","heading":"4.1.1.1 For Difference of Means (\\(\\mu_1-\\mu_2\\)), Independent Samples","text":"","code":""},{"path":"basic-statistical-inference.html","id":"for-difference-of-means-mu_1---mu_2-paired-samples-d-x-y","chapter":"4 Basic Statistical Inference","heading":"4.1.1.2 For Difference of Means (\\(\\mu_1 - \\mu_2\\)), Paired Samples (D = X-Y)","text":"\\(100(1-\\alpha)%\\) Confidence Interval\\[\n\\bar{D} \\pm t_{\\alpha/2}\\frac{s_d}{\\sqrt{n}}\n\\]Hypothesis Testing Test Statistic\\[\nt = \\frac{\\bar{D}-D_0}{s_d / \\sqrt{n}}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"difference-of-two-proprotions","chapter":"4 Basic Statistical Inference","heading":"4.1.1.3 Difference of Two Proprotions","text":"Mean\\[\n\\hat{p_1}-\\hat{p_2}\n\\]Variance\n\\[\n\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}\n\\]\\(100(1-\\alpha)%\\) Confidence Interval\\[\n\\hat{p_1}-\\hat{p_2} + z_{\\alpha/2}\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}\n\\]Sample Sizes, Confidence \\(\\alpha\\), Error d\n(Prior Estimate fo \\(\\hat{p_1},\\hat{p_2}\\))\\[\nn \\approx \\frac{z_{\\alpha/2}^2[p_1(1-p_1)+p_2(1-p_2)]}{d^2}\n\\](Prior Estimates \\(\\hat{p}\\))\\[\nn \\approx \\frac{z_{\\alpha/2}^2}{2d^2}\n\\]Hypothesis Testing - Test StatisticsNull Value \\((p_1 - p_2) \\neq 0\\)\\[\nz = \\frac{(\\hat{p_1} - \\hat{p_2})-(p_1 - p_2)_0}{\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}}\n\\]Null Value \\((p_1 - p_2)_0 = 0\\)\\[\nz = \\frac{\\hat{p_1} - \\hat{p_2}}{\\sqrt{\\hat{p}(1-\\hat{p})(\\frac{1}{n_1}+\\frac{1}{n_2})}}\n\\]\\[\n\\hat{p}= \\frac{x_1 + x_2}{n_1 + n_2} = \\frac{n_1 \\hat{p_1} + n_2 \\hat{p_2}}{n_1 + n_2}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"single-variance","chapter":"4 Basic Statistical Inference","heading":"4.1.2 Single Variance","text":"\\[\n1 - \\alpha = P( \\chi_{1-\\alpha/2;n-1}^2) \\le (n-1)s^2/\\sigma^2 \\le \\chi_{\\alpha/2;n-1}^2) \\\\\n= P(\\frac{(n-1)s^2}{\\chi_{\\alpha/2}^2} \\le \\sigma^2 \\le \\frac{(n-1)s^2}{\\chi_{1-\\alpha/2}^2})\n\\]\\(100(1-\\alpha) \\%\\) confidence interval \\(\\sigma^2\\) :\\[\n(\\frac{(n-1)s^2}{\\chi_{\\alpha/2;n-1}^2},\\frac{(n-1)s^2}{\\chi_{1-\\alpha/2;n-1}^2})\n\\]\nConfidence limits \\(\\sigma^2\\) obtained computing positive square roots limitsEquivalently,\\(100(1-\\alpha)%\\) Confidence Interval\\[\nL_1 = \\frac{(n-1)s^2}{\\chi^2_{\\alpha/2}} \\\\\nL_1 = \\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2}}\n\\]\nHypothesis Testing Test Statistic\\[\n\\chi^2 = \\frac{(n-1)s^2}{\\sigma^2_0}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"single-proportion-p","chapter":"4 Basic Statistical Inference","heading":"4.1.3 Single Proportion (p)","text":"","code":""},{"path":"basic-statistical-inference.html","id":"power","chapter":"4 Basic Statistical Inference","heading":"4.1.4 Power","text":"Formally, power (test mean) given :\\[\n\\pi(\\mu) = 1 - \\beta = P(\\text{test rejects } H_0|\\mu)\n\\]\nevaluate power, one needs know distribution test statistic null hypothesis false.1-sided z-test \\(H_0: \\mu \\le \\mu_0 \\\\ H_A: \\mu >0\\)power :\\[\n\\begin{aligned}\n\\pi(\\mu) &= P(\\bar{y} > \\mu_0 + z_{\\alpha} \\sigma/\\sqrt{n}|\\mu) \\\\\n&= P(Z = \\frac{\\bar{y} - \\mu}{\\sigma / \\sqrt{n}} > z_{\\alpha} + \\frac{\\mu_0 - \\mu}{\\sigma/ \\sqrt{n}}|\\mu) \\\\\n&= 1 - \\Phi(z_{\\alpha} + \\frac{(\\mu_0 - \\mu)\\sqrt{n}}{\\sigma}) \\\\\n&= \\Phi(-z_{\\alpha}+\\frac{(\\mu -\\mu_0)\\sqrt{n}}{\\sigma})\n\\end{aligned}\n\\]\\(1-\\Phi(x) = \\Phi(-x)\\) since normal pdf symmetricPower correlated difference \\(\\mu - \\mu_0\\), sample size n, variance \\(\\sigma^2\\), \\(\\alpha\\)-level test (\\(z_{\\alpha}\\))\nEquivalently, power can increased making \\(\\alpha\\) large, \\(\\sigma^2\\) smaller, n larger.2-sided z-test :\\[\n\\pi(\\mu) = \\Phi(-z_{\\alpha/2} + \\frac{(\\mu_0 - \\mu)\\sqrt{n}}{\\sigma}) + \\Phi(-z_{\\alpha/2}+\\frac{(\\mu - \\mu_0)\\sqrt{n}}{\\sigma})\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"sample-size","chapter":"4 Basic Statistical Inference","heading":"4.1.5 Sample Size","text":"","code":""},{"path":"basic-statistical-inference.html","id":"sided-z-test","chapter":"4 Basic Statistical Inference","heading":"4.1.5.1 1-sided Z-test","text":"Example: show mean response \\(\\mu\\) treatment higher mean response \\(\\mu_0\\) without treatment (show treatment effect \\(\\delta = \\mu -\\mu_0\\) large)power increasing function \\(\\mu - \\mu_0\\), necessary find n makes power equal \\(1- \\beta\\) \\(\\mu = \\mu_0 + \\delta\\)Hence, \\[\n\\pi(\\mu_0 + \\delta) = \\Phi(-z_{\\alpha} + \\frac{\\delta \\sqrt{n}}{\\sigma}) = 1 - \\beta\n\\]Since \\(\\Phi (z_{\\beta})= 1-\\beta\\), \\[\n-z_{\\alpha} + \\frac{\\delta \\sqrt{n}}{\\sigma} = z_{\\beta}\n\\]n \\[\nn = (\\frac{(z_{\\alpha}+z_{\\beta})\\sigma}{\\delta})^2\n\\], need larger samples, whenthe sample variability large (\\(\\sigma\\) large)\\(\\alpha\\) small (\\(z_{\\alpha}\\) large)power \\(1-\\beta\\) large (\\(z_{\\beta}\\) large)magnitude effect smaller (\\(\\delta\\) small)Since don’t know \\(\\delta\\) \\(\\sigma\\). can base \\(\\sigma\\) previous studies, pilot studies. , obtain estimate \\(\\sigma\\) anticipating range observation (without outliers). divide range 4 use resulting number approximate estimate \\(\\sigma\\). normal (distribution) data, reasonable.","code":""},{"path":"basic-statistical-inference.html","id":"sided-z-test-1","chapter":"4 Basic Statistical Inference","heading":"4.1.5.2 2-sided Z-test","text":"want know min n, required guarantee \\(1-\\beta\\) power treatment effect \\(\\delta = |\\mu - \\mu_0|\\) least greater 0. Since power function 2-sided increasing symmetric \\(|\\mu - \\mu_0|\\), need find n makes power equal \\(1-\\beta\\) \\(\\mu = \\mu_0 + \\delta\\)\\[\nn = (\\frac{(z_{\\alpha/2} + z_{\\beta}) \\sigma}{\\delta})^2\n\\]also use confidence interval apporach. reuqire \\(\\alpha\\)-level two-cided CI \\(\\mu\\) \\[\n\\bar{y} \\pm D\n\\]\n\\(D = z_{\\alpha/2}\\sigma/\\sqrt{n}\\) gives\\[\nn = (\\frac{z_{\\alpha/2}\\sigma}{D})^2\n\\]\n(round nearest integer)\\[\nH_0: \\mu \\ge 30 \\\\\nH_a: \\mu < 30\n\\]","code":"\ndata = rnorm(100)\nt.test(data, conf.level=0.95)## \n##  One Sample t-test\n## \n## data:  data\n## t = 0.067865, df = 99, p-value = 0.946\n## alternative hypothesis: true mean is not equal to 0\n## 95 percent confidence interval:\n##  -0.1688418  0.1808004\n## sample estimates:\n##  mean of x \n## 0.00597927\nt.test(data, mu=30,alternative=\"less\")## \n##  One Sample t-test\n## \n## data:  data\n## t = -340.43, df = 99, p-value < 2.2e-16\n## alternative hypothesis: true mean is less than 30\n## 95 percent confidence interval:\n##       -Inf 0.1522694\n## sample estimates:\n##  mean of x \n## 0.00597927"},{"path":"basic-statistical-inference.html","id":"note","chapter":"4 Basic Statistical Inference","heading":"4.1.6 Note","text":"t-tests, sample power easy z-test.\\[\n\\pi(\\mu) = P(\\frac{\\bar{y}-\\mu_0}{s/\\sqrt{n}}> t_{n-1;\\alpha}|\\mu)\n\\]\\(\\mu > \\mu_0\\) (.e., \\(\\mu - \\mu_0 = \\delta\\)), random variable \\((\\bar{y} - \\mu_0)/(s/\\sqrt{n})\\) Student’s t distribution, rather distributed non-central t-distribution non-centrality parameter \\(\\delta \\sqrt{n}/\\sigma\\) d.f. \\(n-1\\)power increasing function non-centrality parameter (note, \\(\\delta = 0\\) distribution usual Student’s t-distribution).evaluate power, one must consider numerical procedure use special chartsApproximate Sample Size Adjustment t-test. use adjustment z-test determination sample size.Let \\(v = n-1\\), n sample size derived based z-test power. 2-sided t-test sample size (apporximate) given:\\[\nn^* = \\frac{(t_{v;\\alpha/2}+t_{v;\\beta})^2 \\sigma^2}{\\delta^2}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"one-sample-non-parametric-methods","chapter":"4 Basic Statistical Inference","heading":"4.1.7 One-sample Non-parametric Methods","text":"","code":"\nlecture.data=c(0.76, 0.82, 0.80, 0.79, 1.06, 0.83, -0.43, -0.34, 3.34, 2.33)"},{"path":"basic-statistical-inference.html","id":"sign-test","chapter":"4 Basic Statistical Inference","heading":"4.1.7.1 Sign Test","text":"want test \\(H_0: \\mu_{(0.5)} = 0; H_a: \\mu_{(0.5)} >0\\) \\(\\mu_{(0.5)}\\) population median. canCount number observation (\\(y_i\\)’s) exceed 0. Denote number \\(s_+\\), called number plus signs. Let \\(s_- = n - s_+\\), number minus signs.Reject \\(H_0\\) \\(s_+\\) large equivalently, \\(s_-\\) small.determine large \\(s_+\\) must reject \\(H_0\\) given significance level, need know distribution corresponding random variable \\(S_+\\) null hypothesis, binomial p = 1/2,w hen null true.work null distribution using binomial formula, \\(\\alpha\\)-level test rejects \\(H_0\\) \\(s_+ \\ge b_{n,\\alpha}\\), \\(b_{n,\\alpha}\\) upper \\(\\alpha\\) critical point \\(Bin(n,1/2)\\) distribution. \\(S_+\\) \\(S_-\\) distribution (\\(S = S_+ = S_-\\)).\\[\n\\text{p-value} = P(S \\ge s_+) = \\sum_{= s_+}^{n} {{n}\\choose{}} (\\frac{1}{2})^n\n\\]\nequivalently,\\[\nP(S \\le s_-) = \\sum_{=0}^{s_-}{{n}\\choose{}} (\\frac{1}{2})^2\n\\]\nlarge sample sizes, use normal approximation binomial, case reject \\(H_0\\) \\[\ns_+ \\ge n/2 + 1/2 + z_{\\alpha}\\sqrt{n/4}\n\\]2-sided test, use tests statistic \\(s_{max} = max(s_+,s_-)\\) \\(s_{min} = min(s_+, s_-)\\). \\(\\alpha\\)-level test rejects \\(H_0\\) p-value \\(\\le \\alpha\\), p-value computed :\\[\np-value = 2 \\sum_{=s_{max}}^{n} {{n}\\choose{}} (\\frac{1}{2})^n = s \\sum_{=0}^{s_{min}} {{n}\\choose{}} (\\frac{1}{2})^n\n\\]\nEquivalently, rejecting \\(H_0\\) \\(s_{max} \\ge b_{n,\\alpha/2}\\)large sample normal approximation can used, \\[\nz = \\frac{s_{max}- n/2 -1/2}{\\sqrt{n/4}}\n\\]\nreject \\(H_0\\) \\(\\alpha\\) \\(z \\ge z_{\\alpha/2}\\)However, treatment 0 problematic test.Solution 1: randomly assign 0 positive negative (2 researchers might get different results).Solution 2: count 0 contribution 1/2 toward \\(s_+\\) \\(s_-\\) (apply binomial distribution)Solution 3: ignore 0 (reduces power test due decreased sample size).","code":"\nbinom.test(sum(lecture.data > 0), length(lecture.data)) # alternative = \"greater\" or alternative = \"less\"## \n##  Exact binomial test\n## \n## data:  sum(lecture.data > 0) and length(lecture.data)\n## number of successes = 8, number of trials = 10, p-value = 0.1094\n## alternative hypothesis: true probability of success is not equal to 0.5\n## 95 percent confidence interval:\n##  0.4439045 0.9747893\n## sample estimates:\n## probability of success \n##                    0.8"},{"path":"basic-statistical-inference.html","id":"wilcoxon-signed-rank-test","chapter":"4 Basic Statistical Inference","heading":"4.1.7.2 Wilcoxon Signed Rank Test","text":"Since Sign Test consider magnitude observation 0, Wilcoxon Signed Rank Test improves taking account ordered magnitudes observation, impose requirement symmetric test (Sign Test )\\[\nH_0: \\mu_{0.5} = 0 \\\\\nH_a: \\mu_{0.5} > 0\n\\]\n(assume ties observations)signed rank test procedure:rank order observation \\(y_i\\) terms absolute values. Let \\(r_i\\) rank \\(y_i\\) ordering. Since assume ties, ranks \\(r_i\\) uniquely determined permutation integers 1,2,…,n.Calculate \\(w_+\\), sum ranks positive values, \\(w_-\\), sum ranks negative values. Note \\(w_+ + w_- = r_1 + r_2 + ... = 1 + 2 + ... + n = n(n+1)/2\\)Reject \\(H_0\\) \\(w_+\\) large (\\(w_-\\) small)know large small regard \\(w_+\\) \\(w_-\\), need distribution \\(W_+\\) \\(W_-\\) null true.Since null distributions identical symmetric, p-value \\(P(W \\ge w_+) = P(W \\le w_-)\\)\\(\\alpha\\)-level test rejects null p-value \\(\\le \\alpha\\), \\(w_+ \\ge w_{n,\\alpha}\\), \\(w_{n,\\alpha}\\) upper \\(\\alpha\\) critical point null distribution W.distribution W special table. large n, distribution W approximately normal.\\[\nz = \\frac{w_+ - n(n+1) /4 -1/2}{\\sqrt{n(n+1)(2n+1)/24}}\n\\]test rejcets \\(H_0\\) level \\(\\alpha\\) \\[\nw_+ \\ge n(n+1)/4 +1/2 + z_{\\alpha}\\sqrt{n(n+1)(2n+1)/24} \\approx w_{n,\\alpha}\n\\]2-sided test, use \\(w_{max}=max(w_+,w_-)\\) \\(w_{min}=min(w_+,w_-)\\), p-value given :\\[\np-value = 2P(W \\ge w_{max}) = 2P(W \\le w_{min})\n\\]\nSign Test,ignore 0. cases \\(|y_i|\\)’s may tied rank, simply assign tied ranks average rank (“midrank”).Example, \\(y_1 = -1\\), \\(y_3 = 3\\) \\(y_3 = -3\\), \\(y_4 =5\\), \\(r_1 = 1\\), \\(r_2 = r_3=(2+3)/2 = 2.5\\), \\(r_4 = 4\\)","code":"\nwilcox.test(lecture.data) #does not use normal approximation (using the underlying W distribution)## \n##  Wilcoxon signed rank exact test\n## \n## data:  lecture.data\n## V = 52, p-value = 0.009766\n## alternative hypothesis: true location is not equal to 0\nwilcox.test(lecture.data,exact=F) #uses normal approximation## \n##  Wilcoxon signed rank test with continuity correction\n## \n## data:  lecture.data\n## V = 52, p-value = 0.01443\n## alternative hypothesis: true location is not equal to 0"},{"path":"basic-statistical-inference.html","id":"two-sample-inference","chapter":"4 Basic Statistical Inference","heading":"4.2 Two Sample Inference","text":"","code":""},{"path":"basic-statistical-inference.html","id":"means","chapter":"4 Basic Statistical Inference","heading":"4.2.1 Means","text":"Suppose 2 sets observations,\\(y_1,..., y_{n_y}\\)\\(x_1,...,x_{n_x}\\)random samples two independent populations means \\(\\mu_y\\) \\(\\mu_x\\) variances \\(\\sigma^2_y\\),\\(\\sigma^2_x\\).\ngoal compare \\(\\mu_x\\) \\(\\mu_y\\) \\(\\sigma^2_y = \\sigma^2_x\\)","code":""},{"path":"basic-statistical-inference.html","id":"large-sample-tests","chapter":"4 Basic Statistical Inference","heading":"4.2.1.1 Large Sample Tests","text":"Assume \\(n_y\\) \\(n_x\\) large (\\(\\ge 30\\)). ,\\[\nE(\\bar{y} - \\bar{x}) = \\mu_y - \\mu_x \\\\\nVar(\\bar{y} - \\bar{x}) = \\sigma^2_y /n_y + \\sigma^2_x/n_x\n\\],\\[\nZ = \\frac{\\bar{y}-\\bar{x} - (\\mu_y - \\mu_x)}{\\sqrt{\\sigma^2_y /n_y + \\sigma^2_x/n_x}} \\sim N(0,1)\n\\]\n(according Central Limit Theorem). large samples, can replace variances unbiased estimators (\\(s^2_y,s^2_x\\)), get large sample distribution.approximate \\(100(1-\\alpha) \\%\\) CI \\(\\mu_y - \\mu_x\\) given :\\[\n\\bar{y} - \\bar{x} \\pm z_{\\alpha/2}\\sqrt{s^2_y/n_y + s^2_x/n_x}\n\\]\\[\nH_0: \\mu_y - \\mu_x = \\delta_0 \\\\\nH_A: \\mu_y - \\mu_x \\neq \\delta_0\n\\]\\(\\alpha\\)-level statistic:\\[\nz = \\frac{\\bar{y}-\\bar{x} - \\delta_0}{\\sqrt{s^2_y /n_y + s^2_x/n_x}}\n\\]reject \\(H_0\\) \\(|z| > z_{\\alpha/2}\\)\\(\\delta = )\\), means testing whether two means equal.","code":""},{"path":"basic-statistical-inference.html","id":"small-sample-tests","chapter":"4 Basic Statistical Inference","heading":"4.2.1.2 Small Sample Tests","text":"two samples normal distribution, iid \\(N(\\mu_y,\\sigma^2_y)\\) iid \\(N(\\mu_x,\\sigma^2_x)\\) two samples independent, can inference based t-distributionThen 2 casesEqual VarianceUnequal Variance","code":""},{"path":"basic-statistical-inference.html","id":"equal-variance","chapter":"4 Basic Statistical Inference","heading":"4.2.1.2.1 Equal variance","text":"Assumptionsiid: \\(var(\\bar{y}) = \\sigma^2_y / n_y ; var(\\bar{x}) = \\sigma^2_x / n_x\\)Independence samples: observation one sample can influence observation sample, \\[\n\\begin{aligned}\nvar(\\bar{y} - \\bar{x}) &= var(\\bar{y}) + var{\\bar{x}} - 2cov(\\bar{y},\\bar{x}) \\\\\n&= var(\\bar{y}) + var{\\bar{x}} \\\\\n&= \\sigma^2_y / n_y + \\sigma^2_x / n_x \n\\end{aligned}\n\\]Normality: Justifies use t-distributionLet \\(\\sigma^2 = \\sigma^2_y = \\sigma^2_x\\). , \\(s^2_y\\) \\(s^2_x\\) unbiased estimators \\(\\sigma^2\\). can pool .pooled variance estimate \n\\[\ns^2 = \\frac{(n_y - 1)s^2_y + (n_x - 1)s^2_x}{(n_y-1)+(n_x-1)}\n\\]\n\\(n_y + n_x -2\\) df.test statistic\\[\nT = \\frac{\\bar{y}- \\bar{x} -(\\mu_y - \\mu_x)}{s\\sqrt{1/n_y + 1/n_x}} \\sim t_{n_y + n_x -2}\n\\]\\(100(1 - \\alpha) \\%\\) CI \\(\\mu_y - \\mu_x\\) \\[\n\\bar{y} - \\bar{x} \\pm (t_{n_y + n_x -2})s\\sqrt{1/n_y + 1/n_x}\n\\]Hypothesis testing:\\[\nH_0: \\mu_y - \\mu_x = \\delta_0 \\\\\nH_1: \\mu_y - \\mu_x \\neq \\delta_0\n\\]reject \\(H_0\\) \\(|t| > t_{n_y + n_x -2;\\alpha/2}\\)","code":""},{"path":"basic-statistical-inference.html","id":"unequal-variance","chapter":"4 Basic Statistical Inference","heading":"4.2.1.2.2 Unequal Variance","text":"AssumptionsTwo samples independent\n1. Scatter plots\n2. Correlation coefficient (normal)Independence observation sample\n1. Test serial correlationFor sample, homogeneity variance\n1. Scatter plots\n2. Formal testsNormalityEquality variances (homogeneity variance samples)\n1. F-test\n2. Barlett test\n3. [Modified Levene Test]compare 2 normal \\(\\sigma^2_y \\neq \\sigma^2_x\\), use test statistic:\\[\nT = \\frac{\\bar{y}- \\bar{x} -(\\mu_y - \\mu_x)}{\\sqrt{s^2_y/n_y + s^2_x/n_x}} \n\\]\ncase, T follow t-distribution (distribution depends ratio unknown variances \\(\\sigma^2_y,\\sigma^2_x\\)). case small sizes, can can approximate tests using Welch-Satterthwaite method (Satterthwaite 1946). assume T can approximated t-distribution, adjust degrees freedom.Let \\(w_y = s^2_y /n_y\\) \\(w_x = s^2_x /n_x\\) (w’s square respective standard errors)\n, degrees freedom \\[\nv = \\frac{(w_y + w_x)^2}{w^2_y / (n_y-1) + w^2_x / (n_x-1)}\n\\]Since v usually fractional, truncate nearest integer.\\(100 (1-\\alpha) \\%\\) CI \\(\\mu_y - \\mu_x\\) \\[\n\\bar{y} - \\bar{x} \\pm t_{v,\\alpha/2} \\sqrt{s^2_y/n_y + s^2_x /n_x}\n\\]Reject \\(H_0\\) \\(|t| > t_{v,\\alpha/2}\\), \\[\nt = \\frac{\\bar{y} - \\bar{x}-\\delta_0}{\\sqrt{s^2_y/n_y + s^2_x /n_x}}\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"variances","chapter":"4 Basic Statistical Inference","heading":"4.2.2 Variances","text":"\\[\nF_{ndf,ddf}= \\frac{s^2_1}{s^2_2}\n\\]\\(s^2_1>s^2_2, ndf = n_1-1,ddf = n_2-1\\)","code":""},{"path":"basic-statistical-inference.html","id":"f-test","chapter":"4 Basic Statistical Inference","heading":"4.2.2.1 F-test","text":"Test\\[\nH_0: \\sigma^2_y = \\sigma^2_x \\\\\nH_a: \\sigma^2_y \\neq \\sigma^2_x\n\\]Consider test statistic,\\[\nF= \\frac{s^2_y}{s^2_x}\n\\]Reject \\(H_0\\) \\(F>f_{n_y -1,n_x -1,\\alpha/2}\\) \\(F<f_{n_y -1,n_x -1,1-\\alpha/2}\\)\\(F>f_{n_y -1,n_x -1,\\alpha/2}\\) \\(F<f_{n_y -1,n_x -1,1-\\alpha/2}\\) upper lower \\(\\alpha/2\\) critical points F-distribution, \\(n_y-1\\) \\(n_x-1\\) degrees freedom.NoteThis test depends heavily assumption Normality.particular, give many significant results observations come long-tailed distributions (.e., positive kurtosis).find support normality, can use nonparametric tests [Modified Levene Test]","code":"\ndata(iris)\nirisVe=iris$Petal.Width[iris$Species==\"versicolor\"] \nirisVi=iris$Petal.Width[iris$Species==\"virginica\"]\n\nvar.test(irisVe,irisVi)## \n##  F test to compare two variances\n## \n## data:  irisVe and irisVi\n## F = 0.51842, num df = 49, denom df = 49, p-value = 0.02335\n## alternative hypothesis: true ratio of variances is not equal to 1\n## 95 percent confidence interval:\n##  0.2941935 0.9135614\n## sample estimates:\n## ratio of variances \n##          0.5184243"},{"path":"basic-statistical-inference.html","id":"modified-levene-test-brown-forsythe-test","chapter":"4 Basic Statistical Inference","heading":"4.2.2.2 Modified Levene Test (Brown-Forsythe Test)","text":"considers averages absolute deviations rather squared deviations. Hence, less sensitive long-tailed distributions.test still good normal dataFor sample, consider absolute deviation observation form median:\\[\nd_{y,} = |y_i - y_{.5}| \\\\\nd_{x,} = |x_i - x_{.5}|\n\\]\n,\\[\nt_L^* = \\frac{\\bar{d}_y-\\bar{d}_x}{s \\sqrt{1/n_y + 1/n_x}}\n\\]pooled variance \\(s^2\\) given :\\[\ns^2 = \\frac{\\sum_i^{n_y}(d_{y,}-\\bar{d}_y)^2 + \\sum_j^{n_x}(d_{x,}-\\bar{d}_x)^2}{n_y + n_x -2}\n\\]error terms constant variance \\(n_y\\) \\(n_x\\) extremely small, \\(t_L^* \\sim t_{n_x + n_y -2}\\)reject null hypothesis \\(|t_L^*| > t_{n_y + n_x -2;\\alpha/2}\\)just two-sample t-test applied absolute deviations.","code":"\ndVe=abs(irisVe-median(irisVe)) \ndVi=abs(irisVi-median(irisVi)) \nt.test(dVe,dVi,var.equal=T)## \n##  Two Sample t-test\n## \n## data:  dVe and dVi\n## t = -2.5584, df = 98, p-value = 0.01205\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -0.12784786 -0.01615214\n## sample estimates:\n## mean of x mean of y \n##     0.154     0.226\n# small samples t-test  \nt.test(irisVe,irisVi,var.equal=F)## \n##  Welch Two Sample t-test\n## \n## data:  irisVe and irisVi\n## t = -14.625, df = 89.043, p-value < 2.2e-16\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##  -0.7951002 -0.6048998\n## sample estimates:\n## mean of x mean of y \n##     1.326     2.026"},{"path":"basic-statistical-inference.html","id":"power-1","chapter":"4 Basic Statistical Inference","heading":"4.2.3 Power","text":"Consider \\(\\sigma^2_y = \\sigma^2_x = \\sigma^2\\)\nassumption equal variances, take size samples groups (\\(n_y = n_x = n\\))1-sided testing,\\[\nH_0: \\mu_y - \\mu_x \\le 0 \\\\\nH_a: \\mu_y - \\mu_x > 0\n\\]\\(\\alpha\\)-level z-test rejects \\(H_0\\) \\[\nz = \\frac{\\bar{y} - \\bar{x}}{\\sigma \\sqrt{2/n}} > z_{\\alpha}\n\\]\\[\n\\pi(\\mu_y - \\mu_x) = \\Phi(-z_{\\alpha} + \\frac{\\mu_y -\\mu_x}{\\sigma}\\sqrt{n/2})\n\\]need sample size n gie least \\(1-\\beta\\) power \\(\\mu_y - \\mu_x = \\delta\\), \\(\\delta\\) smallest difference want see.Power given :\\[\n\\Phi(-z_{\\alpha} + \\frac{\\delta}{\\sigma}\\sqrt{n/2}) = 1 - \\beta\n\\]","code":""},{"path":"basic-statistical-inference.html","id":"sample-size-1","chapter":"4 Basic Statistical Inference","heading":"4.2.4 Sample Size","text":", sample size \\[\nn = 2(\\frac{\\sigma (z_{\\alpha} + z_{\\beta}}{\\delta})^2\n\\]2-sided test, replace \\(z_{\\alpha}\\) \\(z_{\\alpha/2}\\).\none-sample case, perform exact 2-sample t-test sample size calculation, must use non-central t-distribution.correction gives approximate t-test sample size can obtained using z-test n value formula:\\[\nn^* = 2(\\frac{\\sigma (t_{2n-2;\\alpha} + t_{2n-2;\\beta})}{\\delta})^2\n\\]use \\(alpha/2\\) two-sided test","code":""},{"path":"basic-statistical-inference.html","id":"matched-pair-designs","chapter":"4 Basic Statistical Inference","heading":"4.2.5 Matched Pair Designs","text":"two treatmentswe assume \\(y_i \\sim^{iid} N(\\mu_y, \\sigma^2_y)\\) \\(x_i \\sim^{iid} N(\\mu_x,\\sigma^2_x)\\), since \\(y_i\\) \\(x_i\\) measured subject, correlated.Let\\[\n\\mu_D = E(y_i - x_i) = \\mu_y -\\mu_x \\\\\n\\sigma^2_D = var(y_i - x_i) = Var(y_i) + Var(x_i) -2cov(y_i,x_i)\n\\]matching induces positive correlation, variance difference measurements reduced compared independent case. point Matched Pair Designs. Although covariance can negative, giving larger variance difference independent sample case, usually covariance positive. means \\(y_i\\) \\(x_i\\) large many subjects, others, measurement small. (still assume various subjects respond independently , necessary iid assumption within groups).Let \\(d_i = y_i - x_i\\), \\(\\bar{d} = \\bar{y}-\\bar{x}\\) sample mean \\(d_i\\)\\(s_d^2=\\frac{1}{n-1}\\sum_{=1}^n (d_i - \\bar{d})^2\\) sample variance differenceOnce data converted differences, back One Sample Inference can use tests CIs.","code":""},{"path":"basic-statistical-inference.html","id":"nonparametric-tests-for-two-samples","chapter":"4 Basic Statistical Inference","heading":"4.2.6 Nonparametric Tests for Two Samples","text":"Matched Pair Designs, can use One-sample Non-parametric Methods.Assume Y X random variables CDF \\(F_y\\) \\(F_x\\). , Y stochastically larger X real number u, \\(P(Y > u) \\ge P(X > u)\\).Equivalently, \\(P(Y \\le u) \\le P(X \\le u)\\), \\(F_Y(u) \\le F_X(u)\\), thing \\(F_Y < F_X\\)two distributions identical, except one shifted relative , distribution can indexed location parameter, say \\(\\theta_y\\) \\(\\theta_x\\). case, \\(Y>X\\) \\(\\theta_y > \\theta_x\\)Consider hypotheses,\\[\nH_0: F_Y = F_X \\\\\nH_a: F_Y < F_X\n\\]\nalternative upper one-sided alternative.can also consider lower one-sided alternative\\[\nH_a: F_Y > F_X \\text{ } \\\\\nH_a: F_Y < F_X \\text{ } F_Y > F_X\n\\]case, don’t use \\(H_a: F_Y \\neq F_X\\) allows arbitrary differences distributions, without requiring one stochastically larger .distributions differ terms location parameters, can focus hypothesis tests parameters (e.g., \\(H_0: \\theta_y = \\theta_x\\) vs. \\(\\theta_y > \\theta_x\\))2 equivalent nonparametric tests consider hypothesis mentioned aboveWilcoxon rank testMann-Whitney U test","code":""},{"path":"basic-statistical-inference.html","id":"wilcoxon-rank-test","chapter":"4 Basic Statistical Inference","heading":"4.2.6.1 Wilcoxon rank test","text":"Combine \\(n= n_y + n_x\\) observations rank ascending order.Sum ranks y’s x’s separately. Let \\(w_y\\) \\(w_x\\) sums. (\\(w_y + w_x = 1 + 2 + ... + n = n(n+1)/2\\))Reject \\(H_0\\) \\(w_y\\) large (equivalently, \\(w_x\\) small)\\(H_0\\), arrangement y’s x’s equally likely occur, \\((n_y + n_x)!/(n_y! n_x!)\\) possible arrangements.Technically, arrangement can compute values \\(w_y\\) \\(w_x\\), thus generate distribution statistic null hypothesis.lead computationally intensive.","code":"\nwilcox.test(irisVe,irisVi,alternative=\"two.sided\",conf.level=0.95, exact=F,correct=T)## \n##  Wilcoxon rank sum test with continuity correction\n## \n## data:  irisVe and irisVi\n## W = 49, p-value < 2.2e-16\n## alternative hypothesis: true location shift is not equal to 0"},{"path":"basic-statistical-inference.html","id":"mann-whitney-u-test","chapter":"4 Basic Statistical Inference","heading":"4.2.6.2 Mann-Whitney U test","text":"Mann-Whitney test computed follows:Compare \\(y_i\\) wiht \\(x_i\\).\nLet \\(u_y\\) number pairs \\(y_i > x_i\\)\nLet \\(u_x\\) number pairs \\(y_i < x_i\\). (assume ties).\n\\(n_y n_x\\) comparisons \\(u_y + u_x = n_y n_x\\).Reject \\(H_0\\) \\(u_y\\) large (\\(u_x\\) small)Mann-Whitney U test Wilcoxon rank test related:\\[\nu_y = w_y - n_y(n_y+1) /2 \\\\\nu_x = w_x - n_x(n_x +1)/2\n\\]\\(\\alpha\\)-level test rejects \\(H_0\\) \\(u_y \\ge u_{n_y,n_x,\\alpha}\\), \\(u_{n_y,n_x,\\alpha}\\) upper \\(\\alpha\\) critical point null distribution random variable, U.p-value defined \\(P(Y \\ge u_y) = P(U \\le u_x)\\). One advantage Mann-Whitney U test can use either \\(u_y\\) \\(u_x\\) carry test.large \\(n_y\\) \\(n_x\\), null distribution U can well approximated normal distribution mean \\(E(U) = n_y n_x /2\\) variance \\(var(U) = n_y n_x (n+1)/12\\). large sample z-test can based statistic:\\[\nz = \\frac{u_y - n_y n_x /2 -1/2}{\\sqrt{n_y n_x (n+1)/12}}\n\\]test rejects \\(H_0\\) level \\(\\alpha\\) \\(z \\ge z_{\\alpha}\\) \\(u_y \\ge u_{n_y,n_x,\\alpha}\\) \\[\nu_{n_y, n_x, \\alpha} \\approx n_y n_x /2 + 1/2 + z_{\\alpha}\\sqrt{n_y n_x (n+1)/12}\n\\]2-sided test, use test statistic \\(u_{max} = max(u_y,u_x)\\) \\(u_{min} = min(u_y, u_x)\\) p-value given \\[\np-value = 2P(U \\ge u_{max}) = 2P(U \\le u_{min})\n\\]\nSince assume ties (\\(y_i = x_j\\)), count 1/2 towards \\(u_y\\) \\(u_x\\). Even though sampling distribution , large sample approximation still reasonable,","code":""},{"path":"basic-statistical-inference.html","id":"categorical-data-analysis","chapter":"4 Basic Statistical Inference","heading":"4.3 Categorical Data Analysis","text":"Categorical Data Analysis categorical outcomesNominal variables: logical ordering (e.g., sex)Ordinal variables: logical order, relative distances values clear (e.g., small, medium, large)distribution one variable changes level (values) variable change. row percentages different column.","code":""},{"path":"basic-statistical-inference.html","id":"inferences-for-small-samples","chapter":"4 Basic Statistical Inference","heading":"4.3.1 Inferences for Small Samples","text":"approximate tests based asymptotic normality \\(\\hat{p}_1 - \\hat{p}_2\\) apply small samples.Using Fisher’s Exact Test evaluate \\(H_0: p_1 = p_2\\)Assume \\(X_1\\) \\(X_2\\) independent BinomialLet \\(x_1\\) \\(x_2\\) corresponding observed values.Let \\(n= n_1 + n_2\\) total sample size\\(m = x_1 + x_2\\) observed number successes.assuming m (total successes) fixed, conditioning value, one can show conditional distribution number successes sample 1 HypergeometricIf want test \\(H_0: p_1 = p_2\\) \\(H_a: p_1 \\neq p_2\\), \\[\nZ^2 = (\\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\hat{p}(1-\\hat{p})(1/n_1 + 1/n_2)}})^2 \\sim \\chi_{1,\\alpha}^2\n\\]\\(\\chi_{1,\\alpha}^2\\) upper \\(\\alpha\\) percentage point central Chi-squared one d.f.extends contingency table setting: whether observed frequencies equal expected null hypothesis association.","code":""},{"path":"basic-statistical-inference.html","id":"test-of-association","chapter":"4 Basic Statistical Inference","heading":"4.3.2 Test of Association","text":"Pearson Chi-square test statistic \\[\n\\chi^2 = \\sum_{\\text{categories}} \\frac{(observed - epxected)^2}{expected}\n\\]Comparison proportions several independent surveys experiments\\(H_0: p_1 = p_2 = ... = p_k\\) vs. alternative null true (least one pair equal).estimate common value probability success single trial assuming \\(H_0\\) true:\\[\n\\hat{p} = \\frac{x_1 + x_2 + ... + x_k}{n_1 + n_2 + ...+ n_k}\n\\]use table expected counts \\(H_0\\) true:\\[\n\\chi^2 = \\sum_{\\text{cells table}} \\frac{(observed - expected)^2}{expected}\n\\]k-1 degrees freedom","code":""},{"path":"basic-statistical-inference.html","id":"two-way-count-data","chapter":"4 Basic Statistical Inference","heading":"4.3.2.1 Two-way Count Data","text":"Design 1\ntotal sample size fixed n = constant (e.g., survey job satisfaction income); row column totals random variablesDesign 2\nFix sample size group (row) (e.g., Drug treatments success failure); fixed number participants treatment; independent random samples two row populations.different sampling designs imply two different probability models.","code":""},{"path":"basic-statistical-inference.html","id":"total-sample-size-fixed","chapter":"4 Basic Statistical Inference","heading":"4.3.2.2 Total Sample Size Fixed","text":"Design 1random sample size n drawn single population, sample units cross-classified r row categories c columnThis results r x c table observed counts\\(n_{ij} = 1,...,r;j=1,...,c\\)Let \\(p_{ij}\\) probability classification cell (,j) \\(\\sum_{=1}^r \\sum_{j=1}^c p_{ij} = 1\\). Let \\(N_{ij}\\) random variable corresponding \\(n_{ij}\\)\njoint distribution \\(N_{ij}\\) multinomial unknown parameters \\(p_{ij}\\)Denote row variable X column variable Y, \\(p_{ij} = P(X=,Y = j)\\) \\(p_{.} = P(X = )\\) \\(p_{.j} = P(Y = j)\\) marginal probabilities.\nnull hypothesis X Y statistically independent (.e., association) just:\\[\nH_0: p_{ij} = P(X =,Y=j) = P(X =) P(Y =j) = p_{.}p_{.j} \\\\\nH_a: p_{ij} \\neq p_{.}p_{.j}\n\\]\n,j.","code":""},{"path":"basic-statistical-inference.html","id":"row-total-fixed","chapter":"4 Basic Statistical Inference","heading":"4.3.2.3 Row Total Fixed","text":"Design 2Random samples sizes \\(n_1,...,n_r\\) drawn independently \\(r \\ge 2\\) row populations. case, 2-way table row totals \\(n_{.} = n_i\\) \\(= 1,...,r\\).counts row modeled independent multinomial distributions.X fixed, Y observed., \\(p_{ij}\\) represent conditional probabilities \\(p_{ij} = P(Y=j|X=)\\)null hypothesis probability response j , regardless row population (.e., association):\\[\nH_0: p_{ij} = P(Y = j | X = ) = p_j \\text{,j =1,2,...,c} \\\\\n\\text{} H_0: (p_{i1},p_{i2},...,p_{ic}) = (p_1,p_2,...,p_c) \\text{ } \\\\\nH_a: (p_{i1},p_{i2},...,p_{ic}) \\text{ }\n\\]Although hypotheses tested different two sampling designs, chi-square test identicalWe estimated expected frequencies:\\[\n\\hat{e}_{ij} = \\frac{n_{.}n_{.j}}{n}\n\\]Chi-square statistic \\[\n\\chi^2 = \\sum_{=1}^{r} \\sum_{j=1}^{c} \\frac{(n_{ij}-\\hat{e}_{ij})^2}{\\hat{e}_{ij}} \\sim \\chi_{(r-1)(c-1)}\n\\]\\(\\alpha\\)-level test rejects \\(H_0\\) \\(\\chi^2 > \\chi^2_{(r-1)(c-1),\\alpha}\\)","code":""},{"path":"basic-statistical-inference.html","id":"pearson-chi-square-test","chapter":"4 Basic Statistical Inference","heading":"4.3.2.4 Pearson Chi-square Test","text":"Determine whether association existsSometimes, \\(H_0\\) represents model whose validity tested. Contrast conventional formulation \\(H_0\\) hypothesis disproved. goal case disprove model, see whether data consistent model deviation can attributed chance.tests measure strength association.tests depend reflect sample size - double sample size copying observation, double \\(\\chi^2\\) statistic even thought strength association change.Pearson Chi-square Test appropriate 20% cells expected cell frequency less 5 (large-sample p-values appropriate).sample size small exact p-values can calculated (prohibitive large samples); calculation exact p-values assumes column totals row totals fixed.\\[\nH_0: p_J = 0.5 \\\\\nH_a: p_J < 0.5\n\\]\\[\nH_0: p_J = p_S \\\\\nH_a: p_j \\neq p_S\n\\]","code":"\njuly.x=480 \njuly.n=1000 \nsept.x=704 \nsept.n=1600\nprop.test(x=july.x,n=july.n,p=0.5,alternative=\"less\",correct=F)## \n##  1-sample proportions test without continuity correction\n## \n## data:  july.x out of july.n, null probability 0.5\n## X-squared = 1.6, df = 1, p-value = 0.103\n## alternative hypothesis: true p is less than 0.5\n## 95 percent confidence interval:\n##  0.0000000 0.5060055\n## sample estimates:\n##    p \n## 0.48\nprop.test(x=c(july.x,sept.x),n=c(july.n,sept.n),correct=F)## \n##  2-sample test for equality of proportions without continuity\n##  correction\n## \n## data:  c(july.x, sept.x) out of c(july.n, sept.n)\n## X-squared = 3.9701, df = 1, p-value = 0.04632\n## alternative hypothesis: two.sided\n## 95 percent confidence interval:\n##  0.0006247187 0.0793752813\n## sample estimates:\n## prop 1 prop 2 \n##   0.48   0.44"},{"path":"basic-statistical-inference.html","id":"ordinal-association","chapter":"4 Basic Statistical Inference","heading":"4.3.3 Ordinal Association","text":"ordinal association implies one variable increases, tends increase decrease (depending nature association).tests variables two levels, levels must logical ordering.","code":""},{"path":"basic-statistical-inference.html","id":"mantel-haenszel-chi-square-test","chapter":"4 Basic Statistical Inference","heading":"4.3.3.1 Mantel-Haenszel Chi-square Test","text":"Mantel-Haenszel Chi-square Test powerful testing ordinal associations, test strength association.test presented case one series 2 x 2 tables examine effects different conditions (K tables, 2 x 2 x K table)stratum k, given marginal totals \\((n_{.1k},n_{.2k},n_{1.k},n_{2.k})\\), sampling model cell counts Hypergeometric (knowing \\(n_{11k}\\) determines \\((n_{12k},n_{21k},n_{22k})\\), given marginal totals)Assuming conditional independence, Hypergeometric mean variance \\(n_{11k}\\) \\[\nm_{11k} = E(n_{11k}) = \\frac{n_{1.k} n_{.1k}}{n_{..k}} \\\\\nvar(n_{11k}) = \\frac{n_{1.k} n_{2.k} n_{.1k} n_{.2k}}{n_{..k}^2(n_{..k}-1)}\n\\]test conditional independence, Mantel Haenszel proposed\\[\nM^2 = \\frac{(|\\sum_{k} n_{11k} - \\sum_k m_{11k}| -.5)^2}{\\sum_{k}var(n_{11k})} \\sim \\chi^2_{1}\n\\]\nmethod can extended general x J x K tables.(2 x 2 x 3) table","code":"\nBron=array(c(20, 9, 382, 214, 10, 7, 172, 120, 12, 6, 327, 183), dim = c(2, 2, 3), dimnames = list( Particulate = c(\"High\", \"Low\"), Bronchitis = c(\"Yes\", \"No\"), Age = c(\"15-24\", \"25-39\", \"40+\"))) \nmargin.table(Bron,c(1,2))##            Bronchitis\n## Particulate Yes  No\n##        High  42 881\n##        Low   22 517\n# assess whether the relationship between Bronchitis by Particulate Level varies by Age\nlibrary(samplesizeCMH)\nmarginal_table=margin.table(Bron,c(1,2))\nodds.ratio(marginal_table)## [1] 1.120318\n#  whether these odds vary by age. The conditional odds can be calculated using the original table.\napply(Bron,3,odds.ratio)##     15-24     25-39       40+ \n## 1.2449098 0.9966777 1.1192661\n# Mantel-Haenszel Test\nmantelhaen.test(Bron,correct=T)## \n##  Mantel-Haenszel chi-squared test with continuity correction\n## \n## data:  Bron\n## Mantel-Haenszel X-squared = 0.11442, df = 1, p-value = 0.7352\n## alternative hypothesis: true common odds ratio is not equal to 1\n## 95 percent confidence interval:\n##  0.6693022 1.9265813\n## sample estimates:\n## common odds ratio \n##          1.135546"},{"path":"basic-statistical-inference.html","id":"mcnemars-test","chapter":"4 Basic Statistical Inference","heading":"4.3.3.1.1 McNemar’s Test","text":"special case Mantel-Haenszel Chi-square Test","code":"\nvote=cbind(c(682,22),c(86,810))\nmcnemar.test(vote,correct=T)## \n##  McNemar's Chi-squared test with continuity correction\n## \n## data:  vote\n## McNemar's chi-squared = 36.75, df = 1, p-value = 1.343e-09"},{"path":"basic-statistical-inference.html","id":"spearman-rank-correlation","chapter":"4 Basic Statistical Inference","heading":"4.3.3.2 Spearman Rank Correlation","text":"test strength association two ordinally scaled variables, can use Spearman Rank Correlation statisticLet X Y two random variables measured ordinal scale. Consider n pairs observations (\\(x_i,y_i\\)), = 1,…,nThe Spearman Rank Correlation coefficient (denoted \\(r_S\\) calculated using Pearson correlation formula, based ranks \\(x_i\\) \\(y_i\\)).Spearman Rank Correlation calculatedAssign ranks \\(x_i\\)’s \\(y_i\\)’s separately. Let \\(u_i = rank(x_i)\\) \\(v_i = rank(y_i)\\)Calculate \\(r_S\\) using formula Pearson correlation coefficient, applied ranks:\\[\nr_S = \\frac{\\sum_{=1}^{n}(u_i - \\bar{u})(v_i - \\bar{v})}{\\sqrt{(\\sum_{= 1}^{n}(u_i - \\bar{u})^2)(\\sum_{=1}^{n}(v_i - \\bar{v})^2)}}\n\\]\\(r_S\\) ranges -1 +1 , \\(r_S = -1\\) perfect negative monotone association\\(r_S = +1\\) perfect positive monotone association X Y.test\\(H_0:\\) X Y independent\\(H_a\\): X Y positively associatedFor large n (e.g., \\(n \\ge 10\\)),\\[\nr_S \\sim N(0,1/(n-1))\n\\],\\[\nZ = r_s \\sqrt{n-1} \\sim N(0,1)\n\\]","code":""},{"path":"linear-regression.html","id":"linear-regression","chapter":"5 Linear Regression","heading":"5 Linear Regression","text":"Estimator Desirable PropertiesUnbiasedUnbiasedConsistencyConsistency\\(plim\\hat{\\beta_n}=\\beta\\)based law large numbers, can derive consistencyMore observations means precise, closer true value.EfficiencyMinimum variance comparison another estimator.\nOLS BlUE (best linear unbiased estimator) means OLS efficient among class linear unbiased estimator Gauss-Markov Theorem\ncorrect distributional assumptions, Maximum Likelihood asymptotically efficient among consistent estimators.\nOLS BlUE (best linear unbiased estimator) means OLS efficient among class linear unbiased estimator Gauss-Markov TheoremIf correct distributional assumptions, Maximum Likelihood asymptotically efficient among consistent estimators.","code":""},{"path":"linear-regression.html","id":"ordinary-least-squares","chapter":"5 Linear Regression","heading":"5.1 Ordinary Least Squares","text":"fundamental model statistics econometric OLS linear regression.\nOLS = Maximum likelihood error term assumed normally distributed.","code":""},{"path":"linear-regression.html","id":"simple-regression-basic-model","chapter":"5 Linear Regression","heading":"5.1.1 Simple Regression (Basic Model)","text":"\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\n\\]\\(Y_i\\): response (dependent) variable -th observation\\(\\beta_0,\\beta_1\\): regression parameters intercept slope.\\(X_i\\): known constant (independent predictor variable) -th observation\\(\\epsilon_i\\): random error term\\[\nE(\\epsilon_i) = 0 \\\\\nvar(\\epsilon_i) = \\sigma^2\n\\]\\[\ncov(\\epsilon_i,\\epsilon_j) = 0  \\text{ } \\neq j\n\\]\\(Y_i\\) random since \\(\\epsilon_i\\) :\\[\n\\begin{aligned}\nE(Y_i) &= E(\\beta_0 + \\beta_1 X_i + \\epsilon_i) \\\\\n&= E(\\beta_0) + E(\\beta_1 X_i) + E(\\epsilon) \\\\\n&= \\beta_0 + \\beta_1 X_i\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nvar(Y_i) &= var(\\beta_0 + \\beta_1 X_i + \\epsilon_i) \\\\\n&= var(\\epsilon_i) \\\\\n&= \\sigma^2\n\\end{aligned}\n\\]Since \\(cov(\\epsilon_i, \\epsilon_j) = 0\\) (uncorrelated), outcome one trail effect outcome . Hence, \\(Y_i, Y_j\\) uncorrelated well (conditioned X’s)NoteLeast Squares require distributional assumption","code":""},{"path":"linear-regression.html","id":"estimation","chapter":"5 Linear Regression","heading":"5.1.1.1 Estimation","text":"Deviation \\(Y_i\\) expected value:\\[\nY_i - E(Y_i) = Y_i - (\\beta_0 + \\beta_1 X_i)\n\\]Consider sum square deviations:\\[\nQ = \\sum_{=1}^{n} (Y_i - \\beta_0 -\\beta_1 X_i)^2\n\\]\\[\nb_1 = \\frac{\\sum_{=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{=1}^{n}(X_i - \\bar{X})^2} \\\\\nb_0 = \\frac{1}{n}(\\sum_{=1}^{n}Y_i - b_1\\sum_{=1}^{n}X_i) = \\bar{Y} - b_1 \\bar{X}\n\\]","code":""},{"path":"linear-regression.html","id":"properties-of-least-least-estimators","chapter":"5 Linear Regression","heading":"5.1.1.2 Properties of Least Least Estimators","text":"\\[\nE(b_1) = \\beta_1 \\\\\nE(b_0) = E(\\bar{Y}) - \\bar{X}\\beta_1 \\\\\nE(\\bar{Y}) = \\beta_0 + \\beta_1 \\bar{X} \\\\\nE(b_0) = \\beta_0 \\\\\nvar(b_1) = \\frac{\\sigma^2}{\\sum_{=1}^{n}(X_i - \\bar{X})^2} \\\\\nvar(b_0) = \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum (X_i - \\bar{X})^2})\n\\]\\(var(b_1)\\) approaches 0 measurements taken \\(X_i\\) values (unless \\(X_i\\) mean value)\\(var(b_0)\\) approaches 0 n increases \\(X_i\\) values judiciously selected.Mean Square Error\\[\nMSE = \\frac{SSE}{n-2} = \\frac{\\sum_{=1}^{n}e_i^2}{n-2} = \\frac{\\sum(Y_i - \\hat{Y_i})^2}{n-2}\n\\]Unbiased estimator MSE:\\[\nE(MSE) = \\sigma^2\n\\]\\[\ns^2(b_1) = \\widehat{var(b_1)} = \\frac{MSE}{\\sum_{=1}^{n}(X_i - \\bar{X})^2} \\\\\ns^2(b_0) = \\widehat{var(b_0)} = MSE(\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum_{=1}^{n}(X_i - \\bar{X})^2})\n\\]\\[\nE(s^2(b_1)) = var(b_1) \\\\\nE(s^2(b_0))=var(b_0)\n\\]","code":""},{"path":"linear-regression.html","id":"residuals","chapter":"5 Linear Regression","heading":"5.1.1.3 Residuals","text":"\\[\ne_i = Y_i - \\hat{Y} = Y_i - (b_0 + b_1 X_i)\n\\]\\(e_i\\) estimate \\(\\epsilon_i = Y_i - E(Y_i)\\)\\(\\epsilon_i\\) always unknown since don’t know true \\(\\beta_0, \\beta_1\\)\\[\n\\sum_{=1}^{n} e_i = 0 \\\\\n\\sum_{=1}^{n} X_i e_i = 0\n\\]","code":""},{"path":"linear-regression.html","id":"inference","chapter":"5 Linear Regression","heading":"5.1.1.4 Inference","text":"Normality AssumptionLeast Squares estimation require assumptions normality.However, inference parameters, need distributional assumptions.Inference \\(\\beta_0,\\beta_1\\) \\(Y_h\\) extremely sensitive moderate departures normality, especially sample size largeInference \\(Y_{pred}\\) sensitive normality assumptions.Normal Error Regression Model\\[\nY_i \\sim N(\\beta_0+\\beta_1X_i, \\sigma^2) \\\\\n\\]","code":""},{"path":"linear-regression.html","id":"beta_1","chapter":"5 Linear Regression","heading":"5.1.1.4.1 \\(\\beta_1\\)","text":"normal error model,\\[\nb_1 \\sim N(\\beta_1,\\frac{\\sigma^2}{\\sum_{=1}^{n}(X_i - \\bar{X})^2})\n\\]linear combination independent normal random variable normally distributedHence,\\[\n\\frac{b_1 - \\beta_1}{s(b_1)} \\sim t_{n-2}\n\\]\\((1-\\alpha) 100 \\%\\) confidence interval \\(\\beta_1\\) \\[\nb_1 \\pm t_{t-\\alpha/2 ; n-2}s(b_1)\n\\]","code":""},{"path":"linear-regression.html","id":"beta_0","chapter":"5 Linear Regression","heading":"5.1.1.4.2 \\(\\beta_0\\)","text":"normal error model, sampling distribution \\(b_0\\) \\[\nb_0 \\sim N(\\beta_0,\\sigma^2(\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum_{=1}^{n}(X_i - \\bar{X})^2}))\n\\]Hence,\\[\n\\frac{b_0 - \\beta_0}{s(b_0)} \\sim t_{n-2}\n\\]\n\\((1-\\alpha)100 \\%\\) confidence interval \\(\\beta_0\\) \\[\nb_0 \\pm t_{1-\\alpha/2;n-2}s(b_0)\n\\]","code":""},{"path":"linear-regression.html","id":"mean-response","chapter":"5 Linear Regression","heading":"5.1.1.4.3 Mean Response","text":"Let \\(X_h\\) denote level X wish estimate mean responseWe denote mean response \\(X = X_h\\) \\(E(Y_h)\\)point estimator \\(E(Y_h)\\) \\(\\hat{Y}_h\\):\\[\n\\hat{Y}_h = b_0 + b_1 X_h\n\\]\nNote\\[\nE(\\bar{Y}_h)= E(b_0 + b_1X_h) \\\\\n= \\beta_0 + \\beta_1 X_h \\\\\n= E(Y_h)\n\\]\n(unbiased estimator)\\[\n\\begin{aligned}\nvar(\\hat{Y}_h) &= var(b_0 + b_1 X_h) \\\\\n&= var(\\hat{Y} + b_1 (X_h - \\bar{X})) \\\\\n&= var(\\bar{Y}) + (X_h - \\bar{X})^2var(b_1) + 2(X_h - \\bar{X})cov(\\bar{Y},b_1) \\\\\n&= \\frac{\\sigma^2}{n} + (X_h - \\bar{X})^2 \\frac{\\sigma^2}{\\sum(X_i - \\bar{X})^2} \\\\\n&= \\sigma^2(\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2})\n\\end{aligned}\n\\]Since \\(cov(\\bar{Y},b_1) = 0\\) due iid assumption \\(\\epsilon_i\\)estimate variance \\[\ns^2(\\hat{Y}_h) = MSE (\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n}(X_i - \\bar{X})^2})\n\\]sampling distribution mean response \\[\n\\hat{Y}_h \\sim N(E(Y_h),var(\\hat{Y_h})) \\\\\n\\frac{\\hat{Y}_h - E(Y_h)}{s(\\hat{Y}_h)} \\sim t_{n-2}\n\\]\\(100(1-\\alpha) \\%\\) CI \\(E(Y_h)\\) \\[\n\\hat{Y}_h \\pm t_{1-\\alpha/2;n-2}s(\\hat{Y}_h)\n\\]","code":""},{"path":"linear-regression.html","id":"prediction-of-a-new-observation","chapter":"5 Linear Regression","heading":"5.1.1.4.4 Prediction of a new observation","text":"Regarding Mean Response, interested estimating mean distribution Y given certain X.Now, want predict individual outcome distribution Y given X. call \\(Y_{pred}\\)Estimation mean response versus prediction new observation:point estimates cases: \\(\\hat{Y}_{pred} = \\hat{Y}_h\\)variance prediction different; hence, prediction intervals different confidence intervals. prediction variance must consider:\nVariation mean distribution Y\nvariation within distribution Y\nVariation mean distribution Yvariation within distribution YWe want predict: mean response + error\\[\n\\beta_0 + \\beta_1 X_h + \\epsilon\n\\]Since \\(E(\\epsilon) = 0\\), use least squares predictor:\\[\n\\hat{Y}_h = b_0 + b_1 X_h\n\\]variance predictor \\[\n\\begin{aligned}\nvar(b_0 + b_1 X_h + \\epsilon) &= var(b_0 + b_1 X_h) + var(\\epsilon) \\\\\n&= \\sigma^2(\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n}(X_i - \\bar{X})^2}) + \\sigma^2 \\\\\n&= \\sigma^2(1+\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n}(X_i - \\bar{X})^2})\n\\end{aligned}\n\\]estimate variance given \\[\ns^2(pred)= MSE (1+ \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{=1}^{n}(X_i - \\bar{X})^2}) \\\\\n\\frac{Y_{pred}-\\hat{Y}_h}{s(pred)} \\sim t_{n-2}\n\\]\\(100(1-\\alpha) \\%\\) prediction interval \\[\n\\bar{Y}_h \\pm t_{1-\\alpha/2; n-2}s(pred)\n\\]prediction interval sensitive distributional assumption errors, \\(\\epsilon\\)","code":""},{"path":"linear-regression.html","id":"confidence-band","chapter":"5 Linear Regression","heading":"5.1.1.4.5 Confidence Band","text":"want know confidence interval entire regression line, can draw conclusions mean response fo entire regression line \\(E(Y) = \\beta_0 + \\beta_1 X\\) rather given response YWorking-Hotelling Confidence BandFor given \\(X_h\\), band \\[\n\\hat{Y}_h \\pm W s(\\hat{Y}_h)\n\\]\n\\(W^2 = 2F_{1-\\alpha;2,n-2}\\), just 2 times F-stat 2 n-2 degrees freedomthe interval width change \\(X_h\\) (since \\(s(\\hat{Y}_h)\\) changes)boundary values confidence band always define hyperbole containing regression linewill smallest \\(X = \\bar{X}\\)","code":""},{"path":"linear-regression.html","id":"anova","chapter":"5 Linear Regression","heading":"5.1.1.5 ANOVA","text":"Partitioning Total Sum Squares: Consider corrected Total sum squres:\\[\nSSTO = \\sum_{=1}^{n} (Y_i -\\bar{Y})^2\n\\]\nMeasures overall dispersion response variable\nuse term corrected correct mean, uncorrected total sum squares given \\(\\sum Y_i^2\\)use \\(\\hat{Y}_i = b_0 + b_1 X_i\\) estimate conditional mean Y \\(X_i\\)\\[\n\\begin{aligned}\n\\sum_{=1}^n (Y_i - \\bar{Y})^2 &= \\sum_{=1}^n (Y_i - \\hat{Y}_i + \\hat{Y}_i - \\bar{Y})^2 \\\\\n&= \\sum_{=1}^n(Y_i - \\hat{Y}_i)^2 + \\sum_{=1}^n(\\hat{Y}_i - \\bar{Y})^2 + 2\\sum_{=1}^n(Y_i - \\hat{Y}_i)(\\hat{Y}_i-\\bar{Y}) \\\\\n&= \\sum_{=1}^n(Y_i - \\hat{Y}_i)^2 + \\sum_{=1}^n(\\bar{Y}_i -\\bar{Y})^2 \\\\\nSTTO &= SSE + SSR \\\\\n\\end{aligned}\n\\]SSR regression sum squares, measures conditional mean varies central value.cross-product term decomposition 0:\\[\n\\begin{aligned}\n\\sum_{=1}^n (Y_i - \\hat{Y}_i)(\\hat{Y}_i - \\bar{Y}) &= \\sum_{=1}^{n}(Y_i - \\bar{Y} -b_1 (X_i - \\bar{X}))(\\bar{Y} + b_1 (X_i - \\bar{X})-\\bar{Y}) \\\\\n&= b_1 \\sum_{=1}^{n} (Y_i - \\bar{Y})(X_i - \\bar{X}) - b_1^2\\sum_{=1}^{n}(X_i - \\bar{X})^2 \\\\\n&= b_1 \\frac{\\sum_{=1}^{n}(Y_i -\\bar{Y})(X_i - \\bar{X})}{\\sum_{=1}^{n}(X_i - \\bar{X})^2} \\sum_{=1}^{n}(X_i - \\bar{X})^2 - b_1^2\\sum_{=1}^{n}(X_i - \\bar{X})^2 \\\\\n&= b_1^2 \\sum_{=1}^{n}(X_i - \\bar{X})^2 - b_1^2 \\sum_{=1}^{n}(X_i - \\bar{X})^2 \\\\\n&= 0\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nSSTO &= SSR + SSE \\\\\n(n-1 d.f) &= (1 d.f.) + (n-2 d.f.)\n\\end{aligned}\n\\]\\[\nE(MSE) = \\sigma^2 \\\\\nE(MSR) = \\sigma^2 + \\beta_1^2 \\sum_{=1}^{n} (X_i - \\bar{X})^2 \n\\]\\(\\beta_1 = 0\\), two expected values sameif \\(\\beta_1 \\neq 0\\) E(MSR) larger E(MSE)means ratio two quantities, can infer something \\(\\beta_1\\)Distribution theory tells us \\(\\epsilon_i \\sim iid N(0,\\sigma^2)\\) assuming \\(H_0: \\beta_1 = 0\\) true,\\[\n\\frac{MSE}{\\sigma^2} \\sim \\chi_{n-2}^2 \\\\\n\\frac{MSR}{\\sigma^2} \\sim \\chi_{1}^2 \\text{ $\\beta_1=0$}\n\\]two chi-square random variables independent.Since ratio 2 independent chi-square random variable follows F distribution, consider:\\[\nF = \\frac{MSR}{MSE} \\sim F_{1,n-2}\n\\]\n\\(\\beta_1 =0\\). Thus, reject \\(H_0: \\beta_1 = 0\\) (\\(E(Y_i)\\) = constant) \\(\\alpha\\) \\[\nF > F_{1 - \\alpha;1,n-2}\n\\]\nnull hypothesis can tested approach.Coefficient Determination\\[\nR^2 = \\frac{SSR}{SSTO} = 1- \\frac{SSE}{SSTO}\n\\]\\(0 \\le R^2 \\le 1\\)Interpretation: proportionate reduction total variation Y fitting linear model X.\nreally correct say \\(R^2\\) “variation Y explained X.”\\(R^2\\) related correlation coefficient Y X:\\[\nR^2 = (r)^2\n\\]\n\\(r= corr(x,y)\\) estimate Pearson correlation coefficient. Also, note\\[\nb_1 = (\\frac{\\sum_{=1}^{n}(Y_i - \\bar{Y})^2}{\\sum_{=1}^{n} (X_i - \\bar{X})^2})^{1/2} \\\\\nr = \\frac{s_y}{s_x} r\n\\]Lack Fit\\(Y_{11},Y_{21},...,Y_{n_1,1}\\): \\(n_1\\) repeat obs \\(X_1\\)\\(Y_{1c},Y_{2c},...,Y_{n_c,c}\\): \\(n_c\\) repeat obs \\(X_c\\), c distinct X values.Let \\(\\bar{Y}_j\\) mean replicates \\(X_j\\)Partition Error Sum Squares:\\[\n\\begin{aligned}\n\\sum_{} \\sum_{j} (Y_{ij} - \\hat{Y}_{ij})^2 &= \\sum_{} \\sum_{j} (Y_{ij} - \\bar{Y}_j + \\bar{Y}_j + \\hat{Y}_{ij})^2 \\\\\n&=  \\sum_{} \\sum_{j} (Y_{ij} - \\bar{Y}_j)^2 + \\sum_{} \\sum_{j} (\\bar{Y}_j - \\hat{Y}_{ij})^2 + \\text{cross product term} \\\\\n&= \\sum_{} \\sum_{j}(Y_{ij} - \\bar{Y}_j)^2 + \\sum_j n_j (\\bar{Y}_j- \\hat{Y}_{ij})^2 \\\\\nSSE &= SSPE + SSLF \\\\\n\\end{aligned}\n\\]SSPE: “pure error sum squares” n-c degrees freedom since need estimate c meansSSLF: “lack fit sum squares” c - 2 degrees freedom (number unique X values - number parameters used specify conditional mean regression model)\\[\nMSPE = \\frac{SSPE}{df_{pe}} = \\frac{SSPE}{n-c} \\\\\nMSLF = \\frac{SSLF}{df_{lf}} = \\frac{SSLF}{c-2}\n\\]\nF-test Lack--Fit tests\\[\nH_0: Y_{ij} = \\beta_0 + \\beta_1 X_i + \\epsilon_{ij}, \\epsilon_{ij} \\sim iid N(0,\\sigma^2) \\\\\nH_a: Y_{ij} = \\alpha_0 + \\alpha_1 X_i + f(X_i, Z_1,...) + \\epsilon_{ij}^*,\\epsilon_{ij}^* \\sim iid N(0, \\sigma^2)\n\\]\\(E(MSPE) = \\sigma^2\\) either \\(H_0\\), \\(H_a\\)\\(E(MSLF) = \\sigma^2 + \\frac{\\sum n_j(f(X_i,...))^2}{n-2}\\) general \\(E(MSLF) = \\sigma^2\\) \\(H_0\\) trueWe reject \\(H_0\\) (.e., model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) adequate) \\[\nF = \\frac{MSLF}{MSPE} > F_{1-\\alpha;c-2,n-c}\n\\]Failing reject \\(H_0\\) imply \\(H_0: Y_{ij} = \\beta_0 + \\beta_1 X_i + \\epsilon_{ij}\\) exactly true, suggests model may provide reasonable approximation true model.Repeat observations effect \\(R^2\\):impossible \\(R^2\\) attain 1 repeat obs. exist (SSE can’t 0)maximum \\(R^2\\) attainable situation:\\[\nR^2_{max} = \\frac{SSTo - SSPE}{SSTO}\n\\]levels X need repeat observations.Typically, \\(H_0\\) appropriate, one still uses MSE estimate \\(\\sigma^2\\) rather MSPE, Since MSE degrees freedom, sometimes people pool estimates.Joint Inference\nconfidence coefficient \\(\\beta_0\\) \\(\\beta_1\\) considered simultaneously \\(\\le \\alpha\\)Let\\(\\bar{}_1\\) event first interval covers \\(\\beta_0\\)\\(\\bar{}_2\\) event teh second interval covers \\(\\beta_1\\)\\[\nP(\\bar{}_1) = 1 - \\alpha \\\\\nP(\\bar{}_2) = 1 - \\alpha\n\\]probability \\(\\bar{}_1\\) \\(\\bar{}_2\\)\\[\n\\begin{aligned}\nP(\\bar{}_1 \\cap \\bar{}_2) &= 1 - P(\\bar{}_1 \\cup \\bar{}_2) \\\\\n&= 1 - P(A_1) - P(A_2) + P(A_1 \\cap A_2) \\\\\n&\\ge 1 - P(A_1) - P(A_2) \\\\\n&= 1 - 2\\alpha\n\\end{aligned}\n\\]\\(\\beta_0\\) \\(\\beta_1\\) separate 95% confidence intervals, joint (family) confidence coefficient least \\(1 - 2(0.05) = 0.9\\). called Bonferroni Inequality\nuse procedure obtained \\(1-\\alpha/2\\) confidence intervals two regression parameters separately, joint (Bonferroni) family confidence coefficient least \\(1- \\alpha\\)\\(1-\\alpha\\) joint Bonferroni confidence interval \\(\\beta_0\\) \\(\\beta_1\\) given calculating:\\[\nb_0 \\pm B s(b_0) \\\\\nb_1 \\pm B s(b_1) \n\\]\\(B= t_{1-\\alpha/4;n-2}\\)Interpretation: repeated samples taken joint \\((1-\\alpha)\\) intervals \\(\\beta_0\\) \\(\\beta_1\\) obtained, \\((1-\\alpha)100\\)% joint intervals contain true pair \\((\\beta_0, \\beta_1)\\). , \\(\\alpha \\times 100\\)% samples, one intervals contain true value.Bonferroni interval conservative. lower bound joint intervals tend correct \\((1-\\alpha)100\\)% time (lower power). People usually consider larger \\(\\alpha\\) Bonferroni joint tests (e.g, \\(\\alpha=0.1\\))Bonferroni procedure extends testing 2 parameters. Say interested testing \\(\\beta_0,\\beta_1,..., \\beta_{g-1}\\) (g parameters test). , joint Bonferroni interval obtained calculating \\((1-\\alpha/g)\\) 100% level interval separately.example, \\(\\alpha = 0.05\\) \\(g=10\\), individual test done \\(1- \\frac{.05}{10}\\) level. 2-sided intervals, corresponds using \\(t_{1-\\frac{0.05}{2(10)};n-p}\\) CI formula. procedure works best g relatively small, otherwise intervals individual parameter wide teh test way conservative.\\(b_0,b_1\\) usually correlated (negatively \\(\\bar{X} >0\\) positively \\(\\bar{X}<0\\))multiple comparison procedures available.","code":""},{"path":"linear-regression.html","id":"assumptions","chapter":"5 Linear Regression","heading":"5.1.1.6 Assumptions","text":"Linearity regression functionError terms constant varianceError terms independentNo outliersError terms normally distributedNo Omitted variables","code":""},{"path":"linear-regression.html","id":"diagnostics","chapter":"5 Linear Regression","heading":"5.1.1.7 Diagnostics","text":"Constant Variance\nPlot residuals vs. XOutliers\nplot residuals vs. X\nbox plots\nstem-leaf plots\nscatter plotswe use standardize residuals unit variance. standardized residuals called studentized residuals:\\[\nr_i = \\frac{e_i -\\bar{e}}{s(e_i)} = \\frac{e_i}{s(e_i)}\n\\]simplified standardization procedure gives semi-studentized residuals:\\[\ne_i^* = \\frac{e_i - \\bar{e}}{\\sqrt{MSE}} = \\frac{e_i}{\\sqrt{MSE}}\n\\]Non-independent Error Terms\nplot residuals vs. timeResiduals \\(e_i\\) independent random variables involve fitted values \\(\\hat{Y}_i\\), based fitted regression function.sample size large, dependency among \\(e_i\\) relatively unimportant.detect non-independence, helps plot residual -th response vs. (-1)-thNon-normality Error Termsto detect non-normality (distribution plots residuals, box plots residuals, stem-leaf plots residuals, normal probability plots residuals)Need relatively large sample sizes.types departure affect distribution residuals (wrong regression function, non-constant error variance,…)","code":""},{"path":"linear-regression.html","id":"objective-tests-of-model-assumptions","chapter":"5 Linear Regression","heading":"5.1.1.7.1 Objective Tests of Model Assumptions","text":"Normality\nUse Methods based empirical cumulative distribution function test residuals.\nUse Methods based empirical cumulative distribution function test residuals.Constancy error variance\nBrown-Forsythe Test (Modified Levene Test)\nBreusch-Pagan Test (Cook-Weisberg Test)\nBrown-Forsythe Test (Modified Levene Test)Breusch-Pagan Test (Cook-Weisberg Test)","code":""},{"path":"linear-regression.html","id":"remeidal-measures","chapter":"5 Linear Regression","heading":"5.1.1.8 Remeidal Measures","text":"simple linear regression appropriate, one can:complicated modelstransformations X /Y (may “optimal” results)Remedial measures based deviations:Non-linearity:\nTransformations\ncomplicated models\nTransformationsmore complicated modelsNon-constant error variance:\nWeighted Least Squares\nTransformations\nWeighted Least SquaresTransformationsCorrelated errors:\nserially correlated error models (times series )\nserially correlated error models (times series )Non-normality:\n\nAdditional variables: multiple regression.Outliers:\nRobust estimation.\nRobust estimation.","code":""},{"path":"linear-regression.html","id":"transformations","chapter":"5 Linear Regression","heading":"5.1.1.8.1 Transformations","text":"use transformations one variables performing regression analysis.\nproperties least-squares estimates apply transformed regression, original variable.transform Y variable perform regression get:\\[\ng(Y_i) = b_0 + b_1 X_i\n\\]\nTransform back:\\[\n\\hat{Y}_i = g^{-1}(b_0 + b_1 X_i)\n\\]\\(\\hat{Y}_i\\) biased. can correct bias.Box-Cox Family Transformations\\[\nY'= Y^{\\lambda}\n\\]\\(\\lambda\\) parameter determined data.pick \\(\\lambda\\), can estimation :trial errormaximum likelihoodnumerical searchVariance Stabilizing TransformationsA general method finding variance stabilizing transformation, standard deviation function mean, delta method - application Taylor series expansion.\\[\n\\sigma = \\sqrt{var(Y)} = f(\\mu)\n\\]\n\\(\\mu = E(Y)\\) \\(f(\\mu)\\) smooth function mean.COnsider transformation h(Y). Expand function Taylor series \\(\\mu\\). ,\\[\nh(Y) = h(\\mu) + h'(\\mu)(Y-\\mu) + \\text{small terms}\n\\]\nwant select function h(.) variance h(Y) nearly constant values \\(\\mu= E(Y)\\):\\[\n\\begin{aligned}\nconst &= var(h(Y)) \\\\\n&= var(h(\\mu) + h'(\\mu)(Y-\\mu)) \\\\\n&= (h'(\\mu))^2 var(Y-\\mu) \\\\\n&= (h'(\\mu))^2 var(Y) \\\\\n&= (h'(\\mu))^2(f(\\mu))^2 \\\\\n\\end{aligned}\n\\]must ,\\[\nh'(\\mu) \\propto \\frac{1}{f(\\mu)}\n\\],\\[\nh(\\mu) = \\int\\frac{1}{f(\\mu)}d\\mu\n\\]Example: Poisson distribution: \\(\\sigma^2 = var(Y) = E(Y) = \\mu\\),\\[\n\\sigma = f(\\mu) = \\sqrt{mu} \\\\\nh'(\\mu) \\propto \\frac{1}{\\mu} = \\mu^{-.5}\n\\]\n, variance stabilizing transformation :\\[\nh(\\mu) = \\int \\mu^{-.5} d\\mu = \\frac{1}{2} \\sqrt{\\mu}\n\\]hence, \\(\\sqrt{Y}\\) used variance stabilizing transformation.don’t know \\(f(\\mu)\\)Trial error. Look residuals plotsAsk researchers previous studies find published results similar experiments determine transformation used.multiple observations \\(Y_{ij}\\) X values, compute \\(\\bar{Y}_i\\) \\(s_i\\) plot \n\\(s_i \\propto \\bar{Y}_i^{\\lambda}\\) consider \\(s_i = \\bar{Y}_i^{\\lambda}\\) \\(ln(s_i) = ln() + \\lambda ln(\\bar{Y}_i)\\). regression natural log \\(s_i\\) natural log \\(\\bar{Y}_i\\) gives \\(\\hat{}\\) \\(\\hat{\\lambda}\\) suggests form \\(f(\\mu)\\) don’t multiple obs, might still able “group” observations get \\(\\bar{Y}_i\\) \\(s_i\\).","code":""},{"path":"linear-regression.html","id":"multiple-linear-regression","chapter":"5 Linear Regression","heading":"5.1.2 Multiple Linear Regression","text":"Geometry Least Squares\\[\n\\begin{aligned}\n\\mathbf{\\hat{y}} &= \\mathbf{Xb} \\\\\n&= \\mathbf{X(X'X)^{-1}X'y} \\\\\n&= \\mathbf{Hy}\n\\end{aligned}\n\\]sometimes H denoted P.H projection operator.\\[\n\\mathbf{\\hat{y}= Hy}\n\\]\nprojection y onto linear space spanned columns X (model space). dimension model space rank X.Facts:H symmetric (.e., H = H’)HH = H\\[\n\\begin{aligned}\n\\mathbf{HH} &= \\mathbf{X(X'X)^{-1}X'X(X'X)^{-1}X'} \\\\\n&= \\mathbf{X(X'X)^{-1}IX'} \\\\\n&= \\mathbf{X(X'X)^{-1}X'}\n\\end{aligned}\n\\]H n x n matrix rank(H) = rank(X)\\(\\mathbf{(-H) = - X(X'X)^{-1}X'}\\) also projection operator. projects onto n - k dimensional space orthogonal k dimensional space spanned columns X\\(\\mathbf{H(-H)=(-H)H = 0}\\)Partition uncorrected total sum squares:\\[\n\\begin{aligned}\n\\mathbf{y'y} &= \\mathbf{\\hat{y}'\\hat{y} + e'e} \\\\\n&= \\mathbf{(Hy)'(Hy) + ((-H)y)'((-H)y)} \\\\\n&= \\mathbf{y'H'Hy + y'(-H)'(-H)y} \\\\\n&= \\mathbf{y'Hy + y'(-H)y} \\\\\n\\end{aligned}\n\\]partition corrected total sum squares:\\[\n\\mathbf{y'(-H_1)y = y'(H-H_1)y + y'(-H)y}\n\\]\n\\(H_1 = \\frac{1}{n} J = 1'(1'1)1\\)Equivalently, can express\\[\n\\mathbf{Y = X\\hat{\\beta} + (Y - X\\hat{\\beta})}\n\\]\n\\(\\mathbf{\\hat{Y} = X \\hat{\\beta}}\\) = sum vector fitted values\\(\\mathbf{e = ( Y - X \\hat{\\beta})}\\) = residual\\(\\mathbf{Y}\\) n x 1 vector n-dimensional space \\(R^n\\)\\(\\mathbf{X}\\) n x p full rank matrix. columns generate p-dimensional subspace \\(R^n\\). Hence, estimator \\(\\mathbf{X \\hat{\\beta}}\\) also subspace.choose least squares estimator minimize distance \\(\\mathbf{Y}\\) \\(\\mathbf{X \\hat{\\beta}}\\), orthogonal projection \\(\\mathbf{Y}\\) onto \\(\\mathbf{X\\beta}\\).\\[\n\\begin{aligned}\n||\\mathbf{Y} - \\mathbf{X}\\hat{\\beta}||^2 &= \\mathbf{||Y - X\\hat{\\beta}||}^2 + \\mathbf{||X \\hat{\\beta}||}^2 \\\\\n&= \\mathbf{(Y - X\\hat{\\beta})'(Y - X\\hat{\\beta}) +(X \\hat{\\beta})'(X \\hat{\\beta})} \\\\\n&= \\mathbf{(Y - X\\hat{\\beta})'Y - (Y - X\\hat{\\beta})'X\\hat{\\beta} + \\hat{\\beta}' X'X\\hat{\\beta}} \\\\\n&= \\mathbf{(Y-X\\hat{\\beta})'Y + \\hat{\\beta}'X'X(XX')^{-1}X'Y} \\\\\n&= \\mathbf{Y'Y - \\hat{\\beta}'X'Y + \\hat{\\beta}'X'Y}\n\\end{aligned}\n\\]norm (p x 1) vector \\(\\mathbf{}\\) defined :\\[\n\\mathbf{|||| = \\sqrt{'}} = \\sqrt{\\sum_{=1}^p ^2_i}\n\\]Coefficient Multiple Determination\\[\n R^2 = \\frac{SSR}{SSTO}= 1- \\frac{SSE}{SSTO}\n\\]\nAdjusted Coefficient Multiple Determination\\[\nR^2_a = 1 - \\frac{SSE/(n-p)}{SSTO/(n-1)} = 1 - \\frac{(n-1)SSE}{(n-p)SSTO}\n\\]Sequential Partial Sums Squares:regression model coefficients \\(\\beta = (\\beta_0, \\beta_1,...,\\beta_{p-1})'\\), denote uncorrected corrected SS \\[\nSSM = SS(\\beta_0, \\beta_1,...,\\beta_{p-1}) \\\\\nSSM_m = SS(\\beta_0, \\beta_1,...,\\beta_{p-1}|\\beta_0)\n\\]\n2 decompositions \\(SSM_m\\):Sequential SS: (unique -depends order, also referred Type SS, default anova() R)\\[\nSSM_m = SS(\\beta_1 | \\beta_0) + SS(\\beta_2 | \\beta_0, \\beta_1) + ...+ SS(\\beta_{p-1}| \\beta_0,...,\\beta_{p-2})\n\\]**Partial SS: (use practice - contribution given others)\\[\nSSM_m = SS(\\beta_1 | \\beta_0,\\beta_2,...,\\beta_{p-1}) + ... + SS(\\beta_{p-1}| \\beta_0, \\beta_1,...,\\beta_{p-2})\n\\]","code":""},{"path":"linear-regression.html","id":"ols-assumptions","chapter":"5 Linear Regression","heading":"5.1.3 OLS Assumptions","text":"A1 LinearityA2 Full rankA3 Exogeneity Independent VariablesA4 HomoskedasticityA5 Data Generation (random Sampling)A6 Normal Distribution","code":""},{"path":"linear-regression.html","id":"a1-linearity","chapter":"5 Linear Regression","heading":"5.1.3.1 A1 Linearity","text":"\\[\\begin{equation}\nA1: y=\\mathbf{x}\\beta + \\epsilon\n\\tag{5.1}\n\\end{equation}\\]restrictivex can nonlinear transformation including interactions, natural log, quadraticWith A3 (Exogeneity Independent), linearity can restrictive","code":""},{"path":"linear-regression.html","id":"log-model","chapter":"5 Linear Regression","heading":"5.1.3.1.1 Log Model","text":"","code":""},{"path":"linear-regression.html","id":"higher-orders","chapter":"5 Linear Regression","heading":"5.1.3.1.2 Higher Orders","text":"\\(y=\\beta_0 + x_1\\beta_1 + x_1^2\\beta_2 + \\epsilon\\)\\[\n\\frac{\\partial y}{\\partial x_1}=\\beta_1 + 2x_1\\beta_2\n\\]effect \\(x_1\\) y depends level \\(x_1\\)partial effect average = \\(\\beta_1+2E(x_1)\\beta_2\\)Average Partial Effect = \\(E(\\beta_1 + 2x_1\\beta_2)\\)","code":""},{"path":"linear-regression.html","id":"interactions","chapter":"5 Linear Regression","heading":"5.1.3.1.3 Interactions","text":"\\(y=\\beta_0 + x_1\\beta_1 + x_2\\beta_2 + x_1x_2\\beta_3 + \\epsilon\\)\\(\\beta_1\\) average effect y unit change \\(x_1\\) \\(x_2=0\\)\\(\\beta_1 + x_2\\beta_3\\) partial effect \\(x_1\\) y depends level \\(x_2\\)","code":""},{"path":"linear-regression.html","id":"a2-full-rank","chapter":"5 Linear Regression","heading":"5.1.3.2 A2 Full rank","text":"\\[\\begin{equation}\nA2: rank(E(x'x))=k\n\\tag{5.2}\n\\end{equation}\\]also known identification conditioncolumns \\(\\mathbf{x}\\) written linear function columnswhich ensures parameter unique exists population regression equation","code":""},{"path":"linear-regression.html","id":"a3-exogeneity-of-independent-variables","chapter":"5 Linear Regression","heading":"5.1.3.3 A3 Exogeneity of Independent Variables","text":"\\[\\begin{equation}\nA3: E[\\epsilon|x_1,x_2,...,x_k]=E[\\epsilon|\\mathbf{x}]=0\n\\tag{5.3}\n\\end{equation}\\]strict exogeneityalso known mean independence check back Correlation Independenceby Law Iterated Expectations \\(E(\\epsilon)=0\\), can satisfied always including intercept.independent variables carry information prediction \\(\\epsilon\\)A3 implies \\(E(y|x)=x\\beta\\), means conditional mean function must linear function x A1 Linearity","code":""},{"path":"linear-regression.html","id":"a3a","chapter":"5 Linear Regression","heading":"5.1.3.3.1 A3a","text":"Weaker Exogeneity AssumptionExogeneity Independent variablesA3a: \\(E(\\mathbf{x_i'}\\epsilon_i)=0\\)\\(x_i\\) uncorrelated \\(\\epsilon_i\\) Correlation IndependenceWeaker mean independence A3\nA3 implies A3a, reverse\ncausality interpretations\ntest difference\nA3 implies A3a, reverseNo causality interpretationsCannot test difference","code":""},{"path":"linear-regression.html","id":"a4-homoskedasticity","chapter":"5 Linear Regression","heading":"5.1.3.4 A4 Homoskedasticity","text":"\\[\\begin{equation}\nA4: Var(\\epsilon|x)=Var(\\epsilon)=\\sigma^2\n\\tag{5.4}\n\\end{equation}\\]\n* Variation disturbance independent variables","code":""},{"path":"linear-regression.html","id":"a5-data-generation-random-sampling","chapter":"5 Linear Regression","heading":"5.1.3.5 A5 Data Generation (random Sampling)","text":"\\[\\begin{equation}\nA5: {y_i,x_{i1},...,x_{ik-1}: = 1,..., n}\n\\tag{5.5}\n\\end{equation}\\]random samplerandom sample mean samples independent identically distributed (iid) joint distribution \\((y,\\mathbf{x})\\)A3 A4, \nStrict Exogeneity: \\(E(\\epsilon_i|x_1,...,x_n)=0\\). independent variables carry information prediction \\(\\epsilon\\)\nNon-autocorrelation: \\(E(\\epsilon_i\\epsilon_j|x_1,...,x_n)=0\\) error term uncorrelated across draws conditional independent variables \\(\\rightarrow\\) \\(A4: Var(\\epsilon|\\mathbf{X})=Var(\\epsilon)=\\sigma^2I_n\\)\nStrict Exogeneity: \\(E(\\epsilon_i|x_1,...,x_n)=0\\). independent variables carry information prediction \\(\\epsilon\\)Non-autocorrelation: \\(E(\\epsilon_i\\epsilon_j|x_1,...,x_n)=0\\) error term uncorrelated across draws conditional independent variables \\(\\rightarrow\\) \\(A4: Var(\\epsilon|\\mathbf{X})=Var(\\epsilon)=\\sigma^2I_n\\)times series spatial settings, A5 less likely hold.","code":""},{"path":"linear-regression.html","id":"a5a","chapter":"5 Linear Regression","heading":"5.1.3.5.1 A5a","text":"stochastic process \\(\\{x_t\\}_{t=1}^T\\) stationary every collection fo time indices \\(\\{t_1,t_2,...,t_m\\}\\), joint distribution \\[\nx_{t_1},x_{t_2},...,x_{t_m}\n\\]\njoint distribution \\[\nx_{t_1+h},x_{t_2+h},...,x_{t_m+h}\n\\]\\(h \\ge 1\\)joint distribution first ten observation next ten, etc.Independent draws automatically satisfies thisA stochastic process \\(\\{x_t\\}_{t=1}^T\\) weakly stationary \\(x_t\\) \\(x_{t+h}\\) “almost independent” h increases without bounds.\n* two observation far apart “almost independent”Common Weakly Dependent ProcessesMoving Average process order 1 (MA(1))MA(1) means one period lag.\\[\ny_t = u_t + \\alpha_1 u_{t-1} \\\\\nE(y_t) = E(u_t) + \\alpha_1E(u_{t-1}) = 0 \\\\\nVar(y_t) = var(u_t) + \\alpha_1 var(u_{t-1}) = \\sigma^2 + \\alpha_1^2 \\sigma^2 = \\sigma^2(1+\\alpha_1^2)\n\\]\n\\(u_t\\) drawn iid t variance \\(\\sigma^2\\)increase absolute value \\(\\alpha_1\\) increases varianceWhen MA(1) process can inverted (\\(|\\alpha|<1\\) \\[\nu_t = y_t - \\alpha_1u_{t-1}\n\\]\ncalled autoregressive representation (express current observation term past observation).can expand 1 lag, MA(q) process\\[\ny_t = u_t + \\alpha_1 u_{t-1} + ... + \\alpha_q u_{t-q}\n\\]\\(u_t \\sim WN(0,\\sigma^2)\\)Covariance stationary: irrespective value parameters.Invertibility \\(\\alpha < 1\\)conditional mean MA(q) depends q lags (long-term memory).MA(q), autorcorrealtions beyond q 0.\\[\n\\begin{aligned}\nCov(y_t,y_{t-1}) &= Cov(u_t + \\alpha_1 u_{t-1},u_{t-1}+\\alpha_1u_{t-2}) \\\\\n&= \\alpha_1var(u_{t-1}) \\\\\n&= \\alpha_1\\sigma^2\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nCov(y_t,y_{t-2}) &= Cov(u_t + \\alpha_1 u_{t-1},u_{t-2}+\\alpha_{1}u_{t-3}) \\\\\n&= 0\n\\end{aligned}\n\\]MA models linear relationship teh dependent variable teh current past values stochastic term.Auto regressive process order 1 (AR(1))\\[\ny_t = \\rho y_{t-1}+ u_t, |\\rho|<1\n\\]\\(u_t\\) drawn iid t variance \\(\\sigma^2\\)\\[\n\\begin{aligned}\nCov(y_t,y_{t-1}) &= Cov(\\rho y_{t-1} + u-t,y_{t-1}) \\\\\n&= \\rho Var(y_{t-1}) \\\\\n&= \\rho \\frac{\\sigma^2}{1-\\rho^2}\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nCov(y_t,y_{t-h}) &= \\rho^h \\frac{\\sigma^2}{1-\\rho^2}\n\\end{aligned}\n\\]Stationarity:\ncontinuum t, distribution t \\[\nE(y_t) = E(y_{t-1}) = ...= E(y_0) \\\\\ny_1 = \\rho y_0 + u_1\n\\]\ninitial observation \\(y_0=0\\)Assume \\(E(y_t)=0\\)\\[\ny_t = \\rho^t y_{t-t} + \\rho^{t-1}u_1 + \\rho^{t-2}u_2 +...+ \\rho u_{t-1} + u_t \\\\\n= \\rho^t y_0 + \\rho^{t-1}u_1 + \\rho^{t-2}u_2 +...+ \\rho u_{t-1} + u_t\n\\]Hence, \\(y_t\\) weighted \\(u_t\\) time observations .\ny correlated previous observations well future observations.\\[\nVar(y_t) = Var(\\rho y_{t-1} + u_t) \\\\\n= \\rho^2 Var(y_{t-1}) + Var(u_t) + 2\\rho Cov(y_{t-1}u_t) \\\\\n= \\rho^2 Var(y_{t-1}) + \\sigma^2\n\\]\nHence,\\[\nVar(y_t) = \\frac{\\sigma^2}{1-\\rho^2}\n\\]\nVariance constantly time, \\(\\rho \\neq 1\\) \\(-1\\).stationarity requires \\(\\rho \\neq 1\\) -1.\nweakly dependent process \\(|\\rho|<1\\)estimate AR(1) process, use Yule-Walker Equation\\[\ny_t = \\epsilon_t + \\phi y_{t-1} \\\\\ny_t y_{t-\\tau} = \\epsilon_t y_{t-\\tau} + \\phi y_{t-1}y_{t-\\tau} \\\\\n\\]\n\\(\\tau \\ge 1\\), \\[\n\\gamma \\tau = \\phi \\gamma (\\tau -1) \\\\\n\\rho_t = \\phi^t\n\\]\ngeneralize pth order autoregressive process, AR(p):\\[\ny_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} + \\epsilon_t\n\\]AR(p) process covariance stationary, decay autocorrelations.combine MA(q) AR(p), ARMA(p,q) process, can see seasonality. example, ARMA(1,1)\\[\ny_t = \\phi y_{t-1} + \\epsilon_t + \\alpha \\epsilon_{t-1}\n\\]Random Walk process\\[\ny_t = y_0 + \\sum_{s=1}^{t}u_t\n\\]stationary : \\(y_0 = 0\\) \\(E(y_t)= 0\\), \\(Var(y_t)=t\\sigma^2\\). along spectrum, variance largernot weakly dependent: \\(Cov(\\sum_{s=1}^{t}u_s,\\sum_{s=1}^{t-h}u_s) = (t-h)\\sigma^2\\). covariance (fixed) diminishing h increases\\[\nAssumption A5a: \\{y_t,x_{t1},..,x_{tk-1} \\}\n\\]\n\\(t=1,...,T\\) stationary weakly dependent processes.Alternative Weak Law, Central Limit Theorem\n\\(z_t\\) weakly dependent stationary process finite first absolute moment \\(E(z_t) = \\mu\\), \\[\nT^{-1}\\sum_{t=1}^{T}z_t \\^p \\mu\n\\]additional regulatory conditions hold (Greene 1990), \\[\n\\sqrt{T}(\\bar{z}-\\mu) \\^d N(0,B)\n\\]\n\\(B= Var(z_t) + 2\\sum_{h=1}^{\\infty}Cov(z_t,z_{t-h})\\)","code":""},{"path":"linear-regression.html","id":"a6-normal-distribution","chapter":"5 Linear Regression","heading":"5.1.3.6 A6 Normal Distribution","text":"\\[\\begin{equation}\nA6: \\epsilon|\\mathbf{x}\\sim N(0,\\sigma^2I_n)\n\\tag{5.6}\n\\end{equation}\\]error term normally distributedFrom A1-A3, identification (also known Orthogonality Condition) population parameter \\(\\beta\\)\\[\n\\begin{aligned}\ny &= {x}\\beta + \\epsilon && \\text{A1} \\\\\nx'y &= x'x\\beta + x'\\epsilon && \\text{} \\\\\nE(x'y) &= E(x'x)\\beta + E(x'\\epsilon)  && \\text{} \\\\\nE(x'y) &= E(x'x)\\beta && \\text{A3} \\\\\n[E(x'x)]^{-1}E(x'y) &= [E(x'x)]^{-1}E(x'x)\\beta && \\text{A2} \\\\\n[E(x'x)]^{-1}E(x'y) &= \\beta\n\\end{aligned}\n\\]\\(\\beta\\) row vector parameters produces best predictor y\nchoose min \\(\\gamma\\) :\\[\n\\underset{\\gamma}{\\operatorname{argmin}}E((y-x\\gamma)^2)\n\\]\nFirst Order Condition\\[\n\\begin{split}\n\\frac{\\partial((y-x\\gamma)^2)}{\\partial\\gamma}&=0 \\\\\n-2E(x'(y-x\\gamma))&=0 \\\\\nE(x'y)-E(x'x\\gamma) &=0 \\\\\nE(x'y) &= E(x'x)\\gamma \\\\\n(E(x'x))^{-1}E(x'y) &= \\gamma\n\\end{split}\n\\]Second Order Condition\\[\n\\begin{split}\n\\frac{\\partial^2E((y-x\\gamma)^2)}{}&=0 \\\\\nE(\\frac{\\partial(y-x\\partial)^2)}{\\partial\\gamma\\partial\\gamma'}) &= 2E(x'x)\n\\end{split}\n\\]\nA3 holds, \\(2E(x'x)\\) PSD \\(\\rightarrow\\) minimum","code":""},{"path":"linear-regression.html","id":"theorems","chapter":"5 Linear Regression","heading":"5.1.4 Theorems","text":"","code":""},{"path":"linear-regression.html","id":"frisch-waugh-lovell-theorem","chapter":"5 Linear Regression","heading":"5.1.4.1 Frisch-Waugh-Lovell Theorem","text":"\\[\n\\mathbf{y=X\\beta + \\epsilon=X_1\\beta_1+X_2\\beta_2 +\\epsilon}\n\\]Equivalently,\\[\n\\left(\n\\begin{array}\n{cc}\nX_1'X_1 & X_1'X_2 \\\\\nX_2'X_1 & X_2'X_2\n\\end{array}\n\\right)\n\\left(\n\\begin{array}\n{c}\n\\hat{\\beta_1} \\\\\n\\hat{\\beta_2}\n\\end{array}\n\\right)\n=\n\\left(\n\\begin{array}{c}\nX_1'y \\\\\nX_2'y\n\\end{array}\n\\right)\n\\]\nHence,\\[\n\\mathbf{\\hat{\\beta_1}=(X_1'X_1)^{-1}X_1'y - (X_1'X_1)^{-1}X_1'X_2\\hat{\\beta_2}}\n\\]Betas multiple regression betas individual simple regressionDifferent set X affect coefficient estimates.\\(X_1'X_2 = 0\\) \\(\\hat{\\beta_2}=0\\), 1 2 hold.","code":""},{"path":"linear-regression.html","id":"gauss-markov-theorem","chapter":"5 Linear Regression","heading":"5.1.4.2 Gauss-Markov Theorem","text":"linear regression model\\[\n\\mathbf{y=X\\beta + \\epsilon}\n\\]A1, A2, A3, A4, OLS estimator defined \\[\n\\hat{\\beta} = \\mathbf{(X'X)^{-1}X'y}\n\\]\nminimum variance linear (y) unbiased estimator \\(\\beta\\)Let \\(\\tilde{\\beta}=\\mathbf{Cy}\\), another linear estimator \\(\\mathbf{C}\\) k x n function X), unbiased,\\[\n\\begin{split}\nE(\\tilde{\\beta}|\\mathbf{X}) &= E(\\mathbf{Cy|X}) \\\\\n&= E(\\mathbf{CX\\beta + C\\epsilon|X}) \\\\\n&= \\mathbf{CX\\beta}\n\\end{split}\n\\]\nequals true parameter \\(\\beta\\) \\(\\mathbf{CX=}\\)\nEquivalently,\n\\(\\tilde{\\beta} = \\beta + \\mathbf{C}\\epsilon\\)\nvariance estimator \\(Var(\\tilde{\\beta}|\\mathbf{X}) = \\sigma^2\\mathbf{CC'}\\)show minimum variance,\\[\n\\begin{split}\n&=\\sigma^2\\mathbf{(C-(X'X)^{-1}X')(C-(X'X)^{-1}X')'} \\\\\n&= \\sigma^2\\mathbf{(CC' - CX(X'X)^{-1})-(X'X)^{-1}X'C + (X'X)^{-1}X'X(X'X)^{-1})} \\\\\n&= \\sigma^2 (\\mathbf{CC' - (X'X)^{-1}-(X'X)^{-1} + (X'X)^{-1}}) \\\\\n&= \\sigma^2\\mathbf{CC'} - \\sigma^2(\\mathbf{X'X})^{-1} \\\\\n&= Var(\\tilde{\\beta}|\\mathbf{X}) - Var(\\hat{\\beta}|\\mathbf{X})\n\\end{split}\n\\]Hierarchy OLS Assumptions","code":""},{"path":"linear-regression.html","id":"variable-selection","chapter":"5 Linear Regression","heading":"5.1.5 Variable Selection","text":"depends onObjectives goalsPreviously acquired expertiseavailability dataavailability computer softwareLet P - 1 number possible X variables","code":""},{"path":"linear-regression.html","id":"mallowss-c_p-statistic","chapter":"5 Linear Regression","heading":"5.1.5.1 Mallows’s \\(C_p\\) Statistic","text":"(Mallows, 1973, Technometrics, 15, 661-675)measure predictive ability fitted model\nLet \\(\\hat{Y}_{ip}\\) predicted value \\(Y_i\\) using model p parameters. total standardized mean square error prediction :\\[\n\\Gamma_p = \\frac{\\sum_{=1}^n E(\\hat{Y}_{ip}-E(Y_i))^2}{\\sigma^2} \\\\\n= \\frac{\\sum_{=1}^n [E(\\hat{Y}_{ip})-E(Y_i)]^2+\\sum_{=1}^n var(\\hat{Y}_{ip})}{\\sigma^2}\n\\]\nfirst term numerator (bias)^2 term 2nd term prediction variance term.bias term decreases variables added model.assume full model (p=P) true model, \\(E(\\hat{Y}_{ip}) - E(Y_i) = 0\\) bias 0.Prediction variance increase variables added model \\(\\sum var(\\hat{Y}_{ip}) = p \\sigma^2\\)thus, tradeoff bias variance terns achieved minimizing \\(\\Gamma_p\\).Since \\(\\Gamma_p\\) unknown (due \\(\\beta\\)). use estimate: \\(C_p = \\frac{SSE_p}{\\hat{\\sigma^2}}- (n-2p)\\) unbiased estimate \\(\\Gamma_p\\)variables added model, \\(SSE_p\\) decreases 2p increases. \\(\\hat{\\sigma^2}=MSE(X_1,..,X_{P-1})\\) MSE possible X variables model.bias \\(E(C_p) \\approx p\\). Thus, good models \\(C_p\\) close p.Prediction: consider models \\(C_p \\le p\\)Parameter estimation: consider models \\(C_p \\le 2p -(P-1)\\). Fewer variables eliminated model avoid excess bias estimates.","code":""},{"path":"linear-regression.html","id":"akaike-information-criterion-aic","chapter":"5 Linear Regression","heading":"5.1.5.2 Akaike Information Criterion (AIC)","text":"\\[\nAUC = n ln(\\frac{SSE_p}{n}) + 2p\n\\]increasing p (number parameters) leads first-term decreases, second-term increases.want model small values AIC. AIC increases parameter added model, parameter needed.AIC represents tradeoff precision fit number parameters used.","code":""},{"path":"linear-regression.html","id":"bayes-or-schwarz-information-criterion","chapter":"5 Linear Regression","heading":"5.1.5.3 Bayes (or Schwarz) Information Criterion","text":"\\[\nBIC = n \\ln(\\frac{SSE_p}{n})+ (\\ln n)p\n\\]coefficient front p tends penalize heavily models larger number parameters (compared AIC).","code":""},{"path":"linear-regression.html","id":"prrediciton-error-sum-of-squares-press","chapter":"5 Linear Regression","heading":"5.1.5.4 Prrediciton Error Sum of Squares (PRESS)","text":"\\[\nPRESS_p = \\sum_{=1}^{n} (Y_i - \\hat{Y}_{()})^2\n\\]\n\\(\\hat{Y}_{()}\\) prediction -th response -th observation used, obtained model p parameters.evaluates predictive ability postulated model omitting one observation time.want small \\(PRESS_p\\) valuesIt can computationally intensive large p. ","code":""},{"path":"linear-regression.html","id":"best-subsets-algorithm","chapter":"5 Linear Regression","heading":"5.1.5.5 Best Subsets Algorithm","text":"“leap bounds” algorithm (Furnival Wilson 1974) combines comparison SSE different subset models control sequence subset regression computed.Guarantees finding best m subset regressions within subset size less computational burden possible subsets.","code":"\nlibrary(\"leaps\")\nregsubsets()"},{"path":"linear-regression.html","id":"stepwise-selection-procedures","chapter":"5 Linear Regression","heading":"5.1.5.6 Stepwise Selection Procedures","text":"forward stepwise procedure:finds plausible subset sequentially.step, variable added deleted.criterion adding deleting based SSE, \\(R^2\\), T, F-statistic.Note:Instead using exact F-values, computer packages usually specify equivalent “significance” level. example, SLE “significance” level enter, SLS “significance” level stay. SLE SLS guides rather true tests significance.choice SLE SLS represents balancing opposing tendencies. Use large SLE values tends result many predictor variables; models small SLE tend -specified resulting \\(\\sigma^2\\) badly overestimated.choice SLE, can choose 0.05 0.5.SLE > SLS cycling pattern may occur. Although computer packages can detect can stop happens. quick fix: SLS = SLE /2 (Bendel Afifi 1977).SLE < SLS procedure conservative may lead variables low contribution retained.Order variable entry matter.Automated Selection Procedures:Forward selection: idea forward stepwise except doesn’t test variables dropped enter. (good forward stepwise).Backward Elimination: begin variables identifies one smallest F-value dropped.","code":""},{"path":"linear-regression.html","id":"diagnostics-1","chapter":"5 Linear Regression","heading":"5.1.6 Diagnostics","text":"","code":""},{"path":"linear-regression.html","id":"normality-of-errors","chapter":"5 Linear Regression","heading":"5.1.6.1 Normality of errors","text":"use Methods based normal probability plot Methods based empirical cumulative distribution functionor plots ","code":"\ny = 1:100\nx = rnorm(100)\nqqplot(x,y)"},{"path":"linear-regression.html","id":"influential-observationsoutliers","chapter":"5 Linear Regression","heading":"5.1.6.2 Influential observations/outliers","text":"","code":""},{"path":"linear-regression.html","id":"hat-matrix","chapter":"5 Linear Regression","heading":"5.1.6.2.1 Hat matrix","text":"\\[\n\\mathbf{H = X(X'X)^{-1}}\n\\]\n\\(\\mathbf{\\hat{Y}= HY, e = (-H)Y}\\) \\(var(\\mathbf{e}) = \\sigma^2 (\\mathbf{-H})\\)\\(\\sigma^2(e_i) = \\sigma^2 (1-h_{ii})\\), \\(h_{ii}\\) -th element main diagonal \\(\\mathbf{H}\\) (must 0 1).\\(\\sum_{=1}^{n} h_{ii} = p\\)\\(cov(e_i,e_j) = -h_{ii}\\sigma^2\\) \\(\\neq j\\)Estimate: \\(s^2(e_i) = MSE (1-h_{ii})\\)Estimate: \\(\\hat{cov}(e_i,e_j) = -h_{ij}(MSE)\\); model assumption correct, covariance small large data sets.\\(\\mathbf{x}_i = [1 X_{,1} ... X_{,p-1}]'\\) (vector X-values given response), \\(h_{ii} = \\mathbf{x_i'(X'X)^{-1}x_i}\\) (depends relative positions design points \\(X_{,1},...,X_{,p-1}\\))","code":""},{"path":"linear-regression.html","id":"studentized-residuals","chapter":"5 Linear Regression","heading":"5.1.6.2.2 Studentized Residuals","text":"\\[\nr_i = \\frac{e_i}{s(e_i)} \\\\\nr_i \\sim N(0,1)\n\\]\\(s(e_i) = \\sqrt{MSE(1-h_{ii})}\\). \\(r_i\\) called studentized residual standardized residual.can use semi-studentized residual , \\(e_i^*= e_i \\sqrt{MSE}\\). doesn’t take account different variances \\(e_i\\).want see model without particular value. delete -th case, fit regression remaining n-1 cases, get estimated responses -th case, \\(\\hat{Y}_{()}\\), find difference, called deleted residual:\\[\nd_i = Y_i - \\hat{Y}_{()} \\\\\n= \\frac{e_i}{1-h_{ii}} \n\\]\ndon’t need recompute regression model caseAs \\(h_{ii}\\) increases, \\(d_i\\) increases.\\[\ns^2(d_i)= \\frac{MSE_{()}}{1-h_{ii}}\n\\]\n\\(MSE_{()}\\) mean square error -th case omitted.Let\\[\nt_i = \\frac{d_i}{s(d_i)} = \\frac{e_i}{\\sqrt{MSE_{()}(1-h_{ii})}}\n\\]studentized deleted residual, follows t-distribution n-p-1 df.\\[\n(n-p)MSE = (n-p-1)MSE_{()}+ \\frac{e^2_{}}{1-h_{ii}}\n\\]\nhence, need fit regressions case \\[\nt_i = e_i (\\frac{n-p-1}{SSE(1-h_{ii})-e^2_i})^{1/2}\n\\]outlying Y-observations cases whose studentized deleted residuals large absolute value. many residuals consider, Bonferroni critical value can can (\\(t_{1-\\alpha/2n;n-p-1}\\))Outlying X ObservationsRecall, \\(0 \\le h_{ii} \\le 1\\) \\(\\sum_{=1}^{n}h_{ii}=p\\) (total number parameters)large \\(h_{ii}\\) indicates -th case distant center X observations (leverage -th case). , large value suggests observation exercises substantial leverage determining fitted value \\(\\hat{Y}_i\\)\\(\\mathbf{\\hat{Y}=HY}\\), linear combination Y-values; \\(h_{ii}\\) weight observation \\(Y_i\\); \\(h_{ii}\\) measures role X values determining important \\(Y_i\\) affecting \\(\\hat{Y}_i\\).Large \\(h_{ii}\\) implies \\(var(e_i)\\) small, larger \\(h_{ii}\\) implies \\(\\hat{Y}_i\\) close \\(Y_i\\)small data sets: \\(h_{ii} > .5\\) suggests “large.”large data sets: \\(h_{ii} > \\frac{2p}{n}\\) \"large.Using hat matrix identify extrapolation:Let \\(\\mathbf{x_{new}}\\) vector containing X values inference mean response new observation made.Let \\(\\mathbf{X}\\) data design matrix used fit data. , \\(h_{new,new} = \\mathbf{x_{new}(X'X)^{-1}x_{new}}\\) within range leverage values (\\(h_{ii}\\)) cases data set, extrapolation involved; otherwise; extrapolation indicated.Identifying Influential Cases:influential mean exclusion observation causes major changes int fitted regression. (outliers influential)Influence Single Fitted Values: DFFITSInfluence Fitted Values: Cook’s DInfluence Regression Coefficients: DFBETAS","code":""},{"path":"linear-regression.html","id":"dffits","chapter":"5 Linear Regression","heading":"5.1.6.2.3 DFFITS","text":"Influence Single Fitted Values: DFFITS\\[\n(DFFITS)_i = \\frac{\\hat{Y}_i - \\hat{Y}_{()}}{\\sqrt{MSE_{()}h_{ii}}} \\\\\n= t_i (\\frac{h_{ii}}{1-h_{ii}})^{1/2}\n\\]standardized difference -th fitted value observations -th case removed.studentized deleted residual multiplied factor function fo -th leverage value.influence :\nsmall medium data sets: \\(|DFFITS|>1\\)\nlarge data sets: \\(|DFFITS|> 2 \\sqrt{p/n}\\)\nsmall medium data sets: \\(|DFFITS|>1\\)large data sets: \\(|DFFITS|> 2 \\sqrt{p/n}\\)","code":""},{"path":"linear-regression.html","id":"cooks-d","chapter":"5 Linear Regression","heading":"5.1.6.2.4 Cook’s D","text":"Influence Fitted Values: Cook’s D\\[\nD_i = \\frac{\\sum_{j=1}^{n}(\\hat{Y}_j - \\hat{Y}_{j()})^2}{p(MSE)} \\\\\n= \\frac{e^2_i}{p(MSE)}(\\frac{h_{ii}}{(1-h_{ii})^2})\n\\]\ngives influence -th case fitted values.\\(e_i\\) increases \\(h_{ii}\\) increases, \\(D_i\\) increases.\\(D_i\\) percentile \\(F_{(p,n-p)}\\) distribution. percentile greater \\(.5(50\\%)\\) -th case major influence. practice, \\(D_i >4/n\\), -th case major influence.","code":""},{"path":"linear-regression.html","id":"dfbetas","chapter":"5 Linear Regression","heading":"5.1.6.2.5 DFBETAS","text":"Influence Regression Coefficients: DFBETAS\\[\n(DFBETAS)_{k()} = \\frac{b_k - b_{k()}}{\\sqrt{MSE_{()}c_{kk}}}\n\\]\\(k = 0,...,p-1\\) \\(c_{kk}\\) si k-th diagonal element \\(\\mathbf{X'X}^{-1}\\)Influence -th case regression coefficient \\(b_k\\) (k=0,..,p-1) difference estimated regression coefficients based n cases regression coefficients obtained -th case omitted (\\(b_{k()}\\))small data sets: \\(|DFBETA|>1\\)large data sets: \\(|DFBETA| > 2\\sqrt{n}\\)Sign DFBETA inculcates whether inclusion case leads increase decrease estimates regression coefficient.","code":""},{"path":"linear-regression.html","id":"collinearity","chapter":"5 Linear Regression","heading":"5.1.6.3 Collinearity","text":"Multicollinearity refers correlation among explanatory variables.large changes estimated regression coefficient predictor variable added deleted, observation altered deleted.noninsignificant results individual tests regression coefficients important predictor variables.estimated regression coefficients algebraic sign opposite expected theoretical consideration prior experience.large coefficients simple correlation pairs predictor variables correlation matrix.wide confidence intervals regression coefficients representing important predictor variables.X variables highly correlated inverse \\((X'X)^{-1}\\) exist computationally unstable.Correlated Predictor Variables: X variables “perfectly” correlated, system undetermined infinite number models fit data. , \\(X'X\\) singular, \\((X'X)^{-1}\\) doesn’t exist. ,parameters interpreted (\\(\\mathbf{b = (X'X)^{-1}X'y}\\))sampling variability infinite (\\(\\mathbf{s^2(b) = MSE (X'X)^{-1}}\\))","code":""},{"path":"linear-regression.html","id":"vifs","chapter":"5 Linear Regression","heading":"5.1.6.3.1 VIFs","text":"Let \\(R^2_k\\) coefficient multiple determination \\(X_k\\) regressed p - 2 X variables model. ,\\[\nVIF_k = \\frac{1}{1-R^2_k}\n\\]large values indicate near collinearity causing variance \\(b_k\\) inflated, \\(var(b_k) \\propto \\sigma^2 (VIF_k)\\)typically, \\(VIF_k > 10\\) indicates collinearity problem result poor parameters estimates.mean VIF’s provide estimate ratio true multicollinearity model X variables uncorrelatedserious multicollinearity \\(avg(VIF) >>1\\)","code":""},{"path":"linear-regression.html","id":"condition-number","chapter":"5 Linear Regression","heading":"5.1.6.3.2 Condition Number","text":"Condition Numberspectral decomposition\\[\n\\mathbf{X'X}= \\sum_{=1}^{p} \\lambda_i \\mathbf{u_i u_i'}\n\\]\n\\(\\lambda_i\\) eigenvalue \\(\\mathbf{u}_i\\) eigenvector. \\(\\lambda_1 > ...>\\lambda_p\\) eigenvecotrs orthogonal:\\[\n\\begin{cases}\n\\mathbf{u_i'u_j} =\n0&\\text{$\\neq j$}\\\\\n1&\\text{$=j$}\\\\\n\\end{cases}\n\\]condition number \\[\nk = \\sqrt{\\frac{\\lambda_{max}}{\\lambda_{min}}}\n\\]values \\(k>30\\) cause concernvalues \\(30<k<100\\) imply moderate dependencies.values \\(k>100\\) imply strong collinearityCondition index\\[\n\\delta_i = \\sqrt{\\frac{\\lambda_{max}}{\\lambda_i}}\n\\]\n= 1,…,pwe can find proportion total variance associated k-th regression coefficient -th eigenmode:\\[\n\\frac{u_{ik}^2/\\lambda_i}{\\sum_j (u^2_{jk}/\\lambda_j)}\n\\]\nvariance proportions can helpful identifying serious conllienaritythe condition index must largethe variance proportions must large (>,5) least two regression coefficients.","code":""},{"path":"linear-regression.html","id":"constancy-of-error-variance","chapter":"5 Linear Regression","heading":"5.1.6.4 Constancy of Error Variance","text":"","code":""},{"path":"linear-regression.html","id":"brown-forsythe-test-modified-levene-test","chapter":"5 Linear Regression","heading":"5.1.6.4.1 Brown-Forsythe Test (Modified Levene Test)","text":"depend normalityApplicable error variance increases decreases Xrelatively large sample size needed (can ignore dependency residuals)Split residuals 2 groups (\\(e_{i1}, = 1, ..., n_1; e_{i2}, j=1,...,n_2\\))Let \\(d_{i1}= |e_{i1}-\\tilde{e}_{1}|\\) \\(\\tilde{e}_{1}\\) median group 1.Let \\(d_{j2}=|e_{j2}-\\tilde{e}_{2}|\\)., 2-sample t-test:\\[\nt_L = \\frac{\\bar{d}_1 - \\bar{d}_2}{s\\sqrt{1/n_1+1/n_2}}\n\\]\n\n\\[\ns^2 = \\frac{\\sum_i(d_{i1}-\\bar{d}_1)^2+\\sum_j(d_{j2}-\\bar{d}_2)^2}{n-2}\n\\]\n\\(|t_L|>t_{1-\\alpha/2;n-2}\\) conclude error variance constant.","code":""},{"path":"linear-regression.html","id":"breusch-pagan-test-cook-weisberg-test","chapter":"5 Linear Regression","heading":"5.1.6.4.2 Breusch-Pagan Test (Cook-Weisberg Test)","text":"Assume error terms independent normally distributed, \\[\n\\sigma^2_i = \\gamma_0 + \\gamma_1 X_i\n\\]Constant error variance corresponds \\(\\gamma_1 = 0\\), .e., test\\(H_0: \\gamma_1 =0\\)\\(H_1: \\gamma_1 \\neq 0\\)regressing squared residuals X usual manner. Obtain regression sum squares : SSR* (SSR regression \\(e^2_i\\) \\(X_i\\)). , define\\[\nX^2_{BP} = \\frac{SSR*/2}{(SSE/n)^2}\n\\]SSE error sum squares regression Y X.\\(H_0: \\gamma_1 = 0\\) holds n reasonably large, \\(X^2_{BP}\\) follows approximately \\(\\chi^2\\) distribution 1 d.f. reject \\(H_0\\) (Homogeneous variance) \\(X^2_{BP} > \\chi^2_{1-\\alpha;1}\\)","code":""},{"path":"linear-regression.html","id":"independence","chapter":"5 Linear Regression","heading":"5.1.6.5 Independence","text":"","code":""},{"path":"linear-regression.html","id":"plots","chapter":"5 Linear Regression","heading":"5.1.6.5.1 Plots","text":"","code":""},{"path":"linear-regression.html","id":"durbin-watson","chapter":"5 Linear Regression","heading":"5.1.6.5.2 Durbin-Watson","text":"","code":""},{"path":"linear-regression.html","id":"time-series","chapter":"5 Linear Regression","heading":"5.1.6.5.3 Time-series","text":"","code":""},{"path":"linear-regression.html","id":"spatial-statistics","chapter":"5 Linear Regression","heading":"5.1.6.5.4 Spatial Statistics","text":"","code":""},{"path":"linear-regression.html","id":"model-validation","chapter":"5 Linear Regression","heading":"5.1.7 Model Validation","text":"split data 2 groups: training (model building) sample validation (prediction) sample.model MSE tend underestimate inherent variability making future predictions. consider actual predictive ability, consider mean squared prediction error (MSPE):\\[\nMSPE = \\frac{\\sum_{=1}^{n} (Y_i- \\hat{Y}_i)^2}{n^*}\n\\]\n\\(Y_i\\) known value response variable -th validation case.\n\\(\\hat{Y}_i\\) predicted value based model fit training data set.\n\\(n^*\\) number cases validation set.\n\\(Y_i\\) known value response variable -th validation case.\\(\\hat{Y}_i\\) predicted value based model fit training data set.\\(n^*\\) number cases validation set.want MSPE close MSE (MSE biased); look ratio MSPE / MSE (closer 1, better).","code":""},{"path":"linear-regression.html","id":"finite-sample-properties","chapter":"5 Linear Regression","heading":"5.1.8 Finite Sample Properties","text":"n fixedBias average, close estimate true value\n\\(Bias = E(\\hat{\\beta}) -\\beta\\) \\(\\beta\\) true parameter value \\(\\hat{\\beta}\\) estimator \\(\\beta\\)\nestimator unbiased \n\\(Bias = E(\\hat{\\beta}) -\\beta = 0\\) \\(E(\\hat{\\beta})=\\beta\\)\nmeans estimator produce estimates , average, equal value trying estimate\n\n\\(Bias = E(\\hat{\\beta}) -\\beta\\) \\(\\beta\\) true parameter value \\(\\hat{\\beta}\\) estimator \\(\\beta\\)estimator unbiased \n\\(Bias = E(\\hat{\\beta}) -\\beta = 0\\) \\(E(\\hat{\\beta})=\\beta\\)\nmeans estimator produce estimates , average, equal value trying estimate\n\\(Bias = E(\\hat{\\beta}) -\\beta = 0\\) \\(E(\\hat{\\beta})=\\beta\\)means estimator produce estimates , average, equal value trying estimateDistribution estimator: estimator function random variables (data)Standard Deviation: spread estimator.OLSUnder A1 A2 A3, OLS unbiased\\[\n\\begin{aligned}\nE(\\hat{\\beta}) &= E(\\mathbf{(X'X)^{-1}X'y}) && \\text{A2}\\\\\n     &= E(\\mathbf{(X'X)^{-1}X'(X\\beta + \\epsilon)}) && \\text{A1}\\\\\n     &= E(\\mathbf{(X'X)^{-1}X'X\\beta + (X'X)^{-1}X'\\epsilon})  && \\text{} \\\\\n     &= E(\\beta + \\mathbf{(X'X)^{-1}X'\\epsilon}) \\\\\n     &= \\beta + E(\\mathbf{(X'X^{-1}\\epsilon)}) \\\\\n     &= \\beta + E(E((\\mathbf{X'X)^{-1}X'\\epsilon|X})) &&\\text{LIE} \\\\\n     &= \\beta + E((\\mathbf{X'X)^{-1}X'}E\\mathbf{(\\epsilon|X})) \\\\\n     &= \\beta + E((\\mathbf{X'X)^{-1}X'}0)) && \\text{A3} \\\\\n     &= \\beta\n\\end{aligned}\n\\]LIE stands Law Iterated ExpectationIf A3 hold, OLS biasedFrom Frisch-Waugh-Lovell Theorem, omitted variable \\(\\hat{\\beta}_2 \\neq 0\\) \\(\\mathbf{X_1'X_2} \\neq 0\\), omitted variable cause OLS estimator biased.A1 A2 A3 A4, conditional variance OLS estimator follows]\\[\n\\begin{aligned}\nVar(\\hat{\\beta}|\\mathbf{X}) &= Var(\\beta + \\mathbf{(X'X)^{-1}X'\\epsilon|X}) && \\text{A1-A2}\\\\\n    &= Var((\\mathbf{X'X)^{-1}X'\\epsilon|X)} \\\\\n    &= \\mathbf{X'X^{-1}X'} Var(\\epsilon|\\mathbf{X})\\mathbf{X(X'X)^{-1}} \\\\\n    &= \\mathbf{X'X^{-1}X'} \\sigma^2I \\mathbf{X(X'X)^{-1}} && \\text{A4} \\\\\n    &= \\sigma^2\\mathbf{X'X^{-1}X'} \\mathbf{X(X'X)^{-1}} \\\\\n    &= \\sigma^2\\mathbf{(X'X)^{-1}}\n\\end{aligned}\n\\]Sources variation\\(\\sigma^2=Var(\\epsilon_i|\\mathbf{X})\\)\namount unexplained variation \\(\\epsilon_i\\) large relative explained \\(\\mathbf{x_i \\beta}\\) variation\namount unexplained variation \\(\\epsilon_i\\) large relative explained \\(\\mathbf{x_i \\beta}\\) variation“Small” \\(Var(x_{i1}), Var(x_{i1}),..\\)\nlot variation \\(\\mathbf{X}\\) (information)\nsmall sample size\nlot variation \\(\\mathbf{X}\\) (information)small sample size“Strong” correlation explanatory variables\n\\(x_{i1}\\) highly correlated linear combination 1, \\(x_{i2}\\), \\(x_{i3}\\), …\ninclude many irrelevant variables contribute .\n\\(x_1\\) perfectly determined regression \\(\\rightarrow\\) Perfect Collinearity \\(\\rightarrow\\) A2 violated.\n\\(x_1\\) highly correlated linear combination variables, Multicollinearity\n\\(x_{i1}\\) highly correlated linear combination 1, \\(x_{i2}\\), \\(x_{i3}\\), …include many irrelevant variables contribute .\\(x_1\\) perfectly determined regression \\(\\rightarrow\\) Perfect Collinearity \\(\\rightarrow\\) A2 violated.\\(x_1\\) highly correlated linear combination variables, Multicollinearity","code":""},{"path":"linear-regression.html","id":"check-for-multicollinearity","chapter":"5 Linear Regression","heading":"5.1.8.1 Check for Multicollinearity","text":"Variance Inflation Factor (VIF)\nRule thumb \\(VIF \\ge 10\\) large\\[\nVIF = \\frac{1}{1-R_1^2} \n\\]","code":""},{"path":"linear-regression.html","id":"standard-errors","chapter":"5 Linear Regression","heading":"5.1.8.2 Standard Errors","text":"\\(Var(\\hat{\\beta}|\\mathbf{X})=\\sigma^2\\mathbf{(X'X)^{-1}}\\) variance estimate \\(\\hat{\\beta}\\)Standard Errors estimators/estimates standard deviation (square root variance) estimator \\(\\hat{\\beta}\\)A1-A5, can estimate \\(\\sigma^2=Var(\\epsilon^2|\\mathbf{X})\\) standard errors \\[\ns^2 = \\frac{1}{n-k}\\sum_{=1}^{n}e_i^2 \\\\\n= \\frac{1}{n-k}SSR\n\\]degrees freedom adjustment: \\(e_i \\neq \\epsilon_i\\) estimated using k estimates \\(\\beta\\), lose degrees freedom variance estimate.\\(s=\\sqrt{s^2}\\) biased estimator standard deviation ([Jensen’s Inequality])Standard Errors \\(\\hat{\\beta}\\)\\[\nSE(\\hat{\\beta}_{j-1})=s\\sqrt{[(\\mathbf{X'X})^{-1}]_{jj}} \\\\\n= \\frac{s}{\\sqrt{SST_{j-1}(1-R_{j-1}^2)}}\n\\]\n\\(SST_{j-1}\\) \\(R_{j-1}^2\\) following regression\\(x_{j-1}\\) 1, \\(x_1\\),… \\(x_{j-2}\\),\\(x_j\\),\\(x_{j+1}\\), …, \\(x_{k-1}\\)Summary Finite Sample PropertiesUnder A1-A3: OLS unbiasedUnder A1-A4: variance OLS estimator \\(Var(\\hat{\\beta}|\\mathbf{X})=\\sigma^2\\mathbf{(X'X)^{-1}}\\)A1-A4, A6: OLS estimator \\(\\hat{\\beta} \\sim N(\\beta,\\sigma^2\\mathbf{(X'X)^{-1}})\\)A1-A4, Gauss-Markov Theorem holds \\(\\rightarrow\\) OLS BLUEUnder A1-A5, standard errors unbiased estimator standard deviation \\(\\hat{\\beta}\\)","code":""},{"path":"linear-regression.html","id":"large-sample-properties","chapter":"5 Linear Regression","heading":"5.1.9 Large Sample Properties","text":"let \\(n \\rightarrow \\infty\\)perspective allows us evaluate “quality” estimators finite sample properties informative, impossible computeconsistency, asymptotic distribution, asymptotic varianceMotivationFinite Sample Properties need strong assumption A1 A3 A4 A6Other estimation GLS, MLE need analyzed using Large Sample PropertiesLet \\(\\mu(\\mathbf{X})=E(y|\\mathbf{X})\\) Conditional Expectation Function\\(\\mu(\\mathbf{X})\\) minimum mean squared predictor (possible functions)\\[\nminE((y-f(\\mathbf{X}))^2)\n\\]A1 A3,\\[\n\\mu(\\mathbf{X})=\\mathbf{X}\\beta\n\\]linear projection\\[\nL(y|1,\\mathbf{X})=\\gamma_0 + \\mathbf{X}Var(X)^{-1}Cov(X,Y)\n\\]\\(\\mathbf{X}Var(X)^{-1}Cov(X,Y)=\\gamma\\)minimum mean squared linear approximation conditional mean function\\[\n(\\gamma_0,\\gamma) = arg min E((E(y|\\mathbf{X})-(+\\mathbf{Xb})^2)\n\\]OLS always consistent linear projection, necessarily unbiased.Linear projection causal interpretationLinear projection depend assumption A1 A3Evaluating estimator using large sample properties:Consistency: measure centralityLimiting Distribution: shape scaled estimator sample size increasesAsymptotic variance: spread estimator regards limiting distribution.estimator \\(\\hat{\\theta}\\) consistent \\(\\theta\\) \\(\\hat{\\theta}_n \\^p \\theta\\)n increases, estimator converges population parameter value.Unbiased imply consistency consistency imply unbiased.Based Weak Law Large Numbers\\[\n\\begin{split}\n\\hat{\\beta} &= \\mathbf{(X'X)^{-1}X'y} \\\\\n&= \\mathbf{(\\sum_{=1}^{n}x_i'x_i)^{-1} \\sum_{=1}^{n}x_i'y_i} \\\\\n&= (n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'x_i)^{-1}} n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'y_i}\n\\end{split}\n\\]\\[\n\\begin{split}\nplim(\\hat{\\beta}) &= plim((n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'x_i)^{-1}} n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'y_i}) \\\\\n&= plim((n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'x_i)^{-1}})plim(n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'y_i}) \\\\\n&= (plim(n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'x_i)^{-1}})plim(n^{-1}\\mathbf{\\sum_{=1}^{n}x_i'y_i}) \\text{ due A2, A5} \\\\\n&= E(\\mathbf{x_i'x_i})^{-1}E(\\mathbf{x_i'y_i})\n\\end{split}\n\\]\\[\nE(\\mathbf{x_i'x_i})^{-1}E(\\mathbf{x_i'y_i}) = \\beta + E(\\mathbf{x_i'x_i})^{-1}E(\\mathbf{x_i'\\epsilon_i})\n\\]A1, A2, A3a, A5 OLS consistent, guarantee unbiased.A1, A2, A3a, A5, \\(\\mathbf{x_i'x_i}\\) finite first second moments (CLT), \\(Var(\\mathbf{x_i'}\\epsilon_i)=\\mathbf{B}\\)\\((n^{-1}\\sum_{=1}^{n}\\mathbf{x_i'x_i})^{-1} \\^p (E(\\mathbf{x'_ix_i}))^{-1}\\)\\(\\sqrt{n}(n^{-1}\\sum_{=1}^{n}\\mathbf{x_i'}\\epsilon_i) \\^d N(0,\\mathbf{B})\\)\\[\n\\sqrt{n}(\\hat{\\beta}-\\beta) = (n^{-1}\\sum_{=1}^{n}\\mathbf{x_i'x_i})^{-1}\\sqrt{n}(n^{-1}\\sum_{=1}^{n}\\mathbf{x_i'x_i}) \\^{d} N(0,\\Sigma)\n\\]\n\\(\\Sigma=(E(\\mathbf{x_i'x_i}))^{-1}\\mathbf{B}(E(\\mathbf{x_i'x_i}))^{-1}\\)holds A3aDo need A4 A6 apply CLT\nA4 hold, \\(\\mathbf{B}=Var(\\mathbf{x_i'}\\epsilon_i)=\\sigma^2E(x_i'x_i)\\) means \\(\\Sigma=\\sigma^2(E(\\mathbf{x_i'x_i}))^{-1}\\), use standard errors\nA4 hold, \\(\\mathbf{B}=Var(\\mathbf{x_i'}\\epsilon_i)=\\sigma^2E(x_i'x_i)\\) means \\(\\Sigma=\\sigma^2(E(\\mathbf{x_i'x_i}))^{-1}\\), use standard errorsHeteroskedasticity can fromLimited dependent variableDependent variables large/skewed rangesSolving Asymptotic Variance\\[\n\\begin{aligned}\n\\Sigma &= (E(\\mathbf{x_i'x_i}))^{-1}\\mathbf{B}(E(\\mathbf{x_i'x_i}))^{-1} \\\\\n&= (E(\\mathbf{x_i'x_i}))^{-1}Var(\\mathbf{x_i'}\\epsilon_i)(E(\\mathbf{x_i'x_i}))^{-1} \\\\\n&= (E(\\mathbf{x_i'x_i}))^{-1}E[(\\mathbf{x_i'}\\epsilon_i-0)(\\mathbf{x_i'}\\epsilon_i-0)](E(\\mathbf{x_i'x_i}))^{-1} & \\text{A3a} \\\\\n&= (E(\\mathbf{x_i'x_i}))^{-1}E[E(\\mathbf{\\epsilon_i^2|x_i)x_i'x_i]}(E(\\mathbf{x_i'x_i}))^{-1} & \\text{LIE} \\\\\n&= (E(\\mathbf{x_i'x_i}))^{-1}\\sigma^2E(\\mathbf{x_i'x_i})(E(\\mathbf{x_i'x_i}))^{-1} & \\text{A4} \\\\\n&= \\sigma^2(E(\\mathbf{x_i'x_i}))\n\\end{aligned}\n\\]A1, A2, A3a, A4, A5:\\[\n\\sqrt{n}(\\hat{\\beta}-\\beta) \\^d N(0,\\sigma^2(E(\\mathbf{x_i'x_i}))^{-1})\n\\]Asymptotic variance approximation variance scaled random variable \\(\\sqrt{n}(\\hat{\\beta}-\\beta)\\) n large.use \\(Avar(\\sqrt{n}(\\hat{\\beta}-\\beta))/n\\) approximation finite sample variance large n:\\[\nAvar(\\sqrt{n}(\\hat{\\beta}-\\beta)) \\approx Var(\\sqrt{n}(\\hat{\\beta}-\\beta)) \\\\\nAvar(\\sqrt{n}(\\hat{\\beta}-\\beta))/n \\approx Var(\\sqrt{n}(\\hat{\\beta}-\\beta))/n = Var(\\hat{\\beta})\n\\]Avar(.) behave way Var(.)\\[\nAvar(\\sqrt{n}(\\hat{\\beta}-\\beta))/n \\neq Avar(\\sqrt{n}(\\hat{\\beta}-\\beta)/\\sqrt{n}) \\\\\n\\neq Avar(\\hat{\\beta})\n\\]Finite Sample Properties, calculate standard errors estimate conditional standard deviation:\\[\nSE_{fs}(\\hat{\\beta}_{j-1})=\\sqrt{\\hat{Var}}(\\hat{\\beta}_{j-1}|\\mathbf{X}) = \\sqrt{s^2[\\mathbf{(X'X)}^{-1}]_{jj}}\n\\]Large Sample Properties, calculate standard errors estimate square root asymptotic variance\\[\nSE_{ls}(\\hat{\\beta}_{j-1})=\\sqrt{\\hat{Avar}(\\sqrt{n}\\hat{\\beta}_{j-1})/n} = \\sqrt{s^2[\\mathbf{(X'X)}^{-1}]_{jj}}\n\\]Hence, standard error estimator finite sample large sample.\n* estimator, conceptually estimating two different things.\n* Valid weaker assumptions: assumptions needed produce consistent estimator finite sample conditional variance (A1-A5) stronger needed produce consistent estimator asymptotic variance (A1,A2,A3a,A4,A5)Suppose \\(y_1,...,y_n\\) random sample population mean \\(\\mu\\) variance-covariance matrix \\(\\Sigma\\)\\(\\bar{y} = \\frac{1}{n}\\sum_{=1}^{n} y_i\\) consistent estimator \\(\\mu\\)\\(S = \\frac{1}{n-1}\\sum_{=1}^{n} (y_i -\\bar{y})(y_i-\\bar{y})'\\) consistent estimator \\(\\Sigma\\).Multivariate Central limit Theorem: Similar univariate case, \\(\\sqrt{n}(\\bar{y}-\\mu) \\sim N_p(0,\\Sigma)\\), n large relative p (e.g., \\(n \\ge 25p\\)). Equivalently, \\(\\bar{y} \\sim N_p(\\mu,\\Sigma/n)\\).Wald’s Theorem: \\(n(\\bar{y} - \\mu)'S^{-1}(\\bar{y}-\\mu) \\sim \\chi^2_{(p)}\\) n large relative p. ","code":""},{"path":"linear-regression.html","id":"application","chapter":"5 Linear Regression","heading":"5.1.10 Application","text":"","code":""},{"path":"linear-regression.html","id":"feasible-generalized-least-squares","chapter":"5 Linear Regression","heading":"5.2 Feasible Generalized Least Squares","text":"Motivation efficient estimatorGauss-Markov Theorem holds A1-A4A4: \\(Var(\\epsilon| \\mathbf{X} )=\\sigma^2I_n\\)\nHeteroskedasticity: \\(Var(\\epsilon_i|\\mathbf{X}) \\neq \\sigma^2I_n\\)\nSerial Correlation: \\(Cov(\\epsilon_i,\\epsilon_j|\\mathbf{X}) \\neq 0\\)\nHeteroskedasticity: \\(Var(\\epsilon_i|\\mathbf{X}) \\neq \\sigma^2I_n\\)Serial Correlation: \\(Cov(\\epsilon_i,\\epsilon_j|\\mathbf{X}) \\neq 0\\)Without A4, can know unbiased estimator efficient?Original (unweighted) model:\\[\n\\mathbf{y=X\\beta+ \\epsilon}\n\\]\nSuppose A1-A3 hold, A4 hold,\n\\[\n\\mathbf{Var(\\epsilon|X)=\\Omega \\neq \\sigma^2 I_n}\n\\]try use OLS estimate transformed (weighted) model\\[\n\\mathbf{wy=wX\\beta + w\\epsilon}\n\\]\nneed choose \\(\\mathbf{w}\\) \\[\n\\mathbf{w'w = \\Omega^{-1}}\n\\]\n\\(\\mathbf{w}\\) (full-rank matrix) Cholesky decomposition \\(\\mathbf{\\Omega^{-1}}\\) (full-rank matrix)words, \\(\\mathbf{w}\\) squared root \\(\\Omega\\) (squared root version matrix)\\[\n\\Omega = var(\\epsilon | X) \\\\\n\\Omega^{-1} = var(\\epsilon | X)^{-1}\n\\], transformed equation (IGLS) following properties.\\[\n\\begin{split}\n\\mathbf{\\hat{\\beta}_{IGLS}} &= \\mathbf{(X'w'wX)^{-1}X'w'wy} \\\\\n& = \\mathbf{(X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1}y} \\\\\n& = \\mathbf{\\beta + X'\\Omega^{-1}X'\\Omega^{-1}\\epsilon}\n\\end{split}\n\\]Since A1-A3 hold unweighted model\\[\n\\begin{aligned}\n\\mathbf{E(\\hat{\\beta}_{IGLS}|X)} & = E(\\mathbf{\\beta + (X'\\Omega^{-1}X'\\Omega^{-1}\\epsilon)}|X)\\\\\n& = \\mathbf{\\beta + E(X'\\Omega^{-1}X'\\Omega^{-1}\\epsilon)|X)} \\\\\n& = \\mathbf{\\beta + X'\\Omega^{-1}X'\\Omega^{-1}E(\\epsilon|X)}  && \\text{since A3}: E(\\epsilon|X)=0 \\\\\n& = \\mathbf{\\beta}\n\\end{aligned}\n\\]\\(\\rightarrow\\) IGLS estimator unbiased\\[\n\\begin{aligned}\n\\mathbf{Var(w\\epsilon|X)} &= \\mathbf{wVar(\\epsilon|X)w'} \\\\\n& = \\mathbf{w\\Omega w'} \\\\\n& = \\mathbf{w(w'w)^{-1}w'} && \\text{since w full-rank matrix}\\\\\n& = \\mathbf{ww^{-1}(w')^{-1}w'} \\\\\n& = \\mathbf{I_n}\n\\end{aligned}\n\\]\\(\\rightarrow\\) A4 holds transformed (weighted) equationThen, variance estimator \\[\n\\begin{aligned}\nVar(\\hat{\\beta}_{IGLS}|\\mathbf{X}) & = \\mathbf{Var(\\beta + (X'\\Omega ^{-1}X)^{-1}X'\\Omega^{-1}\\epsilon|X)} \\\\\n&= \\mathbf{Var((X'\\Omega ^{-1}X)^{-1}X'\\Omega^{-1}\\epsilon|X)} \\\\\n&= \\mathbf{(X'\\Omega ^{-1}X)^{-1}X'\\Omega^{-1} Var(\\epsilon|X)   \\Omega^{-1}X(X'\\Omega ^{-1}X)^{-1}} && \\text{A4 holds}\\\\\n&= \\mathbf{(X'\\Omega ^{-1}X)^{-1}X'\\Omega^{-1} \\Omega \\Omega^{-1} \\Omega^{-1}X(X'\\Omega ^{-1}X)^{-1}} \\\\\n&= \\mathbf{(X'\\Omega ^{-1}X)^{-1}}\n\\end{aligned}\n\\]Let \\(= \\mathbf{(X'X)^{-1}X'-(X'\\Omega ^{-1} X)X' \\Omega^{-1}}\\) \n\\[\nVar(\\hat{\\beta}_{OLS}|X)- Var(\\hat{\\beta}_{IGLS}|X) = \\Omega '\n\\]\n\\(\\Omega\\) Positive Semi Definite, \\(\\Omega '\\) also PSD, IGLS efficientThe name Infeasible comes fact impossible compute estimator.\\[\n\\mathbf{w} = \n\\left(\n\\begin{array}{ccccc}\nw_{11} & 0 & 0 & ... & 0 \\\\\nw_{21} & w_{22} & 0 & ... & 0 \\\\\nw_{31} & w_{32} & w_{33} & ... & ... \\\\\nw_{n1} & w_{n2} & w_{n3} & ... & w_{nn} \\\\\n\\end{array}\n\\right)\n\\]\\(n(n+1)/2\\) number elements n observations \\(\\rightarrow\\) infeasible estimate. (number equation > data)Hence, need make assumption \\(\\Omega\\) make feasible estimate \\(\\mathbf{w}\\):Heteroskedasticity : multiplicative exponential modelAR(1)Cluster","code":""},{"path":"linear-regression.html","id":"heteroskedasticity","chapter":"5 Linear Regression","heading":"5.2.1 Heteroskedasticity","text":"\\[\\begin{equation}\n\\begin{split}\nVar(\\epsilon_i |x_i) & = E(\\epsilon^2|x_i) \\neq \\sigma^2 \\\\\n& = h(x_i) = \\sigma_i^2 \\text{(variance error term function x)}\n\\end{split}\n\\tag{5.7}\n\\end{equation}\\]model,\\[\ny_i = x_i\\beta + \\epsilon_i \\\\\n(1/\\sigma_i)y_i = (1/\\sigma_i)x_i\\beta + (1/\\sigma_i)\\epsilon_i\n\\], (5.7)\\[\n\\begin{split}\nVar((1/\\sigma_i)\\epsilon_i|X) &= (1/\\sigma_i^2) Var(\\epsilon_i|X) \\\\\n&= (1/\\sigma_i^2)\\sigma_i^2 \\\\\n&= 1\n\\end{split}\n\\]weight matrix \\(\\mathbf{w}\\) matrix equation\\[\n\\mathbf{wy=wX\\beta + w\\epsilon}\n\\]\\[\n\\mathbf{w}= \n\\left(\n\\begin{array}{ccccc}\n1/\\sigma_1 & 0 & 0 & ... & 0 \\\\\n0 & 1/\\sigma_2 & 0 & ... & 0 \\\\\n0 & 0 & 1/\\sigma_3 & ... & . \\\\\n. & . & . & . & 0 \\\\\n0 & 0 & . & . & 1/\\sigma_n\n\\end{array}\n\\right)\n\\]Infeasible Weighted Least SquaresAssume know \\(\\sigma_i^2\\) (Infeasible)IWLS estimator obtained least squared estimated following weighted equation\\[\n(1/\\sigma_i)y_i = (1/\\sigma_i)\\mathbf{x}_i\\beta + (1/\\sigma_i)\\epsilon_i \n\\]Usual standard errors weighted equation valid \\(Var(\\epsilon | \\mathbf{X}) = \\sigma_i^2\\)\\(Var(\\epsilon | \\mathbf{X}) \\neq \\sigma_i^2\\) heteroskedastic robust standard errors valid.Problem: know \\(\\sigma_i^2=Var(\\epsilon_i|\\mathbf{x_i})=E(\\epsilon_i^2|\\mathbf{x}_i)\\)One observation \\(\\epsilon_i\\) estimate sample variance estimate \\(\\sigma_i^2\\)\nModel \\(\\epsilon_i^2\\) reasonable (strictly positive) function \\(x_i\\) independent error \\(v_i\\) (strictly positive)\nModel \\(\\epsilon_i^2\\) reasonable (strictly positive) function \\(x_i\\) independent error \\(v_i\\) (strictly positive)\\[\n\\epsilon_i^2=v_i exp(\\mathbf{x_i\\gamma})\n\\]\ncan apply log transformation recover linear parameters model,\\[\nln(\\epsilon_i^2) = \\mathbf{x_i\\gamma} + ln(v_i)\n\\]\n\\(ln(v_i)\\) independent \\(\\mathbf{x}_i\\)observe \\(\\epsilon_i\\)\n* OLS residual (\\(e_i\\)) approximate","code":""},{"path":"linear-regression.html","id":"serial-correlation","chapter":"5 Linear Regression","heading":"5.2.2 Serial Correlation","text":"\\[\nCov(\\epsilon_i, \\epsilon_j | \\mathbf{X}) \\neq 0\n\\]covariance stationary,\\[\nCov(\\epsilon_i,\\epsilon_j|\\mathbf{X}) = Cov(\\epsilon_i, \\epsilon_{+h}|\\mathbf{x_i,x_{+h}})=\\gamma_h\n\\]variance covariance matrix \\[\nVar(\\epsilon|\\mathbf{X}) = \\Omega = \n\\left(\n\\begin{array}{ccccc}\n\\sigma^2 & \\gamma_1 & \\gamma_2 & ... & \\gamma_{n-1} \\\\\n\\gamma_1 & \\sigma^2 & \\gamma_1 & ... & \\gamma_{n-2} \\\\\n\\gamma_2 & \\gamma_1 & \\sigma^2 & ... & ... \\\\\n. & . & . & . & \\gamma_1 \\\\\n\\gamma_{n-1} & \\gamma_{n-2} & . & \\gamma_1 & \\sigma^2\n\\end{array}\n\\right)\n\\]n parameters estimate - need sort fo structure reduce number parameters estimate.Time Series\nEffect inflation deficit Treasury BIll interest rates\nEffect inflation deficit Treasury BIll interest ratesCross-sectional\nClustering\nClustering","code":""},{"path":"linear-regression.html","id":"ar1","chapter":"5 Linear Regression","heading":"5.2.2.1 AR(1)","text":"\\[\ny_t= \\beta_0 + x_t\\beta_1 + \\epsilon_t \\\\\n\\epsilon_t = \\rho \\epsilon_{t-1} + u_t\n\\]variance covariance matrix \n\\[\nVar(\\epsilon | \\mathbf{X})= \\frac{\\sigma^2_u}{1-\\rho}\n\\left(\n\\begin{array}{ccccc}\n1 & \\rho & \\rho^2 & ... & \\rho^{n-1} \\\\\n\\rho & 1 & \\rho & ... & \\rho^{n-2} \\\\\n\\rho^2 & \\rho & 1 & . & . \\\\\n. & . & . & . & \\rho \\\\\n\\rho^{n-1} & \\rho^{n-2} & . & \\rho & 1 \\\\\n\\end{array}\n\\right)\n\\]Hence, 1 parameter estimate: \\(\\rho\\)A1, A2, A3a, A5a, OLS consistent asymptotically normalUse Newey West Standard Errors valid inference.Apply Infeasible Cochrane Orcutt (knew \\(\\rho\\))\\[\nu_t = \\epsilon_t - \\rho \\epsilon_{t-1}\n\\]\nsatisfies A3, A4, A5 ’d like transform equation one \\(u_t\\) error.\\[\n\\begin{aligned}\ny_t - \\rho y_{t-1} &= (\\beta_0 + x\\beta_1 + \\epsilon_t) - \\rho (\\beta_0 + x_{t-1}\\beta_1 + \\epsilon_{t-1}) \\\\\n& = (1-\\rho)\\beta_0 + (x_t - \\rho x_{t-1})\\beta_1 + u_t\n\\end{aligned}\n\\]","code":""},{"path":"linear-regression.html","id":"infeasible-cochrane-orcutt","chapter":"5 Linear Regression","heading":"5.2.2.1.1 Infeasible Cochrane Orcutt","text":"Assume know \\(\\rho\\) (Infeasible)ICO estimator obtained least squared estimated following weighted first difference equation\\[\ny_t -\\rho y_{t-1} = (1-\\rho)\\beta_0 + (x_t - \\rho x_{t-1})\\beta_1 + u_t\n\\]Usual standard errors weighted first difference equation valid errors truly follow AR(1) processIf serial correlation generated complex dynamic process Newey-West HAC standard errors validProblem\nknow \\(\\rho\\)\\(\\rho\\) correlation \\(\\epsilon_t\\) \\(\\epsilon_{t-1}\\): estimate using OLS residuals (\\(e_i\\)) proxy\\[\n\\hat{\\rho} = \\frac{\\sum_{t=1}^{T}e_te_{t-1}}{\\sum_{t=1}^{T}e_t^2}\n\\]can obtained OLS regression \\[\ne_t = \\rho e_{t-1} + u_t\n\\]\nsuppress intercept.losing observation\ntaking first difference dropping first observation\ntaking first difference dropping first observation\\[\ny_1 = \\beta_0 + x_1 \\beta_1 + \\epsilon_1\n\\]\n+ Feasiable Prais Winsten Transformation applies Infeasible Cochrane Orcutt includes weighted version first observation\\[\n(\\sqrt{1-\\rho^2})y_1 = \\beta_0 + (\\sqrt{1-\\rho^2})x_1 \\beta_1 + (\\sqrt{1-\\rho^2}) \\epsilon_1\n\\]","code":""},{"path":"linear-regression.html","id":"cluster","chapter":"5 Linear Regression","heading":"5.2.2.2 Cluster","text":"\\[\ny_{gi} = \\mathbf{x}_{gi}\\beta + \\epsilon_{gi}\n\\]\\[\nCov(\\epsilon_{gi}, \\epsilon_{hj})\n\\begin{cases}\n= 0 & \\text{$g \\neq h$ pair (,j)} \\\\\n\\neq 0 & \\text{(,j) pair}\\\\\n\\end{cases}\n\\]Intra-group Correlation\nindividual single group may correlated independent across groups.A4 violated. usual standard errors OLS valid.Use cluster robust standard errors OLS.Suppose 3 groups different n\\[\nVar(\\epsilon| \\mathbf{X})= \\Omega =\n\\left(\n\\begin{array}{cccccc}\n\\sigma^2 & \\delta_{12}^1 & \\delta_{13}^1 & 0 & 0 & 0 \\\\\n\\delta_{12}^1 & \\sigma^2 & \\delta_{23}^1 & 0 & 0 & 0 \\\\\n\\delta_{13}^1 & \\delta_{23}^1 & \\sigma^2 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & \\sigma^2 & \\delta_{12}^2 & 0 \\\\\n0 & 0 & 0 & \\delta_{12}^2 & \\sigma^2 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & \\sigma^2\n\\end{array}\n\\right)\n\\]\\(Cov(\\epsilon_{gi}, \\epsilon_{gj}) = \\delta_{ij}^g\\) \\(Cov(\\epsilon_{gi}, \\epsilon_{hj}) = 0\\) jInfeasible Generalized Least Squares (Cluster)Assume \\(\\sigma^2\\) \\(\\delta_{ij}^g\\) known, plug \\(\\Omega\\) solve inverse \\(\\Omega^{-1}\\) (infeasible)Infeasible Generalized Least Squares Estimator \\[\n\\hat{\\beta}_{IGLS} = \\mathbf{(X'\\Omega^{-1}X)^{-1}X'\\Omega^{-1}y}\n\\]Problem\n* know \\(\\sigma^2\\) \\(\\delta_{ij}^g\\)\n+ Can make assumptions data generating process causing clustering behavior.\n- give structure \\(Cov(\\epsilon_{gi},\\epsilon_{gj})= \\delta_{ij}^g\\) makes feasible estimate\n- assumptions wrong use cluster robust standard errors.Solution\nAssume group level random effects specification error\\[\ny_{gi} = \\mathbf{g}_i \\beta + c_g + u_{gi} \\\\\nVar(c_g|\\mathbf{x}_i) = \\sigma^2_c \\\\\nVar(u_{gi}|\\mathbf{x}_i) = \\sigma^2_u\n\\]\\(c_g\\) \\(u_{gi}\\) independent , mean independent \\(\\mathbf{x}_i\\)\\(c_g\\) captures common group shocks (independent across groups)\\(u_{gi}\\) captures individual shocks (independent across individuals groups)error variance \\[\nVar(\\epsilon| \\mathbf{X})= \\Omega =\n\\left(\n\\begin{array}{cccccc}\n\\sigma^2_c + \\sigma^2_u & \\sigma^2_c & \\sigma^2_c & 0 & 0 & 0 \\\\\n\\sigma^2_c & \\sigma^2 + \\sigma^2_u & \\sigma^2_c & 0 & 0 & 0 \\\\\n\\sigma^2_c & \\sigma^2_c  & \\sigma^2+ \\sigma^2_u & 0 & 0 & 0 \\\\\n0 & 0 & 0 & \\sigma^2+ \\sigma^2_u & \\sigma^2_c & 0 \\\\\n0 & 0 & 0 & \\sigma^2_c & \\sigma^2+ \\sigma^2_u & 0 \\\\\n0 & 0 & 0 & 0 & 0 & \\sigma^2+ \\sigma^2_u\n\\end{array}\n\\right)\n\\]Use Feasible group level Random Effects","code":""},{"path":"linear-regression.html","id":"weighted-least-squares","chapter":"5 Linear Regression","heading":"5.3 Weighted Least Squares","text":"Estimate following equation using OLS\\[\ny_i = \\mathbf{x}_i \\beta + \\epsilon_i\n\\]\nobtain residuals \\(e_i=y_i -\\mathbf{x}_i \\hat{\\beta}\\)Transform residual estimate following OLS,\\[\nln(e_i^2)= \\mathbf{x}_i\\gamma + ln(v_i)\n\\]obtain predicted values \\(g_i=\\mathbf{x}_i \\hat{\\gamma}\\)weights untransformed predicted outcome,\\[\n\\hat{\\sigma}_i =\\sqrt{exp(g_i)}\n\\]FWLS (Feasible WLS) estimator obtained least squared estimated following weighted equation\\[\n(1/\\hat{\\sigma}_i)y_i = (1/\\hat{\\sigma}_i) \\mathbf{x}_i\\beta + (1/\\hat{\\sigma}_i)\\epsilon_i\n\\]Properties FWLSThe infeasible WLS estimator unbiased A1-A3 unweighted equation.FWLS estimator unbiased estimator.FWLS estimator consistent A1, A2, (unweighted equation), A5, \\(E(\\mathbf{x}_i'\\epsilon_i/\\sigma^2_i)=0\\)\nA3a sufficient equation\nA3 sufficient equation.\nA3a sufficient equationA3 sufficient equation.FWLS estimator asymptotically efficient OLS errors multiplicative exponential heteroskedasticity.\nerrors truly multiplicative exponential heteroskedasticity, usual standard errors valid\nbelieve may mis-specification multiplicative exponential model, report heteroskedastic robust standard errors.\nerrors truly multiplicative exponential heteroskedasticity, usual standard errors validIf believe may mis-specification multiplicative exponential model, report heteroskedastic robust standard errors.","code":""},{"path":"linear-regression.html","id":"generalized-least-squares","chapter":"5 Linear Regression","heading":"5.4 Generalized Least Squares","text":"Consider\\[\n\\mathbf{y = X\\beta + \\epsilon} \n\\]\n,\\[\nvar(\\epsilon) = \\mathbf{G} = \n\\left(\n\\begin{array}\n{cccc}\ng_{11} & g_{12} & ... & g_{1n} \\\\\ng_{21} & g_{22} & ... & g_{2n} \\\\\n. & . & . & . \\\\\ng_{n1} & . & . & g_{nn}\\\\\n\\end{array}\n\\right)\n\\]variances heterogeneous, errors correlated.\\[\n\\mathbf{\\hat{b}_G = (X'G^{-1}X)^{-1}X'G^{-1}Y}\n\\]know G, can estimate \\(\\mathbf{b}\\) just like OLS. However, know G. Hence, model structure G.","code":""},{"path":"linear-regression.html","id":"feasiable-prais-winsten","chapter":"5 Linear Regression","heading":"5.5 Feasiable Prais Winsten","text":"Weighting Matrix\\[\n\\mathbf{w} = \n\\left(\n\\begin{array}{ccccc}\n\\sqrt{1- \\hat{\\rho}^2} & 0 & 0 &... & 0 \\\\\n-\\hat{\\rho} & 1 & 0 & ... & 0 \\\\\n0 &  -\\hat{\\rho} & 1 & & . \\\\\n. & . & . & . & 0 \\\\\n0 & . & 0 & -\\hat{\\rho} & 1\n\\end{array}\n\\right)\n\\]Estimate following equation using OLS\\[\ny_t = \\mathbf{x}_t \\beta + \\epsilon_t\n\\]\nobtain residuals \\(e_t = y_t - \\mathbf{x}_t \\hat{\\beta}\\)Estimate correlation coefficient AR(1) process estimating following OLS (without intercept)\\[\ne_t = \\rho e_{t-1} + u_t\n\\]Transform outcome independent variables \\(\\mathbf{wy}\\) \\(\\mathbf{wX}\\) respectively (weight matrix stated).FPW estimator obtained least squared estimated following weighted equation\\[\n\\mathbf{wy = wX\\beta + w\\epsilon}\n\\]Properties Feasiable Prais Winsten EstimatorThe Infeasible PW estimator A1-A3 unweighted equationThe FPW estimator biasedThe FPW consistent A1 A2 A5 \\[\nE((\\mathbf{x_t - \\rho x_{t-1}})')(\\epsilon_t - \\rho \\epsilon_{t-1})=0\n\\]\n+ A3a sufficient equation\n+ A3 sufficient equationThe FPW estimator asymptotically efficient OLS errors truly generated AR(1) process\nerrors truly generated AR(1) process usual standard errors valid\nconcerned may complex dependence structure heteroskedasticity, use Newey West Standard Errors\nerrors truly generated AR(1) process usual standard errors validIf concerned may complex dependence structure heteroskedasticity, use Newey West Standard Errors","code":""},{"path":"linear-regression.html","id":"feasible-group-level-random-effects","chapter":"5 Linear Regression","heading":"5.6 Feasible group level Random Effects","text":"Estimate following equation using OLS\\[\ny_{gi} = \\mathbf{x}_{gi}\\beta + \\epsilon_{gi}\n\\]\nobtain residuals \\(e_{gi} = y_{gi} - \\mathbf{x}_{gi}\\hat{\\beta}\\)\n2. Estimate variance using usual $s^2 estimator\\[\ns^2 = \\frac{1}{n-k}\\sum_{=1}^{n}e_i^2\n\\]\nestimator \\(\\sigma^2_c + \\sigma^2_u\\) estimate within group correlation,\\[\n\\hat{\\sigma}^2_c = \\frac{1}{G} \\sum_{g=1}^{G} (\\frac{1}{\\sum_{=1}^{n_g-1}}\\sum_{\\neq j}\\sum_{j}^{n_g}e_{gi}e_{gj})\n\\]plug estimates obtain \\(\\hat{\\Omega}\\)feasible group level RE estimator obtained \\[\n\\hat{\\beta}= \\mathbf{(X'\\hat{\\Omega}^{-1}X)^{-1}X'\\hat{\\Omega}^{-1}y}\n\\]Properties Feasible group level Random Effects EstimatorThe infeasible group RE estimator linear estimator unbiased A1-A3 unweighted equation\nA3 requires \\(E(\\epsilon_{gi}|\\mathbf{x}_i) = E(c_{g}|\\mathbf{x}_i)+ (u_{gi}|\\mathbf{x}_i)=0\\) generally assume \\(E(c_{g}|\\mathbf{x}_i)+ (u_{gi}|\\mathbf{x}_i)=0\\). assumption \\(E(c_{g}|\\mathbf{x}_i)=0\\) generally called random effects assumption\nA3 requires \\(E(\\epsilon_{gi}|\\mathbf{x}_i) = E(c_{g}|\\mathbf{x}_i)+ (u_{gi}|\\mathbf{x}_i)=0\\) generally assume \\(E(c_{g}|\\mathbf{x}_i)+ (u_{gi}|\\mathbf{x}_i)=0\\). assumption \\(E(c_{g}|\\mathbf{x}_i)=0\\) generally called random effects assumptionThe Feasible group level Random Effects biasedThe Feasible group level Random Effects consistent A1-A3a, A5a unweighted equation.\nA3a requires \\(E(\\mathbf{x}_i'\\epsilon_{gi}) = E(\\mathbf{x}_i'c_{g})+ (\\mathbf{x}_i'u_{gi})=0\\)\nA3a requires \\(E(\\mathbf{x}_i'\\epsilon_{gi}) = E(\\mathbf{x}_i'c_{g})+ (\\mathbf{x}_i'u_{gi})=0\\)Feasible group level Random Effects estimator asymptotically efficient OLS errors follow random effects specification\nerrors follow random effects specification usual standard errors consistent\nmight complex dependence structure heteroskedasticity, need cluster robust standard errors.\nerrors follow random effects specification usual standard errors consistentIf might complex dependence structure heteroskedasticity, need cluster robust standard errors.","code":""},{"path":"linear-regression.html","id":"ridge-regression","chapter":"5 Linear Regression","heading":"5.7 Ridge Regression","text":"Collinearity problem, use Ridge regression.main problem multicollinearity \\(\\mathbf{X'X}\\) “ill-conditioned.” idea ridge regression: adding constant diagonal \\(\\mathbf{X'X}\\) improves conditioning\\[\n\\mathbf{X'X} + c\\mathbf{} (c>0)\n\\]\nchoice c hard. estimator\\[\n\\mathbf{b}^R = (\\mathbf{X'X}+c\\mathbf{})^{-1}\\mathbf{X'y}\n\\]\nbiased.smaller variance OLS estimator; c increases, bias increases variance decreases.always exists value c ridge regression estimator smaller total MSE OLSthe optimal c varies application data set.find “optimal” c use “ridge trace.”plot values p - 1 parameter estimates different values c, simultaneously.typically, c increases toward 1 coefficients decreases 0.values VIF tend decrease rapidly c gets bigger 0. VIF values begin change slowly c approaches 1.can examine ridge trace VIF values chooses smallest value c regression coefficients first become stable ridge trace VIF values become sufficiently small (subjective).typically, procedure applied standardized regression model.","code":""},{"path":"linear-regression.html","id":"principal-component-regression","chapter":"5 Linear Regression","heading":"5.8 Principal Component Regression","text":"also addresses problem multicollinearity","code":""},{"path":"linear-regression.html","id":"robust-regression","chapter":"5 Linear Regression","heading":"5.9 Robust Regression","text":"address problem influential cases.can used known functional form fitted, errors normal due outlying cases.","code":""},{"path":"linear-regression.html","id":"least-absolute-residuals-lar-regression","chapter":"5 Linear Regression","heading":"5.9.1 Least Absolute Residuals (LAR) Regression","text":"also known minimum \\(L_1\\)-norm regression.\\[\nL_1 = \\sum_{=1}^{n}|Y_i - (\\beta_0 + \\beta_1 X_{i1} + .. + \\beta_{p-1}X_{,p-1})\n\\]sensitive outliers inadequacies model specification.","code":""},{"path":"linear-regression.html","id":"least-median-of-squares-lms-regression","chapter":"5 Linear Regression","heading":"5.9.2 Least Median of Squares (LMS) Regression","text":"\\[\nmedian\\{[Y_i - (\\beta_0 - \\beta_1X_{i1} + ... + \\beta_{p-1}X_{,p-1})]^2 \\}\n\\]","code":""},{"path":"linear-regression.html","id":"iteratively-reweighted-least-squares-irls-robust-regression","chapter":"5 Linear Regression","heading":"5.9.3 Iteratively Reweighted Least Squares (IRLS) Robust Regression","text":"uses Weighted Least Squares lessen influence outliers.weights \\(w_i\\) inversely proportional far outlying case (e.g., based residual)weights revised iteratively robust fitProcess:Step 1: Choose weight function weighting cases.\nStep 2: obtain starting weights cases.\nStep 3: Use starting weights WLS obtain residuals fitted regression function.\nStep 4: use residuals Step 3 obtain revised weights.\nStep 5: continue convergence.Note:don’t know form regression function, consider using nonparametric regression (e.g., locally weighted regression, regression trees, projection pursuit, neural networks, smoothing splines, loess, wavelets).use detect outliers confirm OLS.","code":""},{"path":"linear-regression.html","id":"maximum-likelihood-regression","chapter":"5 Linear Regression","heading":"5.10 Maximum Likelihood","text":"Premise: find values parameters maximize probability observing data\nwords, try maximize value theta likelihood function\n\\[\nL(\\theta)=\\prod_{=1}^{n}f(y_i|\\theta)\n\\]\n\\(f(y|\\theta)\\) probability density observing single value Y given value \\(\\theta\\)\n\\(f(y|\\theta)\\) can specify various type distributions. can review back section Distributions. example\ny dichotomous variable, \n\\[\nL(\\theta)=\\prod_{=1}^{n}\\theta^{y_i}(1-\\theta)^{1-y_i}\n\\]\\(\\hat{\\theta}\\) Maximum Likelihood estimate \\(L(\\hat{\\theta}) > L(\\theta_0)\\) values \\(\\theta_0\\) parameter space.","code":""},{"path":"linear-regression.html","id":"motivation-for-mle","chapter":"5 Linear Regression","heading":"5.10.1 Motivation for MLE","text":"Suppose know conditional distribution y given x:\\[\nf_{Y|X}(y,x;\\theta)\n\\]\n\\(\\theta\\) unknown parameter distribution. Sometimes concerned unconditional distribution \\(f_{Y}(y;\\theta)\\)given sample iid data, can calculate joint distribution entire sample,\\[\nf_{Y_1,...,Y_n|X_1,...,X_n(y_1,...y_n,x_1,...,x_n;\\theta)}= \\prod_{=1}^{n}f_{Y|X}(y_i,x_i;\\theta)\n\\]\njoint distribution evaluated sample likelihood (probability) observed particular sample (depends \\(\\theta\\))Idea MLE: Given sample, choose estimates parameters gives highest likelihood (probability) observing particular sample\\[\nmax_{\\theta} \\prod_{=1}^{n}f_{Y|X}(y_i,x_i; \\theta)\n\\]Equivalently,\\[\nmax_{\\theta} \\prod_{=1}^{n} ln(f_{Y|X}(y_i,x_i; \\theta))\n\\]\nSolving Maximum Likelihood EstimatorSolve First Order Condition\\[\n\\frac{\\partial}{\\partial \\theta}\\sum_{=1}^{n} ln(f_{Y|X}(y_i,x_i;\\hat{\\theta}_{MLE})) = 0\n\\]\n\\(\\hat{\\theta}_{MLE}\\) defined.Evaluate Second Order Condition\\[\n\\frac{\\partial^2}{\\partial \\theta^2} \\sum_{=1}^{n} ln(f_{Y|X}(y_i,x_i;\\hat{\\theta}_{MLE})) < 0\n\\]condition ensures can solve maximumExamples:\nUnconditional Poisson Distribution: Number products ordered Amazon within hour, number website visits day political campaign.Exponential Distribution: Length time earthquake occurs, length time car battery lasts.\\[\nf_{Y|X}(y,x;\\theta) = exp(-y/x\\theta)/x\\theta \\\\\nf_{Y_1,..Y_n|X_1,...,X_n(y_1,...,y_n,x_1,...,x_n;\\theta)} = \\prod_{=1}^{n}exp(-y_i/x_i \\theta)/x_i \\theta\n\\]","code":""},{"path":"linear-regression.html","id":"assumption","chapter":"5 Linear Regression","heading":"5.10.2 Assumption","text":"High Level Regulatory Assumptions sufficient condition used show large sample properties\nHence, MLE, need either assume verify regulatory assumptiosn holds.\nHence, MLE, need either assume verify regulatory assumptiosn holds.observations independent density function.multivariate normal assumption, ML yields consistent estimates means covariance matrix multivariate distribution finite fourth moments (Little Smith 1987)find MLE, usually differentiate log-likelihood function set equal 0.\\[\n\\frac{d}{d\\theta}l(\\theta) = 0 \n\\]\nscore equationOur confidence MLE quantified “pointedness” log-likelihood\n\\[\nI_O(\\theta)= \\frac{d^2}{d\\theta^2}l(\\theta) = 0 \n\\]\ncalled observed informationwhile\n\\[\n(\\theta)=E[I_O(\\theta;Y)]\n\\]\nexpected information. (also known Fisher Information). base variance estimator.\\[\nV(\\hat{\\Theta}) \\approx (\\theta)^{-1}\n\\]Consistency MLE\nSuppose \\(y_i\\) \\(x_i\\) iid drawn true conditional pdf \\(f_{Y|X}(y_i,x_i;\\theta_0)\\). following regulatory assumptions hold,R1: \\(\\theta \\neq \\theta_0\\) \\(f_{Y|X}(y_i,x_i;\\theta) \\neq f_{Y|X}(y_i,x_i;\\theta_0)\\)\nR2: set \\(\\Theta\\) contains true parameters \\(\\theta_0\\) compact\nR3: log-likelihood \\(ln(f_{Y|X}(y_i,x_i;\\theta_0))\\) continuous \\(\\theta\\) probability 1\nR4: \\(E(sup_{\\theta \\\\Theta}|ln(f_{Y|X}(y_i,x_i;\\theta_0))|)\\)MLE estimator consistent,\\[\n\\hat{\\theta}_{MLE} \\^p \\theta_0\n\\]Asymptotic Normality MLESuppose \\(y_1\\) \\(x_i\\) iid drawn true conditional pdf \\(f_{Y|X}(y_i,x_i;\\theta)\\). R1-R4 following holdR5: \\(\\theta_0\\) interior set \\(\\Theta\\)\nR6: \\(f_{Y|X}(y_i,x_i;\\theta)\\) twice continuously differentiable \\(\\theta\\) \\(f_{Y|X}(y_i,x_i;\\theta) >0\\) neighborhood \\(N \\\\Theta\\) around \\(\\theta_0\\)\nR7: \\(\\int sup_{\\theta \\N}||\\partial f_{Y|X}(y_i,x_i;\\theta)\\partial\\theta||d(y,x) <\\infty\\), \\(\\int sup_{\\theta \\N} || \\partial^2 f_{Y|X}(y_i,x_i;\\theta)/\\partial \\theta \\partial \\theta' || d(y,x) < \\infty\\) \\(E(sup_{\\theta \\N} || \\partial^2ln(f_{Y|X}(y_i,x_i;\\theta)) / \\partial \\theta \\partial \\theta' ||) < \\infty\\)\nR8: information matrix \\((\\theta_0) = Var(\\partial f_{Y|X}(y,x_i; \\theta_0)/\\partial \\theta)\\) exists non-singularthen MLE estimator asymptotically normal,\\[\n\\sqrt{n}(\\hat{\\theta}_{MLE} - \\theta_0) \\^d N(0,(\\theta_0)^{-1})\n\\]","code":""},{"path":"linear-regression.html","id":"properties","chapter":"5 Linear Regression","heading":"5.10.3 Properties","text":"(EJD, Agresti, Finlay 1998)Consistent: estimates approximately unbiased large samplesAsymptotically efficient: approximately smaller standard errors compared estimatorAsymptotically normal: repeated sampling, estimates approximately normal distribution. Suppose \\(\\hat{\\theta}_n\\) MLE \\(\\theta\\) based n independent observations. \\(\\hat{\\theta}_n \\sim N(\\theta,H^{-1})\\).\n+ H called Fisher information matrix. contains expected values second partial derivatives log-likelihood function. (.j)th element H \\(-E(\\frac{\\partial^2l(\\theta)}{\\partial \\theta_i \\partial \\theta_j})\\)\n+ can estimate H finding form determined , evaluating \\(\\theta = \\hat{\\theta}_n\\)Invariance: MLE \\(g(\\theta) = g(\\hat{\\theta})\\) function g(.)\\[\n\\hat{\\Theta} \\approx^d (\\theta,(\\hat{\\theta)^{-1}}))\n\\]Explicit vs Implicit MLEIf solve score equation get expression MLE, ’s called explicitIf closed form MLE, need algorithms derive expression, ’s called implicitLarge Sample Property MLEImplicit theorems assumption know conditional distribution,\\[\nf_{Y|X}(y_i,x_i;\\theta_0)\n\\]\njust now know exact parameter value.Distributional mis-specification result inconsistent parameter estimates.Quasi-MLE: Particular settings/ assumption allow certain types distributional mis-specification (Ex: long distribution part particular class satisfies particular assumption, estimating wrong distribution lead inconsistent parameter estimates).non-parametric/ Semi-parametric estimation: little distributional assumption made. (hard implement, derive properties, interpret)asymptotic variance MLE achieves Cramer-Rao Lower BoundThe Cramer-Rao Lower Bound lower brand asymptotic variance consistent asymptotically normally distributed estimator.estimator achieves lower bound efficient estimator.maximum Likelihood estimator (assuming distribution correctly specified R1-R8 hold) efficient consistent asymptotically normal estimator.\n* efficient among consistent estimators (limited unbiased linear estimators).NoteML better choice binary, strictly positive, count, inherent heteroskedasticity linear model.ML assume know conditional distribution outcome, derive estimator using information.\nAdds assumption know distribution (similar A6 Normal Distribution linear model)\nproduce efficient estimator.\nAdds assumption know distribution (similar A6 Normal Distribution linear model)produce efficient estimator.","code":""},{"path":"linear-regression.html","id":"compare-to-ols","chapter":"5 Linear Regression","heading":"5.10.4 Compare to OLS","text":"MLE cure OLS problems:joint inference MLE, typically use log-likelihood calculation, instead F-scoreFunctional form affects estimation MLE OLS.Perfect Collinearity/Multicollinearity: highly correlated likely yield large standard errors.Endogeneity (Omitted variables bias, Simultaneous equations): Like OLS, MLE also biased problem","code":""},{"path":"linear-regression.html","id":"application-1","chapter":"5 Linear Regression","heading":"5.10.5 Application","text":"applications MLECorner Solution\nEx: hours worked, donations charity\nEstimate Tobit\nEx: hours worked, donations charityEstimate TobitNon-negative count\nEx: Numbers arrest, Number cigarettes smoked day\nEstimate Poisson regression\nEx: Numbers arrest, Number cigarettes smoked dayEstimate Poisson regressionMultinomial Choice\nEx: Demand cars, votes primary election\nEstimate mutinomial probit logit\nEx: Demand cars, votes primary electionEstimate mutinomial probit logitOrdinal Choice\nEx: Levels Happiness, Levels Income\nOrdered Probit\nEx: Levels Happiness, Levels IncomeOrdered ProbitModel binary Response\nbinary variable Bernoulli distribution:\\[\nf_Y(y_i;p) = p^{y_i}(1-p)^{(1-y_i)}\n\\]\np probability success. conditional distribution :\\[\nf_{Y|X}(y_i,x_i;p(.)) = p(x_i)^{y_i} (1-p(x_i))^{(1-y_i)}\n\\]choose \\(p(x_i)\\) reasonable function \\(x_i\\) unknown parameters \\(\\theta\\)can use latent variable model probability functions\\[\ny_i = 1\\{y_i^* > 0 \\}  \\\\\ny_i^* = x_i \\beta-\\epsilon_i\n\\]\\(y_i^*\\) latent variable (unobserved) well-defined terms units/magnitudes\\(\\epsilon_i\\) mean 0 unobserved random variable.can rewrite mdoel without latent variable,\\[\ny_i = 1\\{x_i beta > \\epsilon_i \\}\n\\]probability function,\\[\np(x_i) = P(y_i = 1|x_i) \\\\\n= P(x_i \\beta > \\epsilon_i | x_i) \\\\\n= F_{\\epsilon|X}(x_i \\beta | x_i)\n\\]need choose conditional distribution \\(\\epsilon_i\\). Hence, can make additional strong independence assumption\\(\\epsilon_i\\) independent \\(x_i\\)probability function simply,\\[\np(x_i) = F_\\epsilon(x_i \\beta)\n\\]\nprobability function also conditional expectation function,\\[\nE(y_i | x_i) = P(y_i = 1|x_i) = F_\\epsilon (x_i \\beta)\n\\]allow conditional expectation function non-linear.Common distributional assumptionProbit: Assume \\(\\epsilon_i\\) standard normally distributed, \\(F_\\epsilon(.) = \\Phi(.)\\) standard normal CDF.Logit: Assume \\(\\epsilon_i\\) standard logistically distributed, \\(F_\\epsilon(.) = \\Lambda(.)\\) standard normal CDF.Step deriveChoose distribution (normal logistic) plug following log likelihood,\\[\nln(f_{Y|X} (y_i , x_i; \\beta)) = y_i ln(F_\\epsilon(x_i \\beta)) + (1-y_i)ln(1-F_\\epsilon(x_i \\beta))\n\\]Solve MLE finding Maximum \\[\n\\hat{\\beta}_{MLE} = argmax \\sum_{=1}^{n}ln(f_{Y|X}(y_i,x_i; \\beta))\n\\]Properties Probit Logit EstimatorsProbit Logit consistent asymptotically normal \n[A2][] holds: \\(E(x_i' x_i)\\) exists non-singular\n[A5][] (A5a) holds: {y_i,x_i} iid (stationary weakly dependent).\nDistributional assumptions \\(\\epsilon_i\\) hold: Normal/Logistic independent \\(x_i\\)\n[A2][] holds: \\(E(x_i' x_i)\\) exists non-singular[A5][] (A5a) holds: {y_i,x_i} iid (stationary weakly dependent).Distributional assumptions \\(\\epsilon_i\\) hold: Normal/Logistic independent \\(x_i\\)assumptions, Probit Logit also asymptotically efficient asymptotic variance,\\[\n(\\beta_0)^{-1} = [E(\\frac{(f_\\epsilon(x_i \\beta_0))^2}{F_\\epsilon(x_i\\beta_0)(1-F_\\epsilon(x_i\\beta_0))}x_i' x_i)]^{-1}\n\\]\\(F_\\epsilon(x_i\\beta_0)\\) probability density function (derivative CDF)","code":""},{"path":"linear-regression.html","id":"interpretation","chapter":"5 Linear Regression","heading":"5.10.5.1 Interpretation","text":"\\(\\beta\\) average response latent variable associated change \\(x_i\\)Magnitudes meaningDirection meaningThe partial effect Non-linear binary response model\\[\nE(y_i |x_i) = F_\\epsilon (x_i \\beta) \\\\\nPE(x_{ij}) = \\frac{\\partial E(y_i |x_i)}{\\partial x_{ij}} = f_\\epsilon (x_i \\beta)\\beta_j\n\\]partial effect coefficient parameter \\(\\beta_j\\) multiplied scaling factor \\(f_\\epsilon (x_i \\beta)\\)scaling factor depends \\(x_i\\) partial effect changes depending \\(x_i\\) isSingle value partial effectPartial Effect Average (PEA) partial effect average individual\\[\nf_{\\epsilon}(\\bar{x}\\hat{\\beta})\\hat{\\beta}_j\n\\]Average Partial Effect (APE) average partial effect individual.\\[\n\\frac{1}{n}\\sum_{=1}^{n}f_\\epsilon(x_i \\hat{\\beta})\\hat{\\beta}_j\n\\]linear model, APE = PEA.\nnon-linear model (e.g., binary response), APE \\(\\neq\\) PEA","code":""},{"path":"non-linear-regression.html","id":"non-linear-regression","chapter":"6 Non-linear Regression","heading":"6 Non-linear Regression","text":"Definition: models derivatives mean function respect parameters depend one parameters.approximate data, can approximate functionby high-order polynomialby linear model (e.g., Taylor expansion around X’s)collection locally linear models basis functionbut easy interpret, enough data, can’t interpret globally.intrinsically nonlinear models:\\[\nY_i = f(\\mathbf{x_i;\\theta}) + \\epsilon_i\n\\]\\(f(\\mathbf{x_i;\\theta})\\) nonlinear function relating \\(E(Y_i)\\) independent variables \\(x_i\\)\\(\\mathbf{x}_i\\) k x 1 vector independent variables (fixed).\\(\\mathbf{\\theta}\\) p x 1 vector parameters.\\(\\epsilon_i\\)s iid variables mean 0 variance \\(\\sigma^2\\). (sometimes ’s normal).","code":""},{"path":"non-linear-regression.html","id":"inference-1","chapter":"6 Non-linear Regression","heading":"6.1 Inference","text":"Since \\(Y_i = f(\\mathbf{x}_i,\\theta) + \\epsilon_i\\), \\(\\epsilon_i \\sim iid(0,\\sigma^2)\\). can obtain \\(\\hat{\\theta}\\) minimizing \\(\\sum_{=1}^{n}(Y_i - f(x_i,\\theta))^2\\) estimate \\(s^2 = \\hat{\\sigma}^2_{\\epsilon}=\\frac{\\sum_{=1}^{n}(Y_i - f(x_i,\\theta))^2}{n-p}\\)","code":""},{"path":"non-linear-regression.html","id":"linear-function-of-the-parameters","chapter":"6 Non-linear Regression","heading":"6.1.1 Linear Function of the Parameters","text":"assume \\(\\epsilon_i \\sim N(0,\\sigma^2)\\), \\[\n\\hat{\\theta} \\sim (\\mathbf{\\theta},\\sigma^2[\\mathbf{F}(\\theta)'\\mathbf{F}(\\theta)]^{-1})\n\\]= asymptotic normalityAsymptotic means enough data make inference (sample size increases, becomes accurate (true value)).Since want inference linear combinations parameters contrasts.\\(\\mathbf{\\theta} = (\\theta_0,\\theta_1,\\theta_2)'\\) want look \\(\\theta_1 - \\theta_2\\); can define vector \\(\\mathbf{} = (0,1,-1)'\\), consider inference \\(\\mathbf{'\\theta}\\)Rules expectation variance fixed vector \\(\\mathbf{}\\) random vector \\(\\mathbf{Z}\\);\\[\nE(\\mathbf{'Z}) = \\mathbf{'}E(\\mathbf{Z}) \\\\\nvar(\\mathbf{'Z}) = \\mathbf{'}var(\\mathbf{Z}) \\mathbf{}\n\\],\\[\n\\mathbf{'\\hat{\\theta}} \\sim (\\mathbf{'\\theta},\\sigma^2\\mathbf{'[F(\\theta)'F(\\theta)]^{-1}})\n\\]\\(\\mathbf{'\\hat{\\theta}}\\) asymptotically independent \\(s^2\\) (order 1/n) \\[\n\\frac{\\mathbf{'\\hat{\\theta}-'\\theta}}{s(\\mathbf{'[F(\\theta)'F(\\theta)]^{-1}})^{1/2}} \\sim t_{n-p}\n\\]construct \\(100(1-\\alpha)\\%\\) confidence interval \\(\\mathbf{'\\theta}\\)\\[\n\\mathbf{'\\theta} \\pm t_{(1-\\alpha/2,n-p)}s(\\mathbf{'[F(\\theta)'F(\\theta)]^{-1}})^{1/2}\n\\]Suppose \\(\\mathbf{'} = (0,...,j,...,0)\\). , confidence interval jth element \\(\\mathbf{\\theta}\\) \\[\n\\hat{\\theta}_j \\pm t_{(1-\\alpha/2,n-p)}s\\sqrt{\\hat{c}^{j}}\n\\]\\(\\hat{c}^{j}\\) jth diagonal element \\([\\mathbf{F(\\hat{\\theta})'F(\\hat{\\theta})}]^{-1}\\)example, can get starting values using linearized version function \\(\\log y = \\log + b x\\). , can fit linear regression use estimates starting valueswith nls, can fit nonlinear model via least squares","code":"\n#set a seed value \nset.seed(23)\n\n#Generate x as 100 integers using seq function\nx<-seq(0,100,1)\n\n#Generate y as a*e^(bx)+c\ny<-runif(1,0,20)*exp(runif(1,0.005,0.075)*x)+runif(101,0,5)\n\n# visualize\nplot(x,y)\n#define our data frame\ndatf = data.frame(x,y)\n\n#define our model function\nmod =function(a,b,x) a*exp(b*x)\n#get starting values by linearizing\nlin_mod=lm(log(y)~x,data=datf)\n\n#convert the a parameter back from the log scale; b is ok \nastrt = exp(as.numeric(lin_mod$coef[1]))\nbstrt = as.numeric(lin_mod$coef[2])\nprint(c(astrt,bstrt))## [1] 14.07964761  0.01855635\nnlin_mod = nls(y ~ mod(a, b, x),\n               start = list(a = astrt, b = bstrt),\n               data = datf)\n\n#look at model fit summary\nsummary(nlin_mod)## \n## Formula: y ~ mod(a, b, x)\n## \n## Parameters:\n##    Estimate Std. Error t value Pr(>|t|)    \n## a 13.603909   0.165390   82.25   <2e-16 ***\n## b  0.019110   0.000153  124.90   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.542 on 99 degrees of freedom\n## \n## Number of iterations to convergence: 3 \n## Achieved convergence tolerance: 7.006e-07\n#add prediction to plot\nplot(x, y)\nlines(x, predict(nlin_mod), col = \"red\")"},{"path":"non-linear-regression.html","id":"nonlinear","chapter":"6 Non-linear Regression","heading":"6.1.2 Nonlinear","text":"Suppose \\(h(\\theta)\\) nonlinear function parameters. can use Taylor series \\(\\theta\\)\\[\nh(\\hat{\\theta}) \\approx h(\\theta) + \\mathbf{h}'[\\hat{\\theta}-\\theta]\n\\]\\(\\mathbf{h} = (\\frac{\\partial h}{\\partial \\theta_1},...,\\frac{\\partial h}{\\partial \\theta_p})'\\)\\[\nE( \\hat{\\theta}) \\approx \\theta \\\\\nvar(\\hat{\\theta}) \\approx  \\sigma^2[\\mathbf{F(\\theta)'F(\\theta)}]^{-1} \\\\\nE(h(\\hat{\\theta})) \\approx h(\\theta) \\\\\nvar(h(\\hat{\\theta})) \\approx \\sigma^2 \\mathbf{h'[F(\\theta)'F(\\theta)]^{-1}h}\n\\]Thus,\\[\nh(\\hat{\\theta}) \\sim (h(\\theta),\\sigma^2\\mathbf{h'[F(\\theta)'F(\\theta)]^{-1}h})\n\\]approximate \\(100(1-\\alpha)\\%\\) confidence interval \\(h(\\theta)\\) \\[\nh(\\hat{\\theta}) \\pm t_{(1-\\alpha/2;n-p)}s(\\mathbf{h'[F(\\theta)'F(\\theta)]^{-1}h})^{1/2}\n\\]\\(\\mathbf{h}\\) \\(\\mathbf{F}(\\theta)\\) evaluated \\(\\hat{\\theta}\\)Regarding prediction interval Y \\(x=x_0\\)\\[\nY_0 = f(x_0;\\theta) + \\epsilon_0, \\epsilon_0 \\sim N(0,\\sigma^2) \\\\\n\\hat{Y}_0 = f(x_0,\\hat{\\theta})\n\\]\\(n \\\\infty\\), \\(\\hat{\\theta} \\\\theta\\), \\[\nf(x_0, \\hat{\\theta}) \\approx f(x_0,\\theta) + \\mathbf{f}_0(\\mathbf{\\theta})'[\\hat{\\theta}-\\theta]\n\\]\\[\nf_0(\\theta)= (\\frac{\\partial f(x_0,\\theta)}{\\partial \\theta_1},..,\\frac{\\partial f(x_0,\\theta)}{\\partial \\theta_p})'\n\\](note: \\(f_0(\\theta)\\) different \\(f(\\theta)\\)).\\[\nY_0 - \\hat{Y}_0 \\approx Y_0  - f(x_0,\\theta) - f_0(\\theta)'[\\hat{\\theta}-\\theta]  \\\\\n= \\epsilon_0 - f_0(\\theta)'[\\hat{\\theta}-\\theta]\n\\]\\[\nE(Y_0 - \\hat{Y}_0) \\approx E(\\epsilon_0)E(\\hat{\\theta}-\\theta) = 0 \\\\\nvar(Y_0 - \\hat{Y}_0) \\approx var(\\epsilon_0 - \\mathbf{(f_0(\\theta)'[\\hat{\\theta}-\\theta])}) \\\\\n= \\sigma^2 + \\sigma^2 \\mathbf{f_0 (\\theta)'[F(\\theta)'F(\\theta)]^{-1}f_0(\\theta)} \\\\\n= \\sigma^2 (1 + \\mathbf{f_0 (\\theta)'[F(\\theta)'F(\\theta)]^{-1}f_0(\\theta)})\n\\]Hence, combining\\[\nY_0 - \\hat{Y}_0 \\sim (0,\\sigma^2 (1 + \\mathbf{f_0 (\\theta)'[F(\\theta)'F(\\theta)]^{-1}f_0(\\theta)}))\n\\]Note:Confidence intervals mean response \\(Y_i\\) (different prediction intervals) can obtained similarly.","code":""},{"path":"non-linear-regression.html","id":"non-linear-least-squares","chapter":"6 Non-linear Regression","heading":"6.2 Non-linear Least Squares","text":"LS estimate \\(\\theta\\), \\(\\hat{\\theta}\\) set parameters minimizes residual sum squares:\\[\nS(\\hat{\\theta}) = SSE(\\hat{\\theta}) = \\sum_{=1}^{n}\\{Y_i - f(\\mathbf{x_i};\\hat{\\theta})\\}^2\n\\]obtain solution, can consider partial derivatives \\(S(\\theta)\\) respect \\(\\theta_j\\) set 0, gives system p equations. normal equation \\[\n\\frac{\\partial S(\\theta)}{\\partial \\theta_j} = -2\\sum_{=1}^{n}\\{Y_i -f(\\mathbf{x}_i;\\theta)\\}[\\frac{\\partial(\\mathbf{x}_i;\\theta)}{\\partial \\theta_j}] = 0\n\\]can’t obtain solution directly/analytically equation.Numerical SolutionsGrid search\n“grid” possible parameter values see one minimize residual sum squares.\nfiner grid = greater accuracy\ninefficient, hard p large.\nGrid searchA “grid” possible parameter values see one minimize residual sum squares.finer grid = greater accuracycould inefficient, hard p large.Gauss-Newton Algorithm\ninitial estimate \\(\\theta\\) denoted \\(\\hat{\\theta}^{(0)}\\)\nuse Taylor expansions \\(f(\\mathbf{x}_i;\\theta)\\) function \\(\\theta\\) point \\(\\hat{\\theta}^{(0)}\\)\nGauss-Newton Algorithmwe initial estimate \\(\\theta\\) denoted \\(\\hat{\\theta}^{(0)}\\)use Taylor expansions \\(f(\\mathbf{x}_i;\\theta)\\) function \\(\\theta\\) point \\(\\hat{\\theta}^{(0)}\\)\\[\n\\begin{aligned} \nY_i &= f(x_i;\\theta) + \\epsilon_i \\\\\n&= f(x_i;\\theta) + \\sum_{j=1}^{p}\\{\\frac{\\partial f(x_i;\\theta)}{\\partial \\theta_j}\\}_{\\theta = \\hat{\\theta}^{(0)}} (\\theta_j - \\hat{\\theta}^{(0)}) + \\text{remainder} + \\epsilon_i\n\\end{aligned}\n\\]Equivalently,matrix notation,\\[\n\\mathbf{Y} = \n\\left[ \n\\begin{array}\n{c}\nY_1 \\\\\n. \\\\\nY_n\n\\end{array} \n\\right]\n\\]\\[\n\\mathbf{f}(\\hat{\\theta}^{(0)}) =\n\\left[ \n\\begin{array}\n{c}\nf(\\mathbf{x_1,\\hat{\\theta}}^{(0)}) \\\\\n. \\\\\nf(\\mathbf{x_n,\\hat{\\theta}}^{(0)})\n\\end{array} \n\\right]\n\\]\\[\n\\mathbf{\\epsilon} = \n\\left[ \n\\begin{array}\n{c}\n\\epsilon_1 \\\\\n. \\\\\n\\epsilon_n\n\\end{array} \n\\right]\n\\]\\[\n\\mathbf{F}(\\hat{\\theta}^{(0)}) = \n\\left[ \n\\begin{array}\n{ccc}\n\\frac{\\partial f(x_1,\\mathbf{\\theta})}{\\partial \\theta_1} & ... & \\frac{\\partial f(x_1,\\mathbf{\\theta})}{\\partial \\theta_p}\\\\\n. & . & . \\\\\n\\frac{\\partial f(x_n,\\mathbf{\\theta})}{\\partial \\theta_1} & ... & \\frac{\\partial f(x_n,\\mathbf{\\theta})}{\\partial \\theta_p}\n\\end{array} \\right]_{\\theta = \\hat{\\theta}^{(0)}}\n\\]Hence,\\[\n\\mathbf{Y} = \\mathbf{f}(\\hat{\\theta}^{(0)}) + \\mathbf{F}(\\hat{\\theta}^{(0)})(\\theta - \\hat{\\theta}^{(0)}) + \\epsilon + \\text{remainder}\n\\]assume remainder small error term assumed iid mean 0 variance \\(\\sigma^2\\).can rewrite equation \\[\n\\mathbf{Y} - \\mathbf{f}(\\hat{\\theta}^{(0)}) \\approx \\mathbf{F}(\\hat{\\theta}^{(0)})(\\theta - \\hat{\\theta}^{(0)}) + \\epsilon\n\\]form linear model. solve \\((\\theta - \\hat{\\theta}^{(0)})\\) let equal \\(\\hat{\\delta}^{(1)}\\)\nnew estimate given adding Gauss increment adjustment initial estimate \\(\\hat{\\theta}^{(1)} = \\hat{\\theta}^{(0)} + \\hat{\\delta}^{(1)}\\)\ncan repeat process.Gauss-Newton Algorithm Steps:initial estimate \\(\\hat{\\theta}^{(0)}\\), set j = 0Taylor series expansion calculate \\(\\mathbf{f}(\\hat{\\theta}^{(j)})\\) \\(\\mathbf{F}(\\hat{\\theta}^{(j)})\\)Use OLS get \\(\\hat{\\delta}^{(j+1)}\\)get new estimate \\(\\hat{\\theta}^{(j+1)}\\), return step 2continue “convergence”final parameter estimate \\(\\hat{\\theta}\\), can estimate \\(\\sigma^2\\) \\(\\epsilon \\sim (\\mathbf{0}, \\sigma^2 \\mathbf{})\\) \\[\n\\hat{\\sigma}^2= \\frac{1}{n-p}(\\mathbf{Y}-\\mathbf{f}(x;\\hat{\\theta}))'(\\mathbf{Y}-\\mathbf{f}(x;\\hat{\\theta}))\n\\]Criteria convergenceMinor change objective function (SSE = residual sum squares)\\[\n\\frac{|SSE(\\hat{\\theta}^{(j+1)})-SSE(\\hat{\\theta}^{(j)})|}{SSE(\\hat{\\theta}^{(j)})} < \\gamma_1\n\\]Minor change parameter estimates\\[\n|\\hat{\\theta}^{(j+1)}-\\hat{\\theta}^{(j)}| < \\gamma_2\n\\]“residual projection” criterion (Bates Watts 1981)","code":""},{"path":"non-linear-regression.html","id":"alternative-of-gauss-newton-algorithm","chapter":"6 Non-linear Regression","heading":"6.2.1 Alternative of Gauss-Newton Algorithm","text":"","code":""},{"path":"non-linear-regression.html","id":"gauss-newton-algorithm","chapter":"6 Non-linear Regression","heading":"6.2.1.1 Gauss-Newton Algorithm","text":"Normal equations:\\[\n\\frac{\\partial SSE(\\theta)}{\\partial \\theta} = 2\\mathbf{F}(\\theta)'[\\mathbf{Y}-\\mathbf{f}(\\theta)]\n\\]\\[\n\\begin{aligned}\n\\hat{\\theta}^{(j+1)} &= \\hat{\\theta}^{(j)} + \\hat{\\delta}^{(j+1)} \\\\\n&= \\hat{\\theta}^{(j)} + [\\mathbf{F}((\\hat{\\theta})^{(j)})'\\mathbf{F}(\\hat{\\theta}^{(j)})]^{-1}\\mathbf{F}(\\hat{\\theta})^{(j)} \\\\\n&= \\hat{\\theta}^{(j)} - \\frac{1}{2}[\\mathbf{F}(\\hat{\\theta}^{(j)})'\\mathbf{F}(\\hat{\\theta}^{(j)})]^{-1}\\frac{\\partial SSE(\\hat{\\theta}^{(j)})}{\\partial \\theta}\n\\end{aligned}\n\\]\\(\\frac{\\partial SSE(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\) gradient vecotr (points direction SSE increases rapidly). path known steepest ascent.\\([\\mathbf{F}(\\hat{\\theta}^{(j)})'\\mathbf{F}(\\hat{\\theta}^{(j)})]^{-1}\\) indicates far move\\(-1/2\\): indicator direction steepest descent.","code":""},{"path":"non-linear-regression.html","id":"modified-gauss-newton-algorithm","chapter":"6 Non-linear Regression","heading":"6.2.1.2 Modified Gauss-Newton Algorithm","text":"avoid overstepping (local min), can use modified Gauss-Newton Algorithm. define new proposal \\(\\theta\\)\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} + \\alpha_j \\hat{\\delta}^{(j+1)}, 0 < \\alpha_j < 1\n\\]\\(\\alpha_j\\) (called “learning rate”): used modify step length.also \\(\\alpha *1/2\\), typically assumed absorbed learning rate.way choose \\(\\alpha_j\\), can use step halving\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} + \\frac{1}{2^k}\\hat{\\delta}^{(j+1)}\n\\]wherek smallest non-negative integer \\[\nSSE(\\hat{\\theta}^{(j)}+\\frac{1}{2^k}\\hat{\\delta}^{(j+1)}) < SSE(\\hat{\\theta}^{(j)})\n\\] means try \\(\\hat{\\delta}^{(j+1)}\\), \\(\\hat{\\delta}^{(j+1)}/2\\), \\(\\hat{\\delta}^{(j+1)}/4\\), etc.general form convergence algorithm \\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j \\mathbf{}_j \\frac{\\partial Q(\\hat{\\theta}^{(j)})}{\\partial \\theta} \n\\]\\(\\mathbf{}_j\\) positive definite matrix\\(\\alpha_j\\) learning rate\\(\\frac{\\partial Q(\\hat{\\theta}^{(j)})}{\\partial \\theta}\\)gradient based objective function Q (function \\(\\theta\\)), typically SSE nonlinear regression applications (e.g., cross-entropy classification).Refer back Modified Gauss-Newton Algorithm, can see form\\[\n\\hat{\\theta}^{(j+1)} =\\hat{\\theta}^{(j)} - \\alpha_j[\\mathbf{F}(\\hat{\\theta}^{(j)})'\\mathbf{F}(\\hat{\\theta}^{(j)})]^{-1}\\frac{\\partial SSE(\\hat{\\theta}^{(j)})}{\\partial \\theta}\n\\]Q = SSE, \\([\\mathbf{F}(\\hat{\\theta}^{(j)})'\\mathbf{F}(\\hat{\\theta}^{(j)})]^{-1} = \\mathbf{}\\)","code":""},{"path":"non-linear-regression.html","id":"steepest-descent","chapter":"6 Non-linear Regression","heading":"6.2.1.3 Steepest Descent","text":"(also known just “gradient descent”)\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j \\mathbf{}_{p \\times p}\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}\n\\]slow converge, moves rapidly initially.use starting values","code":""},{"path":"non-linear-regression.html","id":"levenberg--marquardt","chapter":"6 Non-linear Regression","heading":"6.2.1.4 Levenberg -Marquardt","text":"\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j [\\mathbf{F}(\\hat{\\theta}^{(j)})'\\mathbf{F}(\\hat{\\theta}^{(j)})+ \\tau \\mathbf{}_{p \\times p}]\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}\n\\]compromise Gauss-Newton Algorithm Steepest Descent.best \\(\\mathbf{F}(\\hat{\\theta}^{(j)})'\\mathbf{F}(\\hat{\\theta}^{(j)})\\) nearly singular (\\(\\mathbf{F}(\\hat{\\theta}^{(j)})\\) isn’t full rank)similar ridge regressionIf \\(SSE(\\hat{\\theta}^{(j+1)}) < SSE(\\hat{\\theta}^{(j)})\\), \\(\\tau= \\tau/10\\) next iteration. Otherwise, \\(\\tau = 10 \\tau\\)","code":""},{"path":"non-linear-regression.html","id":"newton-raphson","chapter":"6 Non-linear Regression","heading":"6.2.1.5 Newton-Raphson","text":"\\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j [\\frac{\\partial^2Q(\\hat{\\theta}^{(j)})}{\\partial \\theta \\partial \\theta'}]^{-1}\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}\n\\]Hessian matrix can rewritten :\\[\n\\frac{ \\partial^2Q(\\hat{ \\theta}^{(j)})}{ \\partial \\theta \\partial \\theta'} = 2 \\mathbf{F}((\\hat{ \\theta})^{(j)})' \\mathbf{F} ( \\hat{\\theta}^{(j)}) - 2\\sum_{=1}^{n} [Y_i - f(x_i;\\theta)] \\frac{\\partial^2f(x_i;\\theta)}{\\partial \\theta \\partial \\theta'}\n\\]contains term Gauss-Newton Algorithm, combined one containing second partial derivatives f(). (methods require second derivatives objective function known “second-order methods.”)\nHowever, last term \\(\\frac{\\partial^2f(x_i;\\theta)}{\\partial \\theta \\partial \\theta'}\\) can sometimes nonsingular.","code":""},{"path":"non-linear-regression.html","id":"quasi-newton","chapter":"6 Non-linear Regression","heading":"6.2.1.6 Quasi-Newton","text":"update \\(\\theta\\) according \\[\n\\hat{\\theta}^{(j+1)} = \\hat{\\theta}^{(j)} - \\alpha_j \\mathbf{H}_j^{-1}\\frac{\\partial \\mathbf{Q}(\\hat{\\theta}^{(j)})}{\\partial \\theta}\n\\]\\(H_j\\) symmetric positive definite approximation Hessian, gets closer \\(j \\\\infty\\).\\(\\mathbf{H}_j\\) computed iterativelyAMong first-order methods(first derivatives required), method performs best.","code":""},{"path":"non-linear-regression.html","id":"derivative-free-methods","chapter":"6 Non-linear Regression","heading":"6.2.1.7 Derivative Free Methods","text":"secant Method: like Gauss-Newton Algorithm, calculates derivatives numerically past iterations.Simplex MethodsGenetic AlgorithmDifferential Evolution AlgorithmsParticle Swarm OptimizationAnt Colony Optimization","code":""},{"path":"non-linear-regression.html","id":"practical-considerations","chapter":"6 Non-linear Regression","heading":"6.2.2 Practical Considerations","text":"converge, algorithm need good initial estimates.Starting values:\nPrior theoretical info\ngrid search graph \\(SSE(\\theta)\\)\nalso use OLS get starting values.\nModel interpretation: idea regarding form objective function, can try guess initial value.\nExpected Value Parameterization\nStarting values:Prior theoretical infoA grid search graph \\(SSE(\\theta)\\)also use OLS get starting values.Model interpretation: idea regarding form objective function, can try guess initial value.Expected Value ParameterizationConstrained Parameters: (constraints parameters like \\(\\theta_i>,< \\theta_i <b\\))\nfit model first see converged parameter estimates satisfy constraints.\ndont’ satisfy, try re-parameterizing\nConstrained Parameters: (constraints parameters like \\(\\theta_i>,< \\theta_i <b\\))fit model first see converged parameter estimates satisfy constraints.dont’ satisfy, try re-parameterizing","code":""},{"path":"non-linear-regression.html","id":"failure-to-converge","chapter":"6 Non-linear Regression","heading":"6.2.2.1 Failure to converge","text":"\\(SSE(\\theta)\\) may “flat” neighborhood minimum.can try different “better” starting values.Might suggest model complex data, might consider simpler model.","code":""},{"path":"non-linear-regression.html","id":"convergence-to-a-local-minimum","chapter":"6 Non-linear Regression","heading":"6.2.2.2 Convergence to a Local Minimum","text":"Linear least squares property \\(SSE(\\theta) = \\mathbf{(Y-X\\beta)'(Y-X\\beta)}\\), quadratic unique minimum (maximum).Nonlinear east squares need unique minimumUsing different starting values can helpIf dimension \\(\\theta\\) low, graph \\(SSE(\\theta)\\) function \\(\\theta_i\\)Different algorithm can help (e.g., genetic algorithm, particle swarm)converge, algorithms need good initial estimates.Starting values:\nprior theoretical info\ngrid search graph\nOLS estimates starting values\nModel interpretation\nExpected Value Parameterization\nStarting values:prior theoretical infoA grid search graphOLS estimates starting valuesModel interpretationExpected Value ParameterizationConstrained Parameters:\ntry model without constraints first.\nresulted parameter estimates satisfy constraint, try re-parameterizing\nConstrained Parameters:try model without constraints first.resulted parameter estimates satisfy constraint, try re-parameterizingFor prediction intervalBased forms function, can also programmed starting values nls function (e.e.g, logistic growth, asymptotic regression, etc).example, logistic growth model:\\[\nP = \\frac{K}{1+ exp(P_0+ rt)} + \\epsilon\n\\]whereP = population time tK = carrying capacityr = population growth ratebut R slight different parameterization:\\[\nP = \\frac{asym}{1 + exp(\\frac{xmid - t}{scal})}\n\\]whereasym = carrying capacityxmid = x value inflection point curvescal = scaling parameter.Hence, haveK = asymr = -1/scal\\(P_0 = -rxmid\\)parameterizationIf can also define self-starting fucntion models uncommon (built nls)Example based (Schabenberger Pierce 2001)suggested model (known plateau model) \\[\nE(Y_{ij}) = (\\beta_{0j} + \\beta_{1j}N_{ij})I_{N_{ij}\\le \\alpha_j} + (\\beta_{0j} + \\beta_{1j}\\alpha_j)I_{N_{ij} > \\alpha_j}\n\\]whereN observationi particular observationj = 1,2 corresponding depths (30,60)define selfStart function. defined model linear first part plateau (remain constant) can use first half predictors (sorted increasing value) get initial estimate slope intercept model, last predictor value (alpha) can starting value plateau parameter.combine model custom function calculate starting values.Instead modeling depths model separately model together - common slope, intercept, plateau.Examine residual values combined model.can test whether parameters two soil depth fits significantly different? know combined model appropriate, consider parameterization let parameters 60cm model equal parameters 30cm model plus increment:\\[\n\\beta_{02} = \\beta_{01} + d_0 \\\\\n\\beta_{12} = \\beta_{11} + d_1 \\\\\n\\alpha_{2} = \\alpha_{1} + d_a\n\\]can implement following function:Starting values easy now fit model individually., increment parameters, \\(d_1\\),\\(d_2\\),\\(d_a\\) significantly different 0, suggesting two models .","code":"\n# Grid search\n#choose grid of a and b values\naseq = seq(10,18,.2)\nbseq = seq(.001,.075,.001)\n\nna = length(aseq)\nnb = length(bseq)\nSSout = matrix(0,na*nb,3) #matrix to save output\ncnt = 0\nfor (k in 1:na){\n   for (j in 1:nb){\n      cnt = cnt+1\n      ypred = mod(aseq[k],bseq[j],x) #evaluate model w/ these parms\n      ss = sum((y-ypred)^2)  #this is our SSE objective function\n      #save values of a, b, and SSE\n      SSout[cnt,1]=aseq[k]\n      SSout[cnt,2]=bseq[j]\n      SSout[cnt,3]=ss\n   }\n}\n#find minimum SSE and associated a,b values\nmn_indx = which.min(SSout[,3])\nastrt = SSout[mn_indx,1]\nbstrt = SSout[mn_indx,2]\n#now, run nls function with these starting values\nnlin_modG=nls(y~mod(a,b,x),start=list(a=astrt,b=bstrt)) \n\nnlin_modG## Nonlinear regression model\n##   model: y ~ mod(a, b, x)\n##    data: parent.frame()\n##        a        b \n## 13.60391  0.01911 \n##  residual sum-of-squares: 235.5\n## \n## Number of iterations to convergence: 3 \n## Achieved convergence tolerance: 2.293e-07\n# Note, the package `nls_multstart` will allow you to do a grid search without programming your own loop\nplotFit(\n  nlin_modG,\n  interval = \"both\",\n  pch = 19,\n  shade = TRUE,\n  col.conf = \"skyblue4\",\n  col.pred = \"lightskyblue2\",\n  data = datf\n)  \napropos(\"^SS\")##  [1] \"ss\"          \"SSasymp\"     \"SSasympOff\"  \"SSasympOrig\" \"SSbiexp\"    \n##  [6] \"SSD\"         \"SSfol\"       \"SSfpl\"       \"SSgompertz\"  \"SSlogis\"    \n## [11] \"SSmicmen\"    \"SSout\"       \"SSweibull\"\n# simulated data\ntime <- c(1, 2, 3, 5, 10, 15, 20, 25, 30, 35)\npopulation <- c(2.8, 4.2, 3.5, 6.3, 15.7, 21.3, 23.7, 25.1, 25.8, 25.9)\nplot(time, population, las = 1, pch = 16)\n# model fitting\nlogisticModelSS <- nls(population ~ SSlogis(time, Asym, xmid, scal))\nsummary(logisticModelSS)## \n## Formula: population ~ SSlogis(time, Asym, xmid, scal)\n## \n## Parameters:\n##      Estimate Std. Error t value Pr(>|t|)    \n## Asym  25.5029     0.3666   69.56 3.34e-11 ***\n## xmid   8.7347     0.3007   29.05 1.48e-08 ***\n## scal   3.6353     0.2186   16.63 6.96e-07 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.6528 on 7 degrees of freedom\n## \n## Number of iterations to convergence: 1 \n## Achieved convergence tolerance: 1.908e-06\ncoef(logisticModelSS)##      Asym      xmid      scal \n## 25.502890  8.734698  3.635333\n#convert to other parameterization\nKs = as.numeric(coef(logisticModelSS)[1])\nrs = -1/as.numeric(coef(logisticModelSS)[3])\nPos = - rs * as.numeric(coef(logisticModelSS)[2])\n#let's refit with these parameters\nlogisticModel <- nls(population ~ K / (1 + exp(Po + r * time)),start=list(Po=Pos,r=rs,K=Ks))\nsummary(logisticModel)## \n## Formula: population ~ K/(1 + exp(Po + r * time))\n## \n## Parameters:\n##    Estimate Std. Error t value Pr(>|t|)    \n## Po  2.40272    0.12702   18.92 2.87e-07 ***\n## r  -0.27508    0.01654  -16.63 6.96e-07 ***\n## K  25.50289    0.36665   69.56 3.34e-11 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.6528 on 7 degrees of freedom\n## \n## Number of iterations to convergence: 0 \n## Achieved convergence tolerance: 1.924e-06\n#note: initial values =  solution (highly unusual, but ok)\nplot(time, population, las = 1, pch = 16)\nlines(time, predict(logisticModel), col = \"red\")\n#Load data\ndat <- read.table(\"images/dat.txt\", header = T)\n# plot\ndat.plot <-\n  ggplot(dat) + geom_point(aes(\n    x = no3,\n    y = ryp,\n    color = as.factor(depth)\n  )) +\n  labs(color = 'Depth (cm)') + xlab('Soil NO3') + ylab('relative yield percent')\ndat.plot\n#First define model as a function\nnonlinModel <- function(predictor,b0,b1,alpha){\n  ifelse(predictor<=alpha, \n         b0+b1*predictor, #if observation less than cutoff simple linear model\n         b0+b1*alpha) #otherwise flat line\n}\nnonlinModelInit <- function(mCall,LHS,data){\n  #sort data by increasing predictor value - \n  #done so we can just use the low level no3 conc to fit a simple model\n  xy <- sortedXyData(mCall[['predictor']],LHS,data)\n  n <- nrow(xy)\n  #For the first half of the data a simple linear model is fit\n  lmFit <- lm(xy[1:(n/2),'y']~xy[1:(n/2),'x'])\n  b0 <- coef(lmFit)[1]\n  b1 <- coef(lmFit)[2]\n  #for the cut off to the flat part select the last x value used in creating linear model\n  alpha <- xy[(n/2),'x']\n  value <- c(b0,b1,alpha)\n  names(value) <- mCall[c('b0','b1','alpha')]\n  value\n}\nSS_nonlinModel <- selfStart(nonlinModel,nonlinModelInit,c('b0','b1','alpha'))\n#Above code defined model and selfStart now just need to call it for each of the depths\nsep30_nls <-\n  nls(ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha), data = dat[dat$depth ==\n                                                                         30, ])\n\nsep60_nls <-\n  nls(ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha), data = dat[dat$depth ==\n                                                                         60, ])\n\npar(mfrow = c(1, 2))\nplotFit(\n  sep30_nls,\n  interval = \"both\",\n  pch = 19,\n  shade = TRUE,\n  col.conf = \"skyblue4\",\n  col.pred = \"lightskyblue2\",\n  data = dat[dat$depth == 30, ],\n  main = 'Results 30 cm depth',\n  ylab = 'relative yield percent',\n  xlab = 'Soil NO3 concentration',\n  xlim = c(0, 120)\n)\nplotFit(\n  sep60_nls,\n  interval = \"both\",\n  pch = 19,\n  shade = TRUE,\n  col.conf = \"lightpink4\",\n  col.pred = \"lightpink2\",\n  data = dat[dat$depth == 60, ],\n  main = 'Results 60 cm depth',\n  ylab = 'relative yield percent',\n  xlab = 'Soil NO3 concentration',\n  xlim = c(0, 120)\n)\nsummary(sep30_nls)## \n## Formula: ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha)\n## \n## Parameters:\n##       Estimate Std. Error t value Pr(>|t|)    \n## b0     15.1943     2.9781   5.102 6.89e-07 ***\n## b1      3.5760     0.1853  19.297  < 2e-16 ***\n## alpha  23.1324     0.5098  45.373  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 8.258 on 237 degrees of freedom\n## \n## Number of iterations to convergence: 6 \n## Achieved convergence tolerance: 3.608e-09\nsummary(sep60_nls)## \n## Formula: ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha)\n## \n## Parameters:\n##       Estimate Std. Error t value Pr(>|t|)    \n## b0      5.4519     2.9785    1.83   0.0684 .  \n## b1      5.6820     0.2529   22.46   <2e-16 ***\n## alpha  16.2863     0.2818   57.80   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 7.427 on 237 degrees of freedom\n## \n## Number of iterations to convergence: 5 \n## Achieved convergence tolerance: 8.571e-09\nred_nls <-\n  nls(ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha), data = dat)\n\nsummary(red_nls)## \n## Formula: ryp ~ SS_nonlinModel(predictor = no3, b0, b1, alpha)\n## \n## Parameters:\n##       Estimate Std. Error t value Pr(>|t|)    \n## b0      8.7901     2.7688   3.175   0.0016 ** \n## b1      4.8995     0.2207  22.203   <2e-16 ***\n## alpha  18.0333     0.3242  55.630   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 9.13 on 477 degrees of freedom\n## \n## Number of iterations to convergence: 7 \n## Achieved convergence tolerance: 7.126e-09\npar(mfrow = c(1, 1))\nplotFit(\n  red_nls,\n  interval = \"both\",\n  pch = 19,\n  shade = TRUE,\n  col.conf = \"lightblue4\",\n  col.pred = \"lightblue2\",\n  data = dat,\n  main = 'Results combined',\n  ylab = 'relative yield percent',\n  xlab = 'Soil NO3 concentration'\n)\nlibrary(nlstools)\n#using nlstools nlsResiduals function to get some quick residual plots\n#can also use test.nlsResiduals(resid)\n# https://www.rdocumentation.org/packages/nlstools/versions/1.0-2\nresid <- nlsResiduals(red_nls)\nplot(resid)\nnonlinModelF <- function(predictor,soildep,b01,b11,a1,d0,d1,da){\n   b02 = b01 + d0 #make 60cm parms = 30cm parms + increment\n   b12 = b11 + d1\n   a2 = a1 + da\n   \n   y1 = ifelse(predictor<=a1, \n         b01+b11*predictor, #if observation less than cutoff simple linear model\n         b01+b11*a1) #otherwise flat line\n   y2 = ifelse(predictor<=a2, \n               b02+b12*predictor, \n               b02+b12*a2) \n   y =  y1*(soildep == 30) + y2*(soildep == 60)  #combine models\n   return(y)\n}\nSoil_full=nls(ryp~nonlinModelF(predictor=no3,soildep=depth,b01,b11,a1,d0,d1,da),\n              data=dat,\n              start=list(b01=15.2,b11=3.58,a1=23.13,d0=-9.74,d1=2.11,da=-6.85)) \n\nsummary(Soil_full)## \n## Formula: ryp ~ nonlinModelF(predictor = no3, soildep = depth, b01, b11, \n##     a1, d0, d1, da)\n## \n## Parameters:\n##     Estimate Std. Error t value Pr(>|t|)    \n## b01  15.1943     2.8322   5.365 1.27e-07 ***\n## b11   3.5760     0.1762  20.291  < 2e-16 ***\n## a1   23.1324     0.4848  47.711  < 2e-16 ***\n## d0   -9.7424     4.2357  -2.300   0.0219 *  \n## d1    2.1060     0.3203   6.575 1.29e-10 ***\n## da   -6.8461     0.5691 -12.030  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 7.854 on 474 degrees of freedom\n## \n## Number of iterations to convergence: 1 \n## Achieved convergence tolerance: 3.742e-06"},{"path":"non-linear-regression.html","id":"modelestiamtion-adequcy","chapter":"6 Non-linear Regression","heading":"6.2.3 Model/Estiamtion Adequcy","text":"(Bates Watts 1980) assess nonlinearity terms 2 components curvature:Intrinsic nonlinearity: degree bending twisting \\(f(\\theta)\\); estimation approach assumes hte true function relatively flat (planar) neighborhood fo \\(\\hat{\\theta}\\), true \\(f()\\) lot “bending” int neighborhood \\(\\hat{\\theta}\\) (independent parameterizaiton)\nbad, distribution residuals seriously distorted\nslow converge\ndifficult identify ( use function rms.curve)\nSolution:\nuse higher order Taylor expansions estimation\nBayesian method\n\nIntrinsic nonlinearity: degree bending twisting \\(f(\\theta)\\); estimation approach assumes hte true function relatively flat (planar) neighborhood fo \\(\\hat{\\theta}\\), true \\(f()\\) lot “bending” int neighborhood \\(\\hat{\\theta}\\) (independent parameterizaiton)bad, distribution residuals seriously distortedIf bad, distribution residuals seriously distortedslow convergeslow convergedifficult identify ( use function rms.curve)difficult identify ( use function rms.curve)Solution:\nuse higher order Taylor expansions estimation\nBayesian method\nSolution:use higher order Taylor expansions estimationBayesian methodParameter effects nonlinearity: degree curvature (nonlinearity) affected choice \\(\\theta\\) (data dependent; dependent parameterization)\nleads problems inferecne \\(\\hat{\\theta}\\)\nrms.curve MASS can identify\nbootstrap-based inference can also used\nSolution: try reparaemterize.\nParameter effects nonlinearity: degree curvature (nonlinearity) affected choice \\(\\theta\\) (data dependent; dependent parameterization)leads problems inferecne \\(\\hat{\\theta}\\)rms.curve MASS can identifybootstrap-based inference can also usedSolution: try reparaemterize.linear model, Linear Regression, goodness fit measure \\(R^2\\):\\[\nR^2 = \\frac{SSR}{SSTO} = 1- \\frac{SSE}{SSTO} \\\\\n= \\frac{\\sum_{=1}^n (\\hat{Y}_i- \\bar{Y})^2}{\\sum_{=1}^n (Y_i- \\bar{Y})^2} = 1- \\frac{\\sum_{=1}^n ({Y}_i- \\hat{Y})^2}{\\sum_{=1}^n (Y_i- \\bar{Y})^2}\n\\]valid nonlinear case error sum squares model sum squares add total corrected sum squares\\[\nSSR + SSE \\neq SST\n\\]can use pseudo-\\(R^2\\):\\[\nR^2_{pseudo} = 1 - \\frac{\\sum_{=1}^n ({Y}_i- \\hat{Y})^2}{\\sum_{=1}^n (Y_i- \\bar{Y})^2}\n\\]can’t interpret proportion variability explained model. use relative comparison different models.Residual Plots: standardize, similar OLS. useful intrinsic curvature small:studentized residuals\\[\nr_i = \\frac{e_i}{s\\sqrt{1-\\hat{c}_i}}\n\\]\\(\\hat{c}_i\\)-th diagonal \\(\\mathbf{\\hat{H}= F(\\hat{\\theta})[F(\\hat{\\theta})'F(\\hat{\\theta})]^{-1}F(\\hat{\\theta})'}\\)problems ofCollinearity: condition number \\(\\mathbf{[F(\\hat{\\theta})'F(\\hat{\\theta})]^{-1}}\\) less 30. Follow (Magel Hertsgaard 1987); reparameterize possibleCollinearity: condition number \\(\\mathbf{[F(\\hat{\\theta})'F(\\hat{\\theta})]^{-1}}\\) less 30. Follow (Magel Hertsgaard 1987); reparameterize possibleLeverage: Like OLS, consider \\(\\mathbf{\\hat{H}= F(\\hat{\\theta})[F(\\hat{\\theta})'F(\\hat{\\theta})]^{-1}F(\\hat{\\theta})'}\\) (also known “tangent plant hat matrix”) (Laurent Cook 1992)Leverage: Like OLS, consider \\(\\mathbf{\\hat{H}= F(\\hat{\\theta})[F(\\hat{\\theta})'F(\\hat{\\theta})]^{-1}F(\\hat{\\theta})'}\\) (also known “tangent plant hat matrix”) (Laurent Cook 1992)Heterogeneous Errors: weighted Non-linear Least SquaresHeterogeneous Errors: weighted Non-linear Least SquaresCorrelated Errors:\nGeneralized Nonlinear Least Squares\nNonlinear Mixed Models\nBayesian methods\nCorrelated Errors:Generalized Nonlinear Least SquaresNonlinear Mixed ModelsBayesian methods","code":"\n#check parameter effects and intrinsic curvature\n\nmodD = deriv3(~ a*exp(b*x), c(\"a\",\"b\"),function(a,b,x) NULL)\n\nnlin_modD=nls(y~modD(a,b,x),start=list(a=astrt,b=bstrt),data=datf)\n\nrms.curv(nlin_modD)## Parameter effects: c^theta x sqrt(F) = 0.0626 \n##         Intrinsic: c^iota  x sqrt(F) = 0.0062"},{"path":"non-linear-regression.html","id":"application-2","chapter":"6 Non-linear Regression","heading":"6.2.4 Application","text":"\\[\ny_i = \\frac{\\theta_0 + \\theta_1 x_i}{1 + \\theta_2 \\exp(0.4 x_i)} + \\epsilon_i\n\\]\\(= 1,..,n\\)Get starting valuesWe notice \\(Y_{max} = \\theta_0 + \\theta_1 x_i\\) can find x_i datahence, x = 0.0094 y = 2.6722 first equation \\[\n2.6722 = \\theta_0 + 0.0094 \\theta_1 \\\\\n\\theta_0 + 0.0094 \\theta_1 + 0 \\theta_2 = 2.6722\n\\]Secondly, notice can obtain “average” y \\[\n1+ \\theta_2 exp(0.4 x) = 2\n\\]can find average numbers x ywe second equation\\[\n1 + \\theta_2 exp(0.4*11.0648) = 2 \\\\\n0 \\theta_1 + 0 \\theta_1 + 83.58967 \\theta_2 = 1\n\\]Thirdly, can plug value x closest 1 find value yhence \\[\n1.457 = \\frac{\\theta_0 + \\theta_1*0.9895}{1 + \\theta_2 exp(0.4*0.9895)} \\\\\n1.457 + 2.164479 *\\theta_2 = \\theta_0 + \\theta_1*0.9895 \\\\\n\\theta_0 + \\theta_1*0.9895 -  2.164479 *\\theta_2 = 1.457\n\\]3 equations, can solve get starting value \\(\\theta_0,\\theta_1, \\theta_2\\)\\[\n\\theta_0 + 0.0094 \\theta_1 + 0 \\theta_2 = 2.6722 \\\\\n0 \\theta_1 + 0 \\theta_1 + 83.58967 \\theta_2 = 1 \\\\\n\\theta_0 + \\theta_1*0.9895 -  2.164479 *\\theta_2 = 1.457\n\\]Construct manually Gauss-Newton AlgorithmAfter 8 iterations, function converged. objective function value convergence isand parameters \\(\\theta\\)s areand asymptotic variance covariance matrix isIssue encounter problem sensitive starting values. tried value 1 \\(\\theta\\)s, vastly different parameter estimates. , try use model interpretation try find reasonable starting values.Check predefined function nls","code":"## Warning: package 'dplyr' was built under R version 4.0.5## \n## Attaching package: 'dplyr'## The following object is masked from 'package:MASS':\n## \n##     select## The following object is masked from 'package:kableExtra':\n## \n##     group_rows## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\nplot(my_data)\nmax(my_data$y)## [1] 2.6722\nmy_data$x[which.max(my_data$y)]## [1] 0.0094\nmean(my_data$y) #find mean y## [1] -0.0747864\nmy_data$y[which.min(abs(my_data$y-(mean(my_data$y))))] # find y closest to its mean## [1] -0.0773\nmy_data$x[which.min(abs(my_data$y-(mean(my_data$y))))] #find x closest to the mean y## [1] 11.0648\nmy_data$x[which.min(abs(my_data$x-1))] # find value of x closet to 1## [1] 0.9895\nmatch(my_data$x[which.min(abs(my_data$x-1))], my_data$x) # find index of x closest to 1## [1] 14\nmy_data$y[match(my_data$x[which.min(abs(my_data$x-1))], my_data$x)]# find y value## [1] 1.4577\nlibrary(matlib) ## Warning: package 'matlib' was built under R version 4.0.5\nA = matrix(c(0,0.0094, 0, 0,0, 83.58967, 1, 0.9895, - 2.164479), nrow = 3, ncol = 3, byrow = T)\nb = c(2.6722,1,1.457 )\nshowEqn(A, b)## 0*x1 + 0.0094*x2        + 0*x3  =  2.6722 \n## 0*x1      + 0*x2 + 83.58967*x3  =       1 \n## 1*x1 + 0.9895*x2 - 2.164479*x3  =   1.457\nSolve(A, b, fractions = F)## x1      =  -279.80879739 \n##   x2    =   284.27659574 \n##     x3  =      0.0119632\n#starting value\ntheta_0_strt = -279.80879739 \ntheta_1_strt =  284.27659574 \ntheta_2_strt = 0.0119632 \n\n#model\nmod_4 = function(theta_0,theta_1,theta_2,x){\n    (theta_0 + theta_1*x)/(1+ theta_2*exp(0.4*x))\n}\n\n#define a function\nf_4 = expression((theta_0 + theta_1*x)/(1+ theta_2*exp(0.4*x)))\n\n#take the first derivative\ndf_4.d_theta_0=D(f_4,'theta_0')\n\ndf_4.d_theta_1=D(f_4,'theta_1')\n\ndf_4.d_theta_2=D(f_4,'theta_2')\n\n# save the result of all iterations\ntheta_vec = matrix(c(theta_0_strt,theta_1_strt,theta_2_strt))\ndelta= matrix(NA, nrow=3,ncol = 1)\n\nf_theta = as.matrix(eval(f_4,list(x=my_data$x,theta_0 = theta_vec[1,1],theta_1 = theta_vec[2,1],theta_2 = theta_vec[3,1])))\n\ni = 1\n\nrepeat {\n    F_theta_0 = as.matrix(cbind(\n        eval(\n            df_4.d_theta_0,\n            list(\n                x = my_data$x,\n                theta_0 = theta_vec[1, i],\n                theta_1 = theta_vec[2, i],\n                theta_2 = theta_vec[3, i]\n            )\n        ),\n        eval(\n            df_4.d_theta_1,\n            list(\n                x = my_data$x,\n                theta_0 = theta_vec[1, i],\n                theta_1 = theta_vec[2, i],\n                theta_2 = theta_vec[3, i]\n            )\n        ),\n        eval(\n            df_4.d_theta_2,\n            list(\n                x = my_data$x,\n                theta_0 = theta_vec[1, i],\n                theta_1 = theta_vec[2, i],\n                theta_2 = theta_vec[3, i]\n            )\n        )\n    ))\n    delta[, i] = (solve(t(F_theta_0) %*% F_theta_0)) %*% t(F_theta_0) %*% (my_data$y - f_theta[,i])\n    theta_vec = cbind(theta_vec, matrix(NA, nrow = 3, ncol = 1))\n    theta_vec[, i+1] = theta_vec[, i] + delta[, i]\n    i = i + 1\n    \n    f_theta = cbind(f_theta, as.matrix(eval(\n        f_4,\n        list(\n            x = my_data$x,\n            theta_0 = theta_vec[1, i],\n            theta_1 = theta_vec[2, i],\n            theta_2 = theta_vec[3, i]\n        )\n    )))\n    delta = cbind(delta, matrix(NA, nrow = 3, ncol = 1))\n    \n    #convergence criteria based on SSE\n    if (abs(sum((my_data$y - f_theta[,i])^2)-sum((my_data$y - f_theta[,i-1])^2))/(sum((my_data$y - f_theta[,i-1])^2))<0.001){\n        break\n    }\n}\ndelta##               [,1]        [,2]        [,3]       [,4]       [,5]       [,6]\n## [1,]  2.811840e+02 -0.03929013  0.43160654  0.6904856  0.6746748  0.4056460\n## [2,] -2.846545e+02  0.03198446 -0.16403964 -0.2895487 -0.2933345 -0.1734087\n## [3,] -1.804567e-05  0.01530258  0.05137285  0.1183271  0.1613129  0.1160404\n##             [,7] [,8]\n## [1,]  0.09517681   NA\n## [2,] -0.03928239   NA\n## [3,]  0.03004911   NA\ntheta_vec##              [,1]        [,2]        [,3]        [,4]       [,5]       [,6]\n## [1,] -279.8087974  1.37521388  1.33592375  1.76753029  2.4580158  3.1326907\n## [2,]  284.2765957 -0.37788712 -0.34590266 -0.50994230 -0.7994910 -1.0928255\n## [3,]    0.0119632  0.01194515  0.02724773  0.07862059  0.1969477  0.3582607\n##            [,7]       [,8]\n## [1,]  3.5383367  3.6335135\n## [2,] -1.2662342 -1.3055166\n## [3,]  0.4743011  0.5043502\nhead(f_theta)##           [,1]     [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]\n## [1,] -273.8482 1.355410 1.297194 1.633802 2.046023 2.296554 2.389041 2.404144\n## [2,] -209.0859 1.268192 1.216738 1.514575 1.863098 2.059505 2.126009 2.135969\n## [3,] -190.3323 1.242916 1.193433 1.480136 1.810629 1.992095 2.051603 2.060202\n## [4,] -177.1891 1.225196 1.177099 1.456024 1.774000 1.945197 1.999945 2.007625\n## [5,] -148.5872 1.186618 1.141549 1.403631 1.694715 1.844154 1.888953 1.894730\n## [6,] -119.9585 1.147980 1.105961 1.351301 1.615968 1.744450 1.779859 1.783866\n# estimate sigma^2 \n\nsigma2 = 1 / (nrow(my_data) - 3) * (t(my_data$y - (f_theta[, ncol(f_theta)]))) %*%\n    (my_data$y - (f_theta[, ncol(f_theta)])) # p = 3\nsigma2##           [,1]\n## [1,] 0.0801686\nsum((my_data$y - f_theta[,i])^2)## [1] 19.80165\ntheta_vec[,ncol(theta_vec)]## [1]  3.6335135 -1.3055166  0.5043502\nas.numeric(sigma2)*as.matrix(solve(crossprod(F_theta_0)))##             [,1]        [,2]        [,3]\n## [1,]  0.11552571 -0.04817428  0.02685848\n## [2,] -0.04817428  0.02100861 -0.01158212\n## [3,]  0.02685848 -0.01158212  0.00703916\nnlin_4 = nls(y ~ mod_4(theta_0,theta_1, theta_2, x), start = list(theta_0=-279.80879739 ,theta_1=284.27659574 , theta_2=0.0119632), data = my_data)\nnlin_4## Nonlinear regression model\n##   model: y ~ mod_4(theta_0, theta_1, theta_2, x)\n##    data: my_data\n## theta_0 theta_1 theta_2 \n##  3.6359 -1.3064  0.5053 \n##  residual sum-of-squares: 19.8\n## \n## Number of iterations to convergence: 9 \n## Achieved convergence tolerance: 2.294e-07"},{"path":"generalized-linear-models.html","id":"generalized-linear-models","chapter":"7 Generalized Linear Models","heading":"7 Generalized Linear Models","text":"Even though call generalized linear model, still paradigm non-linear regression, form regression model non-linear. name generalized linear model derived fact \\(\\mathbf{x'_i \\beta}\\) (linear form) model.","code":""},{"path":"generalized-linear-models.html","id":"logistic-regression","chapter":"7 Generalized Linear Models","heading":"7.1 Logistic Regression","text":"\\[\np_i = f(\\mathbf{x}_i ; \\beta) = \\frac{exp(\\mathbf{x_i'\\beta})}{1 + exp(\\mathbf{x_i'\\beta})}\n\\]Equivalently,\\[\nlogit(p_i) = log(\\frac{p_i}{1+p_i}) = \\mathbf{x_i'\\beta}\n\\]\\(\\frac{p_i}{1+p_i}\\)odds.form, model specified function mean response linear. Hence, Generalized Linear ModelsThe likelihood function\\[\nL(p_i) = \\prod_{=1}^{n} p_i^{Y_i}(1-p_i)^{1-Y_i}\n\\]\\(p_i = \\frac{\\mathbf{x'_i \\beta}}{1+\\mathbf{x'_i \\beta}}\\) \\(1-p_i = (1+ exp(\\mathbf{x'_i \\beta}))^{-1}\\)Hence, objective function \\[\nQ(\\beta) = log(L(\\beta)) = \\sum_{=1}^n Y_i \\mathbf{x'_i \\beta} - \\sum_{=1}^n  log(1+ exp(\\mathbf{x'_i \\beta}))\n\\]maximize function numerically using optimization method , allows us find numerical MLE \\(\\hat{\\beta}\\). can use standard asymptotic properties MLEs make inference.Property MLEs parameters asymptotically unbiased sample variance-covariance matrix given inverse Fisher information matrix\\[\n\\hat{\\beta} \\dot{\\sim} (\\beta,[\\mathbf{}(\\beta)]^{-1})\n\\]Fisher Information matrix, \\(\\mathbf{}(\\beta)\\) \\[\n\\begin{aligned}\n\\mathbf{}(\\beta) &= E[\\frac{\\partial \\log(L(\\beta))}{\\partial (\\beta)}\\frac{\\partial \\log(L(\\beta))}{\\partial \\beta'}] \\\\\n&= E[(\\frac{\\partial \\log(L(\\beta))}{\\partial \\beta_i} \\frac{\\partial \\log(L(\\beta))}{\\partial \\beta_j})_{ij}]\n\\end{aligned}\n\\]regularity conditions, equivalent negative expected value Hessian Matrix\\[\n\\begin{aligned}\n\\mathbf{}(\\beta) &= -E[\\frac{\\partial^2 \\log(L(\\beta))}{\\partial \\beta \\partial \\beta'}] \\\\\n&= -E[(\\frac{\\partial^2 \\log(L(\\beta))}{\\partial \\beta_i \\partial \\beta_j})_{ij}]\n\\end{aligned}\n\\]Example:\\[\nx_i' \\beta = \\beta_0 + \\beta_1 x_i\n\\]\\[\n- \\frac{\\partial^2 \\ln(L(\\beta))}{\\partial \\beta^2_0} = \\sum_{=1}^n \\frac{\\exp(x'_i \\beta)}{1 + \\exp(x'_i \\beta)} - [\\frac{\\exp(x_i' \\beta)}{1+ \\exp(x'_i \\beta)}]^2 = \\sum_{=1}^n p_i (1-p_i) \\\\\n- \\frac{\\partial^2 \\ln(L(\\beta))}{\\partial \\beta^2_1} = \\sum_{=1}^n \\frac{x_i^2\\exp(x'_i \\beta)}{1 + \\exp(x'_i \\beta)} - [\\frac{x_i\\exp(x_i' \\beta)}{1+ \\exp(x'_i \\beta)}]^2 = \\sum_{=1}^n x_i^2p_i (1-p_i) \\\\\n- \\frac{\\partial^2 \\ln(L(\\beta))}{\\partial \\beta_0 \\partial \\beta_1} = \\sum_{=1}^n \\frac{x_i\\exp(x'_i \\beta)}{1 + \\exp(x'_i \\beta)} - x_i[\\frac{\\exp(x_i' \\beta)}{1+ \\exp(x'_i \\beta)}]^2 = \\sum_{=1}^n x_ip_i (1-p_i) \\\\\n\\]Hence,\\[\n\\mathbf{} (\\beta) = \n\\left[\n\\begin{array}\n{cc}\n\\sum_i p_i(1-p_i) & \\sum_i x_i p_i(1-p_i) \\\\\n\\sum_i x_i p_i(1-p_i) & \\sum_i x_i^2 p_i(1-p_i)\n\\end{array}\n\\right]\n\\]InferenceLikelihood Ratio TestsTo formulate test, let \\(\\beta = [\\beta_1', \\beta_2']'\\). interested testing hypothesis \\(\\beta_1\\), leave \\(\\beta_2\\) unspecified (called nuisance parameters). \\(\\beta_1\\) \\(\\beta_2\\) can either vector scalar, \\(\\beta_2\\) can null.Example: \\(H_0: \\beta_1 = \\beta_{1,0}\\) (\\(\\beta_{1,0}\\) specified) \\(\\hat{\\beta}_{2,0}\\) MLE \\(\\beta_2\\) restriction \\(\\beta_1 = \\beta_{1,0}\\). likelihood ratio test statistic \\[\n-2\\log\\Lambda = -2[\\log(L(\\beta_{1,0},\\hat{\\beta}_{2,0})) - \\log(L(\\hat{\\beta}_1,\\hat{\\beta}_2))]\n\\]wherethe first term value fo likelihood fitted restricted modelthe second term likelihood value fitted unrestricted modelUnder null,\\[\n-2 \\log \\Lambda \\sim \\chi^2_{\\upsilon}\n\\]\\(\\upsilon\\) dimension \\(\\beta_1\\)reject null \\(-2\\log \\Lambda > \\chi_{\\upsilon,1-\\alpha}^2\\)Wald StatisticsBased \\[\n\\hat{\\beta} \\sim (\\beta, [\\mathbf{}(\\beta)^{-1}])\n\\]\\[\nH_0: \\mathbf{L}\\hat{\\beta} = 0 \n\\]\\(\\mathbf{L}\\) q x p matrix q linearly independent rows. \\[\nW = (\\mathbf{L\\hat{\\beta}})'(\\mathbf{L[(\\hat{\\beta})]^{-1}L'})^{-1}(\\mathbf{L\\hat{\\beta}})\n\\]null hypothesisConfidence interval\\[\n\\hat{\\beta}_i \\pm 1.96 \\hat{s}_{ii}^2\n\\]\\(\\hat{s}_{ii}^2\\) -th diagonal \\(\\mathbf{[(\\hat{\\beta})]}^{-1}\\)havelarge sample size, likelihood ratio Wald tests similar results.small sample size, likelihood ratio test better.Logistic Regression: Interpretation \\(\\beta\\)single regressor, model \\[\nlogit\\{\\hat{p}_{x_i}\\} \\equiv logit (\\hat{p}_i) = \\log(\\frac{\\hat{p}_i}{1 - \\hat{p}_i}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\n\\]\\(x= x_i + 1\\)\\[\nlogit\\{\\hat{p}_{x_i +1}\\} = \\hat{\\beta}_0 + \\hat{\\beta}(x_i + 1) = logit\\{\\hat{p}_{x_i}\\} + \\hat{\\beta}_1\n\\],\\[\nlogit\\{\\hat{p}_{x_i +1}\\} - logit\\{\\hat{p}_{x_i}\\} = log\\{odds[\\hat{p}_{x_i +1}]\\} - log\\{odds[\\hat{p}_{x_i}]\\} \\\\\n= log(\\frac{odds[\\hat{p}_{x_i + 1}]}{odds[\\hat{p}_{x_i}]}) = \\hat{\\beta}_1\n\\]\\[\nexp(\\hat{\\beta}_1) = \\frac{odds[\\hat{p}_{x_i + 1}]}{odds[\\hat{p}_{x_i}]}\n\\]estimated odds ratiothe estimated odds ratio, difference c units regressor x, \\(exp(c\\hat{\\beta}_1)\\). multiple covariates, \\(exp(\\hat{\\beta}_k)\\) estimated odds ratio variable \\(x_k\\), assuming variables held constant.Inference Mean ResponseLet \\(x_h = (1, x_{h1}, ...,x_{h,p-1})'\\). \\[\n\\hat{p}_h = \\frac{exp(\\mathbf{x'_h \\hat{\\beta}})}{1 + exp(\\mathbf{x'_h \\hat{\\beta}})}\n\\]\\(s^2(\\hat{p}_h) = \\mathbf{x'_h[(\\hat{\\beta})]^{-1}x_h}\\)new observation, can cutoff point decide whether y = 0 1.","code":""},{"path":"generalized-linear-models.html","id":"application-3","chapter":"7 Generalized Linear Models","heading":"7.1.1 Application","text":"Logistic Regression\\(x \\sim Unif(-0.5,2.5)\\). \\(\\eta = 0.5 + 0.75 x\\)Passing \\(\\eta\\)’s inverse-logit function, get\\[\np = \\frac{\\exp(\\eta)}{1+ \\exp(\\eta)}\n\\]\\(p \\[0,1]\\), generate \\(y \\sim Bernoulli(p)\\)Model FitBased odds ratio, \\(x = 0\\) , odds success 1.59\\(x = 1\\), odds success increase factor 2.19 (.e., 119.29% increase).Deviance Tests\\[\nH_0: \\text{variables related response (.e., model just intercept)} \\\\\nH_1: \\text{least one variable related response}\n\\]Since see p-value 0, reject null variables related responseDeviance residualsHowever, plot informative. Hence, can can see residudals plots grouped bins based prediction values.can also see predicted value residuals.can also look binned plot logistic prediction versus true categoryFormal deviance testHosmer-Lemeshow testNull hypothesis: observed events match expected evens\\[\nX^2_{HL} = \\sum_{j=1}^{J} \\frac{(y_j - m_j \\hat{p}_j)^2}{m_j \\hat{p}_j(1-\\hat{p}_j)}\n\\]wherewithin j-th bin, \\(y_j\\) number successes\\(m_j\\) = number observations\\(\\hat{p}_j\\) = predicted probabilityUnder null hypothesis, \\(X^2_{HLL} \\sim \\chi^2_{J-1}\\)Since p-value = 0.99, reject null hypothesis (.e., model fitting well).","code":"\nlibrary(kableExtra)\nlibrary(dplyr)\nlibrary(pscl)## Classes and Methods for R developed in the\n## Political Science Computational Laboratory\n## Department of Political Science\n## Stanford University\n## Simon Jackman\n## hurdle and zeroinfl functions by Achim Zeileis\nlibrary(ggplot2)\nlibrary(faraway)## \n## Attaching package: 'faraway'## The following object is masked from 'package:investr':\n## \n##     beetle\nlibrary(nnet)## Warning: package 'nnet' was built under R version 4.0.5\nlibrary(agridat)\nlibrary(nlstools)\nset.seed(23) #set seed for reproducibility\nx <- runif(1000,min = -0.5,max = 2.5)\neta1 <- 0.5 + 0.75*x\np <- exp(eta1)/(1+exp(eta1))\ny <- rbinom(1000,1,p)\nBinData <- data.frame(X = x, Y = y)\nLogistic_Model <- glm(formula = Y ~ X,\n                      family = binomial, # family = specifies the response distribution\n                      data = BinData)\nsummary(Logistic_Model)## \n## Call:\n## glm(formula = Y ~ X, family = binomial, data = BinData)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.2317   0.4153   0.5574   0.7922   1.1469  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  0.46205    0.10201   4.530 5.91e-06 ***\n## X            0.78527    0.09296   8.447  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1106.7  on 999  degrees of freedom\n## Residual deviance: 1027.4  on 998  degrees of freedom\n## AIC: 1031.4\n## \n## Number of Fisher Scoring iterations: 4\nnlstools::confint2(Logistic_Model)##                 2.5 %    97.5 %\n## (Intercept) 0.2618709 0.6622204\n## X           0.6028433 0.9676934\nOddsRatio <- coef(Logistic_Model) %>% exp\nOddsRatio ## (Intercept)           X \n##    1.587318    2.192995\nTest_Dev = Logistic_Model$null.deviance - Logistic_Model$deviance\np_val_dev <- 1-pchisq(q = Test_Dev, df = 1)\nLogistic_Resids <- residuals(Logistic_Model, type = \"deviance\")\nplot(\n  y = Logistic_Resids,\n  x = BinData$X,\n  xlab = 'X',\n  ylab = 'Deviance Resids'\n)\nplot_bin <- function(Y,\n                     X,\n                     bins = 100,\n                     return.DF = FALSE) {\n  Y_Name <- deparse(substitute(Y))\n  X_Name <- deparse(substitute(X))\n  Binned_Plot <- data.frame(Plot_Y = Y, Plot_X = X)\n  Binned_Plot$bin <-\n    cut(Binned_Plot$Plot_X, breaks = bins) %>% as.numeric\n  Binned_Plot_summary <- Binned_Plot %>%\n    group_by(bin) %>%\n    summarise(\n      Y_ave = mean(Plot_Y),\n      X_ave = mean(Plot_X),\n      Count = n()\n    ) %>% as.data.frame\n  plot(\n    y = Binned_Plot_summary$Y_ave,\n    x = Binned_Plot_summary$X_ave,\n    ylab = Y_Name,\n    xlab = X_Name\n  )\n  if (return.DF)\n    return(Binned_Plot_summary)\n}\nplot_bin(Y = Logistic_Resids,\n         X = BinData$X,\n         bins = 100)\nLogistic_Predictions <- predict(Logistic_Model, type = \"response\")\nplot_bin(Y = Logistic_Resids, X = Logistic_Predictions, bins = 100)\nNumBins <- 10\nBinned_Data <- plot_bin(\n  Y = BinData$Y,\n  X = Logistic_Predictions,\n  bins = NumBins,\n  return.DF = TRUE\n)\nBinned_Data##    bin     Y_ave     X_ave Count\n## 1    1 0.5833333 0.5382095    72\n## 2    2 0.5200000 0.5795887    75\n## 3    3 0.6567164 0.6156540    67\n## 4    4 0.7014925 0.6579674    67\n## 5    5 0.6373626 0.6984765    91\n## 6    6 0.7500000 0.7373341    72\n## 7    7 0.7096774 0.7786747    93\n## 8    8 0.8503937 0.8203819   127\n## 9    9 0.8947368 0.8601232   133\n## 10  10 0.8916256 0.9004734   203\nabline(0, 1, lty = 2, col = 'blue')\nHL_BinVals <-\n  (Binned_Data$Count * Binned_Data$Y_ave - Binned_Data$Count * Binned_Data$X_ave) ^\n  2 /\n  Binned_Data$Count * Binned_Data$X_ave * (1 - Binned_Data$X_ave)\nHLpval <-\n  pchisq(q = sum(HL_BinVals),\n         df = NumBins,\n         lower.tail = FALSE)\nHLpval## [1] 0.9999989"},{"path":"generalized-linear-models.html","id":"probit-regression","chapter":"7 Generalized Linear Models","heading":"7.2 Probit Regression","text":"\\[\nE(Y_i) = p_i = \\Phi(\\mathbf{x_i'\\theta})\n\\]\\(\\Phi()\\) CDF N(0,1) random variable.models (e..g, t–distribution; log-log; complimentary log-log)let \\(Y_i = 1\\) success, \\(Y_i =0\\) success. assume \\(Y \\sim Ber\\) \\(p_i = P(Y_i =1)\\), success probability. cosnider logistic regression response function \\(logit(p_i) = x'_i \\beta\\)Confusion matrixSensitivity: ability identify positive results\\[\n\\text{Sensitivity} = \\frac{TP}{TP + FN}\n\\]Specificity: ability identify negative results\\[\n\\text{Specificity} = \\frac{TN}{TN + FP}\n\\]False positive rate: Type error (1- specificity)\\[\n\\text{ False Positive Rate} = \\frac{FP}{TN+ FP}\n\\]False Negative Rate: Type II error (1-sensitivity)\\[\n\\text{False Negative Rate} = \\frac{FN}{TP + FN}\n\\]","code":""},{"path":"generalized-linear-models.html","id":"binomial-regression","chapter":"7 Generalized Linear Models","heading":"7.3 Binomial Regression","text":"BinomialHere, cancer case = successes, control case = failures.","code":"\ndata(\"esoph\")\nhead(esoph, n = 3)##   agegp     alcgp    tobgp ncases ncontrols\n## 1 25-34 0-39g/day 0-9g/day      0        40\n## 2 25-34 0-39g/day    10-19      0        10\n## 3 25-34 0-39g/day    20-29      0         6\nplot(\n  esoph$ncases / (esoph$ncases + esoph$ncontrols) ~ esoph$alcgp,\n  ylab = \"Proportion\",\n  xlab = 'Alcohol consumption',\n  main = 'Esophageal Cancer data'\n)\nclass(esoph$agegp) <- \"factor\"\nclass(esoph$alcgp) <- \"factor\"\nclass(esoph$tobgp) <- \"factor\"\n#  only the alcohol consumption as a predictor\nmodel <- glm(cbind(ncases, ncontrols) ~ alcgp, data = esoph, family = binomial)\nsummary(model)## \n## Call:\n## glm(formula = cbind(ncases, ncontrols) ~ alcgp, family = binomial, \n##     data = esoph)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -3.6629  -1.0478  -0.0081   0.6307   3.0296  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  -2.6610     0.1921 -13.854  < 2e-16 ***\n## alcgp40-79    1.1064     0.2303   4.804 1.56e-06 ***\n## alcgp80-119   1.6656     0.2525   6.597 4.20e-11 ***\n## alcgp120+     2.2630     0.2721   8.317  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 227.24  on 87  degrees of freedom\n## Residual deviance: 138.79  on 84  degrees of freedom\n## AIC: 294.27\n## \n## Number of Fisher Scoring iterations: 5\n#Coefficient Odds\ncoefficients(model) %>% exp## (Intercept)  alcgp40-79 alcgp80-119   alcgp120+ \n##  0.06987952  3.02331229  5.28860570  9.61142563\ndeviance(model)/df.residual(model)## [1] 1.652253\nmodel$aic## [1] 294.27\n# alcohol consumption and age as predictors\nbetter_model <- glm(cbind(ncases, ncontrols) ~ agegp + alcgp, data = esoph, family = binomial)\nsummary(better_model)## \n## Call:\n## glm(formula = cbind(ncases, ncontrols) ~ agegp + alcgp, family = binomial, \n##     data = esoph)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.8979  -0.5592  -0.1995   0.5029   2.6250  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  -5.6180     1.0217  -5.499 3.82e-08 ***\n## agegp35-44    1.5376     1.0646   1.444 0.148669    \n## agegp45-54    2.9470     1.0217   2.884 0.003922 ** \n## agegp55-64    3.3116     1.0172   3.255 0.001132 ** \n## agegp65-74    3.5774     1.0209   3.504 0.000458 ***\n## agegp75+      3.5858     1.0620   3.377 0.000734 ***\n## alcgp40-79    1.1392     0.2367   4.814 1.48e-06 ***\n## alcgp80-119   1.4951     0.2600   5.749 8.97e-09 ***\n## alcgp120+     2.2228     0.2843   7.820 5.29e-15 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 227.241  on 87  degrees of freedom\n## Residual deviance:  64.572  on 79  degrees of freedom\n## AIC: 230.05\n## \n## Number of Fisher Scoring iterations: 6\nbetter_model$aic #smaller AIC is better## [1] 230.0526\ncoefficients(better_model) %>% exp##  (Intercept)   agegp35-44   agegp45-54   agegp55-64   agegp65-74     agegp75+ \n##  0.003631855  4.653273722 19.047899816 27.428640745 35.780787582 36.082010052 \n##   alcgp40-79  alcgp80-119    alcgp120+ \n##  3.124334222  4.459579378  9.233256747\npchisq(\n    q = model$deviance - better_model$deviance,\n    df = model$df.residual - better_model$df.residual,\n    lower = FALSE\n)## [1] 1.354906e-14\n# specify link function as probit\nProb_better_model <- glm(\n    cbind(ncases, ncontrols) ~ agegp + alcgp,\n    data = esoph,\n    family = binomial(link = probit)\n)\nsummary(Prob_better_model)## \n## Call:\n## glm(formula = cbind(ncases, ncontrols) ~ agegp + alcgp, family = binomial(link = probit), \n##     data = esoph)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -1.8676  -0.5938  -0.1802   0.4852   2.6056  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  -2.9800     0.4291  -6.945 3.79e-12 ***\n## agegp35-44    0.6991     0.4491   1.557 0.119520    \n## agegp45-54    1.4212     0.4292   3.311 0.000929 ***\n## agegp55-64    1.6512     0.4262   3.874 0.000107 ***\n## agegp65-74    1.8039     0.4297   4.198 2.69e-05 ***\n## agegp75+      1.8025     0.4613   3.908 9.32e-05 ***\n## alcgp40-79    0.6224     0.1247   4.990 6.03e-07 ***\n## alcgp80-119   0.8256     0.1418   5.823 5.80e-09 ***\n## alcgp120+     1.2839     0.1596   8.043 8.77e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 227.241  on 87  degrees of freedom\n## Residual deviance:  61.938  on 79  degrees of freedom\n## AIC: 227.42\n## \n## Number of Fisher Scoring iterations: 6"},{"path":"generalized-linear-models.html","id":"poisson-regression","chapter":"7 Generalized Linear Models","heading":"7.4 Poisson Regression","text":"Poisson distribution\\[\nf(Y_i) = \\frac{\\mu_i^{Y_i}exp(-\\mu_i)}{Y_i!}, Y_i = 0,1,.. \\\\\nE(Y_i) = \\mu_i  \\\\\nvar(Y_i) = \\mu_i\n\\]natural distribution counts. can see variance function mean. let \\(\\mu_i = f(\\mathbf{x_i; \\theta})\\), similar Logistic Regression since can choose \\(f()\\) \\(\\mu_i = \\mathbf{x_i'\\theta}, \\mu_i = \\exp(\\mathbf{x_i'\\theta}), \\mu_i = \\log(\\mathbf{x_i'\\theta})\\)","code":""},{"path":"generalized-linear-models.html","id":"application-4","chapter":"7 Generalized Linear Models","heading":"7.4.1 Application","text":"Count Data Poisson regressionResidual 1634 909 df isn’t great.see Pearson \\(\\chi^2\\)interaction terms, improvementsConsider \\(\\hat{\\phi} = \\frac{\\text{deviance}}{df}\\)evidence -dispersion. Likely cause missing variables. remedies either include variables consider random effects.quick fix force Poisson Regression include value \\(\\phi\\), model called “Quasi-Poisson.”directly rerun model asQuasi-Poisson recommended, Negative Binomial Regression extra parameter account -dispersion .","code":"\ndata(bioChemists, package = \"pscl\")\nbioChemists <- bioChemists %>%\n    rename(\n        Num_Article = art, #articles in last 3 years of PhD\n        Sex = fem, #coded 1 if female\n        Married = mar, #coded 1 if married\n        Num_Kid5 = kid5, #number of childeren under age 6\n        PhD_Quality = phd, #prestige of PhD program\n        Num_MentArticle = ment #articles by mentor in last 3 years\n    )\nhist(bioChemists$Num_Article, breaks = 25, main = 'Number of Articles')\nPoisson_Mod <- glm(Num_Article ~ ., family=poisson, bioChemists)\nsummary(Poisson_Mod)## \n## Call:\n## glm(formula = Num_Article ~ ., family = poisson, data = bioChemists)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -3.5672  -1.5398  -0.3660   0.5722   5.4467  \n## \n## Coefficients:\n##                  Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)      0.304617   0.102981   2.958   0.0031 ** \n## SexWomen        -0.224594   0.054613  -4.112 3.92e-05 ***\n## MarriedMarried   0.155243   0.061374   2.529   0.0114 *  \n## Num_Kid5        -0.184883   0.040127  -4.607 4.08e-06 ***\n## PhD_Quality      0.012823   0.026397   0.486   0.6271    \n## Num_MentArticle  0.025543   0.002006  12.733  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1)\n## \n##     Null deviance: 1817.4  on 914  degrees of freedom\n## Residual deviance: 1634.4  on 909  degrees of freedom\n## AIC: 3314.1\n## \n## Number of Fisher Scoring iterations: 5\nPredicted_Means <- predict(Poisson_Mod,type = \"response\")\nX2 <- sum((bioChemists$Num_Article - Predicted_Means)^2/Predicted_Means)\nX2## [1] 1662.547\npchisq(X2,Poisson_Mod$df.residual, lower.tail = FALSE)## [1] 7.849882e-47\nPoisson_Mod_All2way <- glm(Num_Article ~ .^2, family=poisson, bioChemists)\nPoisson_Mod_All3way <- glm(Num_Article ~ .^3, family=poisson, bioChemists)\nPoisson_Mod$deviance / Poisson_Mod$df.residual## [1] 1.797988\nphi_hat = Poisson_Mod$deviance/Poisson_Mod$df.residual\nsummary(Poisson_Mod,dispersion = phi_hat)## \n## Call:\n## glm(formula = Num_Article ~ ., family = poisson, data = bioChemists)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -3.5672  -1.5398  -0.3660   0.5722   5.4467  \n## \n## Coefficients:\n##                 Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)      0.30462    0.13809   2.206  0.02739 *  \n## SexWomen        -0.22459    0.07323  -3.067  0.00216 ** \n## MarriedMarried   0.15524    0.08230   1.886  0.05924 .  \n## Num_Kid5        -0.18488    0.05381  -3.436  0.00059 ***\n## PhD_Quality      0.01282    0.03540   0.362  0.71715    \n## Num_MentArticle  0.02554    0.00269   9.496  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for poisson family taken to be 1.797988)\n## \n##     Null deviance: 1817.4  on 914  degrees of freedom\n## Residual deviance: 1634.4  on 909  degrees of freedom\n## AIC: 3314.1\n## \n## Number of Fisher Scoring iterations: 5\nquasiPoisson_Mod <- glm(Num_Article ~ ., family=quasipoisson, bioChemists)"},{"path":"generalized-linear-models.html","id":"negative-binomial-regression","chapter":"7 Generalized Linear Models","heading":"7.5 Negative Binomial Regression","text":"can see dispersion 2.264 SE = 0.271, significantly different 1, indicating overdispersion. Check -Dispersion detail","code":"\nlibrary(MASS)\nNegBinom_Mod <- MASS::glm.nb(Num_Article ~ .,bioChemists)\nsummary(NegBinom_Mod)## \n## Call:\n## MASS::glm.nb(formula = Num_Article ~ ., data = bioChemists, init.theta = 2.264387695, \n##     link = log)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.1678  -1.3617  -0.2806   0.4476   3.4524  \n## \n## Coefficients:\n##                  Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)      0.256144   0.137348   1.865 0.062191 .  \n## SexWomen        -0.216418   0.072636  -2.979 0.002887 ** \n## MarriedMarried   0.150489   0.082097   1.833 0.066791 .  \n## Num_Kid5        -0.176415   0.052813  -3.340 0.000837 ***\n## PhD_Quality      0.015271   0.035873   0.426 0.670326    \n## Num_MentArticle  0.029082   0.003214   9.048  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for Negative Binomial(2.2644) family taken to be 1)\n## \n##     Null deviance: 1109.0  on 914  degrees of freedom\n## Residual deviance: 1004.3  on 909  degrees of freedom\n## AIC: 3135.9\n## \n## Number of Fisher Scoring iterations: 1\n## \n## \n##               Theta:  2.264 \n##           Std. Err.:  0.271 \n## \n##  2 x log-likelihood:  -3121.917"},{"path":"generalized-linear-models.html","id":"multinomial","chapter":"7 Generalized Linear Models","heading":"7.6 Multinomial","text":"two categories groups want model relative covariates (e.g., observations \\(= 1,…,n\\) groups/ covariates \\(j = 1,2,…,J\\)), multinomial candidate modelLet\\(p_{ij}\\) probability -th observation belongs j-th group\\(Y_{ij}\\) number observations individual group j; individual observations \\(Y_{i1},Y_{i2},…Y_{iJ}\\)assume probability observing response given multinomial distribution terms probabilities \\(p_{ij}\\), \\(\\sum_{j = 1}^J p_{ij} = 1\\) . interpretation, baseline category \\(p_{i1} = 1 - \\sum_{j = 2}^J p_{ij}\\)link mean response (probability) \\(p_{ij}\\) linear function covariates\\[\n\\eta_{ij} = \\mathbf{x'_i \\beta_j} = \\log \\frac{p_{ij}}{p_{i1}}, j = 2,..,J\n\\]compare \\(p_{ij}\\) baseline \\(p_{i1}\\), suggesting\\[\np_{ij} = \\frac{\\exp(\\eta_{ij})}{1 + \\sum_{=2}^J \\exp(\\eta_{ij})}\n\\]known multinomial logistic model.Note:Softmax coding multinomial logistic regression: rather selecting baseline class, treat K class symmetrically - euqllay important (baseline).\\[\nP(Y = k | X = x) = \\frac{exp(\\beta_{k1} + \\dots + \\beta_{k_p x_p})}{\\sum_{l = 1}^K exp(\\beta_{l0} + \\dots + \\beta_{l_p x_p})}\n\\]log odds ratio k-th k’-th classes \\[\n\\log (\\frac{P(Y=k|X=x)}{P(Y = k' | X=x)}) = (\\beta_{k0} - \\beta_{k'0}) + \\dots + (\\beta_{kp} - \\beta_{k'p}) x_p\n\\]try understand political strengthvisualize political strength variableFit multinomial logistic model:model political strength function age educationAlternatively, stepwise model selection based AICcompare best model full model based devianceWe see significant differencePlot fitted modelIf categories ordered (.e., ordinal data), must use another approach (still multinomial, use cumulative probabilities).Another example\\[\nY \\sim Gamma\n\\]Gamma non-negative opposed Normal. canonical Gamma link function inverse (reciprocal) link\\[\n\\eta_{ij} = \\beta_{0j} + \\beta_{1j}x_{ij} + \\beta_2x_{ij}^2 \\\\\nY_{ij} = \\eta_{ij}^{-1}\n\\]linear predictor quadratic model fit j-th blocks. different model (fitted) one common slopes: glm(y \\(\\sim\\) x + (x^2),…)predict new value x","code":"\nlibrary(faraway)\nlibrary(dplyr)\ndata(nes96, package=\"faraway\")\nhead(nes96,3)##   popul TVnews selfLR ClinLR DoleLR     PID age  educ   income    vote\n## 1     0      7 extCon extLib    Con  strRep  36    HS $3Kminus    Dole\n## 2   190      1 sliLib sliLib sliCon weakDem  20  Coll $3Kminus Clinton\n## 3    31      7    Lib    Lib    Con weakDem  24 BAdeg $3Kminus Clinton\ntable(nes96$PID)## \n##  strDem weakDem  indDem  indind  indRep weakRep  strRep \n##     200     180     108      37      94     150     175\nnes96$Political_Strength <- NA\nnes96$Political_Strength[nes96$PID %in% c(\"strDem\", \"strRep\")] <-\n  \"Strong\"\nnes96$Political_Strength[nes96$PID %in% c(\"weakDem\", \"weakRep\")] <-\n  \"Weak\"\nnes96$Political_Strength[nes96$PID %in% c(\"indDem\", \"indind\", \"indRep\")] <-\n  \"Neutral\"\nnes96 %>% group_by(Political_Strength) %>% summarise(Count = n())## # A tibble: 3 x 2\n##   Political_Strength Count\n##   <chr>              <int>\n## 1 Neutral              239\n## 2 Strong               375\n## 3 Weak                 330\nlibrary(ggplot2)\nPlot_DF <- nes96 %>%\n  mutate(Age_Grp = cut_number(age, 4)) %>%\n  group_by(Age_Grp, Political_Strength) %>%\n  summarise(count = n()) %>%\n  group_by(Age_Grp) %>%\n  mutate(etotal = sum(count), proportion = count / etotal)## `summarise()` has grouped output by 'Age_Grp'. You can override using the `.groups` argument.\nAge_Plot <- ggplot(\n  Plot_DF,\n  aes(\n    x = Age_Grp,\n    y = proportion,\n    group = Political_Strength,\n    linetype = Political_Strength,\n    color = Political_Strength\n  )\n) +\n  geom_line(size = 2)\nAge_Plot\nlibrary(nnet)\nMultinomial_Model <-\n    multinom(Political_Strength ~ age + educ, nes96, trace = F)\nsummary(Multinomial_Model)## Call:\n## multinom(formula = Political_Strength ~ age + educ, data = nes96, \n##     trace = F)\n## \n## Coefficients:\n##        (Intercept)          age     educ.L     educ.Q     educ.C      educ^4\n## Strong -0.08788729  0.010700364 -0.1098951 -0.2016197 -0.1757739 -0.02116307\n## Weak    0.51976285 -0.004868771 -0.1431104 -0.2405395 -0.2411795  0.18353634\n##            educ^5     educ^6\n## Strong -0.1664377 -0.1359449\n## Weak   -0.1489030 -0.2173144\n## \n## Std. Errors:\n##        (Intercept)         age    educ.L    educ.Q    educ.C    educ^4\n## Strong   0.3017034 0.005280743 0.4586041 0.4318830 0.3628837 0.2964776\n## Weak     0.3097923 0.005537561 0.4920736 0.4616446 0.3881003 0.3169149\n##           educ^5    educ^6\n## Strong 0.2515012 0.2166774\n## Weak   0.2643747 0.2199186\n## \n## Residual Deviance: 2024.596 \n## AIC: 2056.596\nMultinomial_Step <- step(Multinomial_Model,trace = 0)## trying - age \n## trying - educ \n## trying - age\nMultinomial_Step## Call:\n## multinom(formula = Political_Strength ~ age, data = nes96, trace = F)\n## \n## Coefficients:\n##        (Intercept)          age\n## Strong -0.01988977  0.009832916\n## Weak    0.59497046 -0.005954348\n## \n## Residual Deviance: 2030.756 \n## AIC: 2038.756\npchisq(q = deviance(Multinomial_Step) - deviance(Multinomial_Model),\ndf = Multinomial_Model$edf-Multinomial_Step$edf,lower=F)## [1] 0.9078172\nPlotData <- data.frame(age = seq(from = 19, to = 91))\nPreds <-\n  PlotData %>% bind_cols(data.frame(predict(\n    object = Multinomial_Step,\n    PlotData, type = \"probs\"\n  )))\nplot(\n  x = Preds$age,\n  y = Preds$Neutral,\n  type = \"l\",\n  ylim = c(0.2, 0.6),\n  col = \"black\",\n  ylab = \"Proportion\",\n  xlab = \"Age\"\n)\nlines(x = Preds$age,\n      y = Preds$Weak,\n      col = \"blue\")\nlines(x = Preds$age,\n      y = Preds$Strong,\n      col = \"red\")\nlegend(\n  'topleft',\n  legend = c('Neutral', 'Weak', 'Strong'),\n  col = c('black', 'blue', 'red'),\n  lty = 1\n)\npredict(Multinomial_Step,data.frame(age = 34)) # predicted result (categoriy of political strength) of 34 year old## [1] Weak\n## Levels: Neutral Strong Weak\npredict(Multinomial_Step,data.frame(age = c(34,35)),type=\"probs\") # predicted result of the probabilities of each level of political strength for a 34 and 35##     Neutral    Strong      Weak\n## 1 0.2597275 0.3556910 0.3845815\n## 2 0.2594080 0.3587639 0.3818281\nlibrary(agridat)\ndat <- agridat::streibig.competition\n# See Schaberger and Pierce, pages 370+\n# Consider only the mono-species barley data (no competition from Sinapis)\ngammaDat <- subset(dat, sseeds < 1)\ngammaDat <-\n  transform(gammaDat,\n            x = bseeds,\n            y = bdwt,\n            block = factor(block))\n# Inverse yield looks like it will be a good fit for Gamma's inverse link\nggplot(gammaDat, aes(x = x, y = 1 / y)) + geom_point(aes(color = block, shape =\n                                                           block)) +\n  xlab('Seeding Rate') + ylab('Inverse yield') + ggtitle('Streibig Competion - Barley only')\n# linear predictor is quadratic, with separate intercept and slope per block\nm1 <- glm(y ~ block + block*x + block*I(x^2), data=gammaDat,family=Gamma(link=\"inverse\"))\nsummary(m1)## \n## Call:\n## glm(formula = y ~ block + block * x + block * I(x^2), family = Gamma(link = \"inverse\"), \n##     data = gammaDat)\n## \n## Deviance Residuals: \n##      Min        1Q    Median        3Q       Max  \n## -1.21708  -0.44148   0.02479   0.17999   0.80745  \n## \n## Coefficients:\n##                  Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)     1.115e-01  2.870e-02   3.886 0.000854 ***\n## blockB2        -1.208e-02  3.880e-02  -0.311 0.758630    \n## blockB3        -2.386e-02  3.683e-02  -0.648 0.524029    \n## x              -2.075e-03  1.099e-03  -1.888 0.072884 .  \n## I(x^2)          1.372e-05  9.109e-06   1.506 0.146849    \n## blockB2:x       5.198e-04  1.468e-03   0.354 0.726814    \n## blockB3:x       7.475e-04  1.393e-03   0.537 0.597103    \n## blockB2:I(x^2) -5.076e-06  1.184e-05  -0.429 0.672475    \n## blockB3:I(x^2) -6.651e-06  1.123e-05  -0.592 0.560012    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for Gamma family taken to be 0.3232083)\n## \n##     Null deviance: 13.1677  on 29  degrees of freedom\n## Residual deviance:  7.8605  on 21  degrees of freedom\n## AIC: 225.32\n## \n## Number of Fisher Scoring iterations: 5\nnewdf <-\n  expand.grid(x = seq(0, 120, length = 50), block = factor(c('B1', 'B2', 'B3')))\nnewdf$pred <- predict(m1, new = newdf, type = 'response')\nggplot(gammaDat, aes(x = x, y = y)) + geom_point(aes(color = block, shape =\n                                                       block)) +\n  xlab('Seeding Rate') + ylab('Inverse yield') + ggtitle('Streibig Competion - Barley only Predictions') +\n  geom_line(data = newdf, aes(\n    x = x,\n    y = pred,\n    color = block,\n    linetype = block\n  ))"},{"path":"generalized-linear-models.html","id":"generalization","chapter":"7 Generalized Linear Models","heading":"7.7 Generalization","text":"can see Poisson regression looks similar logistic regression. Hence, can generalize class modeling. Thanks (Nelder Wedderburn 1972), generalized linear models (GLMs). Estimation generalize models.Exponential Family\ntheory GLMs developed data distribution given y exponential family.\nform data distribution useful GLMs \\[\nf(y;\\theta, \\phi) = \\exp(\\frac{\\theta y - b(\\theta)}{(\\phi)} + c(y, \\phi))\n\\]\\(\\theta\\) called natural parameter\\(\\phi\\) called dispersion parameterNote:family includes Gamma, Normal, Poisson, . parameterization exponential family, check linkExampleif \\(Y \\sim N(\\mu, \\sigma^2)\\)\\[\n\\begin{aligned}\nf(y; \\mu, \\sigma^2) &= \\frac{1}{(2\\pi \\sigma^2)^{1/2}}\\exp(-\\frac{1}{2\\sigma^2}(y- \\mu)^2) \\\\\n&= \\exp(-\\frac{1}{2\\sigma^2}(y^2 - 2y \\mu +\\mu^2)- \\frac{1}{2}\\log(2\\pi \\sigma^2)) \\\\\n&= \\exp(\\frac{y \\mu - \\mu^2/2}{\\sigma^2} - \\frac{y^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi \\sigma^2)) \\\\\n&= \\exp(\\frac{\\theta y - b(\\theta)}{(\\phi)} + c(y , \\phi))\n\\end{aligned}\n\\]\\(\\theta = \\mu\\)\\(b(\\theta) = \\frac{\\mu^2}{2}\\)\\((\\phi) = \\sigma^2 = \\phi\\)\\(c(y , \\phi) = - \\frac{1}{2}(\\frac{y^2}{\\phi}+\\log(2\\pi \\sigma^2))\\)Properties GLM exponential families\\(E(Y) = b' (\\theta)\\) \\(b'(\\theta) = \\frac{\\partial b(\\theta)}{\\partial \\theta}\\) (' “prime,” transpose)\\(E(Y) = b' (\\theta)\\) \\(b'(\\theta) = \\frac{\\partial b(\\theta)}{\\partial \\theta}\\) (' “prime,” transpose)\\(var(Y) = (\\phi)b''(\\theta)= (\\phi)V(\\mu)\\).\n\\(V(\\mu)\\) variance function; however, variance case \\((\\phi) =1\\)\n\\(var(Y) = (\\phi)b''(\\theta)= (\\phi)V(\\mu)\\).\\(V(\\mu)\\) variance function; however, variance case \\((\\phi) =1\\)\\((), b(), c()\\) identifiable, derive expected value variance Y.\\((), b(), c()\\) identifiable, derive expected value variance Y.ExampleNormal distribution\\[\nb'(\\theta) = \\frac{\\partial b(\\mu^2/2)}{\\partial \\mu} = \\mu \\\\\nV(\\mu) = \\frac{\\partial^2 (\\mu^2/2)}{\\partial \\mu^2} = 1 \\\\\n\\var(Y) = (\\phi) = \\sigma^2\n\\]Poisson distribution\\[\n\\begin{aligned}\nf(y, \\theta, \\phi) &= \\frac{\\mu^y \\exp(-\\mu)}{y!} \\\\\n&= \\exp(y\\log(\\mu) - \\mu - \\log(y!)) \\\\\n&= \\exp(y\\theta - \\exp(\\theta) - \\log(y!))\n\\end{aligned}\n\\]\\(\\theta = \\log(\\mu)\\)\\((\\phi) = 1\\)\\(b(\\theta) = \\exp(\\theta)\\)\\(c(y, \\phi) = \\log(y!)\\)Hence,\\[\nE(Y) = \\frac{\\partial b(\\theta)}{\\partial \\theta} = \\exp(\\theta) = \\mu \\\\\nvar(Y) = \\frac{\\partial^2 b(\\theta)}{\\partial \\theta^2} = \\mu\n\\]Since \\(\\mu = E(Y) = b'(\\theta)\\)GLM, take monotone function (typically nonlinear) \\(\\mu\\) linear set covariates\\[\ng(\\mu) = g(b'(\\theta)) = \\mathbf{x'\\beta}\n\\]Equivalently,\\[\n\\mu = g^{-1}(\\mathbf{x'\\beta})\n\\]\\(g(.)\\) link function since links mean response (\\(\\mu = E(Y)\\)) linear expression covariatesSome people use \\(\\eta = \\mathbf{x'\\beta}\\) \\(\\eta\\) = “linear predictor”GLM composed 2 componentsThe random component:distribution chosen model response variables \\(Y_1,...,Y_n\\)distribution chosen model response variables \\(Y_1,...,Y_n\\)specified choice fo \\((), b(), c()\\) exponential formis specified choice fo \\((), b(), c()\\) exponential formNotation:\nAssume n independent response variables \\(Y_1,...,Y_n\\) densities\\[\nf(y_i ; \\theta_i, \\phi) = \\exp(\\frac{\\theta_i y_i - b(\\theta_i)}{(\\phi)}+ c(y_i, \\phi))\n\\] notice observation might different densities\nAssume \\(\\phi\\) constant \\(= 1,...,n\\), \\(\\theta_i\\) vary. \\(\\mu_i = E(Y_i)\\) .\nNotation:Assume n independent response variables \\(Y_1,...,Y_n\\) densities\\[\nf(y_i ; \\theta_i, \\phi) = \\exp(\\frac{\\theta_i y_i - b(\\theta_i)}{(\\phi)}+ c(y_i, \\phi))\n\\] notice observation might different densitiesAssume \\(\\phi\\) constant \\(= 1,...,n\\), \\(\\theta_i\\) vary. \\(\\mu_i = E(Y_i)\\) .systematic componentis portion model gives relation \\(\\mu\\) covariates \\(\\mathbf{x}\\)portion model gives relation \\(\\mu\\) covariates \\(\\mathbf{x}\\)consists 2 parts:\nlink function, \\(g(.)\\)\nlinear predictor, \\(\\eta = \\mathbf{x'\\beta}\\)\nconsists 2 parts:link function, \\(g(.)\\)linear predictor, \\(\\eta = \\mathbf{x'\\beta}\\)Notation:\nassume \\(g(\\mu_i) = \\mathbf{x'\\beta} = \\eta_i\\) \\(\\mathbf{\\beta} = (\\beta_1,..., \\beta_p)'\\)\nparameters estimated \\(\\beta_1,...\\beta_p , \\phi\\)\nNotation:assume \\(g(\\mu_i) = \\mathbf{x'\\beta} = \\eta_i\\) \\(\\mathbf{\\beta} = (\\beta_1,..., \\beta_p)'\\)parameters estimated \\(\\beta_1,...\\beta_p , \\phi\\)Canonical LinkTo choose \\(g(.)\\), can use canonical link function (Remember: Canonical link just special case link function)link function \\(g(.)\\) \\(g(\\mu_i) = \\eta_i = \\theta_i\\), natural parameter, \\(g(.)\\) canonical link.\\(b(\\theta)\\) = cumulant moment generating function\\(g(\\mu)\\) link function, relates linear predictor mean required monotone increasing, continuously differentiable invertible.Equivalently, can think canonical link function \\[\n\\gamma^{-1} \\circ g^{-1} = \n\\]\nidentity. Hence,\\[\n\\theta = \\eta\n\\]inverse link\\(g^{-1}(.)\\) also known mean function, take linear predictor output (ranging \\(-\\infty\\) \\(\\infty\\)) transform different scale.Exponential: converts \\(\\mathbf{\\beta X}\\) curve restricted 0 \\(\\infty\\) (can see useful case want convert linear predictor non-negative value). \\(\\lambda = \\exp(y) = \\mathbf{\\beta X}\\)Inverse Logit (also known logistic): converts \\(\\mathbf{\\beta X}\\) curve restricted 0 1, useful case want convert linear predictor probability. \\(\\theta = \\frac{1}{1 + \\exp(-y)} = \\frac{1}{1 + \\exp(- \\mathbf{\\beta X})}\\)\n\\(y\\) = linear predictor value\n\\(\\theta\\) = transformed value\n\\(y\\) = linear predictor value\\(\\theta\\) = transformed valueThe identity link \\[\n\\eta_i = g(\\mu_i) = \\mu_i \\\\\n\\mu_i = g^{-1}(\\eta_i) = \\eta_i\n\\]Table 15.1 Generalized Linear Models 15.1 Structure Generalized Linear ModelsMore example link functions inverses can found page 380ExampleNormal random componentMean Response: \\(\\mu_i = \\theta_i\\)Mean Response: \\(\\mu_i = \\theta_i\\)Canonical Link: \\(g( \\mu_i) = \\mu_i\\) (identity link)Canonical Link: \\(g( \\mu_i) = \\mu_i\\) (identity link)Binomial random componentMean Response: \\(\\mu_i = \\frac{n_i \\exp( \\theta)}{1+\\exp (\\theta_i)}\\) \\(\\theta(\\mu_i) = \\log(\\frac{p_i }{1-p_i}) = \\log (\\frac{\\mu_i} {n_i - \\mu_i})\\)Mean Response: \\(\\mu_i = \\frac{n_i \\exp( \\theta)}{1+\\exp (\\theta_i)}\\) \\(\\theta(\\mu_i) = \\log(\\frac{p_i }{1-p_i}) = \\log (\\frac{\\mu_i} {n_i - \\mu_i})\\)Canonical link: \\(g(\\mu_i) = \\log(\\frac{\\mu_i} {n_i - \\mu_i})\\) (logit link)Canonical link: \\(g(\\mu_i) = \\log(\\frac{\\mu_i} {n_i - \\mu_i})\\) (logit link)Poisson random componentMean Response: \\(\\mu_i = \\exp(\\theta_i)\\)Mean Response: \\(\\mu_i = \\exp(\\theta_i)\\)Canonical Link: \\(g(\\mu_i) = \\log(\\mu_i)\\)Canonical Link: \\(g(\\mu_i) = \\log(\\mu_i)\\)Gamma random component:Mean response: \\(\\mu_i = -\\frac{1}{\\theta_i}\\) \\(\\theta(\\mu_i) = - \\mu_i^{-1}\\)Mean response: \\(\\mu_i = -\\frac{1}{\\theta_i}\\) \\(\\theta(\\mu_i) = - \\mu_i^{-1}\\)Canonical Link: \\(g(\\mu\\_i) = - \\frac{1}{\\mu_i}\\)Canonical Link: \\(g(\\mu\\_i) = - \\frac{1}{\\mu_i}\\)Inverse Gaussian randomCanonical Link: \\(g(\\mu_i) = \\frac{1}{\\mu_i^2}\\)","code":"## Warning: package 'jpeg' was built under R version 4.0.5"},{"path":"generalized-linear-models.html","id":"estimation-1","chapter":"7 Generalized Linear Models","heading":"7.7.1 Estimation","text":"MLE parameters systematic component (\\(\\beta\\))Unification derivation computation (thanks exponential forms)unification estimation dispersion parameter (\\(\\phi\\))","code":""},{"path":"generalized-linear-models.html","id":"estimation-of-beta","chapter":"7 Generalized Linear Models","heading":"7.7.1.1 Estimation of \\(\\beta\\)","text":"\\[\nf(y_i ; \\theta_i, \\phi) = \\exp(\\frac{\\theta_i y_i - b(\\theta_i)}{(\\phi)}+ c(y_i, \\phi)) \\\\\nE(Y_i) = \\mu_i = b'(\\theta) \\\\\nvar(Y_i) = b''(\\theta)(\\phi) = V(\\mu_i)(\\phi) \\\\\ng(\\mu_i) = \\mathbf{x}_i'\\beta = \\eta_i\n\\]log-likelihood single observation \\(l_i (\\beta,\\phi)\\). log-likelihood n observations \\[\n\\begin{aligned}\nl(\\beta,\\phi) &= \\sum_{=1}^n l_i (\\beta,\\phi) \\\\\n&= \\sum_{=1}^n (\\frac{\\theta_i y_i - b(\\theta_i)}{(\\phi)}+ c(y_i, \\phi))\n\\end{aligned}\n\\]Using MLE find \\(\\beta\\), use chain rule get derivatives\\[\n\\begin{aligned}\n\\frac{\\partial l_i (\\beta,\\phi)}{\\partial \\beta_j} &=  \\frac{\\partial l_i (\\beta, \\phi)}{\\partial \\theta_i} \\times \\frac{\\partial \\theta_i}{\\partial \\mu_i} \\times \\frac{\\partial \\mu_i}{\\partial \\eta_i}\\times \\frac{\\partial \\eta_i}{\\partial \\beta_j} \\\\\n&= \\sum_{=1}^{n}(\\frac{ y_i - \\mu_i}{(\\phi)} \\times \\frac{1}{V(\\mu_i)} \\times \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\times x_{ij})\n\\end{aligned}\n\\]let\\[\nw_i \\equiv ((\\frac{\\partial \\eta_i}{\\partial \\mu_i})^2 V(\\mu_i))^{-1}\n\\],\\[\n\\frac{\\partial l_i (\\beta,\\phi)}{\\partial \\beta_j} = \\sum_{=1}^n (\\frac{y_i \\mu_i}{(\\phi)} \\times w_i \\times \\frac{\\partial \\eta_i}{\\partial \\mu_i} \\times x_{ij})\n\\]can also get second derivatives using chain rule.Example:Newton-Raphson algorithm, need\\[\n- E(\\frac{\\partial^2 l(\\beta,\\phi)}{\\partial \\beta_j \\partial \\beta_k})\n\\]\\((j,k)\\)th element Fisher information matrix \\(\\mathbf{}(\\beta)\\)Hence,\\[\n- E(\\frac{\\partial^2 l(\\beta,\\phi)}{\\partial \\beta_j \\partial \\beta_k}) = \\sum_{=1}^n \\frac{w_i}{(\\phi)}x_{ij}x_{ik}\n\\](j,k)th elementIf Bernoulli model logit link function (canonical link)\\[\nb(\\theta) = \\log(1 + \\exp(\\theta)) = \\log(1 + \\exp(\\mathbf{x'\\beta})) \\\\\n(\\phi) = 1  \\\\\nc(y_i, \\phi) = 0 \\\\\nE(Y) = b'(\\theta) = \\frac{\\exp(\\theta)}{1 + \\exp(\\theta)} = \\mu = p \\\\\n\\eta = g(\\mu) = \\log(\\frac{\\mu}{1-\\mu}) = \\theta = \\log(\\frac{p}{1-p}) = \\mathbf{x'\\beta} \n\\]\\(Y_i\\), = 1,.., log-likelihood \\[\nl_i (\\beta, \\phi) = \\frac{y_i \\theta_i - b(\\theta_i)}{(\\phi)} + c(y_i, \\phi) = y_i \\mathbf{x}'_i \\beta - \\log(1+ \\exp(\\mathbf{x'\\beta}))\n\\]Additionally,\\[\nV(\\mu_i) = \\mu_i(1-\\mu_i)= p_i (1-p_i) \\\\\n\\frac{\\partial \\mu_i}{\\partial \\eta_i} = p_i(1-p_i)\n\\]Hence,\\[\n\\begin{aligned}\n\\frac{\\partial l(\\beta, \\phi)}{\\partial \\beta_j} &= \\sum_{=1}^n[\\frac{y_i - \\mu_i}{(\\phi)} \\times \\frac{1}{V(\\mu_i)}\\times \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\times x_{ij}] \\\\\n&= \\sum_{=1}^n (y_i - p_i) \\times \\frac{1}{p_i(1-p_i)} \\times p_i(1-p_i) \\times x_{ij} \\\\\n&= \\sum_{=1}^n (y_i - p_i) x_{ij} \\\\\n&= \\sum_{=1}^n (y_i - \\frac{\\exp(\\mathbf{x'_i\\beta})}{1+ \\exp(\\mathbf{x'_i\\beta})})x_{ij}\n\\end{aligned}\n\\]\\[\nw_i = ((\\frac{\\partial \\eta_i}{\\partial \\mu_i})^2 V(\\mu_i))^{-1} = p_i (1-p_i)\n\\]\\[\n\\mathbf{}_{jk}(\\mathbf{\\beta}) = \\sum_{=1}^n \\frac{w_i}{(\\phi)} x_{ij}x_{ik} = \\sum_{=1}^n p_i (1-p_i)x_{ij}x_{ik}\n\\]Fisher-scoring algorithm MLE \\(\\mathbf{\\beta}\\) \\[\n\\left(\n\\begin{array}\n{c}\n\\beta_1 \\\\\n\\beta_2 \\\\\n. \\\\\n. \\\\\n. \\\\\n\\beta_p \\\\\n\\end{array}\n\\right)^{(m+1)}\n=\n\\left(\n\\begin{array}\n{c}\n\\beta_1 \\\\\n\\beta_2 \\\\\n. \\\\\n. \\\\\n. \\\\\n\\beta_p \\\\\n\\end{array}\n\\right)^{(m)} +\n\\mathbf{}^{-1}(\\mathbf{\\beta})\n\\left(\n\\begin{array}\n{c}\n\\frac{\\partial l (\\beta, \\phi)}{\\partial \\beta_1} \\\\\n\\frac{\\partial l (\\beta, \\phi)}{\\partial \\beta_2} \\\\\n. \\\\\n. \\\\\n. \\\\\n\\frac{\\partial l (\\beta, \\phi)}{\\partial \\beta_p} \\\\\n\\end{array}\n\\right)|_{\\beta = \\beta^{(m)}}\n\\]Similar Newton-Raphson expect matrix second derivatives expected value second derivative matrix.matrix notation,\\[\n\\begin{aligned}\n\\frac{\\partial l }{\\partial \\beta} &= \\frac{1}{(\\phi)}\\mathbf{X'W\\Delta(y - \\mu)} \\\\\n&= \\frac{1}{(\\phi)}\\mathbf{F'V^{-1}(y - \\mu)} \\\\\n\\end{aligned}\n\\]\\[\n\\mathbf{}(\\beta) = \\frac{1}{(\\phi)}\\mathbf{X'WX} = \\frac{1}{(\\phi)}\\mathbf{F'V^{-1}F} \\\\\n\\]\\(\\mathbf{X}\\) n x p matrix covariates\\(\\mathbf{W}\\) n x n diagonal matrix (,)th element given \\(w_i\\)\\(\\mathbf{\\Delta}\\) n x n diagonal matrix (,)th element given \\(\\frac{\\partial \\eta_i}{\\partial \\mu_i}\\)\\(\\mathbf{F} = \\mathbf{\\frac{\\partial \\mu}{\\partial \\beta}}\\) n x p matrix ith row \\(\\frac{\\partial \\mu_i}{\\partial \\beta} = (\\frac{\\partial \\mu_i}{\\partial \\eta_i})\\mathbf{x}'_i\\)\\(\\mathbf{V}\\) n x n diagonal matrix (,)th element given \\(V(\\mu_i)\\)Setting derivative log-likelihood equal 0, ML estimating equations \\[\n\\mathbf{F'V^{-1}y= F'V^{-1}\\mu}\n\\]components equation expect y depends parameters \\(\\beta\\)Special CasesIf one canonical link, estimating equations reduce \\[\n\\mathbf{X'y= X'\\mu}\n\\]one identity link, \\[\n\\mathbf{X'V^{-1}y = X'V^{-1}X\\hat{\\beta}}\n\\]gives generalized least squares estimatorGenerally, can rewrite Fisher-scoring algorithm \\[\n\\beta^{(m+1)} = \\beta^{(m)} + \\mathbf{(\\hat{F}'\\hat{V}^{-1}\\hat{F})^{-1}\\hat{F}'\\hat{V}^{-1}(y- \\hat{\\mu})}\n\\]Since \\(\\hat{F},\\hat{V}, \\hat{\\mu}\\) depend \\(\\beta\\), evaluate \\(\\beta^{(m)}\\)starting values \\(\\beta^{(0)}\\), can iterate convergence.Notes:\\((\\phi)\\) constant form \\(m_i \\phi\\) known \\(m_i\\), \\(\\phi\\) cancels.","code":""},{"path":"generalized-linear-models.html","id":"estimation-of-phi","chapter":"7 Generalized Linear Models","heading":"7.7.1.2 Estimation of \\(\\phi\\)","text":"2 approaches:MLE\\[\n\\frac{\\partial l_i}{\\partial \\phi} = \\frac{(\\theta_i y_i - b(\\theta_i)'(\\phi))}{^2(\\phi)} + \\frac{\\partial c(y_i,\\phi)}{\\partial \\phi}\n\\]MLE \\(\\phi\\) solves\\[\n\\frac{^2(\\phi)}{'(\\phi)}\\sum_{=1}^n \\frac{\\partial c(y_i, \\phi)}{\\partial \\phi} = \\sum_{=1}^n(\\theta_i y_i - b(\\theta_i))\n\\]Situation others normal error case, expression \\(\\frac{\\partial c(y,\\phi)}{\\partial \\phi}\\) simpleSituation others normal error case, expression \\(\\frac{\\partial c(y,\\phi)}{\\partial \\phi}\\) simpleEven canonical link \\((\\phi)\\) constant, nice general expression \\(-E(\\frac{\\partial^2 l}{\\partial \\phi^2})\\), unification GLMs provide estimation \\(\\beta\\) breaks \\(\\phi\\)Even canonical link \\((\\phi)\\) constant, nice general expression \\(-E(\\frac{\\partial^2 l}{\\partial \\phi^2})\\), unification GLMs provide estimation \\(\\beta\\) breaks \\(\\phi\\)Moment Estimation (“Bias Corrected \\(\\chi^2\\)”)\nMLE conventional approach estimation \\(\\phi\\) GLMS.\nexponential family \\(var(Y) =V(\\mu)(\\phi)\\). implies\\[\n(\\phi) = \\frac{var(Y)}{V(\\mu)} = \\frac{E(Y- \\mu)^2}{V(\\mu)} \\\\\n(\\hat{\\phi})  = \\frac{1}{n-p} \\sum_{=1}^n \\frac{(y_i -\\hat{\\mu}_i)^2}{V(\\hat{\\mu})}\n\\] p dimension \\(\\beta\\)\nGLM canonical link function \\(g(.)= (b'(.))^{-1}\\)\\[\ng(\\mu) = \\theta = \\eta = \\mathbf{x'\\beta} \\\\\n\\mu = g^{-1}(\\eta)= b'(\\eta)\n\\]\nmethod estimator \\((\\phi)=\\phi\\) \nMoment Estimation (“Bias Corrected \\(\\chi^2\\)”)MLE conventional approach estimation \\(\\phi\\) GLMS.exponential family \\(var(Y) =V(\\mu)(\\phi)\\). implies\\[\n(\\phi) = \\frac{var(Y)}{V(\\mu)} = \\frac{E(Y- \\mu)^2}{V(\\mu)} \\\\\n(\\hat{\\phi})  = \\frac{1}{n-p} \\sum_{=1}^n \\frac{(y_i -\\hat{\\mu}_i)^2}{V(\\hat{\\mu})}\n\\] p dimension \\(\\beta\\)GLM canonical link function \\(g(.)= (b'(.))^{-1}\\)\\[\ng(\\mu) = \\theta = \\eta = \\mathbf{x'\\beta} \\\\\n\\mu = g^{-1}(\\eta)= b'(\\eta)\n\\]method estimator \\((\\phi)=\\phi\\) \\[\n\\hat{\\phi} = \\frac{1}{n-p} \\sum_{=1}^n \\frac{(y_i - g^{-1}(\\hat{\\eta}_i))^2}{V(g^{-1}(\\hat{\\eta}_i))}\n\\]","code":""},{"path":"generalized-linear-models.html","id":"inference-2","chapter":"7 Generalized Linear Models","heading":"7.7.2 Inference","text":"\\[\n\\hat{var}(\\beta) = (\\phi)(\\mathbf{\\hat{F}'\\hat{V}\\hat{F}})^{-1}\n\\]\\(\\mathbf{V}\\) n x n diagonal matrix diagonal elements given \\(V(\\mu_i)\\)\\(\\mathbf{F}\\) n x p matrix given \\(\\mathbf{F} = \\frac{\\partial \\mu}{\\partial \\beta}\\)\\(\\mathbf{V,F}\\) dependent mean \\(\\mu\\), thus \\(\\beta\\). Hence, estimates (\\(\\mathbf{\\hat{V},\\hat{F}}\\)) depend \\(\\hat{\\beta}\\).\\[\nH_0: \\mathbf{L\\beta = d}\n\\]\\(\\mathbf{L}\\) q x p matrix Wald test\\[\nW = \\mathbf{(L \\hat{\\beta}-d)'((\\phi)L(\\hat{F}'\\hat{V}^{-1}\\hat{F})L')^{-1}(L \\hat{\\beta}-d)}\n\\]follows \\(\\chi_q^2\\) distribution (asymptotically), q rank \\(\\mathbf{L}\\)simple case \\(H_0: \\beta_j = 0\\) gives \\(W = \\frac{\\hat{\\beta}^2_j}{\\hat{var}(\\hat{\\beta}_j)} \\sim \\chi^2_1\\) asymptoticallyLikelihood ratio test\\[\n\\Lambda = 2 (l(\\hat{\\beta}_f)-l(\\hat{\\beta}_r)) \\sim \\chi^2_q\n\\]whereq number constraints used fit reduced model \\(\\hat{\\beta}_r\\), \\(\\hat{\\beta}_r\\) fit full model.Wald test easier implement, likelihood ratio test better (especially small samples).","code":""},{"path":"generalized-linear-models.html","id":"deviance","chapter":"7 Generalized Linear Models","heading":"7.7.3 Deviance","text":"Deviance necessary goodness fit, inference alternative estimation dispersion parameter. define consider Deviance likelihood ratio perspective.Assume \\(\\phi\\) known. Let \\(\\tilde{\\theta}\\) denote full \\(\\hat{\\theta}\\) denote reduced model MLEs. , likelihood ratio (2 times difference log-likelihoods) \\[\n2\\sum_{=1}^{n} \\frac{y_i (\\tilde{\\theta}_i- \\hat{\\theta}_i)-b(\\tilde{\\theta}_i) + b(\\hat{\\theta}_i)}{a_i(\\phi)}\n\\]Assume \\(\\phi\\) known. Let \\(\\tilde{\\theta}\\) denote full \\(\\hat{\\theta}\\) denote reduced model MLEs. , likelihood ratio (2 times difference log-likelihoods) \\[\n2\\sum_{=1}^{n} \\frac{y_i (\\tilde{\\theta}_i- \\hat{\\theta}_i)-b(\\tilde{\\theta}_i) + b(\\hat{\\theta}_i)}{a_i(\\phi)}\n\\]exponential families, \\(\\mu = E(y) = b'(\\theta)\\), natural parameter function \\(\\mu: \\theta = \\theta(\\mu) = b'^{-1}(\\mu)\\), likelihood ratio turns \\[\n2 \\sum_{=1}^m \\frac{y_i\\{\\theta(\\tilde{\\mu}_i - \\theta(\\hat{\\mu}_i)\\} - b(\\theta(\\tilde{\\mu}_i)) + b(\\theta(\\hat{\\mu}_i))}{a_i(\\phi)}\n\\]exponential families, \\(\\mu = E(y) = b'(\\theta)\\), natural parameter function \\(\\mu: \\theta = \\theta(\\mu) = b'^{-1}(\\mu)\\), likelihood ratio turns \\[\n2 \\sum_{=1}^m \\frac{y_i\\{\\theta(\\tilde{\\mu}_i - \\theta(\\hat{\\mu}_i)\\} - b(\\theta(\\tilde{\\mu}_i)) + b(\\theta(\\hat{\\mu}_i))}{a_i(\\phi)}\n\\]Comparing fitted model “fullest possible model,” saturated model: \\(\\tilde{\\mu}_i = y_i\\), = 1,..,n. \\(\\tilde{\\theta}_i^* = \\theta(y_i), \\hat{\\theta}_i^* = \\theta (\\hat{\\mu})\\), likelihood ratio \\[\n2 \\sum_{=1}^{n} \\frac{y_i (\\tilde{\\theta}_i^* - \\hat{\\theta}_i^* + b(\\hat{\\theta}_i^*))}{a_i(\\phi)}\n\\]Comparing fitted model “fullest possible model,” saturated model: \\(\\tilde{\\mu}_i = y_i\\), = 1,..,n. \\(\\tilde{\\theta}_i^* = \\theta(y_i), \\hat{\\theta}_i^* = \\theta (\\hat{\\mu})\\), likelihood ratio \\[\n2 \\sum_{=1}^{n} \\frac{y_i (\\tilde{\\theta}_i^* - \\hat{\\theta}_i^* + b(\\hat{\\theta}_i^*))}{a_i(\\phi)}\n\\](McCullagh Nelder 2019) specify \\((\\phi) = \\phi\\), likelihood ratio can written \\[\nD^*(\\mathbf{y, \\hat{\\mu}}) = \\frac{2}{\\phi}\\sum_{=1}^n\\{y_i (\\tilde{\\theta}_i^*- \\hat{\\theta}_i^*)- b(\\tilde{\\theta}_i^*) +b(\\hat{\\theta}_i^*)  \\}  \n\\] (McCullagh Nelder 2019) specify \\((\\phi) = \\phi\\), likelihood ratio can written \\[\nD^*(\\mathbf{y, \\hat{\\mu}}) = \\frac{2}{\\phi}\\sum_{=1}^n\\{y_i (\\tilde{\\theta}_i^*- \\hat{\\theta}_i^*)- b(\\tilde{\\theta}_i^*) +b(\\hat{\\theta}_i^*)  \\}  \n\\] \\(D^*(\\mathbf{y, \\hat{\\mu}})\\) = scaled deviance\\(D^*(\\mathbf{y, \\hat{\\mu}})\\) = scaled deviance\\(D(\\mathbf{y, \\hat{\\mu}}) = \\phi D^*(\\mathbf{y, \\hat{\\mu}})\\) = deviance\\(D(\\mathbf{y, \\hat{\\mu}}) = \\phi D^*(\\mathbf{y, \\hat{\\mu}})\\) = devianceNote:random component distributions, can write \\(a_i(\\phi) = \\phi m_i\\), \n\\(m_i\\) known scalar may change observations. , scaled deviance components divided \\(m_i\\):\\[\nD^*(\\mathbf{y, \\hat{\\mu}}) \\equiv 2\\sum_{=1}^n\\{y_i (\\tilde{\\theta}_i^*- \\hat{\\theta}_i^*)- b(\\tilde{\\theta}_i^*) +b(\\hat{\\theta}_i^*)\\} / (\\phi m_i)  \n\\]\nrandom component distributions, can write \\(a_i(\\phi) = \\phi m_i\\), \\(m_i\\) known scalar may change observations. , scaled deviance components divided \\(m_i\\):\\[\nD^*(\\mathbf{y, \\hat{\\mu}}) \\equiv 2\\sum_{=1}^n\\{y_i (\\tilde{\\theta}_i^*- \\hat{\\theta}_i^*)- b(\\tilde{\\theta}_i^*) +b(\\hat{\\theta}_i^*)\\} / (\\phi m_i)  \n\\]\\(D^*(\\mathbf{y, \\hat{\\mu}}) = \\sum_{=1}^n d_i\\)m \\(d_i\\) deviance contribution ith observation.\\(D^*(\\mathbf{y, \\hat{\\mu}}) = \\sum_{=1}^n d_i\\)m \\(d_i\\) deviance contribution ith observation.D used model selectionD used model selection\\(D^*\\) used goodness fit tests (likelihood ratio statistic). \\[\nD^*(\\mathbf{y, \\hat{\\mu}}) = 2\\{l(\\mathbf{y,\\tilde{\\mu}})-l(\\mathbf{y,\\hat{\\mu}})\\}\n\\]\\(D^*\\) used goodness fit tests (likelihood ratio statistic). \\[\nD^*(\\mathbf{y, \\hat{\\mu}}) = 2\\{l(\\mathbf{y,\\tilde{\\mu}})-l(\\mathbf{y,\\hat{\\mu}})\\}\n\\]\\(d_i\\) used form deviance residuals\\(d_i\\) used form deviance residualsExample:NormalWe \\[\n\\theta = \\mu \\\\\n\\phi = \\sigma^2 \\\\\nb(\\theta) = \\frac{1}{2} \\theta^2 \\\\\n(\\phi) = \\phi\n\\]Hence,\\[\n\\tilde{\\theta}_i = y_i \\\\\n\\hat{\\theta}_i = \\hat{\\mu}_i = g^{-1}(\\hat{\\eta}_i) \n\\]\\[\n\\begin{aligned}\nD &= 2 \\sum_{1=1}^n Y^2_i - y_i \\hat{\\mu}_i - \\frac{1}{2}y^2_i + \\frac{1}{2} \\hat{\\mu}_i^2 \\\\\n&= \\sum_{=1}^n y_i^2 - 2y_i \\hat{\\mu}_i + \\hat{\\mu}_i^2 \\\\\n&= \\sum_{=1}^n (y_i - \\hat{\\mu}_i)^2\n\\end{aligned}\n\\]residual sum squaresPoisson\\[\nf(y) = \\exp\\{y\\log(\\mu) - \\mu - \\log(y!)\\} \\\\\n\\theta = \\log(\\mu) \\\\\nb(\\theta) = \\exp(\\theta) \\\\\n(\\phi) = 1 \\\\\n\\tilde{\\theta}_i = \\log(y_i) \\\\\n\\hat{\\theta}_i = \\log(\\hat{\\mu}_i) \\\\\n\\hat{\\mu}_i = g^{-1}(\\hat{\\eta}_i)\n\\],\\[\n\\begin{aligned}\nD &= 2 \\sum_{= 1}^n y_i \\log(y_i) - y_i \\log(\\hat{\\mu}_i) - y_i + \\hat{\\mu}_i \\\\\n&= 2 \\sum_{= 1}^n y_i \\log(\\frac{y_i}{\\hat{\\mu}_i}) - (y_i - \\hat{\\mu}_i)\n\\end{aligned}\n\\]\\[\nd_i = 2\\{y_i \\log(\\frac{y_i}{\\hat{\\mu}})- (y_i - \\hat{\\mu}_i)\\}\n\\]","code":""},{"path":"generalized-linear-models.html","id":"analysis-of-deviance","chapter":"7 Generalized Linear Models","heading":"7.7.3.1 Analysis of Deviance","text":"difference deviance reduced full model, q difference number free parameters, asymptotic \\(\\chi^2_q\\). likelihood ratio test\\[\nD^*(\\mathbf{y;\\hat{\\mu}_r}) - D^*(\\mathbf{y;\\hat{\\mu}_f}) = 2\\{l(\\mathbf{y;\\hat{\\mu}_f})-l(\\mathbf{y;\\hat{\\mu}_r})\\}\n\\]comparison models Analysis Deviance. GLM uses analysis model selection.estimation \\(\\phi\\) \\[\n\\hat{\\phi} = \\frac{D(\\mathbf{y, \\hat{\\mu}})}{n - p}\n\\]p = number parameters fit.Excessive use \\(\\chi^2\\) test problematic since asymptotic (McCullagh Nelder 2019)","code":""},{"path":"generalized-linear-models.html","id":"deviance-residuals","chapter":"7 Generalized Linear Models","heading":"7.7.3.2 Deviance Residuals","text":"\\(D = \\sum_{=1}^{n}d_i\\). , define deviance residuals\\[\nr_{D_i} = \\text{sign}(y_i -\\hat{\\mu}_i)\\sqrt{d_i}\n\\]Standardized version deviance residuals \\[\nr_{s,} = \\frac{y_i -\\hat{\\mu}}{\\hat{\\sigma}(1-h_{ii})^{1/2}}\n\\]Let \\(\\mathbf{H^{GLM} = W^{1/2}X(X'WX)^{-1}X'W^{-1/2}}\\), \\(\\mathbf{W}\\) n x n diagonal matrix (,)th element given \\(w_i\\) (see Estimation \\(\\beta\\)). Standardized deviance residuals equivalently\\[\nr_{s, D_i} = \\frac{r_{D_i}}{\\{\\hat{\\phi}(1-h_{ii}^{glm}\\}^{1/2}}\n\\]\\(h_{ii}^{glm}\\) ith diagonal \\(\\mathbf{H}^{GLM}\\)","code":""},{"path":"generalized-linear-models.html","id":"pearson-chi-square-residuals","chapter":"7 Generalized Linear Models","heading":"7.7.3.3 Pearson Chi-square Residuals","text":"Another \\(\\chi^2\\) statistic Pearson \\(\\chi^2\\) statistics: (assume \\(m_i = 1\\))\\[\nX^2 = \\sum_{=1}^{n} \\frac{(y_i - \\hat{\\mu}_i)^2}{V(\\hat{\\mu}_i)}\n\\]\\(\\hat{\\mu}_i\\) fitted mean response fo model interest.Scaled Pearson \\(\\chi^2\\) statistic given \\(\\frac{X^2}{\\phi} \\sim \\chi^2_{n-p}\\) p number parameters estimated. Hence, Pearson \\(\\chi^2\\) residuals \\[\nX^2_i = \\frac{(y_i - \\hat{\\mu}_i)^2}{V(\\hat{\\mu}_i)}\n\\]following assumptions:Independent samplesNo -dispersion: \\(\\phi = 1\\), \\(\\frac{D(\\mathbf{y;\\hat{\\mu}})}{n-p}\\) \\(\\frac{X^2}{n-p}\\) value substantially larger 1 indicates improperly specified model overdispersionMultiple groupsthen \\(\\frac{X^2}{\\phi}\\) \\(D^*(\\mathbf{y; \\hat{\\mu}})\\) follow \\(\\chi^2_{n-p}\\)","code":""},{"path":"generalized-linear-models.html","id":"diagnostic-plots","chapter":"7 Generalized Linear Models","heading":"7.7.4 Diagnostic Plots","text":"Standardized residual Plots:\nplot(\\(r_{s, D_i}\\), \\(\\hat{\\mu}_i\\)) plot(\\(r_{s, D_i}\\), \\(T(\\hat{\\mu}_i)\\)) \\(T(\\hat{\\mu}_i)\\) transformation(\\(\\hat{\\mu}_i\\)) called constant information scale:\nplot(\\(r_{s, D_i}\\), \\(\\hat{\\eta}_i\\))\nStandardized residual Plots:plot(\\(r_{s, D_i}\\), \\(\\hat{\\mu}_i\\)) plot(\\(r_{s, D_i}\\), \\(T(\\hat{\\mu}_i)\\)) \\(T(\\hat{\\mu}_i)\\) transformation(\\(\\hat{\\mu}_i\\)) called constant information scale:plot(\\(r_{s, D_i}\\), \\(\\hat{\\eta}_i\\))see:\nTrend, means might wrong link function, choice scale\nSystematic change range residuals change \\(T(\\hat{\\mu})\\) (incorrect random component) (systematic \\(\\neq\\) random)\nsee:Trend, means might wrong link function, choice scaleSystematic change range residuals change \\(T(\\hat{\\mu})\\) (incorrect random component) (systematic \\(\\neq\\) random)plot(\\(|r_{D_i}|,\\hat{\\mu}_i\\)) check Variance Function.plot(\\(|r_{D_i}|,\\hat{\\mu}_i\\)) check Variance Function.","code":""},{"path":"generalized-linear-models.html","id":"goodness-of-fit","chapter":"7 Generalized Linear Models","heading":"7.7.5 Goodness of Fit","text":"assess goodness fit, can useDeviancePearson Chi-square ResidualsIn nested model, use likelihood-based information measures:\\[\nAIC = -2l(\\mathbf{\\hat{\\mu}}) + 2p \\\\\nAICC = -2l(\\mathbf{\\hat{\\mu}}) + 2p(\\frac{n}{n-p-1}) \\\\\nBIC = 2l(\\hat{\\mu}) + p \\log(n)\n\\]\\(l(\\hat{\\mu})\\) log-likelihood evaluated parameter estimatesp number parametersn number observations.Note: use data model (.e., link function, random underlying random distribution). can different number parameters.Even though statisticians try come measures similar \\(R^2\\), practice, appropriate. example, comapre log-likelihood fitted model model jsut intercept:\\[\nR^2_p = 1 - \\frac{l(\\hat{\\mu})}{l(\\hat{\\mu}_0)}\n\\]certain specific random components binary response model, **rescaled generalized \\(R^2\\):\\[\n\\bar{R}^2 = \\frac{R^2_*}{\\max(R^2_*)} = \\frac{1-\\exp\\{-\\frac{2}{n}(l(\\hat{\\mu}) - l(\\hat{\\mu}_0) \\}}{1 - \\exp\\{\\frac{2}{n}l(\\hat{\\mu}_0)\\}}\n\\]","code":""},{"path":"generalized-linear-models.html","id":"over-dispersion","chapter":"7 Generalized Linear Models","heading":"7.7.6 Over-Dispersion","text":"cases \\(\\phi = 1\\). Recall \\(b''(\\theta)= V(\\mu)\\) check Estimation \\(\\phi\\).find\\(\\phi >1\\): -dispersion (.e., much variation independent binomial Poisson distribution).\\(\\phi<1\\): -dispersion (.e., little variation independent binomial Poisson distribution).either -dispersion, means might unspecified random component, couldSelect different random component distribution can accommodate -dispersion (e.g., negative binomial, Conway-Maxwell Poisson)use Generalized Linear Mixed Models handle random effects generalized linear models.","code":""},{"path":"linear-mixed-models.html","id":"linear-mixed-models","chapter":"8 Linear Mixed Models","heading":"8 Linear Mixed Models","text":"","code":""},{"path":"linear-mixed-models.html","id":"dependent-data","chapter":"8 Linear Mixed Models","heading":"8.1 Dependent Data","text":"Forms dependent data:Multivariate measurements different individuals: (e.g., person’s blood pressure, fat, etc correlated)Clustered measurements: (e.g., blood pressure measurements people family can correlated).Repeated measurements: (e.g., measurement cholesterol time can correlated) “data collected repeatedly experimental material treatments applied initially, data repeated measure.” (Schabenberger Pierce 2001)Longitudinal data: (e.g., individual’s cholesterol tracked time correlated): “data collected repeatedly time observational study termed longitudinal.” (Schabenberger Pierce 2001)Spatial data: (e.g., measurement individuals living neighborhood correlated)Hence, like account correlations.Linear Mixed Model (LMM), also known Mixed Linear Model 2 components:Fixed effect (e.g, gender, age, diet, time)Fixed effect (e.g, gender, age, diet, time)Random effects representing individual variation auto correlation/spatial effects imply dependent (correlated) errorsRandom effects representing individual variation auto correlation/spatial effects imply dependent (correlated) errorsReview Two-Way Mixed Effects ANOVAWe choose model random subject-specific effect instead including dummy subject covariates model :reduction number parameters estimatewhen inference, make sense can infer population (.e., random effect).LLM MotivationIn repeated measurements analysis \\(Y_{ij}\\) response -th individual measured j-th time,\\(=1,…,N\\) ; \\(j = 1,…,n_i\\)\\[\n\\mathbf{Y}_i = \n\\left(\n\\begin{array}\n{c}\nY_{i1} \\\\\n. \\\\\n.\\\\\n.\\\\\nY_{in_i}\n\\end{array}\n\\right)\n\\]measurements subject .Stage 1: (Regression Model) response changes time ith subject\\[\n\\mathbf{Y_i = Z_i \\beta_i + \\epsilon_i}\n\\]\\(Z_i\\) \\(n_i \\times q\\) matrix known covariates\\(\\beta_i\\) unknown q x 1 vector subjective -specific coefficients (regression coefficients different subject)\\(\\epsilon_i\\) random errors (typically \\(\\sim N(0, \\sigma^2 )\\))notice two many \\(\\beta\\) estimate . Hence, motivation second stageStage 2: (Parameter Model)\\[\n\\mathbf{\\beta_i = K_i \\beta + b_i}\n\\]\\(K_i\\) q x p matrix known covariates\\(\\beta\\) p x 1 vector unknown parameter\\(\\mathbf{b}_i\\) independent \\(N(0,D)\\) random variablesThis model explain observed variability subjects respect subject-specific regression coefficients, \\(\\beta_i\\). model different coefficient (\\(\\beta_i\\)) respect \\(\\beta\\).Example:Stage 1:\\[\nY_{ij} = \\beta_{1i} + \\beta_{2i}t_{ij} + \\epsilon_{ij}\n\\]\\(j = 1,..,n_i\\)matrix notation,\\[\n\\mathbf{Y_i} = \n\\left(\n\\begin{array}\n{c}\nY_{i1} \\\\\n.\\\\\nY_{in_i}\n\\end{array}\n\\right)\n\\]\\[\n\\mathbf{Z}_i = \n\\left(\n\\begin{array}\n{cc}\n1 & t_{i1} \\\\\n. & . \\\\\n1 & t_{in_i} \n\\end{array}\n\\right)\n\\]\\[\n\\beta_i =\n\\left(\n\\begin{array}\n{c}\n\\beta_{1i} \\\\\n\\beta_{2i}\n\\end{array}\n\\right)\n\\]\\[\n\\epsilon_i = \n\\left(\n\\begin{array}\n{c}\n\\epsilon_{i1} \\\\\n. \\\\\n\\epsilon_{in_i}\n\\end{array}\n\\right)\n\\]Thus,\\[\n\\mathbf{Y_i = Z_i \\beta_i + \\epsilon_i}\n\\]Stage 2:\\[\n\\beta_{1i} = \\beta_0 + b_{1i} \\\\\n\\beta_{2i} = \\beta_1 L_i + \\beta_2 H_i + \\beta_3 C_i + b_{2i}\n\\]\\(L_i, H_i, C_i\\) indicator variables defined 1 subject falls different categories.Subject specific intercepts depend upon treatment, \\(\\beta_0\\) (average response start treatment), \\(\\beta_1 , \\beta_2, \\beta_3\\) (average time effects three treatment groups).\\[\n\\mathbf{K}_i = \\left(\n\\begin{array}\n{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & L_i & H_i & C_i \n\\end{array}\n\\right) \\\\ \\beta = (\\beta_0 , \\beta_1, \\beta_2, \\beta_3)' \\\\ \n\\mathbf{b}_i = \n\\left(\n\\begin{array}\n{c}\nb_{1i} \\\\\nb_{2i} \\\\\n\\end{array}\n\\right) \\\\ \n\\beta_i = \\mathbf{K_i \\beta + b_i}\n\\]get \\(\\hat{\\beta}\\), can fit model sequentially:Estimate \\(\\hat{\\beta_i}\\) first stageEstimate \\(\\hat{\\beta}\\) second stage replacing \\(\\beta_i\\) \\(\\hat{\\beta}_i\\)However, problems arise method:information lost summarizing vector \\(\\mathbf{Y}_i\\) solely \\(\\hat{\\beta}_i\\)need account variability replacing \\(\\beta_i\\) estimatedifferent subjects might different number observations.address problems, can use Linear Mixed Model (Laird Ware 1982)Substituting stage 2 stage 1:\\[\n\\mathbf{Y}_i = \\mathbf{Z}_i \\mathbf{K}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i + \\mathbf{\\epsilon}_i\n\\]Let \\(\\mathbf{X}_i = \\mathbf{Z}_i \\mathbf{K}_i\\) \\(n_i \\times p\\) matrix . , LMM \\[\n\\mathbf{Y}_i = \\mathbf{X}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i + \\mathbf{\\epsilon}_i\n\\]\\(= 1,..,N\\)\\(\\beta\\) fixed effects, common subjects\\(\\mathbf{b}_i\\) subject specific random effects. \\(\\mathbf{b}_i \\sim N_q (\\mathbf{0,D})\\)\\(\\mathbf{\\epsilon}_i \\sim N_{n_i}(\\mathbf{0,\\Sigma_i})\\)\\(\\mathbf{b}_i\\) \\(\\epsilon_i\\) independent\\(\\mathbf{Z}_{(n_i \\times q})\\) \\(\\mathbf{X}_{(n_i \\times p})\\) matrices known covariates.Equivalently, hierarchical form, call conditional hierarchical formulation linear mixed model\\[\n\\mathbf{Y}_i | \\mathbf{b}_i \\sim N(\\mathbf{X}_i \\beta+ \\mathbf{Z}_i \\mathbf{b}_i, \\mathbf{\\Sigma}_i) \\\\\n\\mathbf{b}_i \\sim N(\\mathbf{0,D})\n\\]\\(= 1,..,N\\). denote respective functions \\(f(\\mathbf{Y}_i |\\mathbf{b}_i)\\) \\(f(\\mathbf{b}_i)\\)general,\\[\nf(,B) = f(|B)f(B) \\\\\nf() = \\int f(,B)dB = \\int f(|B) f(B) dB\n\\]LMM, marginal density \\(\\mathbf{Y}_i\\) \\[\nf(\\mathbf{Y}_i) = \\int f(\\mathbf{Y}_i | \\mathbf{b}_i) f(\\mathbf{b}_i) d\\mathbf{b}_i\n\\]can shown\\[\n\\mathbf{Y}_i \\sim N(\\mathbf{X_i \\beta, Z_i DZ'_i + \\Sigma_i})\n\\]marginal formulation linear mixed modelNotes:longer \\(Z_i b_i\\) mean, add error variance (marginal dependence Y). kinda averaging common effect. Technically, shouldn’t call averaging error b (adding variance covariance matrix), called adding random effectContinue example\\[\nY_{ij} = (\\beta_0 + b_{1i}) + (\\beta_1L_i + \\beta_2 H_i + \\beta_3 C_i + b_{2i})t_{ij} + \\epsilon_{ij}\n\\]treatment group\\[\nY_{ik}= \n\\begin{cases}\n\\beta_0 + b_{1i} + (\\beta_1 + \\ b_{2i})t_{ij} + \\epsilon_{ij} & L \\\\\n\\beta_0 + b_{1i} + (\\beta_2 + \\ b_{2i})t_{ij} + \\epsilon_{ij} & H\\\\\n\\beta_0 + b_{1i} + (\\beta_3 + \\ b_{2i})t_{ij} + \\epsilon_{ij} & C\n\\end{cases}\n\\]Intercepts slopes subject specificDifferent treatment groups different slops, intercept.hierarchical model form\\[\n\\mathbf{Y}_i | \\mathbf{b}_i \\sim N(\\mathbf{X}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i, \\mathbf{\\Sigma}_i)\\\\\n\\mathbf{b}_i \\sim N(\\mathbf{0,D})\n\\]X form \\[\n\\mathbf{X}_i = \\mathbf{Z}_i \\mathbf{K}_i \\\\\n= \n\\left[\n\\begin{array}\n{cc}\n1 & t_{i1} \\\\\n1 & t_{i2} \\\\\n. & . \\\\\n1 & t_{in_i}\n\\end{array}\n\\right]\n\\times\n\\left[\n\\begin{array}\n{cccc}\n1 & 0 & 0 & 0 \\\\\n0 & L_i & H_i & C_i \\\\\n\\end{array}\n\\right] \\\\\n=\n\\left[ \n\\begin{array}\n{cccc}\n1 & t_{i1}L_i & t_{i1}H_i & T_{i1}C_i \\\\\n1 & t_{i2}L_i & t_{i2}H_i & T_{i2}C_i \\\\\n. &. &. &. \\\\\n1 & t_{in_i}L_i & t_{in_i}H_i & T_{in_i}C_i \\\\\n\\end{array}\n\\right]\n\\]\\[\n\\beta = (\\beta_0, \\beta_1, \\beta_2, \\beta_3)' \\\\\n\\mathbf{b}_i = \n\\left(\n\\begin{array}\n{c}\nb_{1i} \\\\\nb_{2i}\n\\end{array}\n\\right)\n,\nD = \n\\left(\n\\begin{array}\n{cc}\nd_{11} & d_{12}\\\\\nd_{12} & d_{22}\n\\end{array}\n\\right)\n\\]Assuming \\(\\mathbf{\\Sigma}_i = \\sigma^2 \\mathbf{}_{n_i}\\), called conditional independence, meaning response subject independent conditional \\(\\mathbf{b}_i\\) \\(\\beta\\)marginal model form\\[\nY_{ij} = \\beta_0 + \\beta_1 L_i t_{ij} + \\beta_2 H_i t_{ij} + \\beta_3 C_i t_{ij} + \\eta_{ij}\n\\]\\(\\eta_i \\sim N(\\mathbf{0},\\mathbf{Z}_i\\mathbf{DZ}_i'+ \\mathbf{\\Sigma}_i)\\)Equivalently,\\[\n\\mathbf{Y_i \\sim N(X_i \\beta, Z_i DZ_i' + \\Sigma_i})\n\\]case \\(n_i = 2\\)\\[\n\\mathbf{Z_iDZ_i'} = \n\\left(\n\\begin{array}\n{cc}\n1 & t_{i1} \\\\\n1 & t_{i2} \n\\end{array}\n\\right)\n\\left(\n\\begin{array}\n{cc}\nd_{11} & d_{12} \\\\\nd_{12} & d_{22} \n\\end{array}\n\\right)\n\\left(\n\\begin{array}\n{cc}\n1 & 1 \\\\\nt_{i1} & t_{i2} \n\\end{array}\n\\right) \\\\\n=\n\\left(\n\\begin{array}\n{cc}\nd_{11} + 2d_{12}t_{i1} + d_{22}t_{i1}^2 & d_{11} + d_{12}(t_{i1} + t_{i2}) + d_{22}t_{i1}t_{i2} \\\\\nd_{11} + d_{12}(t_{i1} + t_{i2}) + d_{22} t_{i1} t_{i2} & d_{11} + 2d_{12}t_{i2} + d_{22}t_{i2}^2  \n\\end{array}\n\\right)\n\\]\\[\nvar(Y_{i1}) = d_{11} + 2d_{12}t_{i1} + d_{22} t_{i1}^2 + \\sigma^2\n\\]top correlation errors, marginal implies variance function response quadratic time, positive curvature \\(d_{22}\\)","code":""},{"path":"linear-mixed-models.html","id":"random-intercepts-model","chapter":"8 Linear Mixed Models","heading":"8.1.1 Random-Intercepts Model","text":"remove random slopes,assumption variability subject-specific slopes can attributed treatment differencesthe model random-intercepts model. subject specific intercepts, slopes within treatment group.\\[\n\\mathbf{Y}_i | b_i \\sim N(\\mathbf{X}_i \\beta + 1 b_i , \\Sigma_i) \\\\\nb_i \\sim N(0,d_{11})\n\\]marginal model (\\(\\mathbf{\\Sigma}_i = \\sigma^2 \\mathbf{}\\))\\[\n\\mathbf{Y}_i \\sim N(\\mathbf{X}_i \\beta, 11'd_{11} + \\sigma^2 \\mathbf{})\n\\]marginal covariance matrix \\[\ncov(\\mathbf{Y}_i)  = 11'd_{11} + \\sigma^2I \\\\\n=\n\\left(\n\\begin{array}\n{cccc}\nd_{11}+ \\sigma^2 & d_{11} & ... & d_{11} \\\\\nd_{11} & d_{11} + \\sigma^2 & d_{11} & ... \\\\\n. & . & . & . \\\\\nd_{11} & ... & ... & d_{11} + \\sigma^2\n\\end{array}\n\\right)\n\\]associated correlation matrix \\[\ncorr(\\mathbf{Y}_i) = \n\\left(\n\\begin{array}\n{cccc}\n1 & \\rho & ... & \\rho \\\\\n\\rho & 1 & \\rho & ... \\\\\n. & . & . & . \\\\\n\\rho & ... & ... & 1 \\\\\n\\end{array}\n\\right)\n\\]\\(\\rho \\equiv \\frac{d_{11}}{d_{11} + \\sigma^2}\\)Thu, haveconstant variance timeequal, positive correlation two measurements subjecta covariance structure called compound symmetry, \\(\\rho\\) called intra-class correlationthat \\(\\rho\\) large, inter-subject variability (\\(d_{11}\\)) large relative intra-subject variability (\\(\\sigma^2\\))","code":""},{"path":"linear-mixed-models.html","id":"covariance-models","chapter":"8 Linear Mixed Models","heading":"8.1.2 Covariance Models","text":"conditional independence assumption, (\\(\\mathbf{\\Sigma_i= \\sigma^2 I_{n_i}}\\)). Consider, \\(\\epsilon_i = \\epsilon_{(1)} + \\epsilon_{(2)}\\), \\(\\epsilon_{(1)}\\) “serial correlation” component. , part individual’s profile response time-varying stochastic processes.\\(\\epsilon_{(2)}\\) measurement error component, independent \\(\\epsilon_{(1)}\\)\\[\n\\mathbf{Y_i = X_i \\beta + Z_i b_i + \\epsilon_{(1)} + \\epsilon_{(2)}}\n\\]\\(\\mathbf{b_i} \\sim N(\\mathbf{0,D})\\)\\(\\mathbf{b_i} \\sim N(\\mathbf{0,D})\\)\\(\\epsilon_{(2)} \\sim N(\\mathbf{0,\\sigma^2 I_{n_i}})\\)\\(\\epsilon_{(2)} \\sim N(\\mathbf{0,\\sigma^2 I_{n_i}})\\)\\(\\epsilon_{(1)} \\sim N(\\mathbf{0,\\tau^2H_i})\\)\\(\\epsilon_{(1)} \\sim N(\\mathbf{0,\\tau^2H_i})\\)\\(\\mathbf{b}_i\\) \\(\\epsilon_i\\) mutually independent\\(\\mathbf{b}_i\\) \\(\\epsilon_i\\) mutually independentTo model structure \\(n_i \\times n_i\\) correlation (covariance ) matrix \\(\\mathbf{H}_i\\). Let (j,k)th element \\(\\mathbf{H}_i\\) \\(h_{ijk}= g(t_{ij}t_{ik})\\). function times \\(t_{ij}\\) \\(t_{ik}\\) , assumed function \"distance’ times.\\[\nh_{ijk} = g(|t_{ij}-t_{ik}|)\n\\]decreasing function \\(g(.)\\) \\(g(0)=1\\) (correlation matrices).Examples type function:Exponential function: \\(g(|t_{ij}-t_{ik}|) = \\exp(-\\phi|t_{ij} - t_{ik}|)\\)Gaussian function: \\(g(|t_{ij} - t_{ik}|) = \\exp(-\\phi(t_{ij} - t_{ik})^2)\\)Similar structures also used \\(\\mathbf{D}\\) matrix (\\(\\mathbf{b}\\))Example: Autoregressive Covariance StructureA first order Autoregressive Model (AR(1)) form\\[\n\\alpha_t = \\phi \\alpha_{t-1} + \\eta_t\n\\]\\(\\eta_t \\sim iid N (0,\\sigma^2_\\eta)\\), covariance two observations \\[\ncov(\\alpha_t, \\alpha_{t+h}) = \\frac{\\sigma^2_\\eta \\phi^{|h|}}{1- \\phi^2}\n\\]\\(h = 0, \\pm 1, \\pm 2, ...; |\\phi|<1\\)Hence,\\[\ncorr(\\alpha_t, \\alpha_{t+h}) = \\phi^{|h|}\n\\]let \\(\\alpha_T = (\\alpha_1,...\\alpha_T)'\\), \\[\ncorr(\\alpha_T) = \n\\left[\n\\begin{array}\n{ccccc}\n1 & \\phi^1 & \\phi^2 & ... & \\phi^2 \\\\\n\\phi^1 & 1 & \\phi^1 & ... & \\phi^{T-1} \\\\\n\\phi^2 & \\phi^1 & 1 & ... & \\phi^{T-2} \\\\\n. & . & . & . &. \\\\\n\\phi^T & \\phi^{T-1} & \\phi^{T-2} & ... & 1\n\\end{array}\n\\right]\n\\]Notes:correlation decreases time lag increasesThis matrix structure known Toeplitz structureMore complicated covariance structures possible, critical component spatial random effects models time series models.Often, don’t need random effects \\(\\mathbf{b}\\) \\(\\epsilon_{(1)}\\)Time Series section","code":""},{"path":"linear-mixed-models.html","id":"estimation-2","chapter":"8 Linear Mixed Models","heading":"8.2 Estimation","text":"\\[\n\\mathbf{Y}_i = \\mathbf{X}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i + \\epsilon_i\n\\]\\(\\beta, \\mathbf{b}_i, \\mathbf{D}, \\mathbf{\\Sigma}_i\\) must obtain estimation data\\(\\mathbf{\\beta}, \\mathbf{D}, \\mathbf{\\Sigma}_i\\) unknown, fixed, parameters, must estimated data\\(\\mathbf{b}_i\\) random variable. Thus, can’t estimate values, can predict . (.e., can’t estimate random thing).\\(\\hat{\\beta}\\) estimator \\(\\beta\\)\\(\\mathbf{b}_i\\) predictor \\(\\mathbf{b}_i\\),population average estimate \\(\\mathbf{Y}_i\\) \\(\\hat{\\mathbf{Y}_i} = \\mathbf{X}_i \\hat{\\beta}\\)subject-specific prediction \\(\\hat{\\mathbf{Y}_i} = \\mathbf{X}_i \\hat{\\beta} + \\mathbf{Z}_i \\hat{b}_i\\)According (Henderson 1950), estimating equations known mixed model equations:\\[\n\\left[\n\\begin{array}\n{c}\n\\hat{\\beta} \\\\\n\\hat{\\mathbf{b}}\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{X'\\Sigma^{-1}X} & \\mathbf{X'\\Sigma^{-1}Z} \\\\\n\\mathbf{Z'\\Sigma^{-1}X} & \\mathbf{Z'\\Sigma^{-1}Z +B^{-1}}\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{X'\\Sigma^{-1}Y} \\\\\n\\mathbf{Z'\\Sigma^{-1}Y}\n\\end{array}\n\\right]\n\\]\\[\n\\mathbf{Y}\n=\n\\left[\n\\begin{array}\n{c}\n\\mathbf{y}_1 \\\\\n. \\\\\n\\mathbf{y}_N\n\\end{array}\n\\right] ;\n\\mathbf{X}\n=\n\\left[\n\\begin{array}\n{c}\n\\mathbf{X}_1 \\\\\n. \\\\\n\\mathbf{X}_N\n\\end{array}\n\\right];\n\\mathbf{b} = \n\\left[\n\\begin{array}\n{c}\n\\mathbf{b}_1 \\\\\n. \\\\\n\\mathbf{b}_N\n\\end{array}\n\\right] ;\n\\epsilon = \n\\left[\n\\begin{array}\n{c}\n\\epsilon_1 \\\\\n. \\\\\n\\epsilon_N\n\\end{array}\n\\right]\n\\\\\ncov(\\epsilon) = \\mathbf{\\Sigma},\n\\mathbf{Z} = \n\\left[\n\\begin{array}\n{cccc}\n\\mathbf{Z}_1 & 0 &  ... & 0 \\\\\n0 & \\mathbf{Z}_2 & ... & 0 \\\\\n. & . & . & . \\\\\n0 & 0 & ... & \\mathbf{Z}_n\n\\end{array}\n\\right],\n\\mathbf{B} =\n\\left[\n\\begin{array}\n{cccc}\n\\mathbf{D} & 0 & ... & 0 \\\\\n0 & \\mathbf{D} & ... & 0 \\\\\n. & . & . & . \\\\\n0 & 0 & ... & \\mathbf{D}\n\\end{array}\n\\right]\n\\]model form\\[\n\\mathbf{Y = X \\beta + Z b + \\epsilon} \\\\\n\\mathbf{Y} \\sim N(\\mathbf{X \\beta, ZBZ' + \\Sigma})\n\\]\\(\\mathbf{V = ZBZ' + \\Sigma}\\), solutions estimating equations can \\[\n\\hat{\\beta} = \\mathbf{(X'V^{-1}X)^{-1}X'V^{-1}Y} \\\\\n\\hat{\\mathbf{b}} = \\mathbf{BZ'V^{-1}(Y-X\\hat{\\beta}})\n\\]estimate \\(\\hat{\\beta}\\) generalized least squares estimate.predictor, \\(\\hat{\\mathbf{b}}\\) best linear unbiased predictor (BLUP), \\(\\mathbf{b}\\)\\[\nE(\\hat{\\beta}) = \\beta \\\\\nvar(\\hat{\\beta}) = (\\mathbf{X'V^{-1}X})^{-1} \\\\\nE(\\hat{\\mathbf{b}}) = 0 \\\\\nvar(\\mathbf{\\hat{b}-b}) = \\mathbf{B-BZ'V^{-1}ZB + BZ'V^{-1}X(X'V^{-1}X)^{-1}X'V^{-1}B}\n\\]variance variance prediction error (mean squared prediction error, MSPE), meaningful \\(var(\\hat{\\mathbf{b}})\\), since MSPE accounts variance bias prediction.derive mixed model equations, consider\\[\n\\mathbf{\\epsilon = Y - X\\beta - Zb}\n\\]Let \\(T = \\sum_{=1}^N n_i\\) total number observations (.e., length \\(\\mathbf{Y},\\epsilon\\)) \\(Nq\\) length \\(\\mathbf{b}\\). joint distribution \\(\\mathbf{b, \\epsilon}\\) \\[\nf(\\mathbf{b,\\epsilon})= \\frac{1}{(2\\pi)^{(T+ Nq)/2}}\n\\left|\n\\begin{array}\n{cc}\n\\mathbf{B} & 0 \\\\\n0 & \\mathbf{\\Sigma}\n\\end{array}\n\\right| ^{-1/2}\n\\exp\n\\left(\n-\\frac{1}{2}\n\\left[\n\\begin{array}\n{c}\n\\mathbf{b} \\\\\n\\mathbf{Y - X \\beta - Zb}\n\\end{array}\n\\right]'\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{B} & 0 \\\\\n0 & \\mathbf{\\Sigma}\n\\end{array}\n\\right]^{-1}\n\\left[\n\\begin{array}\n{c}\n\\mathbf{b} \\\\\n\\mathbf{Y - X \\beta - Zb}\n\\end{array}\n\\right]\n\\right)\n\\]Maximization \\(f(\\mathbf{b},\\epsilon)\\) respect \\(\\mathbf{b}\\) \\(\\beta\\) requires minimization \\[\nQ = \n\\left[\n\\begin{array}\n{c}\n\\mathbf{b} \\\\\n\\mathbf{Y - X \\beta - Zb}\n\\end{array}\n\\right]'\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{B} & 0 \\\\\n0 & \\mathbf{\\Sigma}\n\\end{array}\n\\right]^{-1}\n\\left[\n\\begin{array}\n{c}\n\\mathbf{b} \\\\\n\\mathbf{Y - X \\beta - Zb}\n\\end{array}\n\\right] \\\\\n= \\mathbf{b'B^{-1}b+(Y-X \\beta-Zb)'\\Sigma^{-1}(Y-X \\beta-Zb)}\n\\]Setting derivatives Q respect \\(\\mathbf{b}\\) \\(\\mathbf{\\beta}\\) zero leads system equations:\\[\n\\begin{aligned}\n\\mathbf{X'\\Sigma^{-1}X\\beta + X'\\Sigma^{-1}Zb} &= \\mathbf{X'\\Sigma^{-1}Y}\\\\\n\\mathbf{(Z'\\Sigma^{-1}Z + B^{-1})b + Z'\\Sigma^{-1}X\\beta} &= \\mathbf{Z'\\Sigma^{-1}Y}\n\\end{aligned}\n\\]Rearranging\\[\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{X'\\Sigma^{-1}X} & \\mathbf{X'\\Sigma^{-1}Z} \\\\\n\\mathbf{Z'\\Sigma^{-1}X} & \\mathbf{Z'\\Sigma^{-1}Z + B^{-1}}\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{c}\n\\beta \\\\\n\\mathbf{b}\n\\end{array}\n\\right]\n= \n\\left[\n\\begin{array}\n{c}\n\\mathbf{X'\\Sigma^{-1}Y} \\\\\n\\mathbf{Z'\\Sigma^{-1}Y}\n\\end{array}\n\\right]\n\\]Thus, solution mixed model equations give:\\[\n\\left[\n\\begin{array}\n{c}\n\\hat{\\beta} \\\\\n\\hat{\\mathbf{b}}\n\\end{array}\n\\right]\n= \n\\left[\n\\begin{array}\n{cc}\n\\mathbf{X'\\Sigma^{-1}X} & \\mathbf{X'\\Sigma^{-1}Z} \\\\\n\\mathbf{Z'\\Sigma^{-1}X} & \\mathbf{Z'\\Sigma^{-1}Z + B^{-1}}\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{c}\n\\mathbf{X'\\Sigma^{-1}Y} \\\\\n\\mathbf{Z'\\Sigma^{-1}Y}\n\\end{array}\n\\right]\n\\]Equivalently,Bayes’ theorem\\[\nf(\\mathbf{b}| \\mathbf{Y}) = \\frac{f(\\mathbf{Y}|\\mathbf{b})f(\\mathbf{b})}{\\int f(\\mathbf{Y}|\\mathbf{b})f(\\mathbf{b}) d\\mathbf{b}}\n\\]\\(f(\\mathbf{Y}|\\mathbf{b})\\) “likelihood”\\(f(\\mathbf{b})\\) priorthe denominator “normalizing constant”\\(f(\\mathbf{b}|\\mathbf{Y})\\) posterior distributionIn case\\[\n\\mathbf{Y} | \\mathbf{b} \\sim N(\\mathbf{X\\beta+Zb,\\Sigma}) \\\\\n\\mathbf{b} \\sim N(\\mathbf{0,B})\n\\]posterior distribution form\\[\n\\mathbf{b}|\\mathbf{Y} \\sim N(\\mathbf{BZ'V^{-1}(Y-X\\beta),(Z'\\Sigma^{-1}Z + B^{-1})^{-1}})\n\\]Hence, best predictor (based squared error loss)\\[\nE(\\mathbf{b}|\\mathbf{Y}) = \\mathbf{BZ'V^{-1}(Y-X\\beta)}\n\\]","code":""},{"path":"linear-mixed-models.html","id":"estimating-mathbfv","chapter":"8 Linear Mixed Models","heading":"8.2.1 Estimating \\(\\mathbf{V}\\)","text":"\\(\\tilde{\\mathbf{V}}\\) (estimate \\(\\mathbf{V}\\)), can estimate:\\[\n\\hat{\\beta} = \\mathbf{(X'\\tilde{V}^{-1}X)^{-1}X'\\tilde{V}^{-1}Y} \\\\\n\\hat{\\mathbf{b}} = \\mathbf{BZ'\\tilde{V}^{-1}(Y-X\\hat{\\beta})}\n\\]\\({\\mathbf{b}}\\) EBLUP (estimated BLUP) empirical Bayes estimateNote:\\(\\hat{var}(\\hat{\\beta})\\) consistent estimator \\(var(\\hat{\\beta})\\) \\(\\tilde{\\mathbf{V}}\\) consistent estimator \\(\\mathbf{V}\\)However, \\(\\hat{var}(\\hat{\\beta})\\) biased since variability arises estimating \\(\\mathbf{V}\\) accounted estimate.Hence, \\(\\hat{var}(\\hat{\\beta})\\) underestimates true variabilityWays estimate \\(\\mathbf{V}\\)Maximum Likelihood Estimation (MLE)Restricted Maximum Likelihood (REML)Estimated Generalized Least SquaresBayesian Hierarchical Models (BHM)","code":""},{"path":"linear-mixed-models.html","id":"maximum-likelihood-estimation-mle","chapter":"8 Linear Mixed Models","heading":"8.2.1.1 Maximum Likelihood Estimation (MLE)","text":"Grouping unknown parameters \\(\\Sigma\\) \\(B\\) parameter vector \\(\\theta\\). MLE, \\(\\hat{\\theta}\\) \\(\\hat{\\beta}\\) maximize likelihood \\(\\mathbf{y} \\sim N(\\mathbf{X\\beta, V(\\theta))}\\). Synonymously, \\(-2\\log L(\\mathbf{y;\\theta,\\beta})\\):\\[\n-2l(\\mathbf{\\beta,\\theta,y}) = \\log |\\mathbf{V(\\theta)}| + \\mathbf{(y-X\\beta)'V(\\theta)^{-1}(y-X\\beta)} + N \\log(2\\pi)\n\\]Step 1: Replace \\(\\beta\\) maximum likelihood (\\(\\theta\\) known \\(\\hat{\\beta}= (\\mathbf{X'V(\\theta)^{-1}X)^{-1}X'V(\\theta)^{-1}y}\\)Step 2: Minimize equation respect \\(\\theta\\) get estimator \\(\\hat{\\theta}_{MLE}\\)Step 3: Substitute \\(\\hat{\\theta}_{MLE}\\) back get \\(\\hat{\\beta}_{MLE} = (\\mathbf{X'V(\\theta_{MLE})^{-1}X)^{-1}X'V(\\theta_{MLE})^{-1}y}\\)Step 4: Get \\(\\hat{\\mathbf{b}}_{MLE} = \\mathbf{B(\\hat{\\theta}_{MLE})Z'V(\\hat{\\theta}_{MLE})^{-1}(y-X\\hat{\\beta}_{MLE})}\\)Note:\\(\\hat{\\theta}\\) typically negatively biased due unaccounted fixed effects estimated, try account .","code":""},{"path":"linear-mixed-models.html","id":"restricted-maximum-likelihood-reml","chapter":"8 Linear Mixed Models","heading":"8.2.1.2 Restricted Maximum Likelihood (REML)","text":"REML accounts number estimated mean parameters adjusting objective function. Specifically, likelihood linear combination elements \\(\\mathbf{y}\\) accounted .\\(\\mathbf{K'y}\\), \\(\\mathbf{K}\\) \\(N \\times (N - p)\\) full-rank contrast matrix, columns orthogonal \\(\\mathbf{X}\\) matrix (\\(\\mathbf{K'X} = 0\\)). ,\\[\n\\mathbf{K'y} \\sim N(0,\\mathbf{K'V(\\theta)K})\n\\]\\(\\beta\\) longer distributionWe can proceed maximize likelihood contrasts get \\(\\hat{\\theta}_{REML}\\), depend choice \\(\\mathbf{K}\\). \\(\\hat{\\beta}\\) based \\(\\hat{\\theta}\\)Comparison REML MLEBoth methods based upon likelihood principle, desired properties estimates:\nconsistency\nasymptotic normality\nefficiency\nmethods based upon likelihood principle, desired properties estimates:consistencyconsistencyasymptotic normalityasymptotic normalityefficiencyefficiencyML estimation provides estimates fixed effects, REML can’tML estimation provides estimates fixed effects, REML can’tIn balanced models, REML identical ANOVAIn balanced models, REML identical ANOVAREML accounts df fixed effects int eh model, important \\(\\mathbf{X}\\) large relative sample sizeREML accounts df fixed effects int eh model, important \\(\\mathbf{X}\\) large relative sample sizeChanging \\(\\mathbf{\\beta}\\) effect REML estimates \\(\\theta\\)Changing \\(\\mathbf{\\beta}\\) effect REML estimates \\(\\theta\\)REML less sensitive outliers MLEREML less sensitive outliers MLEMLE better REML regarding model comparisons (e.g., AIC BIC)MLE better REML regarding model comparisons (e.g., AIC BIC)","code":""},{"path":"linear-mixed-models.html","id":"estimated-generalized-least-squares","chapter":"8 Linear Mixed Models","heading":"8.2.1.3 Estimated Generalized Least Squares","text":"MLE REML rely upon Gaussian assumption. overcome issue, EGLS uses first second moments.\\[\n\\mathbf{Y}_i = \\mathbf{X}_i \\beta + \\mathbf{Z}_i \\mathbf{b}_i + \\epsilon_i\n\\]\\(\\epsilon_i \\sim (\\mathbf{0,\\Sigma_i})\\)\\(\\mathbf{b}_i \\sim (\\mathbf{0,D})\\)\\(cov(\\epsilon_i, \\mathbf{b}_i) = 0\\)EGLS estimator \\[\n\\begin{aligned}\n\\hat{\\beta}_{GLS} &= \\{\\sum_{=1}^n \\mathbf{X'_iV_i(\\theta)^{-1}X_i}  \\}^{-1} \\sum_{=1}^n \\mathbf{X'_iV_i(\\theta)^{-1}Y_i} \\\\\n&=\\{\\mathbf{X'V(\\theta)^{-1}X} \\}^{-1} \\mathbf{X'V(\\theta)^{-1}Y}\n\\end{aligned}\n\\]depends first two moments\\(E(\\mathbf{Y}_i) = \\mathbf{X}_i \\beta\\)\\(var(\\mathbf{Y}_i)= \\mathbf{V}_i\\)EGLS use \\(\\hat{\\mathbf{V}}\\) \\(\\mathbf{V(\\theta)}\\)\\[\n\\hat{\\beta}_{EGLS} = \\{ \\mathbf{X'\\hat{V}^{-1}X} \\}^{-1} \\mathbf{X'\\hat{V}^{-1}Y}\n\\]Hence, fixed effects estimators MLE, REML, EGLS form, except estimate \\(\\mathbf{V}\\)case non-iterative approach, EGLS can appealing \\(\\mathbf{V}\\) can estimated without much computational burden.","code":""},{"path":"linear-mixed-models.html","id":"bayesian-hierarchical-models-bhm","chapter":"8 Linear Mixed Models","heading":"8.2.1.4 Bayesian Hierarchical Models (BHM)","text":"Joint distribution cane decomposed hierarchically terms product conditional distributions marginal distribution\\[\nf(,B,C) = f(|B,C) f(B|C)f(C)\n\\]Applying estimate \\(\\mathbf{V}\\)\\[\n\\begin{aligned}\nf(\\mathbf{Y, \\beta, b, \\theta}) &= f(\\mathbf{Y|\\beta,b, \\theta})f(\\mathbf{b|\\theta,\\beta})f(\\mathbf{\\beta|\\theta})f(\\mathbf{\\theta}) & \\text{based probability decomposition} \\\\\n&= f(\\mathbf{Y|\\beta,b, \\theta})f(\\mathbf{b|\\theta})f(\\mathbf{\\beta})f(\\mathbf{\\theta}) & \\text{based simplifying modeling assumptions}\n\\end{aligned}\n\\]elaborate second equality, assume conditional independence (e.g., given \\(\\theta\\), additional info \\(\\mathbf{b}\\) given knowing \\(\\beta\\)), can simply first equalityUsing Bayes’ rule\\[\nf(\\mathbf{\\beta, b, \\theta|Y}) \\propto f(\\mathbf{Y|\\beta,b, \\theta})f(\\mathbf{b|\\theta})f(\\mathbf{\\beta})f(\\mathbf{\\theta})\n\\]\\[\n\\mathbf{Y| \\beta, b, \\theta \\sim N(X\\beta+ Zb, \\Sigma(\\theta))} \\\\\n\\mathbf{b | \\theta \\sim N(0, B(\\theta))}\n\\]also prior distributions \\(f(\\beta), f(\\theta)\\)normalizing constant, can obtain posterior distribution. Typically, can’t get analytical solution right away. Hence, can use Markov Chain Monte Carlo (MCMC) obtain samples posterior distribution.Bayesian Methods:account uncertainty parameters estimates accommodate propagation uncertainty modelcan adjust prior information (.e., priori) parametersCan extend beyond Gaussian distributionsbut hard implement algorithms might problem converging","code":""},{"path":"linear-mixed-models.html","id":"inference-3","chapter":"8 Linear Mixed Models","heading":"8.3 Inference","text":"","code":""},{"path":"linear-mixed-models.html","id":"parameters-beta","chapter":"8 Linear Mixed Models","heading":"8.3.1 Parameters \\(\\beta\\)","text":"","code":""},{"path":"linear-mixed-models.html","id":"wald-test-GLMM","chapter":"8 Linear Mixed Models","heading":"8.3.1.1 Wald test","text":"\\[\n\\mathbf{\\hat{\\beta}(\\theta) = \\{X'V^{-1}(\\theta) X\\}^{-1}X'V^{-1}(\\theta) Y} \\\\\nvar(\\hat{\\beta}(\\theta)) = \\mathbf{\\{X'V^{-1}(\\theta) X\\}^{-1}}\n\\]can use \\(\\hat{\\theta}\\) place \\(\\theta\\) approximate Wald test\\[\nH_0: \\mathbf{\\beta =d} \n\\]\\[\nW = \\mathbf{(\\hat{\\beta} - d)'[(X'\\hat{V}^{-1}X)^{-1}']^{-1}(\\hat{\\beta} - d)}\n\\]\\(W \\sim \\chi^2_{rank()}\\) \\(H_0\\) true. However, take account variability using \\(\\hat{\\theta}\\) place \\(\\theta\\), hence standard errors underestimated","code":""},{"path":"linear-mixed-models.html","id":"f-test-1","chapter":"8 Linear Mixed Models","heading":"8.3.1.2 F-test","text":"Alternatively, can use modified F-test, suppose \\(var(\\mathbf{Y}) = \\sigma^2 \\mathbf{V}(\\theta)\\), \\[\nF^* = \\frac{\\mathbf{(\\hat{\\beta} - d)'[(X'\\hat{V}^{-1}X)^{-1}']^{-1}(\\hat{\\beta} - d)}}{\\hat{\\sigma}^2 \\text{rank}()}\n\\]\\(F^* \\sim f_{rank(), den(df)}\\) null hypothesis. den(df) needs approximated data either:Satterthwaite methodKenward-Roger approximationUnder balanced cases, Wald F tests similar. small sample sizes, can differ p-values. can reduced t-test single \\(\\beta\\)","code":""},{"path":"linear-mixed-models.html","id":"likelihood-ratio-test","chapter":"8 Linear Mixed Models","heading":"8.3.1.3 Likelihood Ratio Test","text":"\\[\nH_0: \\beta \\\\Theta_{\\beta,0}\n\\]\\(\\Theta_{\\beta, 0}\\) subspace parameter space, \\(\\Theta_{\\beta}\\) fixed effects \\(\\beta\\) . \\[\n-2\\log \\lambda_N = -2\\log\\{\\frac{\\hat{L}_{ML,0}}{\\hat{L}_{ML}}\\}\n\\]\\(\\hat{L}_{ML,0}\\) , \\(\\hat{L}_{ML}\\) maximized likelihood obtained maximizing \\(\\Theta_{\\beta,0}\\) \\(\\Theta_{\\beta}\\)\\(-2 \\log \\lambda_N \\dot{\\sim} \\chi^2_{df}\\) df difference dimension (.e., number parameters) \\(\\Theta_{\\beta,0}\\) \\(\\Theta_{\\beta}\\)method applicable REML. REML can still used test covariance parameters nested models.","code":""},{"path":"linear-mixed-models.html","id":"variance-components","chapter":"8 Linear Mixed Models","heading":"8.3.2 Variance Components","text":"ML REML estimator, \\(\\hat{\\theta} \\sim N(\\theta, (\\theta))\\) large samplesFor ML REML estimator, \\(\\hat{\\theta} \\sim N(\\theta, (\\theta))\\) large samplesWald test variance components analogous fixed effects case (see 8.3.1.1 )\nHowever, normal approximation depends largely true value \\(\\theta\\). fail true value \\(\\theta\\) close boundary parameter space \\(\\Theta_{\\theta}\\) (.e., \\(\\sigma^2 \\approx 0\\))\nTypically works better covariance parameter, vairance prarmetesr.\nWald test variance components analogous fixed effects case (see 8.3.1.1 )However, normal approximation depends largely true value \\(\\theta\\). fail true value \\(\\theta\\) close boundary parameter space \\(\\Theta_{\\theta}\\) (.e., \\(\\sigma^2 \\approx 0\\))However, normal approximation depends largely true value \\(\\theta\\). fail true value \\(\\theta\\) close boundary parameter space \\(\\Theta_{\\theta}\\) (.e., \\(\\sigma^2 \\approx 0\\))Typically works better covariance parameter, vairance prarmetesr.Typically works better covariance parameter, vairance prarmetesr.likelihood ratio tests can also used ML REML estimates. However, problem parametersThe likelihood ratio tests can also used ML REML estimates. However, problem parameters","code":""},{"path":"linear-mixed-models.html","id":"information-criteria","chapter":"8 Linear Mixed Models","heading":"8.4 Information Criteria","text":"account likelihood number parameters assess model comparison.","code":""},{"path":"linear-mixed-models.html","id":"akaikes-information-criteria-aic","chapter":"8 Linear Mixed Models","heading":"8.4.1 Akaike’s Information Criteria (AIC)","text":"Derived estimator expected Kullback discrepancy true model fitted candidate model\\[\nAIC = -2l(\\hat{\\theta}, \\hat{\\beta}) + 2q\n\\]\\(l(\\hat{\\theta}, \\hat{\\beta})\\) log-likelihoodq = effective number parameters; total fixed associated random effects (variance/covariance; estimated boundary constraint)Note:comparing models differ random effects, method advised due inability get correct number effective parameters).prefer smaller AIC values.program uses \\(l-q\\) prefer larger AIC values (rarely).can used mixed model section, (e.g., selection covariance structure), sample size must large adequate comparison based criterionCan large negative bias (e.g., sample size small number parameters large) due penalty term can’t approximate bias adjustment adequately","code":""},{"path":"linear-mixed-models.html","id":"corrected-aic-aicc","chapter":"8 Linear Mixed Models","heading":"8.4.2 Corrected AIC (AICC)","text":"developed (HURVICH TSAI 1989)correct small-sample adjustmentdepends candidate model classOnly fixed covariance structure, AICC justified, general covariance structure","code":""},{"path":"linear-mixed-models.html","id":"bayesian-information-criteria-bic","chapter":"8 Linear Mixed Models","heading":"8.4.3 Bayesian Information Criteria (BIC)","text":"\\[\nBIC = -2l(\\hat{\\theta}, \\hat{\\beta}) + q \\log n\n\\]n = number observations.prefer smaller BIC valueBIC AIC used REML MLE mean structure. Otherwise, general, prefer MLEWith example presented beginning Linear Mixed Models,\\[\nY_{ik}= \n\\begin{cases}\n\\beta_0 + b_{1i} + (\\beta_1 + \\ b_{2i})t_{ij} + \\epsilon_{ij} & L \\\\\n\\beta_0 + b_{1i} + (\\beta_2 + \\ b_{2i})t_{ij} + \\epsilon_{ij} & H\\\\\n\\beta_0 + b_{1i} + (\\beta_3 + \\ b_{2i})t_{ij} + \\epsilon_{ij} & C\n\\end{cases}\n\\]\\(= 1,..,N\\)\\(j = 1,..,n_i\\) (measures time \\(t_{ij}\\))Note:subject-specific intercepts,\\[\n\\mathbf{Y}_i |b_i \\sim N(\\mathbf{X}_i \\beta + 1 b_i, \\sigma^2 \\mathbf{}) \\\\\nb_i \\sim N(0,d_{11})\n\\], want estimate \\(\\beta, \\sigma^2, d_{11}\\) predict \\(b_i\\)","code":""},{"path":"linear-mixed-models.html","id":"split-plot-designs","chapter":"8 Linear Mixed Models","heading":"8.5 Split-Plot Designs","text":"Typically used case two factors one needs much larger units .Example:: 3 levels (large units)B: 2 levels (small units)B levels randomized 4 blocks.differs Randomized Block Designs. block, one 6 (3x2) treatment combinations. Randomized Block Designs assign block randomly, split-plot randomize step.Moreover, needs applied large units, factor applied block B can applied multiple times.Hence, modelIf factor interest\\[\nY_{ij} = \\mu + \\rho_i + \\alpha_j + e_{ij}\n\\]wherei = replication (block subject)j = level Factor \\(\\mu\\) = overall mean\\(\\rho_i\\) = variation due -th block\\(e_{ij} \\sim N(0, \\sigma^2_e)\\) = whole plot errorIf B factor interest\\[\nY_{ijk} = \\mu + \\phi_{ij} + \\beta_k + \\epsilon_{ijk}\n\\]\\(\\phi_{ij}\\) = variation due ij-th main plot\\(\\beta_k\\) = Factor B effect\\(\\epsilon_{ijk} \\sim N(0, \\sigma^2_\\epsilon)\\) = subplot error\\(\\phi_{ij} = \\rho_i + \\alpha_j + e_{ij}\\)Together, split-plot model\\[\nY_{ijk} = \\mu + \\rho_i + \\alpha_j + e_{ij} + \\beta_k + (\\alpha \\beta)_{jk} + \\epsilon_{ijk}\n\\]wherei = replicate (blocks subjects)j = level factor Ak = level factor B\\(\\mu\\) = overall mean\\(\\rho_i\\) = effect block\\(\\alpha_j\\) = main effect factor (fixed)\\(e_{ij} = (\\rho \\alpha)_{ij}\\) = block factor interaction (whole plot error, random)\\(\\beta_k\\) = main effect factor B (fixed)\\((\\alpha \\beta)_{jk}\\) = interaction factors B (fixed)\\(\\epsilon_{ijk}\\) = subplot error (random)can approach sub-plot analysis based onthe ANOVA perspective\nWhole plot comparisons\nCompare factor whole plot error (.e., \\(\\alpha_j\\) \\(e_{ij}\\))\nCompare block whole plot error (.e., \\(\\rho_i\\) \\(e_{ij}\\))\n\nSub-plot comparisons:\nCompare factor B subplot error (\\(\\beta\\) \\(\\epsilon_{ijk}\\))\nCompare AB interaction subplot error (\\((\\alpha \\beta)_{jk}\\) \\(\\epsilon_{ijk}\\))\n\nANOVA perspectiveWhole plot comparisons\nCompare factor whole plot error (.e., \\(\\alpha_j\\) \\(e_{ij}\\))\nCompare block whole plot error (.e., \\(\\rho_i\\) \\(e_{ij}\\))\nWhole plot comparisonsCompare factor whole plot error (.e., \\(\\alpha_j\\) \\(e_{ij}\\))Compare factor whole plot error (.e., \\(\\alpha_j\\) \\(e_{ij}\\))Compare block whole plot error (.e., \\(\\rho_i\\) \\(e_{ij}\\))Compare block whole plot error (.e., \\(\\rho_i\\) \\(e_{ij}\\))Sub-plot comparisons:\nCompare factor B subplot error (\\(\\beta\\) \\(\\epsilon_{ijk}\\))\nCompare AB interaction subplot error (\\((\\alpha \\beta)_{jk}\\) \\(\\epsilon_{ijk}\\))\nSub-plot comparisons:Compare factor B subplot error (\\(\\beta\\) \\(\\epsilon_{ijk}\\))Compare factor B subplot error (\\(\\beta\\) \\(\\epsilon_{ijk}\\))Compare AB interaction subplot error (\\((\\alpha \\beta)_{jk}\\) \\(\\epsilon_{ijk}\\))Compare AB interaction subplot error (\\((\\alpha \\beta)_{jk}\\) \\(\\epsilon_{ijk}\\))mixed model perspectivethe mixed model perspective\\[\n\\mathbf{Y = X \\beta + Zb + \\epsilon}\n\\]","code":""},{"path":"linear-mixed-models.html","id":"application-5","chapter":"8 Linear Mixed Models","heading":"8.5.1 Application","text":"","code":""},{"path":"linear-mixed-models.html","id":"example-1","chapter":"8 Linear Mixed Models","heading":"8.5.1.1 Example 1","text":"\\[\ny_{ijk} = \\mu + i_i + v_j + (iv)_{ij} + f_k + \\epsilon_{ijk}\n\\]\\(y_{ijk}\\) = observed yield\\(\\mu\\) = overall average yield\\(i_i\\) = irrigation effect\\(v_j\\) = variety effect\\((iv)_{ij}\\) = irrigation variety interaction\\(f_k\\) = random field (block) effect\\(\\epsilon_{ijk}\\) = residualbecause variety-field combination observed , can’t random interaction effects variety fieldSince p-value interaction term insignificant, consider fitting without .Since p-value Chi-square test insignificant, can’t reject additive model already sufficient. Looking AIC BIC, can also see prefer additive modelRandom Effect ExaminationexactRLRT test\\(H_0\\): Var(random effect) (.e., \\(\\sigma^2\\))= 0\\(H_a\\): Var(random effect) (.e., \\(\\sigma^2\\)) > 0Since p-value significant, reject \\(H_0\\)","code":"\nlibrary(ggplot2)\ndata(irrigation, package = \"faraway\")\nsummary(irrigation)##      field   irrigation variety     yield      \n##  f1     :2   i1:4       v1:8    Min.   :34.80  \n##  f2     :2   i2:4       v2:8    1st Qu.:37.60  \n##  f3     :2   i3:4               Median :40.15  \n##  f4     :2   i4:4               Mean   :40.23  \n##  f5     :2                      3rd Qu.:42.73  \n##  f6     :2                      Max.   :47.60  \n##  (Other):4\nhead(irrigation, 4)##   field irrigation variety yield\n## 1    f1         i1      v1  35.4\n## 2    f1         i1      v2  37.9\n## 3    f2         i2      v1  36.7\n## 4    f2         i2      v2  38.2\nggplot(irrigation,\n       aes(\n         x = field,\n         y = yield,\n         shape = irrigation,\n         color = variety\n       )) +\n  geom_point(size = 3)\nsp_model <- lmerTest::lmer(yield ~ irrigation * variety + (1 | field), irrigation)\nsummary(sp_model)## Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n## lmerModLmerTest]\n## Formula: yield ~ irrigation * variety + (1 | field)\n##    Data: irrigation\n## \n## REML criterion at convergence: 45.4\n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -0.7448 -0.5509  0.0000  0.5509  0.7448 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev.\n##  field    (Intercept) 16.200   4.025   \n##  Residual              2.107   1.452   \n## Number of obs: 16, groups:  field, 8\n## \n## Fixed effects:\n##                        Estimate Std. Error     df t value Pr(>|t|)    \n## (Intercept)              38.500      3.026  4.487  12.725 0.000109 ***\n## irrigationi2              1.200      4.279  4.487   0.280 0.791591    \n## irrigationi3              0.700      4.279  4.487   0.164 0.877156    \n## irrigationi4              3.500      4.279  4.487   0.818 0.454584    \n## varietyv2                 0.600      1.452  4.000   0.413 0.700582    \n## irrigationi2:varietyv2   -0.400      2.053  4.000  -0.195 0.855020    \n## irrigationi3:varietyv2   -0.200      2.053  4.000  -0.097 0.927082    \n## irrigationi4:varietyv2    1.200      2.053  4.000   0.584 0.590265    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##             (Intr) irrgt2 irrgt3 irrgt4 vrtyv2 irr2:2 irr3:2\n## irrigation2 -0.707                                          \n## irrigation3 -0.707  0.500                                   \n## irrigation4 -0.707  0.500  0.500                            \n## varietyv2   -0.240  0.170  0.170  0.170                     \n## irrgtn2:vr2  0.170 -0.240 -0.120 -0.120 -0.707              \n## irrgtn3:vr2  0.170 -0.120 -0.240 -0.120 -0.707  0.500       \n## irrgtn4:vr2  0.170 -0.120 -0.120 -0.240 -0.707  0.500  0.500\nanova(sp_model,ddf = c(\"Kenward-Roger\"))## Type III Analysis of Variance Table with Kenward-Roger's method\n##                    Sum Sq Mean Sq NumDF DenDF F value Pr(>F)\n## irrigation         2.4545 0.81818     3     4  0.3882 0.7685\n## variety            2.2500 2.25000     1     4  1.0676 0.3599\n## irrigation:variety 1.5500 0.51667     3     4  0.2452 0.8612\nlibrary(lme4)## Warning: package 'lme4' was built under R version 4.0.5## Loading required package: Matrix## Warning: package 'Matrix' was built under R version 4.0.5\nsp_model_additive <- lmer(yield ~ irrigation + variety + (1 | field), irrigation)\nanova(sp_model_additive,sp_model,ddf = \"Kenward-Roger\")## refitting model(s) with ML (instead of REML)## Data: irrigation\n## Models:\n## sp_model_additive: yield ~ irrigation + variety + (1 | field)\n## sp_model: yield ~ irrigation * variety + (1 | field)\n##                   npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)\n## sp_model_additive    7 83.959 89.368 -34.980   69.959                     \n## sp_model            10 88.609 96.335 -34.305   68.609 1.3503  3     0.7172\nsp_model <- lme4::lmer(yield ~ irrigation * variety + (1 | field), irrigation)\nlibrary(RLRsim)\nexactRLRT(sp_model)## \n##  simulated finite sample distribution of RLRT.\n##  \n##  (p-value based on 10000 simulated values)\n## \n## data:  \n## RLRT = 6.1118, p-value = 0.0088"},{"path":"linear-mixed-models.html","id":"repeated-measures-in-mixed-models","chapter":"8 Linear Mixed Models","heading":"8.6 Repeated Measures in Mixed Models","text":"\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\delta_{(k)}+ \\epsilon_{ijk}\n\\]wherei-th group (fixed)j-th (repeated measure) time effect (fixed)k-th subject\\(\\delta_{(k)} \\sim N(0,\\sigma^2_\\delta)\\) (k-th subject -th group) \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) (independent error) random effects (\\(= 1,..,n_A, j = 1,..,n_B, k = 1,...,n_i\\))hence, variance-covariance matrix repeated observations k-th subject -th group, \\(\\mathbf{Y}_{ik} = (Y_{i1k},..,Y_{in_Bk})'\\), \\[\n\\begin{aligned}\n\\mathbf{\\Sigma}_{subject} &=\n\\left(\n\\begin{array}\n{cccc}\n\\sigma^2_\\delta + \\sigma^2 & \\sigma^2_\\delta & ... & \\sigma^2_\\delta \\\\\n\\sigma^2_\\delta & \\sigma^2_\\delta +\\sigma^2 & ... & \\sigma^2_\\delta \\\\\n. & . & . & . \\\\\n\\sigma^2_\\delta & \\sigma^2_\\delta & ... & \\sigma^2_\\delta + \\sigma^2 \\\\\n\\end{array}\n\\right) \\\\\n&= (\\sigma^2_\\delta + \\sigma^2)\n\\left(\n\\begin{array}\n{cccc}\n1 & \\rho & ... & \\rho \\\\\n\\rho & 1 & ... & \\rho \\\\\n. & . & . & . \\\\\n\\rho & \\rho & ... & 1 \\\\\n\\end{array}\n\\right) \n& \\text{product scalar correlation matrix}\n\\end{aligned}\n\\]\\(\\rho = \\frac{\\sigma^2_\\delta}{\\sigma^2_\\delta + \\sigma^2}\\), compound symmetry structure discussed Random-Intercepts ModelBut repeated measurements subject time, AR(1) structure might appropriateMixed model repeated measure\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}\n\\]\\(\\epsilon_{ijk}\\) combines random error whole subplots.general,\\[\n\\mathbf{Y = X \\beta + \\epsilon}\n\\]\\(\\epsilon \\sim N(0, \\sigma^2 \\mathbf{\\Sigma})\\) \\(\\mathbf{\\Sigma}\\) block diagonal random error covariance subjectThe variance covariance matrix AR(1) structure \\[\n\\mathbf{\\Sigma}_{subject} =\n\\sigma^2\n\\left(\n\\begin{array}\n{ccccc}\n1  & \\rho & \\rho^2 & ... & \\rho^{n_B-1} \\\\\n\\rho & 1 & \\rho & ... & \\rho^{n_B-2} \\\\\n. & . & . & . & . \\\\\n\\rho^{n_B-1} & \\rho^{n_B-2} & \\rho^{n_B-3} & ... & 1 \\\\\n\\end{array}\n\\right)\n\\]Hence, mixed model repeated measure can written \\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}\n\\]\\(\\epsilon_{ijk}\\) = random error whole subplotsGenerally,\\[\n\\mathbf{Y = X \\beta + \\epsilon} \n\\]\\(\\epsilon \\sim N(0, \\mathbf{\\sigma^2 \\Sigma})\\) \\(\\Sigma\\) = block diagonal random error covariance subject.","code":""},{"path":"linear-mixed-models.html","id":"unbalanced-or-unequally-spaced-data","chapter":"8 Linear Mixed Models","heading":"8.7 Unbalanced or Unequally Spaced Data","text":"Consider model\\[\nY_{ikt} = \\beta_0 + \\beta_{0i} + \\beta_{1}t + \\beta_{1i}t + \\beta_{2} t^2 + \\beta_{2i} t^2 + \\epsilon_{ikt}\n\\]wherei = 1,2 (groups)\\(k = 1,…, n_i\\) ( individuals)\\(t = (t_1,t_2,t_3,t_4)\\) (times)\\(\\beta_{2i}\\) = common quadratic term\\(\\beta_{1i}\\) = common linear time trends\\(\\beta_{0i}\\) = common interceptsThen, assume variance-covariance matrix repeated measurements collected particular subject time form\\[\n\\mathbf{\\Sigma}_{ik} = \\sigma^2\n\\left(\n\\begin{array}\n{cccc}\n1 & \\rho^{t_2-t_1} & \\rho^{t_3-t_1} & \\rho^{t_4-t_1} \\\\\n\\rho^{t_2-t_1} & 1 & \\rho^{t_3-t_2} & \\rho^{t_4-t_2} \\\\\n\\rho^{t_3-t_1} & \\rho^{t_3-t_2} & 1 & \\rho^{t_4-t_3} \\\\\n\\rho^{t_4-t_1} & \\rho^{t_4-t_2} & \\rho^{t_4-t_3} & 1\n\\end{array}\n\\right)\n\\]called “power” covariance modelWe can consider \\(\\beta_{2i} , \\beta_{1i}, \\beta_{0i}\\) accordingly see whether terms needed final model","code":""},{"path":"linear-mixed-models.html","id":"application-6","chapter":"8 Linear Mixed Models","heading":"8.8 Application","text":"R Packages mixed modelsnlme\nnested structure\nflexible complex design\nuser-friendly\nnlmehas nested structurehas nested structureflexible complex designflexible complex designnot user-friendlynot user-friendlylme4\ncomputationally efficient\nuser-friendly\ncan handle nonnormal response\ndetailed application, check Fitting Linear Mixed-Effects Models Using lme4\nlme4computationally efficientcomputationally efficientuser-friendlyuser-friendlycan handle nonnormal responsecan handle nonnormal responsefor detailed application, check Fitting Linear Mixed-Effects Models Using lme4for detailed application, check Fitting Linear Mixed-Effects Models Using lme4Others\nBayesian setting: MCMCglmm, brms\ngenetics: ASReml\nOthersBayesian setting: MCMCglmm, brmsBayesian setting: MCMCglmm, brmsFor genetics: ASRemlFor genetics: ASReml","code":""},{"path":"linear-mixed-models.html","id":"example-1-pulps","chapter":"8 Linear Mixed Models","heading":"8.8.1 Example 1 (Pulps)","text":"Model:\\[\ny_{ij} = \\mu + \\alpha_i + \\epsilon_{ij}\n\\]\\(= 1,..,\\) groups random effect \\(\\alpha_i\\)\\(j = 1,...,n\\) individuals group\\(\\alpha_i \\sim N(0, \\sigma^2_\\alpha)\\) random effects\\(\\epsilon_{ij} \\sim N(0, \\sigma^2_\\epsilon)\\) random effectsImply compound symmetry model intraclass correlation coefficient : \\(\\rho = \\frac{\\sigma^2_\\alpha}{\\sigma^2_\\alpha + \\sigma^2_\\epsilon}\\)factor \\(\\) explain much variation, low correlation within levels: \\(\\sigma^2_\\alpha \\0\\) \\(\\rho \\0\\)factor \\(\\) explain much variation, high correlation within levels \\(\\sigma^2_\\alpha \\\\infty\\) hence, \\(\\rho \\1\\)lmer applicationTo Satterthwaite approximation denominator df, use lmerTestIn example, can see confidence interval computed confint lmer package close confint lmerTest model. MCMglmm applicationunder Bayesian frameworkthis method offers confidence interval slightly positive lmer lmerTest","code":"\ndata(pulp, package = \"faraway\")\nplot(\n  y = pulp$bright,\n  x = pulp$operator,\n  xlab = \"Operator\",\n  ylab = \"Brightness\"\n)\npulp %>% dplyr::group_by(operator) %>% dplyr::summarise(average = mean(bright))## # A tibble: 4 x 2\n##   operator average\n##   <fct>      <dbl>\n## 1 a           60.2\n## 2 b           60.1\n## 3 c           60.6\n## 4 d           60.7\nlibrary(lme4)\nmixed_model <- lmer(formula = bright ~ 1 + (1 | operator), # pipe (i..e, | ) denotes random-effect terms\n                    data = pulp)\nsummary(mixed_model)## Linear mixed model fit by REML ['lmerMod']\n## Formula: bright ~ 1 + (1 | operator)\n##    Data: pulp\n## \n## REML criterion at convergence: 18.6\n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -1.4666 -0.7595 -0.1244  0.6281  1.6012 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev.\n##  operator (Intercept) 0.06808  0.2609  \n##  Residual             0.10625  0.3260  \n## Number of obs: 20, groups:  operator, 4\n## \n## Fixed effects:\n##             Estimate Std. Error t value\n## (Intercept)  60.4000     0.1494   404.2\ncoef(mixed_model)## $operator\n##   (Intercept)\n## a    60.27806\n## b    60.14088\n## c    60.56767\n## d    60.61340\n## \n## attr(,\"class\")\n## [1] \"coef.mer\"\nfixef(mixed_model) # fixed effects## (Intercept) \n##        60.4\nconfint(mixed_model) # confidence interval## Computing profile confidence intervals ...##                 2.5 %     97.5 %\n## .sig01       0.000000  0.6178987\n## .sigma       0.238912  0.4821845\n## (Intercept) 60.071299 60.7287012\nranef(mixed_model) # random effects## $operator\n##   (Intercept)\n## a  -0.1219403\n## b  -0.2591231\n## c   0.1676679\n## d   0.2133955\n## \n## with conditional variances for \"operator\"\nVarCorr(mixed_model) # random effects standard deviation##  Groups   Name        Std.Dev.\n##  operator (Intercept) 0.26093 \n##  Residual             0.32596\nre_dat = as.data.frame(VarCorr(mixed_model))\nrho = re_dat[1,'vcov']/(re_dat[1,'vcov'] + re_dat[2,'vcov']) # rho based on the above formula\nrho## [1] 0.3905354\nlibrary(lmerTest)## \n## Attaching package: 'lmerTest'## The following object is masked from 'package:lme4':\n## \n##     lmer## The following object is masked from 'package:stats':\n## \n##     step\nsummary(lmerTest::lmer(bright ~ 1 + (1 | operator), pulp))$coefficients##             Estimate Std. Error df  t value     Pr(>|t|)\n## (Intercept)     60.4  0.1494434  3 404.1664 3.340265e-08\nconfint(mixed_model)[3,]## Computing profile confidence intervals ...##   2.5 %  97.5 % \n## 60.0713 60.7287\nlibrary(MCMCglmm)## Warning: package 'MCMCglmm' was built under R version 4.0.5## Loading required package: coda## Loading required package: ape## Warning: package 'ape' was built under R version 4.0.5\nmixed_model_bayes <- MCMCglmm(bright~1,random=~operator, data=pulp, verbose=FALSE)\nsummary(mixed_model_bayes)$solutions##             post.mean l-95% CI u-95% CI eff.samp pMCMC\n## (Intercept)  60.39357 60.07742 60.66047 1184.638 0.001"},{"path":"linear-mixed-models.html","id":"prediction","chapter":"8 Linear Mixed Models","heading":"8.8.1.1 Prediction","text":"use bootMer() get bootstrap-based confidence intervals predictions.Another example using GLMM context blockingPenicillin dataExamine treatment effectSince p-value greater 0.05, can’t reject null hypothesis treatment effect.Since p-value greater 0.05, consistent previous observation, conclude can’t reject null hypothesis treatment effect.","code":"\n# random effects prediction (BLUPs)\nranef(mixed_model)$operator##   (Intercept)\n## a  -0.1219403\n## b  -0.2591231\n## c   0.1676679\n## d   0.2133955\nfixef(mixed_model) + ranef(mixed_model)$operator #prediction for each categories##   (Intercept)\n## a    60.27806\n## b    60.14088\n## c    60.56767\n## d    60.61340\npredict(mixed_model, newdata=data.frame(operator=c('a','b','c','d'))) # equivalent to the above method##        1        2        3        4 \n## 60.27806 60.14088 60.56767 60.61340\ndata(penicillin, package = \"faraway\")\nsummary(penicillin)##  treat    blend       yield   \n##  A:5   Blend1:4   Min.   :77  \n##  B:5   Blend2:4   1st Qu.:81  \n##  C:5   Blend3:4   Median :87  \n##  D:5   Blend4:4   Mean   :86  \n##        Blend5:4   3rd Qu.:89  \n##                   Max.   :97\nlibrary(ggplot2)\nggplot(penicillin, aes(\n  y = yield,\n  x = treat,\n  shape = blend,\n  color = blend\n)) + # treatment = fixed effect, blend = random effects\n  geom_point(size = 3) +\n  xlab(\"Treatment\")\nlibrary(lmerTest) # for p-values\nmixed_model <- lmerTest::lmer(yield ~ treat + (1 | blend),\n                              data = penicillin)\nsummary(mixed_model)## Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n## lmerModLmerTest]\n## Formula: yield ~ treat + (1 | blend)\n##    Data: penicillin\n## \n## REML criterion at convergence: 103.8\n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -1.4152 -0.5017 -0.1644  0.6830  1.2836 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev.\n##  blend    (Intercept) 11.79    3.434   \n##  Residual             18.83    4.340   \n## Number of obs: 20, groups:  blend, 5\n## \n## Fixed effects:\n##             Estimate Std. Error     df t value Pr(>|t|)    \n## (Intercept)   84.000      2.475 11.075  33.941 1.51e-12 ***\n## treatB         1.000      2.745 12.000   0.364   0.7219    \n## treatC         5.000      2.745 12.000   1.822   0.0935 .  \n## treatD         2.000      2.745 12.000   0.729   0.4802    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##        (Intr) treatB treatC\n## treatB -0.555              \n## treatC -0.555  0.500       \n## treatD -0.555  0.500  0.500\n#The BLUPs for the each blend\nranef(mixed_model)$blend##        (Intercept)\n## Blend1   4.2878788\n## Blend2  -2.1439394\n## Blend3  -0.7146465\n## Blend4   1.4292929\n## Blend5  -2.8585859\nanova(mixed_model) # p-value based on lmerTest## Type III Analysis of Variance Table with Satterthwaite's method\n##       Sum Sq Mean Sq NumDF DenDF F value Pr(>F)\n## treat     70  23.333     3    12  1.2389 0.3387\nlibrary(pbkrtest)## Warning: package 'pbkrtest' was built under R version 4.0.5\nfull_model <- lmer(yield ~ treat + (1 | blend), penicillin, REML=FALSE) #REML is not appropriate for testing fixed effects, it should be ML\nnull_model <- lmer(yield ~ 1 + (1 | blend), penicillin, REML=FALSE)\nKRmodcomp(full_model, null_model) # use  Kenward-Roger approximation for df## large : yield ~ treat + (1 | blend)\n## small : yield ~ 1 + (1 | blend)\n##          stat     ndf     ddf F.scaling p.value\n## Ftest  1.2389  3.0000 12.0000         1  0.3387"},{"path":"linear-mixed-models.html","id":"example-2-rats","chapter":"8 Linear Mixed Models","heading":"8.8.2 Example 2 (Rats)","text":"interested whether treatment effect induces changes time.Since p-value significant, can confident concluding treatment effect","code":"\nrats <- read.csv(\n    \"images/rats.dat\",\n    header = F,\n    sep = ' ',\n    col.names = c('Treatment', 'rat', 'age', 'y')\n)\nrats$t <- log(1 + (rats$age - 45)/10) #log transformed age\nrat_model <- lmerTest::lmer(y~t:Treatment+(1|rat),data=rats) #treatment = fixed effect, rat = random effects\nsummary(rat_model)## Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n## lmerModLmerTest]\n## Formula: y ~ t:Treatment + (1 | rat)\n##    Data: rats\n## \n## REML criterion at convergence: 932.4\n## \n## Scaled residuals: \n##      Min       1Q   Median       3Q      Max \n## -2.25574 -0.65898 -0.01163  0.58356  2.88309 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev.\n##  rat      (Intercept) 3.565    1.888   \n##  Residual             1.445    1.202   \n## Number of obs: 252, groups:  rat, 50\n## \n## Fixed effects:\n##                Estimate Std. Error       df t value Pr(>|t|)    \n## (Intercept)     68.6074     0.3312  89.0275  207.13   <2e-16 ***\n## t:Treatmentcon   7.3138     0.2808 247.2762   26.05   <2e-16 ***\n## t:Treatmenthig   6.8711     0.2276 247.7097   30.19   <2e-16 ***\n## t:Treatmentlow   7.5069     0.2252 247.5196   33.34   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##             (Intr) t:Trtmntc t:Trtmnth\n## t:Tretmntcn -0.327                    \n## t:Tretmnthg -0.340  0.111             \n## t:Tretmntlw -0.351  0.115     0.119\nanova(rat_model)## Type III Analysis of Variance Table with Satterthwaite's method\n##             Sum Sq Mean Sq NumDF  DenDF F value    Pr(>F)    \n## t:Treatment 3181.9  1060.6     3 223.21  734.11 < 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"linear-mixed-models.html","id":"example-3-agridat","chapter":"8 Linear Mixed Models","heading":"8.8.3 Example 3 (Agridat)","text":"Remove outliersPlot age specieslme function nlme packagelmer function lme4 packageNotes:|| double pipes= uncorrelated random effects|| double pipes= uncorrelated random effectsTo remove intercept term:\n(0+ti|tree)\n(ti-1|tree)\nremove intercept term:(0+ti|tree)(0+ti|tree)(ti-1|tree)(ti-1|tree)include structured covariance terms, can use following way","code":"\nlibrary(agridat)\nlibrary(latticeExtra)## Loading required package: lattice## Warning: package 'lattice' was built under R version 4.0.5## \n## Attaching package: 'lattice'## The following object is masked from 'package:faraway':\n## \n##     melanoma## \n## Attaching package: 'latticeExtra'## The following object is masked from 'package:ggplot2':\n## \n##     layer\ndat <- harris.wateruse\n# Compare to Schabenberger & Pierce, fig 7.23\nuseOuterStrips(\n  xyplot(\n    water ~ day | species * age,\n    dat,\n    as.table = TRUE,\n    group = tree,\n    type = c('p', 'smooth'),\n    main = \"harris.wateruse 2 species, 2 ages (10 trees each)\"\n  )\n)\ndat <- subset(dat, day!=268)\nxyplot(\n  water ~ day | tree,\n  dat,\n  subset = age == \"A2\" & species == \"S2\",\n  as.table = TRUE,\n  type = c('p', 'smooth'),\n  ylab = \"Water use profiles of individual trees\",\n  main = \"harris.wateruse (Age 2, Species 2)\"\n)\n# Rescale day for nicer output, and convergence issues, add quadratic term\ndat <- transform(dat, ti = day / 100)\ndat <- transform(dat, ti2 = ti * ti)\n# Start with a subgroup: age 2, species 2\nd22 <- droplevels(subset(dat, age == \"A2\" & species == \"S2\"))\nlibrary(nlme)## Warning: package 'nlme' was built under R version 4.0.5## \n## Attaching package: 'nlme'## The following object is masked from 'package:lme4':\n## \n##     lmList## The following object is masked from 'package:dplyr':\n## \n##     collapse\n## We use pdDiag() to get uncorrelated random effects\nm1n <- lme(\n    water ~ 1 + ti + ti2, #intercept, time and time-squared = fixed effects\n    data = d22,\n    na.action = na.omit,\n    random = list(tree = pdDiag( ~ 1 + ti + ti2)) # random intercept, time and time squared per tree = random effects\n)\nranef(m1n)##     (Intercept)            ti           ti2\n## T04   0.1985796  1.609864e-09  4.990101e-10\n## T05   0.3492827  2.487690e-10 -4.845287e-11\n## T19  -0.1978989 -7.681202e-10 -1.961453e-10\n## T23   0.4519003 -3.270426e-10 -2.413583e-10\n## T38  -0.6457494 -1.608770e-09 -3.298010e-10\n## T40   0.3739432  3.264705e-10 -2.543109e-11\n## T49   0.8620648  9.021831e-10 -5.402247e-12\n## T53  -0.5655049 -8.279040e-10 -4.579291e-11\n## T67  -0.4394623 -3.485113e-10  2.147434e-11\n## T71  -0.3871552  7.930610e-10  3.718993e-10\nfixef(m1n)## (Intercept)          ti         ti2 \n##  -10.798799   12.346704   -2.838503\nsummary(m1n)## Linear mixed-effects model fit by REML\n##   Data: d22 \n##        AIC     BIC    logLik\n##   276.5142 300.761 -131.2571\n## \n## Random effects:\n##  Formula: ~1 + ti + ti2 | tree\n##  Structure: Diagonal\n##         (Intercept)           ti          ti2  Residual\n## StdDev:   0.5187869 1.438333e-05 3.864019e-06 0.3836614\n## \n## Fixed effects:  water ~ 1 + ti + ti2 \n##                  Value Std.Error  DF   t-value p-value\n## (Intercept) -10.798799 0.8814666 227 -12.25094       0\n## ti           12.346704 0.7827112 227  15.77428       0\n## ti2          -2.838503 0.1720614 227 -16.49704       0\n##  Correlation: \n##     (Intr) ti    \n## ti  -0.979       \n## ti2  0.970 -0.997\n## \n## Standardized Within-Group Residuals:\n##         Min          Q1         Med          Q3         Max \n## -3.07588246 -0.58531056  0.01210209  0.65402695  3.88777402 \n## \n## Number of Observations: 239\n## Number of Groups: 10\nm1lmer <- lmer(water~1+ti+ti2+(ti+ti2||tree),data = d22,na.action = na.omit)## boundary (singular) fit: see ?isSingular\nranef(m1lmer)## $tree\n##     (Intercept) ti ti2\n## T04   0.1985796  0   0\n## T05   0.3492827  0   0\n## T19  -0.1978989  0   0\n## T23   0.4519003  0   0\n## T38  -0.6457494  0   0\n## T40   0.3739432  0   0\n## T49   0.8620648  0   0\n## T53  -0.5655049  0   0\n## T67  -0.4394623  0   0\n## T71  -0.3871552  0   0\n## \n## with conditional variances for \"tree\"\nfixef(m1lmer)## (Intercept)          ti         ti2 \n##  -10.798799   12.346704   -2.838503\nm1l <- lmer(water ~ 1 + ti + ti2 + (1 | tree) + (0 + ti | tree) + (0 + ti2 | tree), data = d22)## boundary (singular) fit: see ?isSingular\nranef(m1l)## $tree\n##     (Intercept) ti ti2\n## T04   0.1985796  0   0\n## T05   0.3492827  0   0\n## T19  -0.1978989  0   0\n## T23   0.4519003  0   0\n## T38  -0.6457494  0   0\n## T40   0.3739432  0   0\n## T49   0.8620648  0   0\n## T53  -0.5655049  0   0\n## T67  -0.4394623  0   0\n## T71  -0.3871552  0   0\n## \n## with conditional variances for \"tree\"\nfixef(m1l)## (Intercept)          ti         ti2 \n##  -10.798799   12.346704   -2.838503\nm2n <- lme(\n    water ~ 1 + ti + ti2,\n    data = d22,\n    random = ~ 1 | tree,\n    cor = corExp(form =  ~ day | tree),\n    na.action = na.omit\n)\nranef(m2n)##     (Intercept)\n## T04   0.1929971\n## T05   0.3424631\n## T19  -0.1988495\n## T23   0.4538660\n## T38  -0.6413664\n## T40   0.3769378\n## T49   0.8410043\n## T53  -0.5528236\n## T67  -0.4452930\n## T71  -0.3689358\nfixef(m2n)## (Intercept)          ti         ti2 \n##  -11.223310   12.712094   -2.913682\nsummary(m2n)## Linear mixed-effects model fit by REML\n##   Data: d22 \n##        AIC      BIC   logLik\n##   263.3081 284.0911 -125.654\n## \n## Random effects:\n##  Formula: ~1 | tree\n##         (Intercept)  Residual\n## StdDev:   0.5154042 0.3925777\n## \n## Correlation Structure: Exponential spatial correlation\n##  Formula: ~day | tree \n##  Parameter estimate(s):\n##    range \n## 3.794624 \n## Fixed effects:  water ~ 1 + ti + ti2 \n##                  Value Std.Error  DF   t-value p-value\n## (Intercept) -11.223310 1.0988725 227 -10.21348       0\n## ti           12.712094 0.9794235 227  12.97916       0\n## ti2          -2.913682 0.2148551 227 -13.56115       0\n##  Correlation: \n##     (Intr) ti    \n## ti  -0.985       \n## ti2  0.976 -0.997\n## \n## Standardized Within-Group Residuals:\n##         Min          Q1         Med          Q3         Max \n## -3.04861039 -0.55703950  0.00278101  0.62558762  3.80676991 \n## \n## Number of Observations: 239\n## Number of Groups: 10"},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"nonlinear-and-generalized-linear-mixed-models","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9 Nonlinear and Generalized Linear Mixed Models","text":"NLMMs extend nonlinear model include fixed effects random effectsGLMMs extend generalized linear model include fixed effects random effects.nonlinear mixed model form \\[\nY_{ij} = f(\\mathbf{x_{ij} , \\theta, \\alpha_i}) + \\epsilon_{ij}\n\\]j-th response cluster (sujbect) (\\(= 1,...,n\\)), \\(j = 1,...,n_i\\)\\(\\mathbf{\\theta}\\) fixed effects\\(\\mathbf{\\alpha}_i\\) random effects cluster \\(\\mathbf{x}_{ij}\\) regressors design variables\\(f(.)\\) nonlinear mean response functionA GLMM can written :assume\\[\ny_i |\\alpha_i \\sim \\text{indep } f(y_i | \\alpha)\n\\]\\(f(y_i | \\mathbf{\\alpha})\\) exponential family distribution,\\[\nf(y_i | \\alpha) = \\exp [\\frac{y_i \\theta_i - b(\\theta_i)}{(\\phi)} - c(y_i, \\phi)]\n\\]conditional mean \\(y_i\\) related \\(\\theta_i\\)\\[\n\\mu_i = \\frac{\\partial b(\\theta_i)}{\\partial \\theta_i}\n\\]transformation mean give us desired linear model model fixed random effects.\\[\nE(y_i |\\alpha) = \\mu_i \\\\\ng(\\mu_i) = \\mathbf{x_i' \\beta + z'_i \\alpha}\n\\]\\(g()\\) known link function \\(\\mu_i\\) conditional mean. can see similarity GLMWe also specify random effects distribution\\[\n\\alpha \\sim f(\\alpha)\n\\]similar specification mixed models.Moreover, law large number applies fixed effects know normal distribution. , can specify \\(\\alpha\\) subjectively.Hence, can show NLMM special case GLMM\\[\n\\mathbf{Y}_i = \\mathbf{f}(\\mathbf{x}_i, \\mathbf{\\theta, \\alpha}_i) + \\mathbf{\\epsilon}_i \\\\\n\\mathbf{Y}_i = \\mathbf{g}^{-1} (\\mathbf{x}_i' \\beta + \\mathbf{z}_i' \\mathbf{\\alpha}_i) + \\mathbf{\\epsilon}_i\n\\]inverse link function corresponds nonlinear transformation fixed random effects.Note:can’t derive analytical formulation marginal distribution nonlinear combination normal variables normally distributed, even case additive error (\\(e_i\\)) random effects (\\(\\alpha_i\\)) normal.Consequences random effectsThe marginal mean \\(y_i\\) \\[\nE(y_i) = E_\\alpha(E(y_i | \\alpha)) = E_\\alpha (\\mu_i) = E(g^{-1}(\\mathbf{x_i' \\beta + z_i' \\alpha}))\n\\]\\(g^{-1}()\\) nonlinear, simplified version can go .special cases log link (\\(g(\\mu) = \\log \\mu\\) \\(g^{-1}() = \\exp()\\)) \\[\nE(y_i) = E(\\exp(\\mathbf{x_i' \\beta + z_i' \\alpha})) = \\exp(\\mathbf{x'_i \\beta})E(\\exp(\\mathbf{z}_i'\\alpha))\n\\]moment generating function \\(\\alpha\\) evaluated \\(\\mathbf{z}_i\\)Marginal variance \\(y_i\\)\\[\n\\begin{aligned}\nvar(y_i) &= var_\\alpha (E(y_i | \\alpha)) + E_\\alpha (var(y_i | \\alpha)) \\\\\n&= var(\\mu_i) + E((\\phi) V(\\mu_i)) \\\\\n&= var(g^{-1} (\\mathbf{x'_i \\beta + z'_i \\alpha})) + E((\\phi)V(g^{-1} (\\mathbf{x'_i \\beta + z'_i \\alpha})))\n\\end{aligned}\n\\]Without specific assumption \\(g()\\) /conditional distribution \\(\\mathbf{y}\\), simplified version.Marginal covariance \\(\\mathbf{y}\\)linear mixed model, random effects introduce dependence among observations share random effect common\\[\n\\begin{aligned}\ncov(y_i, y_j) &= cov_{\\alpha}(E(y_i | \\mathbf{\\alpha}),E(y_j | \\mathbf{\\alpha})) + E_{\\alpha}(cov(y_i, y_j | \\mathbf{\\alpha})) \\\\\n&= cov(\\mu_i, \\mu_j) + E(0) \\\\\n&= cov(g^{-1}(\\mathbf{x}_i' \\beta + \\mathbf{z}_i' \\mathbf{\\alpha}), g^{-1}(\\mathbf{x}'_j \\beta + \\mathbf{z}_j' \\mathbf{\\alpha}))\n\\end{aligned}\n\\]Important: conditioning induce covariabilityExample:Repeated measurements subjects. Let \\(y_{ij}\\) j-th count taken -th subject., model \\(y_{ij} | \\mathbf{\\alpha} \\sim \\text{indep } Pois(\\mu_{ij})\\). \\[\n\\log(\\mu_{ij}) = \\mathbf{x}_{ij}' \\beta + \\alpha_i \n\\]\\(\\alpha_i \\sim iid N(0,\\sigma^2_{\\alpha})\\)log-link random patient effect.","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"estimation-3","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.1 Estimation","text":"linear mixed models, marginal likelihood \\(\\mathbf{y}\\) integration random effects hierarchical formulation\\[\nf(\\mathbf{y}) = \\int f(\\mathbf{y}| \\alpha) f(\\alpha) d \\alpha\n\\]linear mixed models, assumed 2 component distributions Gaussian linear relationships, implied marginal distribution also linear Gaussian allows us solve integral analytically.hand, GLMMs, distribution \\(f(\\mathbf{y} | \\alpha)\\) Gaussian general, NLMMs, functional form mean response random (fixed) effects nonlinear. cases, can’t perform integral analytically, means solve itnumerically /ornumerically /orlinearize inverse link function.linearize inverse link function.","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"estimation-by-numerical-integration","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.1.1 Estimation by Numerical Integration","text":"marginal likelihood \\[\nL(\\beta; \\mathbf{y}) = \\int f(\\mathbf{y} | \\alpha) f(\\alpha) d \\alpha\n\\]Estimation fo fixed effects requires \\(\\frac{\\partial l}{\\partial \\beta}\\), \\(l\\) log-likelihoodOne way obtain marginal inference numerically integrate random effects throughnumerical quadraturenumerical quadratureLaplace approximationLaplace approximationMonte Carlo methodsMonte Carlo methodsWhen dimension \\(\\mathbf{\\alpha}\\) relatively low, easy. dimension \\(\\alpha\\) high, additional approximation required.","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"estimation-by-linearization","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.1.2 Estimation by Linearization","text":"Idea: Linearized version response (known working response, pseudo-response) called \\(\\tilde{y}_i\\) conditional mean \\[\nE(\\tilde{y}_i | \\alpha) = \\mathbf{x}_i' \\beta + \\mathbf{z}_i' \\alpha\n\\]also estimate \\(var(\\tilde{y}_i | \\alpha)\\). , apply Linear Mixed Models estimation usual.difference linearization done (.e., expand \\(f(\\mathbf{x, \\theta, \\alpha})\\) inverse link function","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"penalized-quasi-likelihood","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.1.2.1 Penalized quasi-likelihood","text":"(PQL)popular method\\[\n\\tilde{y}_i^{(k)} = \\hat{\\eta}_i^{(k-1)} + ( y_i - \\hat{\\mu}_i^{(k-1)})\\frac{d \\eta}{d \\mu}| \\hat{\\eta}_i^{(k-1)}\n\\]\\(\\eta_i = g(\\mu_i)\\) linear predictor\\(\\eta_i = g(\\mu_i)\\) linear predictork = iteration optimization algorithmk = iteration optimization algorithmThe algorithm updates \\(\\tilde{y}_i\\) linear mixed model fit using \\(E(\\tilde{y}_i | \\alpha)\\) \\(var(\\tilde{y}_i | \\alpha)\\)Comments:Easy implementEasy implementInference asymptotically correct due linearizatonInference asymptotically correct due linearizatonBiased estimates likely binomial response small groups worst Bernoulli response. Similarly Poisson models small counts. (Faraway 2016)Biased estimates likely binomial response small groups worst Bernoulli response. Similarly Poisson models small counts. (Faraway 2016)Hypothesis testing confidence intervals also problems.Hypothesis testing confidence intervals also problems.","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"generalized-estimating-equations","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.1.2.2 Generalized Estimating Equations","text":"(GEE)Let marginal generalized linear model mean y function predictors, means linearize mean response function assume dependent error structureExample\nBinary data:\\[\nlogit (E(\\mathbf{y})) = \\mathbf{X} \\beta\n\\]assume “working covariance matrix,” \\(\\mathbf{V}\\) elements \\(\\mathbf{y}\\), maximum likelihood equations estimating \\(\\beta\\) \\[\n\\mathbf{X'V^{-1}y} = \\mathbf{X'V^{-1}} E(\\mathbf{y})\n\\]\\(\\mathbf{V}\\) correct, unbiased estimating equationsWe typically define \\(\\mathbf{V} = \\mathbf{}\\). Solutions unbiased estimating equation give consistent estimators.practice, assume covariance structure, logistic regression, calculate large sample varianceLet \\(y_{ij} , j = 1,..,n_i, = 1,..,K\\) j-th measurement -th subject.\\[\n\\mathbf{y}_i = \n\\left(\n\\begin{array}\n{c}\ny_{i1} \\\\\n. \\\\\ny_{in_i}\n\\end{array}\n\\right)\n\\]mean\\[\n\\mathbf{\\mu}_i =\n\\left(\n\\begin{array}\n{c}\n\\mu_{i1} \\\\\n. \\\\\n\\mu_{in_i}\n\\end{array}\n\\right)\n\\]\\[\n\\mathbf{x}_{ij} = \n\\left(\n\\begin{array}\n{c}\nX_{ij1} \\\\\n. \\\\\nX_{ijp}\n\\end{array}\n\\right)\n\\]Let \\(\\mathbf{V}_i = cov(\\mathbf{y}_i)\\), based (LIANG ZEGER 1986) GEE estimates \\(\\beta\\) can obtained solving equation:\\[\nS(\\beta) = \\sum_{=1}^K \\frac{\\partial \\mathbf{\\mu}_i'}{\\partial \\beta} \\mathbf{V}^{-1}(\\mathbf{y}_i - \\mathbf{\\mu}_i) = 0\n\\]Let \\(\\mathbf{R}_i (\\mathbf{c})\\) \\(n_i \\times n_i\\) “working” correlation matrix specified parameters \\(\\mathbf{c}\\). , \\(\\mathbf{V}_i = (\\phi) \\mathbf{B}_i^{1/2}\\mathbf{R}(\\mathbf{c}) \\mathbf{B}_i^{1/2}\\), \\(\\mathbf{B}_i\\) \\(n_i \\times n_i\\) diagonal matrix \\(V(\\mu_{ij})\\) j-th diagonalIf \\(\\mathbf{R}(\\mathbf{c})\\) true correlation matrix \\(\\mathbf{y}_i\\), \\(\\mathbf{V}_i\\) true covariance matrixThe working correlation matrix must estimated iteratively fitting algorithm:Compute initial estimate \\(\\beta\\) (using GLM independence assumption)Compute initial estimate \\(\\beta\\) (using GLM independence assumption)Compute working correlation matrix \\(\\mathbf{R}\\) based upon studentized residualsCompute working correlation matrix \\(\\mathbf{R}\\) based upon studentized residualsCompute estimate covariance \\(\\hat{\\mathbf{V}}_i\\)Compute estimate covariance \\(\\hat{\\mathbf{V}}_i\\)Update \\(\\beta\\) according \n\\[\n\\beta_{r+1} = \\beta_r + (\\sum_{=1}^K \\frac{\\partial \\mathbf{\\mu}'_i}{\\partial \\beta} \\hat{\\mathbf{V}}_i^{-1} \\frac{\\partial \\mathbf{\\mu}_i}{\\partial \\beta})\n\\]Update \\(\\beta\\) according \\[\n\\beta_{r+1} = \\beta_r + (\\sum_{=1}^K \\frac{\\partial \\mathbf{\\mu}'_i}{\\partial \\beta} \\hat{\\mathbf{V}}_i^{-1} \\frac{\\partial \\mathbf{\\mu}_i}{\\partial \\beta})\n\\]Iterate algorithm convergesIterate algorithm convergesNote: Inference based likelihoods appropriate likelihood estimator","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"estimation-by-bayesian-hierarchical-models","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.1.3 Estimation by Bayesian Hierarchical Models","text":"Bayesian Estimation\\[\nf(\\mathbf{\\alpha}, \\mathbf{\\beta} | \\mathbf{y}) \\propto f(\\mathbf{y} | \\mathbf{\\alpha}, \\mathbf{\\beta}) f(\\mathbf{\\alpha})f(\\mathbf{\\beta})\n\\]Numerical techniques (e.g., MCMC) can used find posterior distribution. method best terms make simplifying approximation fully accounting uncertainty estimation prediction, complex, time-consuming, computationally intensive.Implementation Issues:valid joint distribution can constructed given conditional model random parametersNo valid joint distribution can constructed given conditional model random parametersThe mean/ variance relationship random effects lead constraints marginal covariance modelThe mean/ variance relationship random effects lead constraints marginal covariance modelDifficult fit computationallyDifficult fit computationally2 types estimation approaches:Approximate objective function (marginal likelihood) integral approximation\nLaplace methods\nQuadrature methods\nMonte Carlo integration\nApproximate objective function (marginal likelihood) integral approximationLaplace methodsLaplace methodsQuadrature methodsQuadrature methodsMonte Carlo integrationMonte Carlo integrationApproximate model (based Taylor series linearizations)Approximate model (based Taylor series linearizations)Packages RGLMM: MASS:glmmPQL lme4::glmer glmmTMBGLMM: MASS:glmmPQL lme4::glmer glmmTMBNLMM: nlme::nlme; lme4::nlmer brms::brmNLMM: nlme::nlme; lme4::nlmer brms::brmBayesian: MCMCglmm ; brms:brmBayesian: MCMCglmm ; brms:brmExample: Non-Gaussian Repeated measurementsWhen data Gaussian, Linear Mixed ModelsWhen data Gaussian, Linear Mixed ModelsWhen data non-Gaussian, Nonlinear Generalized Linear Mixed ModelsWhen data non-Gaussian, Nonlinear Generalized Linear Mixed Models","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"application-7","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.2 Application","text":"","code":""},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"binomial-cbpp-data","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.2.1 Binomial (CBPP Data)","text":"PQLPro:Linearizes response pseudo-response mean response (like LMM)Linearizes response pseudo-response mean response (like LMM)computationally efficientcomputationally efficientCons:biased binary, Poisson data small countsbiased binary, Poisson data small countsrandom effects interpreted link scalerandom effects interpreted link scalecan’t interpret AIC/BIC valuecan’t interpret AIC/BIC valueis herd specific outcome odds varies.can interpret fixed effect coefficients just like GLM. use logit link function , can say log odds probability case period 2 -1.016 less period 1 (baseline).Numerical IntegrationPro:accurateCon:computationally expensivecomputationally expensivewon’t work complex models.won’t work complex models.small data set, difference two approaches minimalIn numerical integration, can set nAGQ > 1 switch method likelihood evaluation, might increase accuracyBayesian approach GLMMsassume fixed effects parameters distributionassume fixed effects parameters distributioncan handle models intractable result traditional methodscan handle models intractable result traditional methodscomputationally expensivecomputationally expensiveMCMCglmm fits residual variance component (useful dispersion issues)interpret Bayesian “credible intervals” similarly confidence intervalsMake sure make post-hoc diagnosesThere trend, well-mixedFor herd variable, lot 0, suggests problem. fix instability herd effect sampling, can eithermodify prior distribution herd variationmodify prior distribution herd variationincreases number iterationincreases number iterationTo change shape priors, MCMCglmm use:V controls location distribution (default = 1)V controls location distribution (default = 1)nu controls concentration around V (default = 0)nu controls concentration around V (default = 0)","code":"\ndata(cbpp,package = \"lme4\")\nhead(cbpp)##   herd incidence size period\n## 1    1         2   14      1\n## 2    1         3   12      2\n## 3    1         4    9      3\n## 4    1         0    5      4\n## 5    2         3   22      1\n## 6    2         1   18      2\nlibrary(MASS)## Warning: package 'MASS' was built under R version 4.0.5\npql_cbpp <-\n    glmmPQL(\n        cbind(incidence, size - incidence) ~ period,\n        random = ~ 1 | herd,\n        data = cbpp,\n        family = binomial(link = \"logit\"),\n        verbose = F\n    )\nsummary(pql_cbpp)## Linear mixed-effects model fit by maximum likelihood\n##   Data: cbpp \n##   AIC BIC logLik\n##    NA  NA     NA\n## \n## Random effects:\n##  Formula: ~1 | herd\n##         (Intercept) Residual\n## StdDev:   0.5563535 1.184527\n## \n## Variance function:\n##  Structure: fixed weights\n##  Formula: ~invwt \n## Fixed effects:  cbind(incidence, size - incidence) ~ period \n##                 Value Std.Error DF   t-value p-value\n## (Intercept) -1.327364 0.2390194 38 -5.553372  0.0000\n## period2     -1.016126 0.3684079 38 -2.758156  0.0089\n## period3     -1.149984 0.3937029 38 -2.920944  0.0058\n## period4     -1.605217 0.5178388 38 -3.099839  0.0036\n##  Correlation: \n##         (Intr) perid2 perid3\n## period2 -0.399              \n## period3 -0.373  0.260       \n## period4 -0.282  0.196  0.182\n## \n## Standardized Within-Group Residuals:\n##        Min         Q1        Med         Q3        Max \n## -2.0591168 -0.6493095 -0.2747620  0.5170492  2.6187632 \n## \n## Number of Observations: 56\n## Number of Groups: 15\nexp(0.556)## [1] 1.743684\nsummary(pql_cbpp)$tTable##                 Value Std.Error DF   t-value      p-value\n## (Intercept) -1.327364 0.2390194 38 -5.553372 2.333216e-06\n## period2     -1.016126 0.3684079 38 -2.758156 8.888179e-03\n## period3     -1.149984 0.3937029 38 -2.920944 5.843007e-03\n## period4     -1.605217 0.5178388 38 -3.099839 3.637000e-03\nlibrary(lme4)## Warning: package 'lme4' was built under R version 4.0.5## Loading required package: Matrix## Warning: package 'Matrix' was built under R version 4.0.5\nnumint_cbpp <-\n    glmer(\n        cbind(incidence, size - incidence) ~ period + (1 | herd),\n        data = cbpp,\n        family = binomial(link = \"logit\")\n    )\nsummary(numint_cbpp)## Generalized linear mixed model fit by maximum likelihood (Laplace\n##   Approximation) [glmerMod]\n##  Family: binomial  ( logit )\n## Formula: cbind(incidence, size - incidence) ~ period + (1 | herd)\n##    Data: cbpp\n## \n##      AIC      BIC   logLik deviance df.resid \n##    194.1    204.2    -92.0    184.1       51 \n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -2.3816 -0.7889 -0.2026  0.5142  2.8791 \n## \n## Random effects:\n##  Groups Name        Variance Std.Dev.\n##  herd   (Intercept) 0.4123   0.6421  \n## Number of obs: 56, groups:  herd, 15\n## \n## Fixed effects:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  -1.3983     0.2312  -6.048 1.47e-09 ***\n## period2      -0.9919     0.3032  -3.272 0.001068 ** \n## period3      -1.1282     0.3228  -3.495 0.000474 ***\n## period4      -1.5797     0.4220  -3.743 0.000182 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##         (Intr) perid2 perid3\n## period2 -0.363              \n## period3 -0.340  0.280       \n## period4 -0.260  0.213  0.198\nlibrary(rbenchmark)\nbenchmark(\n    \"MASS\" = {\n        pql_cbpp <-\n            glmmPQL(\n                cbind(incidence, size - incidence) ~ period,\n                random = ~ 1 | herd,\n                data = cbpp,\n                family = binomial(link = \"logit\"),\n                verbose = F\n            )\n    },\n    \"lme4\" = {\n        glmer(\n            cbind(incidence, size - incidence) ~ period + (1 | herd),\n            data = cbpp,\n            family = binomial(link = \"logit\")\n        )\n    },\n    replications = 50,\n    columns = c(\"test\", \"replications\", \"elapsed\", \"relative\"),\n    order = \"relative\"\n)##   test replications elapsed relative\n## 1 MASS           50    2.62    1.000\n## 2 lme4           50    5.30    2.023\nlibrary(lme4)\nnumint_cbpp_GH <-\n    glmer(\n        cbind(incidence, size - incidence) ~ period + (1 | herd),\n        data = cbpp,\n        family = binomial(link = \"logit\"),\n        nAGQ = 20\n    )\nsummary(numint_cbpp_GH)$coefficients[, 1] - summary(numint_cbpp)$coefficients[, 1]##   (Intercept)       period2       period3       period4 \n## -0.0008808634  0.0005160912  0.0004066218  0.0002644629\nlibrary(MCMCglmm)## Warning: package 'MCMCglmm' was built under R version 4.0.5## Loading required package: coda## Loading required package: ape## Warning: package 'ape' was built under R version 4.0.5\nBayes_cbpp <-\n    MCMCglmm(\n        cbind(incidence, size - incidence) ~ period,\n        random = ~ herd,\n        data = cbpp,\n        family = \"multinomial2\",\n        verbose = FALSE\n    )\nsummary(Bayes_cbpp)## \n##  Iterations = 3001:12991\n##  Thinning interval  = 10\n##  Sample size  = 1000 \n## \n##  DIC: 537.7511 \n## \n##  G-structure:  ~herd\n## \n##      post.mean  l-95% CI  u-95% CI eff.samp\n## herd  0.001022 9.579e-17 7.923e-05      287\n## \n##  R-structure:  ~units\n## \n##       post.mean l-95% CI u-95% CI eff.samp\n## units     1.155   0.3415    2.194    363.5\n## \n##  Location effects: cbind(incidence, size - incidence) ~ period \n## \n##             post.mean l-95% CI u-95% CI eff.samp  pMCMC    \n## (Intercept)   -1.5404  -2.2180  -0.9078    755.1 <0.001 ***\n## period2       -1.2751  -2.2910  -0.1619    775.6  0.016 *  \n## period3       -1.4065  -2.4474  -0.3088    746.1  0.002 ** \n## period4       -1.9516  -3.2032  -0.6183    704.3  0.002 ** \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\napply(Bayes_cbpp$VCV,2,sd) #explains less variability##       herd      units \n## 0.01664994 0.52416759\nsummary(Bayes_cbpp)$solutions##             post.mean  l-95% CI   u-95% CI eff.samp pMCMC\n## (Intercept) -1.540398 -2.218031 -0.9078279 755.1080 0.001\n## period2     -1.275133 -2.291032 -0.1618755 775.6122 0.016\n## period3     -1.406450 -2.447372 -0.3087735 746.1049 0.002\n## period4     -1.951610 -3.203185 -0.6183007 704.2754 0.002\nlibrary(lattice)## Warning: package 'lattice' was built under R version 4.0.5\nxyplot(as.mcmc(Bayes_cbpp$Sol), layout = c(2, 2))\nxyplot(as.mcmc(Bayes_cbpp$VCV),layout=c(2,1))\nlibrary(MCMCglmm)\nBayes_cbpp2 <-\n    MCMCglmm(\n        cbind(incidence, size - incidence) ~ period,\n        random = ~ herd,\n        data = cbpp,\n        family = \"multinomial2\",\n        nitt = 20000,\n        burnin = 10000,\n        prior = list(G = list(list(\n            V = 1, nu = .1\n        ))),\n        verbose = FALSE\n    )\nxyplot(as.mcmc(Bayes_cbpp2$VCV), layout = c(2, 1))"},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"count-owl-data","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.2.2 Count (Owl Data)","text":"typical Poisson model, \\(\\lambda\\) (Poisson mean), model \\(\\log(\\lambda) = \\mathbf{x'\\beta}\\) response rate (e.g., counts per BroodSize), model \\(\\log(\\lambda / b) = \\mathbf{x'\\beta}\\) , equivalently \\(\\log(\\lambda) = \\log(b) + \\mathbf{x'\\beta}\\) \\(b\\) BroodSize. Hence, “offset” mean log variable.nest explains relatively large proportion variability (standard deviation larger coefficients)nest explains relatively large proportion variability (standard deviation larger coefficients)model fit isn’t great (deviance 5202 594 df)model fit isn’t great (deviance 5202 594 df)improvement using negative binomial considering overdispersionTo account many 0s data, can use zero-inflated Poisson (ZIP) model.glmmTMB can handle ZIP GLMMs since adds automatic differentiation existing estimation strategies.can see ZIP GLMM arrival time covariate zero best.arrival time positive effect observing nonzero number callsarrival time positive effect observing nonzero number callsinteractions non significant, food treatment significant (fewer calls eating)interactions non significant, food treatment significant (fewer calls eating)nest variability large magnitude (without , parameter estimates change)nest variability large magnitude (without , parameter estimates change)","code":"\nlibrary(glmmTMB)## Warning: package 'glmmTMB' was built under R version 4.0.5\nlibrary(dplyr)## Warning: package 'dplyr' was built under R version 4.0.5## \n## Attaching package: 'dplyr'## The following object is masked from 'package:MASS':\n## \n##     select## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\ndata(Owls, package = \"glmmTMB\")\nOwls <- Owls %>% rename(Ncalls = SiblingNegotiation)\nowls_glmer <-\n    glmer(\n        Ncalls ~ offset(log(BroodSize)) + FoodTreatment * SexParent +\n            (1 | Nest),\n        family = poisson,\n        data = Owls\n    )\nsummary(owls_glmer)## Generalized linear mixed model fit by maximum likelihood (Laplace\n##   Approximation) [glmerMod]\n##  Family: poisson  ( log )\n## Formula: Ncalls ~ offset(log(BroodSize)) + FoodTreatment * SexParent +  \n##     (1 | Nest)\n##    Data: Owls\n## \n##      AIC      BIC   logLik deviance df.resid \n##   5212.8   5234.8  -2601.4   5202.8      594 \n## \n## Scaled residuals: \n##     Min      1Q  Median      3Q     Max \n## -3.5529 -1.7971 -0.6842  1.2689 11.4312 \n## \n## Random effects:\n##  Groups Name        Variance Std.Dev.\n##  Nest   (Intercept) 0.2063   0.4542  \n## Number of obs: 599, groups:  Nest, 27\n## \n## Fixed effects:\n##                                     Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)                          0.65585    0.09567   6.855 7.12e-12 ***\n## FoodTreatmentSatiated               -0.65612    0.05606 -11.705  < 2e-16 ***\n## SexParentMale                       -0.03705    0.04501  -0.823   0.4104    \n## FoodTreatmentSatiated:SexParentMale  0.13135    0.07036   1.867   0.0619 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##             (Intr) FdTrtS SxPrnM\n## FdTrtmntStt -0.225              \n## SexParentMl -0.292  0.491       \n## FdTrtmS:SPM  0.170 -0.768 -0.605\n# Negative binomial model\nowls_glmerNB <-\n    glmer.nb(Ncalls ~ offset(log(BroodSize)) + FoodTreatment * SexParent\n             + (1 | Nest), data = Owls)\nc(Deviance = round(summary(owls_glmerNB)$AICtab[\"deviance\"], 3),\n  df = summary(owls_glmerNB)$AICtab[\"df.resid\"])## Deviance.deviance       df.df.resid \n##          3483.616           593.000\nhist(Owls$Ncalls,breaks=30)\nlibrary(glmmTMB)\nowls_glmm <-\n    glmmTMB(\n        Ncalls ~ FoodTreatment * SexParent + offset(log(BroodSize)) +\n            (1 | Nest),\n        ziformula =  ~ 0,\n        family = nbinom2(link = \"log\"),\n        data = Owls\n    )\nowls_glmm_zi <-\n    glmmTMB(\n        Ncalls ~ FoodTreatment * SexParent + offset(log(BroodSize)) +\n            (1 | Nest),\n        ziformula =  ~ 1,\n        family = nbinom2(link\n                         = \"log\"),\n        data = Owls\n    )\n# Scale Arrival time to use as a covariate for zero-inflation parameter\nOwls$ArrivalTime <- scale(Owls$ArrivalTime)\nowls_glmm_zi_cov <- glmmTMB(\n    Ncalls ~ FoodTreatment * SexParent +\n        offset(log(BroodSize)) +\n        (1 | Nest),\n    ziformula =  ~ ArrivalTime,\n    family = nbinom2(link\n                     = \"log\"),\n    data = Owls\n)\nas.matrix(anova(owls_glmm, owls_glmm_zi))##              Df      AIC      BIC    logLik deviance    Chisq Chi Df\n## owls_glmm     6 3495.610 3521.981 -1741.805 3483.610       NA     NA\n## owls_glmm_zi  7 3431.646 3462.413 -1708.823 3417.646 65.96373      1\n##                Pr(>Chisq)\n## owls_glmm              NA\n## owls_glmm_zi 4.592983e-16\nas.matrix(anova(owls_glmm_zi,owls_glmm_zi_cov))##                  Df      AIC      BIC    logLik deviance    Chisq Chi Df\n## owls_glmm_zi      7 3431.646 3462.413 -1708.823 3417.646       NA     NA\n## owls_glmm_zi_cov  8 3422.532 3457.694 -1703.266 3406.532 11.11411      1\n##                    Pr(>Chisq)\n## owls_glmm_zi               NA\n## owls_glmm_zi_cov 0.0008567362\nsummary(owls_glmm_zi_cov)##  Family: nbinom2  ( log )\n## Formula:          \n## Ncalls ~ FoodTreatment * SexParent + offset(log(BroodSize)) +      (1 | Nest)\n## Zero inflation:          ~ArrivalTime\n## Data: Owls\n## \n##      AIC      BIC   logLik deviance df.resid \n##   3422.5   3457.7  -1703.3   3406.5      591 \n## \n## Random effects:\n## \n## Conditional model:\n##  Groups Name        Variance Std.Dev.\n##  Nest   (Intercept) 0.07487  0.2736  \n## Number of obs: 599, groups:  Nest, 27\n## \n## Dispersion parameter for nbinom2 family (): 2.22 \n## \n## Conditional model:\n##                                     Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)                          0.84778    0.09961   8.511  < 2e-16 ***\n## FoodTreatmentSatiated               -0.39529    0.13742  -2.877  0.00402 ** \n## SexParentMale                       -0.07025    0.10435  -0.673  0.50079    \n## FoodTreatmentSatiated:SexParentMale  0.12388    0.16449   0.753  0.45138    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Zero-inflation model:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)  -1.3018     0.1261  -10.32  < 2e-16 ***\n## ArrivalTime   0.3545     0.1074    3.30 0.000966 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"binomial-1","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.2.3 Binomial","text":"Fixed effects (\\(\\beta\\)) = genotypeFixed effects (\\(\\beta\\)) = genotypeRandom effects (\\(\\alpha\\)) = blockRandom effects (\\(\\alpha\\)) = blockEquivalently, can use MCMCglmm , Bayesian approach","code":"\nlibrary(agridat)\nlibrary(ggplot2)## Warning: package 'ggplot2' was built under R version 4.0.5\nlibrary(lme4)\nlibrary(spaMM)## Warning: package 'spaMM' was built under R version 4.0.5## Registered S3 methods overwritten by 'registry':\n##   method               from \n##   print.registry_field proxy\n##   print.registry_entry proxy## spaMM (Rousset & Ferdy, 2014, version 3.9.0) is loaded.\n## Type 'help(spaMM)' for a short introduction,\n## 'news(package='spaMM')' for news,\n## and 'citation('spaMM')' for proper citation.\ndata(gotway.hessianfly)\ndat <- gotway.hessianfly\ndat$prop <- dat$y / dat$n\nggplot(dat, aes(x = lat, y = long, fill = prop)) +\n    geom_tile() +\n    scale_fill_gradient(low = 'white', high = 'black') +\n    geom_text(aes(label = gen, color = block)) +\n    ggtitle('Gotway Hessian Fly')\nflymodel <-\n    glmer(\n        cbind(y, n - y) ~ gen + (1 | block),\n        data = dat,\n        family = binomial,\n        nAGQ = 5\n    )\nsummary(flymodel)## Generalized linear mixed model fit by maximum likelihood (Adaptive\n##   Gauss-Hermite Quadrature, nAGQ = 5) [glmerMod]\n##  Family: binomial  ( logit )\n## Formula: cbind(y, n - y) ~ gen + (1 | block)\n##    Data: dat\n## \n##      AIC      BIC   logLik deviance df.resid \n##    162.2    198.9    -64.1    128.2       47 \n## \n## Scaled residuals: \n##      Min       1Q   Median       3Q      Max \n## -2.38644 -1.01188  0.09631  1.03468  2.75479 \n## \n## Random effects:\n##  Groups Name        Variance Std.Dev.\n##  block  (Intercept) 0.001022 0.03196 \n## Number of obs: 64, groups:  block, 4\n## \n## Fixed effects:\n##             Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)   1.5035     0.3914   3.841 0.000122 ***\n## genG02       -0.1939     0.5302  -0.366 0.714644    \n## genG03       -0.5408     0.5103  -1.060 0.289260    \n## genG04       -1.4342     0.4714  -3.043 0.002346 ** \n## genG05       -0.2037     0.5429  -0.375 0.707486    \n## genG06       -0.9783     0.5046  -1.939 0.052533 .  \n## genG07       -0.6041     0.5111  -1.182 0.237235    \n## genG08       -1.6774     0.4907  -3.418 0.000630 ***\n## genG09       -1.3984     0.4725  -2.960 0.003078 ** \n## genG10       -0.6817     0.5333  -1.278 0.201181    \n## genG11       -1.4630     0.4843  -3.021 0.002522 ** \n## genG12       -1.4591     0.4918  -2.967 0.003010 ** \n## genG13       -3.5528     0.6600  -5.383 7.31e-08 ***\n## genG14       -2.5073     0.5264  -4.763 1.90e-06 ***\n## genG15       -2.0872     0.4851  -4.302 1.69e-05 ***\n## genG16       -2.9697     0.5383  -5.517 3.46e-08 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1## \n## Correlation matrix not shown by default, as p = 16 > 12.\n## Use print(x, correlation=TRUE)  or\n##     vcov(x)        if you need it\nlibrary(coda)\nBayes_flymodel <- MCMCglmm(\n    cbind(y, n - y) ~ gen ,\n    random = ~ block,\n    data = dat,\n    family = \"multinomial2\",\n    verbose = FALSE\n)\nplot(Bayes_flymodel$Sol[, 1], main = dimnames(Bayes_flymodel$Sol)[[2]][1])\nautocorr.plot(Bayes_flymodel$Sol[,1],main=dimnames(Bayes_flymodel$Sol)[[2]][1])"},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"example-from-schabenberger_2001-section-8.4.1","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.2.4 Example from (Schabenberger and Pierce 2001) section 8.4.1","text":"cumulative volume relates complementary diameter (subplots created based total tree height)proposed non-linear model:\\[\nV_{id_j} = (\\beta_0 + (\\beta_1 + b_{1i})\\frac{D^2_i H_i}{1000})(\\exp[-(\\beta_2 + b_{2i})t_{ij} \\exp(\\beta_3 t_{ij})]) + e_{ij}\n\\]\\(b_{1i}, b_{2i}\\) random effects\\(b_{1i}, b_{2i}\\) random effects\\(e_{ij}\\) random errors\\(e_{ij}\\) random errorsLittle different book different implementation nonlinear mixed models.red line = predicted observations based common fixed effectsteal line = tree-specific predictions random effects","code":"\ndat2 <- read.table(\"images/YellowPoplarData_r.txt\")\nnames(dat2) <- c('tn', 'k', 'dbh', 'totht', 'dob', 'ht', 'maxd', 'cumv')\ndat2$t <- dat2$dob / dat2$dbh\ndat2$r <- 1 - dat2$dob / dat2$totht\nlibrary(ggplot2)\nlibrary(dplyr)\ndat2 <- dat2 %>% group_by(tn) %>% mutate(\n    z = case_when(\n        totht < 74 & totht >= 0 ~ 'a: 0-74ft',\n        totht < 88 & totht >= 74 ~ 'b: 74-88',\n        totht < 95 & totht >= 88 ~ 'c: 88-95',\n        totht < 99 & totht >= 95 ~ 'd: 95-99',\n        totht < 104 & totht >= 99 ~ 'e: 99-104',\n        totht < 109 & totht >= 104 ~ 'f: 104-109',\n        totht < 115 & totht >= 109 ~ 'g: 109-115',\n        totht < 120 & totht >= 115 ~ 'h: 115-120',\n        totht < 140 & totht >= 120 ~ 'i: 120-150',\n    )\n)\nggplot(dat2, aes(x = r, y = cumv)) + geom_point(size = 0.5) + facet_wrap(vars(z))\nlibrary(nlme)## Warning: package 'nlme' was built under R version 4.0.5## \n## Attaching package: 'nlme'## The following object is masked from 'package:dplyr':\n## \n##     collapse## The following object is masked from 'package:lme4':\n## \n##     lmList\ntmp <-\n    nlme(\n        cumv ~ (b0 + (b1 + u1) * (dbh * dbh * totht / 1000)) * (exp(-(b2 + u2) *\n                                                                        (t / 1000) * exp(b3 * t))),\n        data = dat2,\n        fixed = b0 + b1 + b2 + b3 ~ 1,\n        # 1 on the right hand side of the formula indicates a single fixed effects for the corresponding parameters\n        random = list(pdDiag(u1 + u2 ~ 1)),\n        #uncorrelated random effects\n        groups = ~ tn,\n        #group on trees so each tree w/ have u1 and u2\n        start = list(fixed = c(\n            b0 = 0.25,\n            b1 = 2.3,\n            b2 = 2.87,\n            b3 = 6.7\n        ))\n    )\nsummary(tmp)## Nonlinear mixed-effects model fit by maximum likelihood\n##   Model: cumv ~ (b0 + (b1 + u1) * (dbh * dbh * totht/1000)) * (exp(-(b2 +      u2) * (t/1000) * exp(b3 * t))) \n##   Data: dat2 \n##        AIC      BIC    logLik\n##   31103.73 31151.33 -15544.86\n## \n## Random effects:\n##  Formula: list(u1 ~ 1, u2 ~ 1)\n##  Level: tn\n##  Structure: Diagonal\n##                u1       u2 Residual\n## StdDev: 0.1508094 0.447829 2.226361\n## \n## Fixed effects:  b0 + b1 + b2 + b3 ~ 1 \n##       Value  Std.Error   DF  t-value p-value\n## b0 0.249386 0.12894686 6297   1.9340  0.0532\n## b1 2.288832 0.01266805 6297 180.6776  0.0000\n## b2 2.500497 0.05606685 6297  44.5985  0.0000\n## b3 6.848871 0.02140677 6297 319.9395  0.0000\n##  Correlation: \n##    b0     b1     b2    \n## b1 -0.639              \n## b2  0.054  0.056       \n## b3 -0.011 -0.066 -0.850\n## \n## Standardized Within-Group Residuals:\n##           Min            Q1           Med            Q3           Max \n## -6.694575e+00 -3.081861e-01 -8.910696e-05  3.469469e-01  7.855665e+00 \n## \n## Number of Observations: 6636\n## Number of Groups: 336\nnlme::intervals(tmp)## Approximate 95% confidence intervals\n## \n##  Fixed effects:\n##           lower      est.     upper\n## b0 -0.003318095 0.2493855 0.5020891\n## b1  2.264006138 2.2888323 2.3136585\n## b2  2.390619987 2.5004970 2.6103740\n## b3  6.806919317 6.8488712 6.8908232\n## attr(,\"label\")\n## [1] \"Fixed effects:\"\n## \n##  Random Effects:\n##   Level: tn \n##            lower      est.     upper\n## sd(u1) 0.1376080 0.1508094 0.1652772\n## sd(u2) 0.4056135 0.4478290 0.4944382\n## \n##  Within-group standard error:\n##    lower     est.    upper \n## 2.187260 2.226361 2.266161\nlibrary(cowplot)\nnlmmfn <- function(fixed,rand,dbh,totht,t){\n  b0 <- fixed[1]\n  b1 <- fixed[2]\n  b2 <- fixed[3]\n  b3 <- fixed[4]\n  u1 <- rand[1]\n  u2 <- rand[2]\n  #just made so we can predict w/o random effects\n  return((b0+(b1+u1)*(dbh*dbh*totht/1000))*(exp(-(b2+u2)*(t/1000)*exp(b3*t))))\n}\n\n\n#Tree 1\npred1 <- data.frame(seq(1,24,length.out=100))\nnames(pred1) <- 'dob'\npred1$tn <- 1\npred1$dbh <- unique(dat2[dat2$tn==1,]$dbh)\npred1$t <- pred1$dob/pred1$dbh\npred1$totht <- unique(dat2[dat2$tn==1,]$totht)\npred1$r <- 1-pred1$dob/pred1$totht\n\n\npred1$test <- predict(tmp,pred1)\npred1$testno <- nlmmfn(fixed=tmp$coefficients$fixed, rand = c(0,0),pred1$dbh,pred1$totht,pred1$t)\n\np1 <- ggplot(pred1)+geom_line(aes(x=r,y=test,color='with random'))+geom_line(aes(x=r,y=testno,color='No random'))+labs(colour = \"\") + geom_point(data=dat2[dat2$tn==1,],aes(x=r,y=cumv)) +ggtitle('Tree 1')+ theme(legend.position = \"none\")\n\n\n#Tree 151\npred151 <- data.frame(seq(1,21,length.out=100))\nnames(pred151) <- 'dob'\npred151$tn <- 151\npred151$dbh <- unique(dat2[dat2$tn==151,]$dbh)\npred151$t <- pred151$dob/pred151$dbh\npred151$totht <- unique(dat2[dat2$tn==151,]$totht)\npred151$r <- 1-pred151$dob/pred151$totht\n\n\npred151$test <- predict(tmp,pred151)\npred151$testno <- nlmmfn(fixed=tmp$coefficients$fixed, rand = c(0,0),pred151$dbh,pred151$totht,pred151$t)\n\np2 <- ggplot(pred151)+geom_line(aes(x=r,y=test,color='with random'))+geom_line(aes(x=r,y=testno,color='No random'))+labs(colour = \"\") + geom_point(data=dat2[dat2$tn==151,],aes(x=r,y=cumv)) + ggtitle('Tree 151')+ theme(legend.position = \"none\")\n\n\n#Tree 279\npred279 <- data.frame(seq(1,9,length.out=100))\nnames(pred279) <- 'dob'\npred279$tn <- 279\npred279$dbh <- unique(dat2[dat2$tn==279,]$dbh)\npred279$t <- pred279$dob/pred279$dbh\npred279$totht <- unique(dat2[dat2$tn==279,]$totht)\npred279$r <- 1-pred279$dob/pred279$totht\n\n\npred279$test <- predict(tmp,pred279)\npred279$testno <- nlmmfn(fixed=tmp$coefficients$fixed, rand = c(0,0),pred279$dbh,pred279$totht,pred279$t)\n\np3 <- ggplot(pred279)+geom_line(aes(x=r,y=test,color='with random'))+geom_line(aes(x=r,y=testno,color='No random'))+labs(colour = \"\") + geom_point(data=dat2[dat2$tn==279,],aes(x=r,y=cumv)) + ggtitle('Tree 279')+ theme(legend.position = \"none\")\n\nplot_grid(p1,p2,p3)"},{"path":"nonlinear-and-generalized-linear-mixed-models.html","id":"summary-1","chapter":"9 Nonlinear and Generalized Linear Mixed Models","heading":"9.3 Summary","text":"","code":""},{"path":"model-specification.html","id":"model-specification","chapter":"10 Model Specification","heading":"10 Model Specification","text":"Test whether underlying assumptions hold trueNested Model (A1/A3)Non-Nested Model (A1/A3)Heteroskedasticity (A4)","code":""},{"path":"model-specification.html","id":"nested-model","chapter":"10 Model Specification","heading":"10.1 Nested Model","text":"\\[\n\\begin{aligned}\ny = \\beta_0 + x_1\\beta_1 + x_2\\beta-2 + x_3\\beta_3 + \\epsilon && \\text{unrestricted model} \\\\\ny = \\beta_0 + x_1\\beta_1 + \\epsilon && \\text{restricted model}\n\\end{aligned}\n\\]Unrestricted model always longer restricted model\nrestricted model “nested” within unrestricted model\ndetermine variables included exclude, use Wald TestAdjusted \\(R^2\\)\\(R^2\\) always increase variables includedAdjusted \\(R^2\\) tries correct penalizing inclusion unnecessary variables.\\[\n{R}^2 = 1 - \\frac{SSR/n}{SST/n} \\\\\n{R}^2_{adj}= 1- \\frac{SSR/(n-k)}{SST/(n-1)} = 1 - \\frac{(n-1)(1-R^2)}{(n-k)}\n\\]\\({R}^2_{adj}\\) increases t-statistic additional variable greater 1 absolute value.\\({R}^2_{adj}\\) valid models heteroskedasticitythere fore used determining variables included model (t F-tests appropriate)","code":""},{"path":"model-specification.html","id":"chow-test","chapter":"10 Model Specification","heading":"10.1.1 Chow test","text":"run two different regressions two groups?","code":""},{"path":"model-specification.html","id":"non-nested-model","chapter":"10 Model Specification","heading":"10.2 Non-Nested Model","text":"compare models different non-nested specifications","code":""},{"path":"model-specification.html","id":"davidson-mackinnon-test","chapter":"10 Model Specification","heading":"10.2.1 Davidson-Mackinnon test","text":"","code":""},{"path":"model-specification.html","id":"independent-variable","chapter":"10 Model Specification","heading":"10.2.1.1 Independent Variable","text":"independent variables logged?\ndecide non-nested alternatives\\[\n\\begin{aligned}\ny =  \\beta_0 + x_1\\beta_1 + x_2\\beta_2 + \\epsilon && \\text{(level eq)} \\\\\ny =  \\beta_0 + ln(x_1)\\beta_1 + x_2\\beta_2 + \\epsilon && \\text{(log eq)}\n\\end{aligned}\n\\]Obtain predict outcome estimating model log equation \\(\\check{y}\\) estimate following auxiliary equation,\\[\ny = \\beta_0 + x_1\\beta_1 + x_2\\beta_2 + \\check{y}\\gamma + error\n\\]evaluate t-statistic null hypothesis \\(H_0: \\gamma = 0\\)Obtain predict outcome estimating model level equation \\(\\hat{y}\\), estimate following auxiliary equation,\\[\ny = \\beta_0 + ln(x_1)\\beta_1 + x_2\\beta_2 + \\check{y}\\gamma + error\n\\]\nevaluate t-statistic null hypothesis \\(H_0: \\gamma = 0\\)reject null (1) step fail reject null second step, log equation preferred.fail reject null (1) step reject null (2) step , level equation preferred.reject steps, statistical evidence neither model used re-evaluate functional form model.fail reject steps, sufficient evidence prefer one model . can compare \\(R^2_{adj}\\) choose two models.\\[\ny = \\beta_0 + ln(x)\\beta_1 + \\epsilon \\\\\ny = \\beta_0 + x(\\beta_1) + x^2\\beta_2 + \\epsilon\n\\]\n* Compare better fits data\n* Compare standard \\(R^2\\) unfair second model less parsimonious (parameters estimate)\n* \\(R_{adj}^2\\) penalize second model less parsimonious\n+ valid heteroskedasticity (A4 holds)\n* compare Davidson-Mackinnon test","code":""},{"path":"model-specification.html","id":"dependent-variable","chapter":"10 Model Specification","heading":"10.2.1.2 Dependent Variable","text":"\\[\n\\begin{aligned}\ny = \\beta_0 + x_1\\beta_1 + \\epsilon && \\text{level eq} \\\\\nln(y) = \\beta_0 + x_1\\beta_1 + \\epsilon && \\text{log eq} \\\\\n\\end{aligned}\n\\]level model, regardless big y , x constant effect (.e., one unit change \\(x_1\\) results \\(\\beta_1\\) unit change y)log model, larger y , effect x stronger (.e., one unit change \\(x_1\\) increase y 1 \\(1+\\beta_1\\) 100 100+100x\\(\\beta_1\\))compare \\(R^2\\) \\(R^2_{adj}\\) outcomes complement different, scaling different (SST different)need “un-transform” \\(ln(y)\\) back scale y compare,Estimate model log equation obtain predicted outcome \\(\\hat{ln(y)}\\)“Un-transform” predicted outcome\\[\n\\hat{m} = exp(\\hat{ln(y)})\n\\]\n3. Estimate following model (without intercept)\\[\ny = \\alpha\\hat{m} + error\n\\]\nobtain predicted outcome \\(\\hat{y}\\)take square correlation \\(\\hat{y}\\) y scaled version \\(R^2\\) log model can now compare usual \\(R^2\\) level model.","code":""},{"path":"model-specification.html","id":"heteroskedasticity-1","chapter":"10 Model Specification","heading":"10.3 Heteroskedasticity","text":"Using roust standard errors always validIf significant evidence heteroskedasticity implying A4 hold\nGauss-Markov Theorem longer holds, OLS BLUE.\nconsider using better linear unbiased estimator (Weighted Least Squares Generalized Least Squares)\nGauss-Markov Theorem longer holds, OLS BLUE.consider using better linear unbiased estimator (Weighted Least Squares Generalized Least Squares)","code":""},{"path":"model-specification.html","id":"breusch-pagan-test","chapter":"10 Model Specification","heading":"10.3.1 Breusch-Pagan test","text":"A4 implies\n\\[\nE(\\epsilon_i^2|\\mathbf{x_i})=\\sigma^2\n\\]\\[\n\\epsilon_i^2 = \\gamma_0 + x_{i1}\\gamma_1 + ... + x_{ik -1}\\gamma_{k-1} + error\n\\]\ndetermining whether \\(\\mathbf{x}_i\\) predictive valueif \\(\\mathbf{x}_i\\) predictive value, variance changes levels \\(\\mathbf{x}_i\\) evidence heteroskedasticityif \\(\\mathbf{x}_i\\) predictive value, variance constant levels \\(\\mathbf{x}_i\\)Breusch-Pagan test heteroskedasticity compute F-test total significance following model\\[\ne_i^2 = \\gamma_0 + x_{i1}\\gamma_1 + ... + x_{ik -1}\\gamma_{k-1} + error\n\\]\nlow p-value means reject null homoskedasticityHowever, Breusch-Pagan test detect heteroskedasticity non-linear form","code":""},{"path":"model-specification.html","id":"white-test","chapter":"10 Model Specification","heading":"10.3.2 White test","text":"test heteroskedasticity allow non-linear relationship computing F-test total significance following model (assume three independent random variables)\\[\ne_i^2=\\gamma_0 + x_i \\gamma_1 + x_{i2}\\gamma_2 + x_{i3}\\gamma_3 + x_{i1}^2\\gamma_4 + x_{i2}^2\\gamma_5 + x_{i3}^2\\gamma_6 + (x_{i1} \\times x_{i2})\\gamma_7 + (x_{i1} \\times x_{i3})\\gamma_8 + (x_{i2} \\times x_{i3})\\gamma_9 + error\n\\]\nlow p-value means reject null homoskedasticityEquivalently, can compute LM \\(LM = nR^2_{e^2}\\)\n\\(R^2_{e^2}\\) come regression squared residual outcomeThe LM statistic \\(\\chi_k^2\\) distribution","code":""},{"path":"imputation-missing-data.html","id":"imputation-missing-data","chapter":"11 Imputation (Missing Data)","heading":"11 Imputation (Missing Data)","text":"Imputation usually seen illegitimate child statistical analysis. Several reasons contribute negative views :Peopled hardly imputation correctlyImputation can applied small range problems correctlyIf missing data y (dependent variable), probability able imputation appropriately. However, certain type missing data (e.g., non-random missing data) xs variable (independent variables), can still salvage collected data points imputation.also need talk want imputation first place. purpose inference/ explanation (valid statistical inference optimal point prediction), imputation offer much help (Rubin 1996). However, purpose prediction, want standard error reduced including information (non-missing data) variables data point. imputation tool ’re looking .software packages, use listwise deletion casewise deletion complete case analysis (analysis observations information). recently statistician can propose methods bit better listwise deletion maximum likelihood multiple imputation.“Judging quality missing data procedures ability recreate individual missing values (according hit rate, mean square error, etc) lead choosing procedures result valid inference,” (Rubin 1996)","code":""},{"path":"imputation-missing-data.html","id":"assumptions-1","chapter":"11 Imputation (Missing Data)","heading":"11.1 Assumptions","text":"","code":""},{"path":"imputation-missing-data.html","id":"missing-completely-at-random-mcar","chapter":"11 Imputation (Missing Data)","heading":"11.1.1 Missing Completely at Random (MCAR)","text":"Missing Completely Random, MCAR, means relationship missingness data values, observed missing. missing data points random subset data. nothing systematic going makes data likely missing others.probability missing data variable unrelated value values variables data set.Note: “missingness” Y can correlated “missingness” X can compare value variables observations missing data, observations without missing data. reject t-test mean difference, can say evidence data MCAR. say data MCAR fail reject t-test.propensity data point missing completely random.’s relationship whether data point missing values data set, missing observed.missing data just random subset data.","code":""},{"path":"imputation-missing-data.html","id":"missing-at-random-mar","chapter":"11 Imputation (Missing Data)","heading":"11.1.2 Missing at Random (MAR)","text":"Missing Random, MAR, means systematic relationship propensity missing values observed data, missing data. Whether observation missing nothing missing values, values individual’s observed variables. , example, men likely tell weight women, weight MAR.MAR weaker MCAR\\[\nP(Y_{missing}|Y,X)= P(Y_{missing}|X)\n\\]probability Y missing given Y X equal probability Y missing given X. However, impossible provide evidence MAR condition.propensity data point missing related missing data, related observed data. another word, systematic relationship propensity missing values observed data, missing data.\nexample, men likely tell weight women, weight MAR\npropensity data point missing related missing data, related observed data. another word, systematic relationship propensity missing values observed data, missing data.example, men likely tell weight women, weight MARMAR requires cause missing data unrelated missing values may related observed values variables.MAR requires cause missing data unrelated missing values may related observed values variables.MAR means missing values related observed values variables. example CD missing data, missing income data may unrelated actual income values related education. Perhaps people education less likely reveal income less educationMAR means missing values related observed values variables. example CD missing data, missing income data may unrelated actual income values related education. Perhaps people education less likely reveal income less education","code":""},{"path":"imputation-missing-data.html","id":"ignorable","chapter":"11 Imputation (Missing Data)","heading":"11.1.3 Ignorable","text":"missing data mechanism ignorable whenThe data MARthe parameters function missing data process unrelated parameters (interest) need estimated.case, actually don’t need model missing data mechanisms unless like improve accuracy, case still need rigorous approach improve efficiency parameters.","code":""},{"path":"imputation-missing-data.html","id":"nonignorable","chapter":"11 Imputation (Missing Data)","heading":"11.1.4 Nonignorable","text":"Missing Random, MNAR, means relationship propensity value missing values.Example: people lowest education missing education sickest people likely drop study.MNAR called Nonignorable missing data mechanism modeled deal missing data. include model data missing likely values .Hence, case nonignorable, data MAR. , parameters interest biased model missing data mechanism. One widely used approach nonignorable missing data (Heckman 1976)Another name: Missing Random (MNAR): relationship propensity value missing values\nexample, people low education less likely report .\nAnother name: Missing Random (MNAR): relationship propensity value missing valuesFor example, people low education less likely report .need model data missing likely values .need model data missing likely values .missing data mechanism related missing valuesthe missing data mechanism related missing valuesIt commonly occurs people want reveal something personal unpopular themselvesIt commonly occurs people want reveal something personal unpopular themselvesComplete case analysis can give highly biased results NI missing data. proportionally low moderate income individuals left sample high income people missing, estimate mean income lower actual population mean.Complete case analysis can give highly biased results NI missing data. proportionally low moderate income individuals left sample high income people missing, estimate mean income lower actual population mean.","code":""},{"path":"imputation-missing-data.html","id":"solutions-to-missing-data","chapter":"11 Imputation (Missing Data)","heading":"11.2 Solutions to Missing data","text":"","code":""},{"path":"imputation-missing-data.html","id":"listwise-deletion","chapter":"11 Imputation (Missing Data)","heading":"11.2.1 Listwise Deletion","text":"Advantages:Can applied statistical test (SEM, multi-level regression, etc.)Can applied statistical test (SEM, multi-level regression, etc.)case MCAR, parameters estimates standard errors unbiased.case MCAR, parameters estimates standard errors unbiased.case MAR among independent variables (depend values dependent variables), listwise deletion parameter estimates can still unbiased. (Little 1992) example, model \\(y=\\beta_{0}+\\beta_1X_1 + \\beta_2X_2 +\\epsilon\\) probability missing data X1 independent Y, dependent value X1 X2, model estimates still unbiased.\nmissing data mechanism depends values independent variables stratified sampling. stratified sampling bias estimates\ncase logistic regression, probability missing data variable depends value dependent variable, independent value independent variables, listwise deletion yield biased intercept estimate, consistent estimates slope standard errors (Vach 1994). However, logistic regression still fail probability missing data dependent value dependent independent variables.\nregression analysis, listwise deletion robust maximum likelihood multiple imputation MAR assumption violated.\ncase MAR among independent variables (depend values dependent variables), listwise deletion parameter estimates can still unbiased. (Little 1992) example, model \\(y=\\beta_{0}+\\beta_1X_1 + \\beta_2X_2 +\\epsilon\\) probability missing data X1 independent Y, dependent value X1 X2, model estimates still unbiased.missing data mechanism depends values independent variables stratified sampling. stratified sampling bias estimatesIn case logistic regression, probability missing data variable depends value dependent variable, independent value independent variables, listwise deletion yield biased intercept estimate, consistent estimates slope standard errors (Vach 1994). However, logistic regression still fail probability missing data dependent value dependent independent variables.regression analysis, listwise deletion robust maximum likelihood multiple imputation MAR assumption violated.Disadvantages:yield larger standard errors sophisticated methods discussed later.data MCAR, MAR, listwise deletion can yield biased estimates.cases regression analysis, sophisticated methods can yield better estimates compared listwise deletion.","code":""},{"path":"imputation-missing-data.html","id":"pairwise-deletion","chapter":"11 Imputation (Missing Data)","heading":"11.2.2 Pairwise Deletion","text":"method used case linear models linear regression, factor analysis, SEM. premise method based coefficient estimates calculated based means, standard deviations, correlation matrix. Compared listwise deletion, still utilized many correlation variables possible compute correlation matrix.Advantages:true missing data mechanism MCAR, pair wise deletion yield consistent estimates, unbiased large samplesIf true missing data mechanism MCAR, pair wise deletion yield consistent estimates, unbiased large samplesCompared listwise deletion: (Glasser 1964)\ncorrelation among variables low, pairwise deletion efficient estimates listwise\ncorrelations among variables high, listwise deletion efficient pairwise.\nCompared listwise deletion: (Glasser 1964)correlation among variables low, pairwise deletion efficient estimates listwiseIf correlations among variables high, listwise deletion efficient pairwise.Disadvantages:data mechanism MAR, pairwise deletion yield biased estimates.small sample, sometimes covariance matrix might positive definite, means coefficients estimates calculated.Note: need read carefully software specify sample size alter standard errors.","code":""},{"path":"imputation-missing-data.html","id":"dummy-variable-adjustment","chapter":"11 Imputation (Missing Data)","heading":"11.2.3 Dummy Variable Adjustment","text":"Also known Missing Indicator Method Proxy VariableAdd another variable database indicate whether value missing.Create 2 variables\\[\\begin{equation}\nD=\n\\begin{cases}\n1 & \\text{data X missing} \\\\\n0 & \\text{otherwise}\\\\\n\\end{cases}\n\\end{equation}\\]\\[\\begin{equation}\nX^* = \n\\begin{cases}\nX & \\text{data available} \\\\\nc & \\text{data missing}\\\\\n\\end{cases}\n\\end{equation}\\]Note: typical choice c usually mean XInterpretation:Coefficient D difference expected value Y group data group without data X.Coefficient X* effect group data YDisadvantages:method yields bias estimates coefficient even case MCAR (Jones 1996)","code":""},{"path":"imputation-missing-data.html","id":"imputation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4 Imputation","text":"","code":""},{"path":"imputation-missing-data.html","id":"mean-mode-median-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.1 Mean, Mode, Median Imputation","text":"Bad:\nMean imputation preserve relationships among variables\nMean imputation leads Underestimate Standard Errors → ’re making Type errors without realizing .\nBiased estimates variances covariances (Haitovsky 1968)\nBad:Mean imputation preserve relationships among variablesMean imputation leads Underestimate Standard Errors → ’re making Type errors without realizing .Biased estimates variances covariances (Haitovsky 1968)","code":""},{"path":"imputation-missing-data.html","id":"maximum-likelihood","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.2 Maximum Likelihood","text":"missing data MAR monotonic (case panel studies), ML can adequately estimating coefficients.Monotonic means missing data X1, observation also missing data variables come .ML can generally handle linear models, log-linear model, beyond , ML still lacks theory software implement.","code":""},{"path":"imputation-missing-data.html","id":"expectation-maximization-algorithm-em-algorithm","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.2.1 Expectation-Maximization Algorithm (EM Algorithm)","text":"iterative process:variables used impute value (Expectation).Check whether value likely (Maximization)., re-imputes likely value.start regression estimates based either listwise deletion pairwise deletion. regressing missing variables available variables, obtain regression model. Plug missing data back original model, modified variances covariances example, missing data \\(X_{ij}\\) regress available data \\(X_{(j)}\\), plug expected value \\(X_{ij}\\) back \\(X_{ij}^2\\) turn \\(X_{ij}^2 + s_{j(j)}^2\\) \\(s_{j(j)}^2\\) stands residual variance regressing \\(X_{ij}\\) \\(X_{(j)}\\) new estimated model, rerun process estimates converge.Advantages:easy usepreserves relationship variables (important use Factor Analysis Linear Regression later ), best case Factor Analysis, doesn’t require standard error individuals item.Disadvantages:Standard errors coefficients incorrect (biased usually downward - underestimate)Models overidentification, estimates efficient","code":""},{"path":"imputation-missing-data.html","id":"direct-ml-raw-maximum-likelihood","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.2.2 Direct ML (raw maximum likelihood)","text":"Advantagesefficient estimates correct standard errors.Disadvantages:Hard implements","code":""},{"path":"imputation-missing-data.html","id":"multiple-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.3 Multiple Imputation","text":"MI designed use “Bayesian model-based approach create procedures, frequentist (randomization-based approach) evaluate procedures.” (Rubin 1996)MI estimates properties ML data MARConsistentAsymptotically efficientAsymptotically normalMI can applied type model, unlike Maximum Likelihood limited small set models.drawback MI produce slightly different estimates every time run . avoid problem, can set seed analysis ensure reproducibility.","code":""},{"path":"imputation-missing-data.html","id":"single-random-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.3.1 Single Random Imputation","text":"Random draws form residual distribution imputed variable add random numbers imputed values.example, missing data X, ’s MCAR, thenregress X Y (Listwise Deletion method) get residual distribution.regress X Y (Listwise Deletion method) get residual distribution.every missing value X, substitute \\(\\tilde{x_i}=\\hat{x_i} + \\rho u_i\\) \n\\(u_i\\) random draw standard normal distribution\n\\(x_i\\) predicted value regression X Y\n\\(\\rho\\) standard deviation residual distribution X regressed Y.\nevery missing value X, substitute \\(\\tilde{x_i}=\\hat{x_i} + \\rho u_i\\) \\(u_i\\) random draw standard normal distribution\\(x_i\\) predicted value regression X Y\\(\\rho\\) standard deviation residual distribution X regressed Y.However, model run imputed data still thinks data collected, imputed, leads standard error estimates low test statistics high.address problem, need repeat imputation process leads us repeated imputation multiple random imputation.","code":""},{"path":"imputation-missing-data.html","id":"repeated-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.3.2 Repeated Imputation","text":"“Repeated imputations draws posterior predictive distribution missing values specific model , particular Bayesian model data missing mechanism.”(Rubin 1996)Repeated imputation, also known , multiple random imputation, allows us multiple “completed” data sets. variability across imputations adjust standard errors upward.estimate standard error \\(\\bar{r}\\) (mean correlation estimates X Y) \\[\nSE(\\bar{r})=\\sqrt{\\frac{1}{M}\\sum_{k}s_k^2+ (1+\\frac{1}{M})(\\frac{1}{M-1})\\sum_{k}(r_k-\\bar{r})^2}\n\\] M number replications, \\(r_k\\) correlation replication k, \\(s_k\\) estimated standard error replication k.However, method still considers parameter predicting \\(\\tilde{x}\\) still fixed, means assume using true parameters predict \\(\\tilde{x}\\). overcome challenge, need introduce variability model \\(\\tilde{x}\\) treating parameters random variables use Bayesian posterior distribution parameters predict parameters.However, sample large proportion missing data small, extra Bayesian step might necessary. sample small proportion missing data large, extra Bayesian step necessary.Two algorithms get random draws regression parameters posterior distribution:Data AugmentationSampling importance/resampling (SIR)Authors argued SIR superiority due computer time (G. King et al. 2001)","code":""},{},{"path":"imputation-missing-data.html","id":"nonparametric-semiparametric-methods","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.4 Nonparametric/ Semiparametric Methods","text":"","code":""},{"path":"imputation-missing-data.html","id":"hot-deck-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.4.1 Hot Deck Imputation","text":"Used U.S. Census Bureau public datasetsapproximate Bayesian bootstrap\nrandomly chosen value individual sample similar values variables. words, find sample subjects similar variables, randomly choose one values missing variable.\\(n_1\\) cases complete data Y \\(n_0\\) cases missing data YStep 1: \\(n_1\\), take random sample (replacement) \\(n_1\\) casesStep 2: retrieved sample take random sample (replacement) \\(n_0\\) casesStep 3: Assign \\(n_0\\) cases step 2 \\(n_0\\) missing data cases.Step 4: Repeat process every variable.Step 5: multiple imputation, repeat four steps multiple times.Note:skip step 1, reduce variability estimating standard errors.skip step 1, reduce variability estimating standard errors.Good:\nConstrained possible values.\nSince value picked random, adds variability, might come handy calculating standard errors.\nGood:Constrained possible values.Since value picked random, adds variability, might come handy calculating standard errors.","code":""},{"path":"imputation-missing-data.html","id":"cold-deck-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.4.2 Cold Deck Imputation","text":"Contrary Hot Deck, Cold Deck choose value systematically observation similar values variables, remove random variation want.","code":""},{"path":"imputation-missing-data.html","id":"predictive-mean-matching","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.4.3 Predictive Mean Matching","text":"Steps:Regress Y X (matrix covariates) \\(n_1\\) (.e., non-missing cases) get coefficients \\(b\\) (\\(k \\times 1\\) vector) residual variance estimates \\(s^2\\)Draw randomly posterior predictive distribution residual variance (assuming noninformative prior) calculating \\(\\frac{(n_1-k)s^2}{\\chi^2}\\), \\(\\chi^2\\) random draw \\(\\chi^2_{n_1-k}\\) let \\(s^2_{[1]}\\) -th random drawRandomly draw posterior distribution coefficients \\(b\\), drawing \\(MVN(b, s^2_{[1]}(X'X)^{-1})\\), X \\(n_1 \\times k\\) matrix X values. \\(b_{1}\\)Using step 1, can calculate standardized residuals \\(n_1\\) cases: \\(e_i = \\frac{y_i - bx_i}{\\sqrt{s^2(1-k/n_1)}}\\)Randomly draw sample (replacement) \\(n_0\\) \\(n_1\\) residuals step 4With \\(n_0\\) cases, can calculate imputed values Y: \\(y_i = b_{[1]}x_i + s_{[1]}e_i\\) \\(e_i\\) taken step 5, \\(b_{[1]}\\) taken step 3, \\(s_{[1]}\\) taken step 2.Repeat steps 2 6 except step 4.Notes:can used multiple variables variable imputed using variables predictor.can also used heteroskedasticity imputed values.Example Statistics GlobleExample UCLA Statistical Consulting (Bruin 2011)","code":"\nset.seed(918273)                                # Seed\nN <- 3000                                       # Sample size\ny <- round(runif(N, -10, 10))                   # Target variable Y\nx1 <- y + round(runif(N, 0, 50))                # Auxiliary variable 1\nx2 <- round(y + 0.25 * x1 + rnorm(N, - 3, 15))  # Auxiliary variable 2\nx3 <- round(0.1 * x1 + rpois(N, 2))             # Auxiliary variable 3\nx4 <- as.factor(round(0.02 * y + runif(N)))     # Auxiliary variable 4 (categorical variable)\ny[rbinom(N, 1, 0.2) == 1] <- NA                 # Insert 20% missing data in Y\ndata <- data.frame(y, x1, x2, x3, x4)           # Store data in dataset\nhead(data)                                      # First 6 rows of our data##    y x1  x2 x3 x4\n## 1  8 38  -3  6  1\n## 2  1 50  -9  5  0\n## 3  5 43  20  5  1\n## 4 NA  9  13  3  0\n## 5 -4 40 -10  6  0\n## 6 NA 29  -6  5  1\nlibrary(\"mice\")                                 # Load mice package## \n## Attaching package: 'mice'## The following object is masked from 'package:stats':\n## \n##     filter## The following objects are masked from 'package:base':\n## \n##     cbind, rbind\n##### Impute data via predictive mean matching (single imputation)#####\n \nimp_single <- mice(data, m = 1, method = \"pmm\") # Impute missing values## \n##  iter imp variable\n##   1   1  y\n##   2   1  y\n##   3   1  y\n##   4   1  y\n##   5   1  y\ndata_imp_single <- complete(imp_single)         # Store imputed data\n# head(data_imp_single)\n\n# SInce single imputation underestiamtes stnadard errors, we use multiple imputaiton\n\n##### Predictive mean matching (multiple imputation)#####\n \nimp_multi <- mice(data, m = 5, method = \"pmm\")  # Impute missing values multiple times## \n##  iter imp variable\n##   1   1  y\n##   1   2  y\n##   1   3  y\n##   1   4  y\n##   1   5  y\n##   2   1  y\n##   2   2  y\n##   2   3  y\n##   2   4  y\n##   2   5  y\n##   3   1  y\n##   3   2  y\n##   3   3  y\n##   3   4  y\n##   3   5  y\n##   4   1  y\n##   4   2  y\n##   4   3  y\n##   4   4  y\n##   4   5  y\n##   5   1  y\n##   5   2  y\n##   5   3  y\n##   5   4  y\n##   5   5  y\ndata_imp_multi_all <- complete(imp_multi,       # Store multiply imputed data\n                           \"repeated\",\n                           include = TRUE)## New names:\n## * y -> y...1\n## * x1 -> x1...2\n## * x2 -> x2...3\n## * x3 -> x3...4\n## * x4 -> x4...5\n## * ...\ndata_imp_multi <- data.frame(                   # Combine imputed Y and X1-X4 (for convenience)\n  data_imp_multi_all[ , 1:6], data[, 2:5])\nhead(data_imp_multi)                            # First 6 rows of our multiply imputed data##   y.0 y.1 y.2 y.3 y.4 y.5 x1  x2 x3 x4\n## 1   8   8   8   8   8   8 38  -3  6  1\n## 2   1   1   1   1   1   1 50  -9  5  0\n## 3   5   5   5   5   5   5 43  20  5  1\n## 4  NA   1  -2  -4   9  -8  9  13  3  0\n## 5  -4  -4  -4  -4  -4  -4 40 -10  6  0\n## 6  NA   4   7   7   6   0 29  -6  5  1\nlibrary(mice)\nlibrary(VIM)## Warning: package 'VIM' was built under R version 4.0.5## Loading required package: colorspace## Warning: package 'colorspace' was built under R version 4.0.5## Loading required package: grid## VIM is ready to use.## Suggestions and bug-reports can be submitted at: https://github.com/statistikat/VIM/issues## \n## Attaching package: 'VIM'## The following object is masked from 'package:datasets':\n## \n##     sleep\nlibrary(lattice)## Warning: package 'lattice' was built under R version 4.0.5\nlibrary(ggplot2)## Warning: package 'ggplot2' was built under R version 4.0.5\n## set observations to NA\nanscombe <- within(anscombe, {\n    y1[1:3] <- NA\n    y4[3:5] <- NA\n})\n## view\nhead(anscombe)##   x1 x2 x3 x4   y1   y2    y3   y4\n## 1 10 10 10  8   NA 9.14  7.46 6.58\n## 2  8  8  8  8   NA 8.14  6.77 5.76\n## 3 13 13 13  8   NA 8.74 12.74   NA\n## 4  9  9  9  8 8.81 8.77  7.11   NA\n## 5 11 11 11  8 8.33 9.26  7.81   NA\n## 6 14 14 14  8 9.96 8.10  8.84 7.04\n## check missing data patterns\nmd.pattern(anscombe)##   x1 x2 x3 x4 y2 y3 y1 y4  \n## 6  1  1  1  1  1  1  1  1 0\n## 2  1  1  1  1  1  1  1  0 1\n## 2  1  1  1  1  1  1  0  1 1\n## 1  1  1  1  1  1  1  0  0 2\n##    0  0  0  0  0  0  3  3 6\n## Number of observations per patterns for all pairs of variables\np <- md.pairs(anscombe)\np # rr = number of observations where both pairs of values are observed## $rr\n##    x1 x2 x3 x4 y1 y2 y3 y4\n## x1 11 11 11 11  8 11 11  8\n## x2 11 11 11 11  8 11 11  8\n## x3 11 11 11 11  8 11 11  8\n## x4 11 11 11 11  8 11 11  8\n## y1  8  8  8  8  8  8  8  6\n## y2 11 11 11 11  8 11 11  8\n## y3 11 11 11 11  8 11 11  8\n## y4  8  8  8  8  6  8  8  8\n## \n## $rm\n##    x1 x2 x3 x4 y1 y2 y3 y4\n## x1  0  0  0  0  3  0  0  3\n## x2  0  0  0  0  3  0  0  3\n## x3  0  0  0  0  3  0  0  3\n## x4  0  0  0  0  3  0  0  3\n## y1  0  0  0  0  0  0  0  2\n## y2  0  0  0  0  3  0  0  3\n## y3  0  0  0  0  3  0  0  3\n## y4  0  0  0  0  2  0  0  0\n## \n## $mr\n##    x1 x2 x3 x4 y1 y2 y3 y4\n## x1  0  0  0  0  0  0  0  0\n## x2  0  0  0  0  0  0  0  0\n## x3  0  0  0  0  0  0  0  0\n## x4  0  0  0  0  0  0  0  0\n## y1  3  3  3  3  0  3  3  2\n## y2  0  0  0  0  0  0  0  0\n## y3  0  0  0  0  0  0  0  0\n## y4  3  3  3  3  2  3  3  0\n## \n## $mm\n##    x1 x2 x3 x4 y1 y2 y3 y4\n## x1  0  0  0  0  0  0  0  0\n## x2  0  0  0  0  0  0  0  0\n## x3  0  0  0  0  0  0  0  0\n## x4  0  0  0  0  0  0  0  0\n## y1  0  0  0  0  3  0  0  1\n## y2  0  0  0  0  0  0  0  0\n## y3  0  0  0  0  0  0  0  0\n## y4  0  0  0  0  1  0  0  3\n# rm = the number of observations where both variables are missing values\n# mr = the number of observations where the first variable’s value (e.g. the row variable) is observed and second (or column) variable is missing\n# mm = the number of observations where the second variable’s value (e.g. the col variable) is observed and first (or row) variable is missing\n\n## Margin plot of y1 and y4\nmarginplot(anscombe[c(5, 8)], col = c(\"blue\", \"red\", \"orange\"))\n## 5 imputations for all missing values\nimp1 <- mice(anscombe, m = 5)## \n##  iter imp variable\n##   1   1  y1  y4\n##   1   2  y1  y4\n##   1   3  y1  y4\n##   1   4  y1  y4\n##   1   5  y1  y4\n##   2   1  y1  y4\n##   2   2  y1  y4\n##   2   3  y1  y4\n##   2   4  y1  y4\n##   2   5  y1  y4\n##   3   1  y1  y4\n##   3   2  y1  y4\n##   3   3  y1  y4\n##   3   4  y1  y4\n##   3   5  y1  y4\n##   4   1  y1  y4\n##   4   2  y1  y4\n##   4   3  y1  y4\n##   4   4  y1  y4\n##   4   5  y1  y4\n##   5   1  y1  y4\n##   5   2  y1  y4\n##   5   3  y1  y4\n##   5   4  y1  y4\n##   5   5  y1  y4## Warning: Number of logged events: 52\n## linear regression for each imputed data set - 5 regression are run\nfitm <- with(imp1, lm(y1 ~ y4 + x1))\nsummary(fitm)## # A tibble: 15 x 6\n##    term        estimate std.error statistic p.value  nobs\n##    <chr>          <dbl>     <dbl>     <dbl>   <dbl> <int>\n##  1 (Intercept)    8.60      2.67      3.23  0.0121     11\n##  2 y4            -0.533     0.251    -2.12  0.0667     11\n##  3 x1             0.334     0.155     2.16  0.0628     11\n##  4 (Intercept)    4.19      2.93      1.43  0.190      11\n##  5 y4            -0.213     0.273    -0.782 0.457      11\n##  6 x1             0.510     0.167     3.05  0.0159     11\n##  7 (Intercept)    6.51      2.35      2.77  0.0244     11\n##  8 y4            -0.347     0.215    -1.62  0.145      11\n##  9 x1             0.395     0.132     3.00  0.0169     11\n## 10 (Intercept)    5.48      3.02      1.81  0.107      11\n## 11 y4            -0.316     0.282    -1.12  0.295      11\n## 12 x1             0.486     0.173     2.81  0.0230     11\n## 13 (Intercept)    7.12      1.81      3.92  0.00439    11\n## 14 y4            -0.436     0.173    -2.53  0.0355     11\n## 15 x1             0.425     0.102     4.18  0.00308    11\n## pool coefficients and standard errors across all 5 regression models\npool(fitm)## Class: mipo    m = 5 \n##          term m   estimate       ubar           b           t dfcom       df\n## 1 (Intercept) 5  6.3808015 6.72703243 2.785088109 10.06913816     8 3.902859\n## 2          y4 5 -0.3690455 0.05860053 0.014674911  0.07621042     8 4.716160\n## 3          x1 5  0.4301588 0.02191260 0.004980516  0.02788922     8 4.856052\n##         riv    lambda       fmi\n## 1 0.4968172 0.3319158 0.5254832\n## 2 0.3005074 0.2310693 0.4303733\n## 3 0.2727480 0.2142985 0.4143230\n## output parameter estimates\nsummary(pool(fitm))##          term   estimate std.error statistic       df    p.value\n## 1 (Intercept)  6.3808015 3.1731905  2.010847 3.902859 0.11643863\n## 2          y4 -0.3690455 0.2760624 -1.336819 4.716160 0.24213491\n## 3          x1  0.4301588 0.1670007  2.575791 4.856052 0.05107581"},{"path":"imputation-missing-data.html","id":"stochastic-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.4.4 Stochastic Imputation","text":"Regression imputation + random residual = Stochastic ImputationMost multiple imputation based form stochastic regression imputation.Good:advantage Regression Imputationand also random componentsBad:might lead implausible values (e.g. negative values)can’t handle heteroskadastic dataNote\nMultiple Imputation usually based form stochastic regression imputation.Single stochastic regression imputationSingle predictive mean matchingStochastic regression imputation contains negative valuesProof heteroskadastic dataSingle stochastic regression imputationSingle predictive mean matchingComparison predictive mean matching stochastic regression imputation","code":"\n# Income data\n \nset.seed(91919)                              # Set seed\nN <- 1000                                    # Sample size\n \nincome <- round(rnorm(N, 0, 500))            # Create some synthetic income data\nincome[income < 0] <- income[income < 0] * (- 1)\n \nx1 <- income + rnorm(N, 1000, 1500)          # Auxiliary variables\nx2 <- income + rnorm(N, - 5000, 2000)\n \nincome[rbinom(N, 1, 0.1) == 1] <- NA         # Create 10% missingness in income\n \ndata_inc_miss <- data.frame(income, x1, x2)\nimp_inc_sri <- mice(data_inc_miss, method = \"norm.nob\", m = 1)## \n##  iter imp variable\n##   1   1  income\n##   2   1  income\n##   3   1  income\n##   4   1  income\n##   5   1  income\ndata_inc_sri <- complete(imp_inc_sri)\nimp_inc_pmm <- mice(data_inc_miss, method = \"pmm\", m = 1)## \n##  iter imp variable\n##   1   1  income\n##   2   1  income\n##   3   1  income\n##   4   1  income\n##   5   1  income\ndata_inc_pmm <- complete(imp_inc_pmm)\ndata_inc_sri$income[data_inc_sri$income < 0]## [1]  -66.055957  -96.980053  -28.921432   -4.175686  -54.480798  -27.207102\n## [7] -143.603500  -80.960488\ndata_inc_pmm$income[data_inc_pmm$income < 0] # No values below 0## numeric(0)\n# Heteroscedastic data\n \nset.seed(654654)                             # Set seed\nN <- 1:5000                                  # Sample size\n \na <- 0\nb <- 1\nsigma2 <- N^2\neps <- rnorm(N, mean = 0, sd = sqrt(sigma2))\n \ny <- a + b * N + eps                         # Heteroscedastic variable\nx <- 30 * N + rnorm(N[length(N)], 1000, 200) # Correlated variable\n \ny[rbinom(N[length(N)], 1, 0.3) == 1] <- NA   # 30% missings\n \ndata_het_miss <- data.frame(y, x)\nimp_het_sri <- mice(data_het_miss, method = \"norm.nob\", m = 1)## \n##  iter imp variable\n##   1   1  y\n##   2   1  y\n##   3   1  y\n##   4   1  y\n##   5   1  y\ndata_het_sri <- complete(imp_het_sri)\nimp_het_pmm <- mice(data_het_miss, method = \"pmm\", m = 1)## \n##  iter imp variable\n##   1   1  y\n##   2   1  y\n##   3   1  y\n##   4   1  y\n##   5   1  y\ndata_het_pmm <- complete(imp_het_pmm)\npar(mfrow = c(1, 2))                              # Both plots in one graphic\n \nplot(x[!is.na(data_het_sri$y)],                   # Plot of observed values\n     data_het_sri$y[!is.na(data_het_sri$y)],\n     main = \"\",\n     xlab = \"X\", ylab = \"Y\")\npoints(x[is.na(y)], data_het_sri$y[is.na(y)],     # Plot of missing values\n       col = \"red\")\ntitle(\"Stochastic Regression Imputation\",         # Title of plot\n      line = 0.5)\nabline(lm(y ~ x, data_het_sri),                   # Regression line\n       col = \"#1b98e0\", lwd = 2.5)\nlegend(\"topleft\",                                 # Legend\n       c(\"Observed Values\", \"Imputed Values\", \"Regression Y ~ X\"),\n       pch = c(1, 1, NA),\n       lty = c(NA, NA, 1),\n       col = c(\"black\", \"red\", \"#1b98e0\"))\n \nplot(x[!is.na(data_het_pmm$y)],                   # Plot of observed values\n     data_het_pmm$y[!is.na(data_het_pmm$y)],\n     main = \"\",\n     xlab = \"X\", ylab = \"Y\")\npoints(x[is.na(y)], data_het_pmm$y[is.na(y)],     # Plot of missing values\n       col = \"red\")\ntitle(\"Predictive Mean Matching\",                 # Title of plot\n      line = 0.5)\nabline(lm(y ~ x, data_het_pmm),\n       col = \"#1b98e0\", lwd = 2.5)\nlegend(\"topleft\",                                 # Legend\n       c(\"Observed Values\", \"Imputed Values\", \"Regression Y ~ X\"),\n       pch = c(1, 1, NA),\n       lty = c(NA, NA, 1),\n       col = c(\"black\", \"red\", \"#1b98e0\"))\n \nmtext(\"Imputation of Heteroscedastic Data\",       # Main title of plot\n      side = 3, line = - 1.5, outer = TRUE, cex = 2)"},{"path":"imputation-missing-data.html","id":"regression-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.5 Regression Imputation","text":"Also known conditional mean imputation Missing value based (regress) variables.Good:\nMaintain relationship variables\ndata MCAR, least-squares coefficients estimates consistent, approximately unbiased large samples (Gourieroux Monfort 1981)\nCan improvement efficiency using weighted least squares (Beale Little 1975) generalized least squares (Gourieroux Monfort 1981).\n\nGood:Maintain relationship variablesMaintain relationship variablesIf data MCAR, least-squares coefficients estimates consistent, approximately unbiased large samples (Gourieroux Monfort 1981)\nCan improvement efficiency using weighted least squares (Beale Little 1975) generalized least squares (Gourieroux Monfort 1981).\ndata MCAR, least-squares coefficients estimates consistent, approximately unbiased large samples (Gourieroux Monfort 1981)Can improvement efficiency using weighted least squares (Beale Little 1975) generalized least squares (Gourieroux Monfort 1981).Bad:\nvariability left. treated data collected.\nUnderestimate standard errors overestimate test statistics\nBad:variability left. treated data collected.Underestimate standard errors overestimate test statistics","code":""},{"path":"imputation-missing-data.html","id":"interpolation-and-extrapolation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.6 Interpolation and Extrapolation","text":"estimated value observations individual. usually works longitudinal data.","code":""},{"path":"imputation-missing-data.html","id":"k-nearest-neighbor-knn-imputation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.7 K-nearest neighbor (KNN) imputation","text":"methods model-based imputation (regression).\nexample neighbor-based imputation (K-nearest neighbor).every observation needs imputed, algorithm identifies ‘k’ closest observations based types distance (e.g., Euclidean) computes weighted average (weighted based distance) ‘k’ obs.discrete variable, uses frequent value among k nearest neighbors.Distance metrics: Hamming distance.continuous variable, uses mean mode.Distance metrics:\nEuclidean\nMahalanobis\nManhattan\nDistance metrics:EuclideanMahalanobisManhattan","code":""},{"path":"imputation-missing-data.html","id":"bayesian-ridge-regression-implementation","chapter":"11 Imputation (Missing Data)","heading":"11.2.4.8 Bayesian Ridge regression implementation","text":"","code":""},{"path":"imputation-missing-data.html","id":"other-methods","chapter":"11 Imputation (Missing Data)","heading":"11.2.5 Other methods","text":"panel data, clustered data, use pan package Schafer (1997)","code":""},{"path":"imputation-missing-data.html","id":"criteria-for-choosing-an-effective-approach","chapter":"11 Imputation (Missing Data)","heading":"11.3 Criteria for Choosing an Effective Approach","text":"Criteria ideal technique treating missing data:Unbiased parameter estimatesAdequate powerAccurate standard errors (p-values, confidence intervals)Multiple Imputation Full Information Maximum Likelihood ideal candidate. Single imputation generally lead underestimation standard errors.","code":""},{"path":"imputation-missing-data.html","id":"another-perspective","chapter":"11 Imputation (Missing Data)","heading":"11.4 Another Perspective","text":"Model bias can arisen various factors including:Imputation methodMissing data mechanism (MCAR vs. MAR)Proportion missing dataInformation available data setSince imputed observations estimates, values corresponding random error. put estimate data point, software doesn’t know . overlooks extra source error, resulting -small standard errors -small p-values. multiple imputation comes multiple estimates.multiple imputation random component, multiple estimates slightly different. re-introduces variation software can incorporate order give model accurate estimates standard error. Multiple imputation huge breakthrough statistics 20 years ago. solves lot problems missing data (though, unfortunately ) done well, leads unbiased parameter estimates accurate standard errors. rate missing data , small (2-3%) doesn’t matter technique use.Remember three goals multiple imputation, missing data technique:Unbiased parameter estimates final analysis (regression coefficients, group means, odds ratios, etc.)accurate standard errors parameter estimates, therefore, accurate p-values analysisadequate power find meaningful parameter values significant.Hence,Don’t round imputations dummy variables. Many common imputation techniques, like MCMC, require normally distributed variables. Suggestions imputing categorical variables dummy code , impute , round imputed values 0 1. Recent research, however, found rounding imputed values actually leads biased parameter estimates analysis model. actually get better results leaving imputed values impossible values, even though ’s counter-intuitive.Don’t round imputations dummy variables. Many common imputation techniques, like MCMC, require normally distributed variables. Suggestions imputing categorical variables dummy code , impute , round imputed values 0 1. Recent research, however, found rounding imputed values actually leads biased parameter estimates analysis model. actually get better results leaving imputed values impossible values, even though ’s counter-intuitive.Don’t transform skewed variables. Likewise, transform variable meet normality assumptions imputing, changing distribution variable relationship variable others use impute. can lead imputing outliers, creating bias just imputing skewed variable.Don’t transform skewed variables. Likewise, transform variable meet normality assumptions imputing, changing distribution variable relationship variable others use impute. can lead imputing outliers, creating bias just imputing skewed variable.Use imputations. advice years 5-10 imputations adequate. true unbiasedness, can get inconsistent results run multiple imputation . (Bodner 2008) recommends many imputations percentage missing data. Since running imputations isn’t work data analyst, ’s reason .Use imputations. advice years 5-10 imputations adequate. true unbiasedness, can get inconsistent results run multiple imputation . (Bodner 2008) recommends many imputations percentage missing data. Since running imputations isn’t work data analyst, ’s reason .Create multiplicative terms imputing. analysis model contains multiplicative term, like interaction term quadratic, create multiplicative terms first, impute. Imputing first, creating multiplicative terms actually biases regression parameters multiplicative term (Hippel 2009)Create multiplicative terms imputing. analysis model contains multiplicative term, like interaction term quadratic, create multiplicative terms first, impute. Imputing first, creating multiplicative terms actually biases regression parameters multiplicative term (Hippel 2009)","code":""},{"path":"imputation-missing-data.html","id":"diagnosing-the-mechanism","chapter":"11 Imputation (Missing Data)","heading":"11.5 Diagnosing the Mechanism","text":"","code":""},{"path":"imputation-missing-data.html","id":"mar-vs.-mnar","chapter":"11 Imputation (Missing Data)","heading":"11.5.1 MAR vs. MNAR","text":"true way distinguish MNAR MAR measure missing data.’s common practice among professional surveyors , example, follow-paper survey phone calls group non-respondents ask key survey items. allows compare respondents non-respondents.responses key items differ much, ’s good evidence data MNAR.However missing data situations, can’t get hold missing data. can’t test directly, can examine patterns data get idea ’s likely mechanism.first thing diagnosing randomness missing data use substantive scientific knowledge data field. sensitive issue, less likely people tell . ’re going tell much cocaine usage phone usage.Likewise, many fields common research situations non-ignorable data common. Educate field’s literature.","code":""},{"path":"imputation-missing-data.html","id":"mcar-vs.-mar","chapter":"11 Imputation (Missing Data)","heading":"11.5.2 MCAR vs. MAR","text":"useful test MCAR, Little’s test.second technique create dummy variables whether variable missing.1 = missing 0 = observedYou can run t-tests chi-square tests variable variables data set see missingness variable related values variables.example, women really less likely tell weight men, chi-square test tell percentage missing data weight variable higher women men.","code":""},{"path":"imputation-missing-data.html","id":"application-8","chapter":"11 Imputation (Missing Data)","heading":"11.6 Application","text":"many imputation:Usually 5. (unless extremely high portion missing, case probably need check data )According Rubin, relative efficiency estimate based m imputations infinity imputation approximately\\[\n(1+\\frac{\\lambda}{m})^{-1}\n\\]\\(\\lambda\\) rate missing dataExample 50% missing data means estimate based 5 imputation standard deviation 5% wider compared estimate based infinity imputation\n(\\(\\sqrt{1+0.5/5}=1.049\\))","code":"\nlibrary(missForest)## Loading required package: randomForest## Warning: package 'randomForest' was built under R version 4.0.5## randomForest 4.6-14## Type rfNews() to see new features/changes/bug fixes.## \n## Attaching package: 'randomForest'## The following object is masked from 'package:ggplot2':\n## \n##     margin## Loading required package: foreach## Loading required package: itertools## Loading required package: iterators## \n## Attaching package: 'missForest'## The following object is masked from 'package:VIM':\n## \n##     nrmse\n#load data\ndata <- iris\n\n#Generate 10% missing values at Random\nset.seed(1)\niris.mis <- prodNA(iris, noNA = 0.1)\n\n#remove categorical variables\niris.mis.cat <- iris.mis\niris.mis <- subset(iris.mis, select = -c(Species))"},{"path":"imputation-missing-data.html","id":"imputation-with-mean-median-mode","chapter":"11 Imputation (Missing Data)","heading":"11.6.1 Imputation with mean / median / mode","text":"check accurary","code":"\n# whole data set\ne1071::impute(iris.mis, what = \"mean\") # replace with mean\ne1071::impute(iris.mis, what = \"median\") # replace with median\n\n# by variables\nHmisc::impute(iris.mis$Sepal.Length, mean)  # mean\nHmisc::impute(iris.mis$Sepal.Length, median)  # median\nHmisc::impute(iris.mis$Sepal.Length, 0)  # replace specific number\nlibrary(DMwR)## Registered S3 method overwritten by 'quantmod':\n##   method            from\n##   as.zoo.data.frame zoo## \n## Attaching package: 'DMwR'## The following object is masked from 'package:VIM':\n## \n##     kNN\nactuals <- iris$Sepal.Width[is.na(iris.mis$Sepal.Width)]\npredicteds <- rep(mean(iris$Sepal.Width, na.rm=T), length(actuals))\nregr.eval(actuals, predicteds)##       mae       mse      rmse      mape \n## 0.2870303 0.1301598 0.3607767 0.1021485"},{"path":"imputation-missing-data.html","id":"knn","chapter":"11 Imputation (Missing Data)","heading":"11.6.2 KNN","text":"Compared mape (mean absolute percentage error) mean imputation, see almost always see improvements.","code":"\nlibrary(DMwR)\n# iris.mis[,!names(iris.mis) %in% c(\"Sepal.Length\")] \n# data should be this line. But since knn cant work with 3 or less variables, we need to use at least 4 variables. \n\n# knn is not appropriate for categorical variables\nknnOutput <-\n    knnImputation(data = iris.mis.cat, \n                  #k = 10, \n                  meth = \"median\" # could use \"median\" or \"weighAvg\"\n                  )  # should exclude the dependent variable: Sepal.Length\nanyNA(knnOutput)## [1] FALSE\nlibrary(DMwR)\nactuals <- iris$Sepal.Width[is.na(iris.mis$Sepal.Width)]\npredicteds <- knnOutput[is.na(iris.mis$Sepal.Width), \"Sepal.Width\"]\nregr.eval(actuals, predicteds)##       mae       mse      rmse      mape \n## 0.2318182 0.1038636 0.3222788 0.0823571"},{"path":"imputation-missing-data.html","id":"rpart","chapter":"11 Imputation (Missing Data)","heading":"11.6.3 rpart","text":"categorical (factor) variables, rpart can handle","code":"\nlibrary(rpart)\nclass_mod <- rpart(Species ~ . - Sepal.Length, data=iris.mis.cat[!is.na(iris.mis.cat$Species), ], method=\"class\", na.action=na.omit)  # since Species is a factor, and exclude dependent variable \"Sepal.Length\"\n\nanova_mod <- rpart(Sepal.Width ~ . - Sepal.Length, data=iris.mis[!is.na(iris.mis$Sepal.Width), ], method=\"anova\", na.action=na.omit)  # since Sepal.Width is numeric.\nspecies_pred <- predict(class_mod, iris.mis.cat[is.na(iris.mis.cat$Species), ])\nwidth_pred <- predict(anova_mod, iris.mis[is.na(iris.mis$Sepal.Width), ])"},{"path":"imputation-missing-data.html","id":"mice-multivariate-imputation-via-chained-equations","chapter":"11 Imputation (Missing Data)","heading":"11.6.4 MICE (Multivariate Imputation via Chained Equations)","text":"Assumption: data MARIt imputes data per variable specifying imputation model variableExampleWe \\(X_1, X_2,..,X_k\\). \\(X_1\\) missing data, regressed rest variables. procedure applies \\(X_2\\) missing data. , predicted values used place missing values.default,Continuous variables use linear regression.Categorical Variables use logistic regression.Methods MICE:PMM (Predictive Mean Matching) – numeric variableslogreg(Logistic Regression) – Binary Variables( 2 levels)polyreg(Bayesian polytomous regression) – Factor Variables (>= 2 levels)Proportional odds model (ordered, >= 2 levels)Impute DataCheck imputed datasetRegression model using imputed datasets","code":"\n# load package\nlibrary(mice)\nlibrary(VIM)\n\n# check missing values\nmd.pattern(iris.mis)##     Sepal.Width Sepal.Length Petal.Length Petal.Width   \n## 100           1            1            1           1  0\n## 15            1            1            1           0  1\n## 8             1            1            0           1  1\n## 2             1            1            0           0  2\n## 11            1            0            1           1  1\n## 1             1            0            1           0  2\n## 1             1            0            0           1  2\n## 1             1            0            0           0  3\n## 7             0            1            1           1  1\n## 3             0            1            0           1  2\n## 1             0            0            1           1  2\n##              11           15           15          19 60\n#plot the missing values\naggr(iris.mis, col=mdc(1:2), numbers=TRUE, sortVars=TRUE, labels=names(iris.mis), cex.axis=.7, gap=3, ylab=c(\"Proportion of missingness\",\"Missingness Pattern\"))## \n##  Variables sorted by number of missings: \n##      Variable      Count\n##   Petal.Width 0.12666667\n##  Sepal.Length 0.10000000\n##  Petal.Length 0.10000000\n##   Sepal.Width 0.07333333\nmice_plot <- aggr(iris.mis, col=c('navyblue','yellow'),\n                    numbers=TRUE, sortVars=TRUE,\n                    labels=names(iris.mis), cex.axis=.7,\n                    gap=3, ylab=c(\"Missing data\",\"Pattern\"))## \n##  Variables sorted by number of missings: \n##      Variable      Count\n##   Petal.Width 0.12666667\n##  Sepal.Length 0.10000000\n##  Petal.Length 0.10000000\n##   Sepal.Width 0.07333333\nimputed_Data <-\n    mice(\n        iris.mis,\n        m = 5, # number of imputed datasets\n        maxit = 50, # number of iterations taken to impute missing values\n        method = 'pmm', # method used in imputation. Here, we used predictive mean matching\n        # other methods can be \n        # \"pmm\": Predictive mean matching\n        # \"midastouch\" : weighted predictive mean matching\n        # \"sample\": Random sample from observed values\n        # \"cart\": classification and regression trees\n        # \"rf\": random forest imputations.\n        # \"2lonly.pmm\": Level-2 class predictive mean matching\n        # Other methods based on whether variables are (1) numeric, (2) binary, (3) ordered, (4), unordered\n        seed = 500\n    )\nsummary(imputed_Data)## Class: mids\n## Number of multiple imputations:  5 \n## Imputation methods:\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width \n##        \"pmm\"        \"pmm\"        \"pmm\"        \"pmm\" \n## PredictorMatrix:\n##              Sepal.Length Sepal.Width Petal.Length Petal.Width\n## Sepal.Length            0           1            1           1\n## Sepal.Width             1           0            1           1\n## Petal.Length            1           1            0           1\n## Petal.Width             1           1            1           0\n#make a density plot\ndensityplot(imputed_Data)\n#the red (imputed values) should be similar to the blue (observed)\n# 1st dataset \ncompleteData <- complete(imputed_Data,1)\n\n# 2nd dataset\ncomplete(imputed_Data,2)\n# regression model\nfit <- with(data = imputed_Data, exp = lm(Sepal.Width ~ Sepal.Length + Petal.Width)) \n\n#combine results of all 5 models\ncombine <- pool(fit)\nsummary(combine)##           term   estimate  std.error statistic       df      p.value\n## 1  (Intercept)  1.8963130 0.32453912  5.843095 131.0856 3.838556e-08\n## 2 Sepal.Length  0.2974293 0.06679204  4.453066 130.2103 1.802241e-05\n## 3  Petal.Width -0.4811603 0.07376809 -6.522608 108.8253 2.243032e-09"},{"path":"imputation-missing-data.html","id":"amelia","chapter":"11 Imputation (Missing Data)","heading":"11.6.5 Amelia","text":"Use bootstrap based EMB algorithm (faster robust impute many variables including cross sectional, time series data etc)Use parallel imputation feature using multicore CPUs.AssumptionsAll variables follow Multivariate Normal Distribution (MVN). Hence, package works best data MVN, transformation normality.Missing data Missing Random (MAR)Steps:m bootstrap samples applies EMB algorithm sample. m different estimates mean variances.first set estimates used impute first set missing values using regression, second set estimates used second set .However, Amelia different MICEMICE imputes data variable variable basis whereas MVN uses joint modeling approach based multivariate normal distribution.MICE can handle different types variables variables MVN need normally distributed transformed approximate normality.MICE can manage imputation variables defined subset data whereas MVN .","code":"\nlibrary(Amelia)## Warning: package 'Amelia' was built under R version 4.0.5## Loading required package: Rcpp## Warning: package 'Rcpp' was built under R version 4.0.5## ## \n## ## Amelia II: Multiple Imputation\n## ## (Version 1.8.0, built: 2021-05-26)\n## ## Copyright (C) 2005-2021 James Honaker, Gary King and Matthew Blackwell\n## ## Refer to http://gking.harvard.edu/amelia/ for more information\n## ##\ndata(\"iris\")\n#seed 10% missing values\niris.mis <- prodNA(iris, noNA = 0.1)\n\n# idvars – keep all ID variables and other variables which you don’t want to impute\n# noms – keep nominal variables here\n\n#specify columns and run amelia\namelia_fit <- amelia(iris.mis, m=5, parallel = \"multicore\", noms = \"Species\")## -- Imputation 1 --\n## \n##   1  2  3  4  5  6  7  8\n## \n## -- Imputation 2 --\n## \n##   1  2  3  4  5  6  7  8\n## \n## -- Imputation 3 --\n## \n##   1  2  3  4  5\n## \n## -- Imputation 4 --\n## \n##   1  2  3  4  5  6  7\n## \n## -- Imputation 5 --\n## \n##   1  2  3  4  5  6  7\n# access imputed outputs\n# amelia_fit$imputations[[1]]"},{"path":"imputation-missing-data.html","id":"missforest","chapter":"11 Imputation (Missing Data)","heading":"11.6.6 missForest","text":"implementation random forest algorithm (non parametric imputation method applicable various variable types). Hence, assumption function form f. Instead, tries estimate f can close data points possible.builds random forest model variable. uses model predict missing values variable help observed values.yields bag imputation error estimate. Moreover, provides high level control imputation process.Since bagging works well categorical variable , don’t need remove .means categorical variables imputed 5% error continuous variables imputed 14% error.can improved tuning values mtry ntree parameter.mtry refers number variables randomly sampled split.ntree refers number trees grow forest.","code":"\nlibrary(missForest)\n#impute missing values, using all parameters as default values\niris.imp <- missForest(iris.mis)##   missForest iteration 1 in progress...done!\n##   missForest iteration 2 in progress...done!\n##   missForest iteration 3 in progress...done!\n##   missForest iteration 4 in progress...done!\n# check imputed values\n# iris.imp$ximp\n\n\n# check imputation error\n# NRMSE is normalized mean squared error. It is used to represent error derived from imputing continuous values. \n# PFC (proportion of falsely classified) is used to represent error derived from imputing categorical values.\niris.imp$OOBerror##      NRMSE        PFC \n## 0.13631893 0.04477612\n#comparing actual data accuracy\niris.err <- mixError(iris.imp$ximp, iris.mis, iris)\niris.err##     NRMSE       PFC \n## 0.1501524 0.0625000"},{"path":"imputation-missing-data.html","id":"hmisc","chapter":"11 Imputation (Missing Data)","heading":"11.6.7 Hmisc","text":"impute() function imputes missing value using user defined statistical method (mean, max, mean). ’s default median.aregImpute() allows mean imputation using additive regression, bootstrapping, predictive mean matching.bootstrapping, different bootstrap resamples used multiple imputations. , flexible additive model (non parametric regression method) fitted samples taken replacements original data missing values (acts dependent variable) predicted using non-missing values (independent variable).uses predictive mean matching (default) impute missing values. Predictive mean matching works well continuous categorical (binary & multi-level) without need computing residuals maximum likelihood fit.NoteFor predicting categorical variables, Fisher’s optimum scoring method used.Hmisc automatically recognizes variables types uses bootstrap sample predictive mean matching impute missing values.missForest can outperform Hmisc observed variables sufficient information.Assumptionlinearity variables predicted.","code":"\nlibrary(Hmisc)## Warning: package 'Hmisc' was built under R version 4.0.5## Loading required package: survival## Warning: package 'survival' was built under R version 4.0.5## Loading required package: Formula## \n## Attaching package: 'Hmisc'## The following objects are masked from 'package:base':\n## \n##     format.pval, units\n# impute with mean value\niris.mis$imputed_age <- with(iris.mis, impute(Sepal.Length, mean))\n\n# impute with random value\niris.mis$imputed_age2 <- with(iris.mis, impute(Sepal.Length, 'random'))\n\n# could also use min, max, median to impute missing value\n\n# using argImpute\nimpute_arg <- aregImpute(~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width +\nSpecies, data = iris.mis, n.impute = 5) # argImpute() automatically identifies the variable type and treats them accordingly.## Iteration 1 \nIteration 2 \nIteration 3 \nIteration 4 \nIteration 5 \nIteration 6 \nIteration 7 \nIteration 8 \nimpute_arg # R-squares are for predicted missing values.## \n## Multiple Imputation using Bootstrap and PMM\n## \n## aregImpute(formula = ~Sepal.Length + Sepal.Width + Petal.Length + \n##     Petal.Width + Species, data = iris.mis, n.impute = 5)\n## \n## n: 150   p: 5    Imputations: 5      nk: 3 \n## \n## Number of NAs:\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n##           11           11           13           24           16 \n## \n##              type d.f.\n## Sepal.Length    s    2\n## Sepal.Width     s    2\n## Petal.Length    s    2\n## Petal.Width     s    2\n## Species         c    2\n## \n## Transformation of Target Variables Forced to be Linear\n## \n## R-squares for Predicting Non-Missing Values for Each Variable\n## Using Last Imputations of Predictors\n## Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n##        0.907        0.660        0.978        0.963        0.993\n# check imputed variable Sepal.Length\nimpute_arg$imputed$Sepal.Length##     [,1] [,2] [,3] [,4] [,5]\n## 19   5.2  5.2  5.2  5.8  5.7\n## 21   5.1  5.0  5.1  5.7  5.4\n## 31   4.8  5.0  5.2  5.0  4.8\n## 35   4.6  4.9  4.9  4.9  4.8\n## 49   5.0  5.1  5.1  5.1  5.1\n## 62   6.2  5.7  6.0  6.4  5.6\n## 65   5.5  5.5  5.2  5.8  5.5\n## 67   6.5  5.8  5.8  6.3  6.5\n## 82   5.2  5.1  5.7  5.8  5.5\n## 113  6.4  6.5  7.4  7.2  6.3\n## 122  6.2  5.8  5.5  5.8  6.7"},{"path":"imputation-missing-data.html","id":"mi","chapter":"11 Imputation (Missing Data)","heading":"11.6.8 mi","text":"allows graphical diagnostics imputation models convergence imputation process.uses Bayesian version regression models handle issue separation.automatically detects irregularities data (e.g., high collinearity among variables).adds noise imputation process solve problem additive constraints.","code":"\nlibrary(mi)\n# default values of parameters\n# 1. rand.imp.method as “bootstrap”\n# 2. n.imp (number of multiple imputations) as 3\n# 3. n.iter ( number of iterations) as 30\nmi_data <- mi(iris.mis, seed = 335)\nsummary(mi_data)"},{"path":"data.html","id":"data","chapter":"12 Data","heading":"12 Data","text":"multiple ways categorize data. example,Qualitative vs. Quantitative:","code":""},{"path":"data.html","id":"cross-sectional","chapter":"12 Data","heading":"12.1 Cross-Sectional","text":"","code":""},{"path":"data.html","id":"time-series-1","chapter":"12 Data","heading":"12.2 Time Series","text":"\\[\ny_t = \\beta_0 + x_{t1}\\beta_1 + x_{t2}\\beta_2 + ... + x_{t(k-1)}\\beta_{k-1} + \\epsilon_t\n\\]ExamplesStatic Model\n\\(y_t=\\beta_0 + x_1\\beta_1 + x_2\\beta_2 - x_3\\beta_3 - \\epsilon_t\\)\nStatic Model\\(y_t=\\beta_0 + x_1\\beta_1 + x_2\\beta_2 - x_3\\beta_3 - \\epsilon_t\\)Finite Distributed Lag model\n\\(y_t=\\beta_0 + pe_t\\delta_0 + pe_{t-1}\\delta_1 +pe_{t-2}\\delta_2 + \\epsilon_t\\)\nLong Run Propensity (LRP) \\(LRP = \\delta_0 + \\delta_1 + \\delta_2\\)\nFinite Distributed Lag model\\(y_t=\\beta_0 + pe_t\\delta_0 + pe_{t-1}\\delta_1 +pe_{t-2}\\delta_2 + \\epsilon_t\\)Long Run Propensity (LRP) \\(LRP = \\delta_0 + \\delta_1 + \\delta_2\\)Dynamic Model\n\\(GDP_t = \\beta_0 + \\beta_1GDP_{t-1} - \\epsilon_t\\)\nDynamic Model\\(GDP_t = \\beta_0 + \\beta_1GDP_{t-1} - \\epsilon_t\\)Finite Sample Properties Time Series:A1-A3: OLS unbiasedA1-A4: usual standard errors consistent Gauss-Markov Theorem holds (OLS BLUE)A1-A6, A6: Finite Sample Wald Test (t-test F-test) validA3 might hold time series settingSpurious Time Trend - solvableStrict vs Contemporaneous Exogeneity - solvableIn time series data, many processes:Autoregressive model order p: AR(p)Moving average model order q: MA(q)Autoregressive model order p moving average model order q: ARMA(p,q)Autoregressive conditional heteroskedasticity model order p: ARCH(p)Generalized Autoregressive conditional heteroskedasticity orders p q; GARCH(p.q)","code":""},{"path":"data.html","id":"deterministic-time-trend","chapter":"12 Data","heading":"12.2.1 Deterministic Time trend","text":"dependent independent variables trending timeSpurious Time Series Regression\\[\ny_t = \\alpha_0 + t\\alpha_1 + v_t\n\\]x takes form\\[\nx_t = \\lambda_0 + t\\lambda_1 + u_t\n\\]\\(\\alpha_1 \\neq 0\\) \\(\\lambda_1 \\neq 0\\)\\(v_t\\) \\(u_t\\) independentthere relationship \\(y_t\\) \\(x_t\\)estimate regression,\\[\ny_t = \\beta_0 + x_t\\beta_1 + \\epsilon_t\n\\]true \\(\\beta_1=0\\)Inconsistent: \\(plim(\\hat{\\beta}_1)=\\frac{\\alpha_1}{\\lambda_1}\\)Invalid Inference: \\(|t| \\^d \\infty\\) \\(H_0: \\beta_1=0\\), always reject null \\(n \\\\infty\\)Uninformative \\(R^2\\): \\(plim(R^2) = 1\\) able perfectly predict \\(n \\\\infty\\)can rewrite equation \\[\ny_t=\\beta_0 + \\beta_1x_t+\\epsilon_t \\\\\n\\epsilon_t = \\alpha_1t + v_t\n\\]\\(\\beta_0 = \\alpha_0\\) \\(\\beta_1=0\\). Since \\(x_t\\) deterministic function time, \\(\\epsilon_t\\) correlated \\(x_t\\) usual omitted variable bias.\nEven \\(y_t\\) \\(x_t\\) related (\\(\\beta_1 \\neq 0\\)) trending time, still get spurious results simple regression \\(y_t\\) \\(x_t\\)Solutions Spurious TrendInclude time trend t additional control\nconsistent parameter estimates valid inference\nInclude time trend t additional controlconsistent parameter estimates valid inferenceDetrend dependent independent variables regress detrended outcome detrended independent variables (.e., regress residuals \\(\\hat{u}_t\\) residuals \\(\\hat{v}_t\\))\nDetrending partialling Frisch-Waugh-Lovell Theorem\nallow non-linear time trends including t \\(t^2\\), exp(t)\nAllow seasonality including indicators relevant “seasons” (quarters, months, weeks).\n\nDetrend dependent independent variables regress detrended outcome detrended independent variables (.e., regress residuals \\(\\hat{u}_t\\) residuals \\(\\hat{v}_t\\))Detrending partialling Frisch-Waugh-Lovell Theorem\nallow non-linear time trends including t \\(t^2\\), exp(t)\nAllow seasonality including indicators relevant “seasons” (quarters, months, weeks).\nDetrending partialling Frisch-Waugh-Lovell TheoremCould allow non-linear time trends including t \\(t^2\\), exp(t)Allow seasonality including indicators relevant “seasons” (quarters, months, weeks).A3 hold :Feedback Effect\n\\(\\epsilon_t\\) influences next period’s independent variables\nFeedback Effect\\(\\epsilon_t\\) influences next period’s independent variablesDynamic Specification\ninclude last time period outcome explanatory variable\nDynamic Specificationinclude last time period outcome explanatory variableDynamically Complete\nfinite distrusted lag model, number lags needs absolutely correct.\nDynamically CompleteFor finite distrusted lag model, number lags needs absolutely correct.","code":""},{"path":"data.html","id":"feedback-effect","chapter":"12 Data","heading":"12.2.2 Feedback Effect","text":"\\[\ny_t = \\beta_0 + x_t\\beta_1 + \\epsilon_t\n\\]A3\\[\nE(\\epsilon_t|\\mathbf{X})= E(\\epsilon_t| x_1,x_2, ...,x_t,x_{t+1},...,x_T)\n\\]equal 0, \\(y_t\\) likely influence \\(x_{t+1},..,x_T\\)A3 violated require error uncorrelated time observation independent regressors (strict exogeneity)","code":""},{"path":"data.html","id":"dynamic-specification","chapter":"12 Data","heading":"12.2.3 Dynamic Specification","text":"\\[\ny_t = \\beta_0 + y_{t-1}\\beta_1 + \\epsilon_t\n\\]\\[\nE(\\epsilon_t|\\mathbf{X})= E(\\epsilon_t| y_1,y_2, ...,y_t,y_{t+1},...,y_T)\n\\]equal 0, \\(y_t\\) \\(\\epsilon_t\\) inherently correlatedA3 violated require error uncorrelated time observation independent regressors (strict exogeneity)Dynamic Specification allowed A3","code":""},{"path":"data.html","id":"dynamically-complete","chapter":"12 Data","heading":"12.2.4 Dynamically Complete","text":"\\[\ny_t = \\beta_0 + x_t\\delta_0 + x_{t-1}\\delta_1 + \\epsilon_t\n\\]\\[\nE(\\epsilon_t|\\mathbf{X})= E(\\epsilon_t| x_1,x_2, ...,x_t,x_{t+1},...,x_T)\n\\]equal 0, include enough lags, \\(x_{t-2}\\) \\(\\epsilon_t\\) correlatedA3 violated require error uncorrelated time observation independent regressors (strict exogeneity)Can corrected including lags (stop? )Without A3OLS biasedGauss-Markov TheoremFinite Sample Properties invalidthen, canFocus Large Sample PropertiesCan use A3a instead A3A3a time series become\\[\nA3a: E(\\mathbf{x}_t'\\epsilon_t)= 0\n\\]regressors time period need independent error time period (Contemporaneous Exogeneity)\\(\\epsilon_t\\) can correlated \\(...,x_{t-2},x_{t-1},x_{t+1}, x_{t+2},...\\)can dynamic specification \\(y_t = \\beta_0 + y_{t-1}\\beta_1 + \\epsilon_t\\)Deriving Large Sample Properties Time SeriesAssumptions A1, A2, A3aAssumptions A1, A2, A3aWeak Law Central Limit Theorem depend A5\n\\(x_t\\) \\(\\epsilon_t\\) dependent t\nwithout Weak Law Central Limit Theorem depend A5, Large Sample Properties OLS\nInstead A5, consider A5a\nWeak Law Central Limit Theorem depend A5\\(x_t\\) \\(\\epsilon_t\\) dependent twithout Weak Law Central Limit Theorem depend A5, Large Sample Properties OLSInstead A5, consider A5aDerivation Asymptotic Variance depends A4\ntime series setting introduces Serial Correlation: \\(Cov(\\epsilon_t, \\epsilon_s) \\neq 0\\)\nDerivation Asymptotic Variance depends A4time series setting introduces Serial Correlation: \\(Cov(\\epsilon_t, \\epsilon_s) \\neq 0\\)A1, A2, A3a, A5a, OLS estimator consistent, asymptotically normal","code":""},{"path":"data.html","id":"highly-persistent-data","chapter":"12 Data","heading":"12.2.5 Highly Persistent Data","text":"\\(y_t, \\mathbf{x}_t\\) weakly dependent stationary process\n* \\(y_t\\) \\(y_{t-h}\\) almost independent large h * A5a hold OLS consistent limiting distribution. * Example + Random Walk \\(y_t = y_{t-1} + u_t\\) + Random Walk drift: \\(y_t = \\alpha+ y_{t-1} + u_t\\)Solution First difference stationary process\\[\ny_t - y_{t-1} = u_t\n\\]\\(u_t\\) weakly dependent process (also called integrated order 0) \\(y_t\\) said difference-stationary process (integrated order 1)regression, \\(\\{y_t, \\mathbf{x}_t \\}\\) random walks (integrated order 1), can consistently estimate first difference equation\\[\n\\begin{aligned}\ny_t - y_{t-1} &= (\\mathbf{x}_t - \\mathbf{x}_{t-1}\\beta + \\epsilon_t - \\epsilon_{t-1}) \\\\\n\\Delta y_t &= \\Delta \\mathbf{x}\\beta + \\Delta u_t\n\\end{aligned}\n\\]Unit Root Test\\[\ny_t = \\alpha + \\alpha y_{t-1} + u_t\n\\]tests \\(\\rho=1\\) (integrated order 1)null \\(H_0: \\rho = 1\\), OLS consistent asymptotically normal.alternative \\(H_a: \\rho < 1\\), OLS consistent asymptotically normal.usual t-test valid, need use transformed equation produce valid test.Dickey-Fuller Test \\[\n\\Delta y_t= \\alpha + \\theta y_{t-1} + v_t\n\\] \\(\\theta = \\rho -1\\)\\(H_0: \\theta = 0\\) \\(H_a: \\theta < 0\\)null, \\(\\Delta y_t\\) weakly dependent \\(y_{t-1}\\) .Dickey Fuller derived non-normal asymptotic distribution. reject null \\(y_t\\) random walk.Concerns standard Dickey Fuller Test\n1. considers fairly simplistic dynamic relationship\\[\n\\Delta y_t = \\alpha + \\theta y_{t-1} + \\gamma_1 \\Delta_{t-1} + ..+ \\gamma_p \\Delta_{t-p} +v_t\n\\]one additional lag, null \\(\\Delta_{y_t}\\) AR(1) process alternative \\(y_t\\) AR(2) process.Solution: include lags \\(\\Delta_{y_t}\\) controls.allow time trend \\[\n\\Delta y_t = \\alpha + \\theta y_{t-1} + \\delta t + v_t\n\\]allows \\(y_t\\) quadratic relationship tSolution: include time trend (changes critical values).Adjusted Dickey-Fuller Test \\[\n\\Delta y_t = \\alpha + \\theta y_{t-1} + \\delta t + \\gamma_1 \\Delta y_{t-1} + ... + \\gamma_p \\Delta y_{t-p} + v_t\n\\] \\(\\theta = 1 - \\rho\\)\\(H_0: \\theta_1 = 0\\) \\(H_a: \\theta_1 < 0\\)null, \\(\\Delta y_t\\) weakly dependent \\(y_{t-1}\\) notCritical values different time trend, reject null \\(y_t\\) random walk.","code":""},{"path":"data.html","id":"newey-west-standard-errors","chapter":"12 Data","heading":"12.2.5.0.1 Newey West Standard Errors","text":"A4 hold, can use Newey West Standard Errors (HAC - Heteroskedasticity Autocorrelation Consistent)\\[\n\\hat{B} = T^{-1} \\sum_{t=1}^{T} e_t^2 \\mathbf{x'_tx_t} + \\sum_{h=1}^{g}(1-\\frac{h}{g+1})T^{-1}\\sum_{t=h+1}^{T} e_t e_{t-h}(\\mathbf{x_t'x_{t-h}+ x_{t-h}'x_t})\n\\]estimates covariances distance g partestimates covariances distance g partdownweights insure \\(\\hat{B}\\) PSDdownweights insure \\(\\hat{B}\\) PSDHow choose g:\nyearly data: g = 1 2 likely account correlation\nquarterly monthly data: g larger (g = 4 8 quarterly g = 12 14 monthly)\ncan also take integer part \\(4(T/100)^{2/9}\\) integer part \\(T^{1/4}\\)\nchoose g:yearly data: g = 1 2 likely account correlationFor quarterly monthly data: g larger (g = 4 8 quarterly g = 12 14 monthly)can also take integer part \\(4(T/100)^{2/9}\\) integer part \\(T^{1/4}\\)Testing Serial CorrelationRun OLS regression \\(y_t\\) \\(\\mathbf{x_t}\\) obtain residuals \\(e_t\\)Run OLS regression \\(y_t\\) \\(\\mathbf{x_t}\\) obtain residuals \\(e_t\\)Run OLS regression \\(e_t\\) \\(\\mathbf{x}_t, e_{t-1}\\) test whether coefficient \\(e_{t-1}\\) significant.Run OLS regression \\(e_t\\) \\(\\mathbf{x}_t, e_{t-1}\\) test whether coefficient \\(e_{t-1}\\) significant.Reject null serial correlation coefficient significant 5% level.\nTest using heteroskedastic robust standard errors\ncan include \\(e_{t-2},e_{t-3},..\\) step 2 test higher order serial correlation (t-test now F-test joint significance)\nReject null serial correlation coefficient significant 5% level.Test using heteroskedastic robust standard errorscan include \\(e_{t-2},e_{t-3},..\\) step 2 test higher order serial correlation (t-test now F-test joint significance)","code":""},{"path":"data.html","id":"repeated-cross-sections","chapter":"12 Data","heading":"12.3 Repeated Cross Sections","text":"time point (day, month, year, etc.), set data sampled. set data can different among different time points.example, can sample different groups students time survey.Allowing structural change pooled cross section\\[\ny_i = \\mathbf{x}_i \\beta + \\delta_1 y_1 + ... + \\delta_T y_T + \\epsilon_i\n\\]Dummy variables one time periodallows different intercept time periodallows outcome change average time periodAllowing structural change pooled cross section\\[\ny_i = \\mathbf{x}_i \\beta + \\mathbf{x}_i y_1 \\gamma_1 + ... + \\mathbf{x}_i y_T \\gamma_T + \\delta_1 y_1 + ...+ \\delta_T y_T + \\epsilon_i\n\\]Interact \\(x_i\\) time period dummy variablesallows different slopes time periodallows effects change based time period (structural break)Interacting time period dummies \\(x_i\\) can produce many variables - use hypothesis testing determine structural breaks needed.","code":""},{"path":"data.html","id":"pooled-cross-section","chapter":"12 Data","heading":"12.3.1 Pooled Cross Section","text":"\\[\ny_i=\\mathbf{x_i\\beta +x_i \\times y1\\gamma_1 + ...+ x_i \\times yT\\gamma_T + \\delta_1y_1+...+ \\delta_Ty_T + \\epsilon_i}\n\\]Interact \\(x_i\\) time period dummy variablesallows different slopes time periodallows different slopes time periodallows effect change based time period (structural break)\ninteracting time period dummies \\(x_i\\) can produce many variables - use hypothesis testing determine structural breaks needed.\nallows effect change based time period (structural break)interacting time period dummies \\(x_i\\) can produce many variables - use hypothesis testing determine structural breaks needed.","code":""},{"path":"data.html","id":"panel-data","chapter":"12 Data","heading":"12.4 Panel Data","text":"Detail notes R can found hereFollows individual T time periods.Panel data structure like n samples time series dataCharacteristicsInformation across individuals time (cross-sectional time-series)Information across individuals time (cross-sectional time-series)N individuals T time periodsN individuals T time periodsData can either\nBalanced: individuals observed time periods\nUnbalanced: individuals observed time periods.\nData can eitherBalanced: individuals observed time periodsUnbalanced: individuals observed time periods.Assume correlation (clustering) time given individual, independence individuals.Assume correlation (clustering) time given individual, independence individuals.TypesShort panel: many individuals time periods.Long panel: many time periods individualsBoth: many time periods many individualsTime Trends Time EffectsNonlinearSeasonalityDiscontinuous shocksRegressorsTime-invariant regressors \\(x_{}=x_i\\) t (e.g., gender, race, education) zero within variationIndividual-invariant regressors \\(x_{}=x_{t}\\) (e.g., time trend, economy trends) zero variationVariation dependent variable regressorsOverall variation: variation time individuals.variation: variation individualsWithin variation: variation within individuals (time).Note: \\(s_O^2 \\approx s_B^2 + s_W^2\\)Since n observation time period t, can control time effect separately including time dummies (time effects)\\[\ny_{}=\\mathbf{x_{}\\beta} + d_1\\delta_1+...+d_{T-1}\\delta_{T-1} + \\epsilon_{}\n\\]Note: use many time dummies time series data time series data, n 1. Hence, variation, sometimes enough data compared variables estimate coefficients.Unobserved Effects Model Similar group clustering, assume random effect captures differences across individuals constant time.\\[\ny_it=\\mathbf{x_{}\\beta} + d_1\\delta_1+...+d_{T-1}\\delta_{T-1} + c_i + u_{}\n\\]\\(c_i + u_{} = \\epsilon_{}\\)\\(c_i\\) unobserved individual heterogeneity (effect)\\(u_{}\\) idiosyncratic shock\\(\\epsilon_{}\\) unobserved error term.","code":""},{"path":"data.html","id":"pooled-ols-estimator","chapter":"12 Data","heading":"12.4.1 Pooled OLS Estimator","text":"\\(c_i\\) uncorrelated \\(x_{}\\)\\[\nE(\\mathbf{x_{}'}(c_i+u_{})) = 0\n\\]A3a still holds. Pooled OLS consistent.A4 hold, OLS still consistent, efficient, need cluster robust SE.Sufficient A3a hold, needExogeneity \\(u_{}\\) A3a (contemporaneous exogeneity): \\(E(\\mathbf{x_{}'}u_{})=0\\) time varying errorRandom Effect Assumption (time constant error): \\(E(\\mathbf{x_{}'}c_{})=0\\)Pooled OLS give consistent coefficient estimates A1, A2, A3a (\\(u_{}\\) RE assumption), A5 (randomly sampling across ).","code":""},{"path":"data.html","id":"individual-specific-effects-model","chapter":"12 Data","heading":"12.4.2 Individual-specific effects model","text":"believe unobserved heterogeneity across individual (e.g., unobserved ability individual affects \\(y\\)), individual-specific effects correlated regressors, Fixed Effects Estimator. correlated Random Effects Estimator.","code":""},{"path":"data.html","id":"random-effects-estimator","chapter":"12 Data","heading":"12.4.2.1 Random Effects Estimator","text":"Random Effects estimator Feasible GLS estimator assumes \\(u_{}\\) serially uncorrelated homoskedasticUnder A1, A2, A3a (\\(u_{}\\) RE assumption) A5 (randomly sampling across ), RE estimator consistent.\nA4 holds \\(u_{}\\), RE efficient estimator\nA4 fails hold (may heteroskedasticity across , serial correlation t), RE efficient, still efficient pooled OLS.\nA1, A2, A3a (\\(u_{}\\) RE assumption) A5 (randomly sampling across ), RE estimator consistent.A4 holds \\(u_{}\\), RE efficient estimatorIf A4 fails hold (may heteroskedasticity across , serial correlation t), RE efficient, still efficient pooled OLS.","code":""},{"path":"data.html","id":"fixed-effects-estimator","chapter":"12 Data","heading":"12.4.2.2 Fixed Effects Estimator","text":"also known Within Estimator uses within variation (time)RE assumption hold (\\(E(\\mathbf{x_{}'}c_i) \\neq 0\\)), A3a hold (\\(E(\\mathbf{x_{}'}\\epsilon_i) \\neq 0\\)).Hence, OLS RE inconsistent/biased (omitted variable bias)However, FE can fix bias due time-invariant factors (observables unobservables) correlated treatment (time-variant factors correlated treatment).","code":""},{"path":"data.html","id":"demean-approach","chapter":"12 Data","heading":"12.4.2.2.1 Demean Approach","text":"deal violation \\(c_i\\), \\[\ny_{}= \\mathbf{x_{}\\beta} + c_i + u_{}\n\\]\\[\n\\bar{y_i}=\\bar{\\mathbf{x_i}} \\beta + c_i + \\bar{u_i}\n\\]second equation time averaged equationusing within transformation, \\[\ny_{} - \\bar{y_i} = \\mathbf{(x_{} - \\bar{x_i})}\\beta + u_{} - \\bar{u_i}\n\\]\\(c_i\\) time constant.Fixed Effects estimator uses POLS transformed equation\\[\ny_{} - \\bar{y_i} = \\mathbf{(x_{} - \\bar{x_i})} \\beta + d_1\\delta_1 + ... + d_{T-2}\\delta_{T-2} + u_{} - \\bar{u_i}\n\\]need A3 (strict exogeneity) (\\(E((\\mathbf{x_{}-\\bar{x_i}})'(u_{}-\\bar{u_i})=0\\)) FE consistent.need A3 (strict exogeneity) (\\(E((\\mathbf{x_{}-\\bar{x_i}})'(u_{}-\\bar{u_i})=0\\)) FE consistent.Variables time constant absorbed \\(c_i\\). Hence make inference time constant independent variables.\ninterested effects time-invariant variables, consider OLS estimator\nVariables time constant absorbed \\(c_i\\). Hence make inference time constant independent variables.interested effects time-invariant variables, consider OLS estimatorIt’s recommended still use cluster robust standard errors.’s recommended still use cluster robust standard errors.","code":""},{"path":"data.html","id":"dummy-approach","chapter":"12 Data","heading":"12.4.2.2.2 Dummy Approach","text":"Equivalent within transformation (.e., mathematically equivalent Demean Approach), can fixed effect estimator dummy regression\\[\ny_{} = x_{}\\beta + d_1\\delta_1 + ... + d_{T-2}\\delta_{T-2} + c_1\\gamma_1 + ... + c_{n-1}\\gamma_{n-1} + u_{}\n\\]whereThe standard error incorrectly calculated.FE within transformation controlling difference across individual allowed correlated observables.","code":""},{"path":"data.html","id":"first-difference-approach","chapter":"12 Data","heading":"12.4.2.2.3 First-difference Approach","text":"Economists typically use approach\\[\ny_{} - y_{(t-1)} = (\\mathbf{x}_{} - \\mathbf{x}_{(t-1)}) \\beta +  + (u_{} - u_{(t-1)})\n\\]","code":""},{"path":"data.html","id":"fixed-effects-summary","chapter":"12 Data","heading":"12.4.2.2.4 Fixed Effects Summary","text":"three approaches almost equivalent.\nDemean Approach mathematically equivalent Dummy Approach\n1 period, 3 .\nthree approaches almost equivalent.Demean Approach mathematically equivalent Dummy ApproachDemean Approach mathematically equivalent Dummy ApproachIf 1 period, 3 .1 period, 3 .Since fixed effect within estimator, status changes can contribute \\(\\beta\\) variation.\nHence, small number changes standard error \\(\\beta\\) explode\nSince fixed effect within estimator, status changes can contribute \\(\\beta\\) variation.Hence, small number changes standard error \\(\\beta\\) explodeStatus changes mean subjects change (1) control treatment group (2) treatment control group. status change, call switchers. (\nTreatment effect typically non-directional.\ncan give parameter direction needed.\nStatus changes mean subjects change (1) control treatment group (2) treatment control group. status change, call switchers. (inTreatment effect typically non-directional.Treatment effect typically non-directional.can give parameter direction needed.can give parameter direction needed.Issues:\nfundamental difference switchers non-switchers. Even though can’t definitive test , providing descriptive statistics switchers non-switchers can give us confidence conclusion.\nfixed effects focus bias reduction, might larger variance (typically, fixed effects less df)\nIssues:fundamental difference switchers non-switchers. Even though can’t definitive test , providing descriptive statistics switchers non-switchers can give us confidence conclusion.fundamental difference switchers non-switchers. Even though can’t definitive test , providing descriptive statistics switchers non-switchers can give us confidence conclusion.fixed effects focus bias reduction, might larger variance (typically, fixed effects less df)fixed effects focus bias reduction, might larger variance (typically, fixed effects less df)true model random effect, economists typically don’t care, especially \\(c_i\\) random effect \\(c_i \\perp x_{}\\) (RE assumption unrelated \\(x_{}\\)). reason economists don’t care RE wouldn’t correct bias, improves efficiency OLS.true model random effect, economists typically don’t care, especially \\(c_i\\) random effect \\(c_i \\perp x_{}\\) (RE assumption unrelated \\(x_{}\\)). reason economists don’t care RE wouldn’t correct bias, improves efficiency OLS.can estimate FE different units (just individuals).can estimate FE different units (just individuals).FE removes bias time invariant factors without costs uses within variation, imposes strict exogeneity assumption \\(u_{}\\): \\(E[(x_{} - \\bar{x}_{})(u_{} - \\bar{u}_{})]=0\\)FE removes bias time invariant factors without costs uses within variation, imposes strict exogeneity assumption \\(u_{}\\): \\(E[(x_{} - \\bar{x}_{})(u_{} - \\bar{u}_{})]=0\\)Recall\\[\nY_{} = \\beta_0 + X_{}\\beta_1 + \\alpha_i + u_{}\n\\]\\(\\epsilon_{} = \\alpha_i + u_{}\\)\\[\n\\hat{\\sigma}^2_\\epsilon = \\frac{SSR_{OLS}}{NT - K}\n\\]\\[\n\\hat{\\sigma}^2_u = \\frac{SSR_{FE}}{NT - (N+K)} = \\frac{SSR_{FE}}{N(T-1)-K}\n\\]’s ambiguous whether variance error changes SSR can increase denominator decreases.FE can unbiased, consistent (.e., converging true effect)","code":""},{"path":"data.html","id":"fe-examples","chapter":"12 Data","heading":"12.4.2.2.5 FE Examples","text":"","code":""},{"path":"data.html","id":"blau1999","chapter":"12 Data","heading":"12.4.2.2.6 Blau (1999)","text":"Intergenerational mobilityIntergenerational mobilityIf transfer resources low income family, can generate upward mobility (increase ability)?transfer resources low income family, can generate upward mobility (increase ability)?Mechanisms intergenerational mobilityGenetic (policy can’t affect) (.e., ability endowment)Environmental indirectEnvironmental direct\\[\n\\frac{\\% \\Delta \\text{Human capital}}{\\% \\Delta \\text{income}}\n\\]Financial transferIncome measures:Total household incomeWage incomeNon-wage incomeAnnual versus permanent incomeCore control variables:Bad controls jointly determined dependent variableControl mother = choice motherUncontrolled mothers:mother racemother racelocation birthlocation birtheducation parentseducation parentshousehold structure age 14household structure age 14\\[\nY_{ijt} = X_{jt} \\beta_i + I_{jt} \\alpha_i + \\epsilon_{ijt}\n\\]\\(\\) = test\\(\\) = test\\(j\\) = individual (child)\\(j\\) = individual (child)\\(t\\) = time\\(t\\) = timeGrandmother’s modelSince child nested within mother mother nested within grandmother, fixed effect child included fixed effect mother, included fixed-effect grandmother\\[\nY_{ijgmt} = X_{} \\beta_{} + I_{jt} \\alpha_i + \\gamma_g + u_{ijgmt}\n\\]wherei = test, j = kid, m = mother, g = grandmotheri = test, j = kid, m = mother, g = grandmotherwhere \\(\\gamma_g\\) includes \\(\\gamma_m\\) includes \\(\\gamma_j\\)\\(\\gamma_g\\) includes \\(\\gamma_m\\) includes \\(\\gamma_j\\)Grandma fixed-effectPros:control genetics + fixed characteristics mother raisedcontrol genetics + fixed characteristics mother raisedcan estimate effect parameter incomecan estimate effect parameter incomeCon:Might sufficient controlCommon cluster fixed-effect level (common correlated component)Fixed effect exaggerates attenuation biasError rate survey can help fix (plug number , uncertainty associated number).","code":""},{"path":"data.html","id":"babcock2010","chapter":"12 Data","heading":"12.4.2.2.7 BABCOCK (2010)","text":"\\[\nT_{ijct} = \\alpha_0 + S_{jct} \\alpha_1 + X_{ijct} \\alpha_2 + u_{ijct}\n\\]\\(S_{jct}\\) average class expectation\\(S_{jct}\\) average class expectation\\(X_{ijct}\\alpha_2\\) individual characteristics\\(X_{ijct}\\alpha_2\\) individual characteristics\\(\\) student\\(\\) student\\(j\\) instructor\\(j\\) instructor\\(c\\) course\\(c\\) course\\(t\\) time\\(t\\) time\\[\nT_{ijct} = \\beta_0+ S_{jct} \\beta_1+ X_{ijct} \\beta_2 +\\mu_{jc} + \\epsilon_{ijct}\n\\]\\(\\mu_{jc}\\) instructor course fixed effect (unique id), different \\((\\theta_j + \\delta_c)\\)Decrease course shopping conditioned available information (\\(\\mu_{jc}\\)) (class grade instructor’s info).Grade expectation change even though class materials stay sameIdentification strategy isUnder (fixed) time-varying factor bias coefficient (simultaneity)\\[\nY_{ijt} = X_{} \\beta_1 + \\text{Teacher Experience}_{jt} \\beta_2 + \\text{Teacher education}_{jt} \\beta_3 + \\text{Teacher score}_{}\\beta_4 + \\dots + \\epsilon_{ijt}\n\\]Drop teacher characteristics, include teacher dummy effect\\[\nY_{ijt} = X_{} \\alpha + \\Gamma_{} \\theta_j + u_{ijt}\n\\]\\(\\alpha\\) within teacher (conditional teacher fixed effect) \\(j = 1 \\(J-1)\\)Nuisance sense don’t interpretation \\(\\alpha\\)least can say \\(\\theta_j\\) teacher effect conditional student test score.\\[\nY_{ijt} = X_{} \\gamma + \\epsilon_{ijt}\n\\]\\(\\gamma\\) within (unconditional) \\(e_{ijt}\\) prediction error\\[\ne_{ijt} = T_{} \\delta_j + \\tilde{e}_{ijt}\n\\]\\(\\delta_j\\) mean group\\[\nY_{ijkt} = Y_{ijkt-1} + X_{} \\beta + T_{} \\tau_j + (W_i + P_k + \\epsilon_{ijkt})\n\\]\\(Y_{ijkt-1}\\) = lag control\\(Y_{ijkt-1}\\) = lag control\\(\\tau_j\\) = teacher fixed time\\(\\tau_j\\) = teacher fixed time\\(W_i\\) student fixed effect\\(W_i\\) student fixed effect\\(P_k\\) school fixed effect\\(P_k\\) school fixed effect\\(u_{ijkt} = W_i + P_k + \\epsilon_{ijkt}\\)\\(u_{ijkt} = W_i + P_k + \\epsilon_{ijkt}\\)worry selection class schoolBias \\(\\tau\\) (1 teacher) \\[\n\\frac{1}{N_j} \\sum_{= 1}^N (W_i + P_k + \\epsilon_{ijkt})\n\\]\\(N_j\\) = number student class teacher \\(j\\)can \\(P_k + \\frac{1}{N_j} \\sum_{= 1}^{N_j} (W_i + \\epsilon_{ijkt})\\)Shocks small class can bias \\(\\tau\\)\\[\n\\frac{1}{N_j} \\sum_{= 1}^{N_j} \\epsilon_{ijkt} \\neq 0\n\\]inflate teacher fixed effectEven create random teacher fixed effect put model, still contains bias mentioned can still \\(\\tau\\) (know way affect - whether positive negative).teachers switch schools, can estimate teacher school fixed effect (mobility web thin vs. thick)Mobility web refers web switchers (.e., one status another).\\[\nY_{ijkt} = Y_{ijk(t-1)} \\alpha + X_{}\\beta + T_{} \\tau + P_k + \\epsilon_{ijkt}\n\\]demean (fixed-effect), \\(\\tau\\) (teacher fixed effect) go awayIf want examine teacher fixed effect, include teacher fixed effectControl school, article argues selection biasFor \\(\\frac{1}{N_j} \\sum_{=1}^{N_j} \\epsilon_{ijkt}\\) (teacher-level average residuals), \\(var(\\tau)\\) change \\(N_j\\) (Figure 2 paper). words, quality teachers function number studentsIf \\(var(\\tau) =0\\) means teacher quality matterSpin-Measurement Error: Sampling error estimation error\\[\n\\hat{\\tau}_j = \\tau_j + \\lambda_j\n\\]\\[\nvar(\\hat{\\tau}) = var(\\tau + \\lambda)\n\\]Assume \\(cov(\\tau_j, \\lambda_j)=0\\) (reasonable) words, randomness getting children correlation teacher quality.Hence,\\[\n\\begin{aligned}\nvar(\\hat{\\tau}) &= var(\\tau) + var(\\lambda) \\\\\nvar(\\tau) &= var(\\hat{\\tau}) - var(\\lambda) \\\\\n\\end{aligned}\n\\]\\(var(\\hat{\\tau})\\) need estimate \\(var(\\lambda)\\)\\[\nvar(\\lambda) = \\frac{1}{J} \\sum_{j=1}^J \\hat{\\sigma}^2_j\n\\] \\(\\hat{\\sigma}^2_j\\) squared standard error teacher \\(j\\) (function \\(n\\))Hence,\\[\n\\frac{var(\\tau)}{var(\\hat{\\tau})} = \\text{reliability} = \\text{true variance signal}\n\\] also known much noise \\(\\hat{\\tau}\\) \\[\n1 - \\frac{var(\\tau)}{var(\\hat{\\tau})} = \\text{noise}\n\\]Even cases true relationship \\(\\tau\\) function \\(N_j\\), recovery method \\(\\lambda\\) still affectedTo examine assumption\\[\n\\hat{\\tau}_j = \\beta_0 + X_j \\beta_1 + \\epsilon_j\n\\]Regressing teacher fixed-effect teacher characteristics give us \\(R^2\\) close 0, teacher characteristics predict sampling error (\\(\\hat{\\tau}\\) contain sampling error)","code":""},{"path":"data.html","id":"tests-for-assumptions","chapter":"12 Data","heading":"12.4.3 Tests for Assumptions","text":"typically don’t test heteroskedasticity use robust covariance matrix estimation anyway.Dataset","code":"\nlibrary(\"plm\")\ndata(\"EmplUK\", package=\"plm\")\ndata(\"Produc\", package=\"plm\")\ndata(\"Grunfeld\", package=\"plm\")\ndata(\"Wages\", package=\"plm\")"},{"path":"data.html","id":"poolability","chapter":"12 Data","heading":"12.4.3.1 Poolability","text":"also known F test stability (Chow test) coefficients\\(H_0\\): individuals coefficients (.e., equal coefficients individuals).\\(H_a\\) Different individuals different coefficients.Notes:within (.e., fixed) model, different intercepts individual assumedUnder random model, intercept assumedHence, reject null hypothesis coefficients stable. , use random model.","code":"\nlibrary(plm)\nplm::pooltest(inv~value+capital, data=Grunfeld, model=\"within\")## \n##  F statistic\n## \n## data:  inv ~ value + capital\n## F = 5.7805, df1 = 18, df2 = 170, p-value = 1.219e-10\n## alternative hypothesis: unstability"},{"path":"data.html","id":"individual-and-time-effects","chapter":"12 Data","heading":"12.4.3.2 Individual and time effects","text":"use Lagrange multiplier test test presence individual time (.e., individual time).Types:honda: (Honda 1985) Defaultbp: (T. S. Breusch Pagan 1980) unbalanced panelskw: (M. L. King Wu 1997) unbalanced panels, two-way effectsghm: (Gourieroux, Holly, Monfort 1982): two-way effects","code":"\npFtest(inv~value+capital, data=Grunfeld, effect=\"twoways\")## \n##  F test for twoways effects\n## \n## data:  inv ~ value + capital\n## F = 17.403, df1 = 28, df2 = 169, p-value < 2.2e-16\n## alternative hypothesis: significant effects\npFtest(inv~value+capital, data=Grunfeld, effect=\"individual\")## \n##  F test for individual effects\n## \n## data:  inv ~ value + capital\n## F = 49.177, df1 = 9, df2 = 188, p-value < 2.2e-16\n## alternative hypothesis: significant effects\npFtest(inv~value+capital, data=Grunfeld, effect=\"time\")## \n##  F test for time effects\n## \n## data:  inv ~ value + capital\n## F = 0.23451, df1 = 19, df2 = 178, p-value = 0.9997\n## alternative hypothesis: significant effects"},{"path":"data.html","id":"cross-sectional-dependencecontemporaneous-correlation","chapter":"12 Data","heading":"12.4.3.3 Cross-sectional dependence/contemporaneous correlation","text":"Null hypothesis: residuals across entities correlated.","code":""},{"path":"data.html","id":"global-cross-sectional-dependence","chapter":"12 Data","heading":"12.4.3.3.1 Global cross-sectional dependence","text":"","code":"\npcdtest(inv~value+capital, data=Grunfeld, model=\"within\")## \n##  Pesaran CD test for cross-sectional dependence in panels\n## \n## data:  inv ~ value + capital\n## z = 4.6612, p-value = 3.144e-06\n## alternative hypothesis: cross-sectional dependence"},{"path":"data.html","id":"local-cross-sectional-dependence","chapter":"12 Data","heading":"12.4.3.3.2 Local cross-sectional dependence","text":"use command, supply matrix w argument.","code":"\npcdtest(inv~value+capital, data=Grunfeld, model=\"within\")## \n##  Pesaran CD test for cross-sectional dependence in panels\n## \n## data:  inv ~ value + capital\n## z = 4.6612, p-value = 3.144e-06\n## alternative hypothesis: cross-sectional dependence"},{"path":"data.html","id":"serial-correlation-1","chapter":"12 Data","heading":"12.4.3.4 Serial Correlation","text":"Null hypothesis: serial correlationNull hypothesis: serial correlationusually seen macro panels long time series (large N T), seen micro panels (small T large N)usually seen macro panels long time series (large N T), seen micro panels (small T large N)Serial correlation can arise individual effects(.e., time-invariant error component), idiosyncratic error terms (e..g, case AR(1) process). typically, refer serial correlation, refer second one.Serial correlation can arise individual effects(.e., time-invariant error component), idiosyncratic error terms (e..g, case AR(1) process). typically, refer serial correlation, refer second one.Can \nmarginal test: 1 two dependence (can biased towards rejection)\njoint test: dependencies (don’t know one causing problem)\nconditional test: assume correctly specify one dependence structure, test whether departure present.\nCan bemarginal test: 1 two dependence (can biased towards rejection)marginal test: 1 two dependence (can biased towards rejection)joint test: dependencies (don’t know one causing problem)joint test: dependencies (don’t know one causing problem)conditional test: assume correctly specify one dependence structure, test whether departure present.conditional test: assume correctly specify one dependence structure, test whether departure present.","code":""},{"path":"data.html","id":"unobserved-effect-test","chapter":"12 Data","heading":"12.4.3.4.1 Unobserved effect test","text":"semi-parametric test (test statistic \\(W \\dot{\\sim} N\\) regardless distribution errors) \\(H_0: \\sigma^2_\\mu = 0\\) (.e., unobserved effects residuals), favors pooled OLS.\nnull, covariance matrix residuals = diagonal (-diagonal = 0)\nsemi-parametric test (test statistic \\(W \\dot{\\sim} N\\) regardless distribution errors) \\(H_0: \\sigma^2_\\mu = 0\\) (.e., unobserved effects residuals), favors pooled OLS.null, covariance matrix residuals = diagonal (-diagonal = 0)robust unobserved effects constant within every group, kind serial correlation.robust unobserved effects constant within every group, kind serial correlation., reject null hypothesis unobserved effects residuals. Hence, exclude using pooled OLS.","code":"\npwtest(log(gsp)~log(pcap)+log(pc)+log(emp)+unemp, data=Produc)## \n##  Wooldridge's test for unobserved individual effects\n## \n## data:  formula\n## z = 3.9383, p-value = 8.207e-05\n## alternative hypothesis: unobserved effect"},{"path":"data.html","id":"locally-robust-tests-for-random-effects-and-serial-correlation","chapter":"12 Data","heading":"12.4.3.4.2 Locally robust tests for random effects and serial correlation","text":"joint LM test random effects serial correlation assuming normality homoskedasticity idiosyncratic errors (Baltagi Li 1991)(Baltagi Li 1995), reject null hypothesis presence serial correlation, random effects. still know whether serial correlation, random effects bothTo know departure null assumption, can use (Bera, Sosa-Escudero, Yoon 2001)’s test first-order serial correlation random effects (normality homoskedasticity assumption error).BSY serial correlationBSY random effectsSince BSY locally robust, “know” serial correlation, test based LM test superior:hand, know random effects, test serial correlation, use (BREUSCH 1978)-(Godfrey 1978)’s testIf “know” random effects, use (Baltagi Li 1995)’s. test serial correlation AR(1) MA(1) processes.\\(H_0\\): Uncorrelated errors.Note:one-sided power positive serial correlation.applicable balanced panels.General serial correlation testsapplicable random effects model, OLS, FE (large T, also known long panel).can also test higher-order serial correlationin case short panels (small T large n), can use","code":"\npbsytest(log(gsp)~log(pcap)+log(pc)+log(emp)+unemp, data=Produc, test=\"j\")## \n##  Baltagi and Li AR-RE joint test - balanced panel\n## \n## data:  formula\n## chisq = 4187.6, df = 2, p-value < 2.2e-16\n## alternative hypothesis: AR(1) errors or random effects\npbsytest(log(gsp)~log(pcap)+log(pc)+log(emp)+unemp, data=Produc)## \n##  Bera, Sosa-Escudero and Yoon locally robust test - balanced panel\n## \n## data:  formula\n## chisq = 52.636, df = 1, p-value = 4.015e-13\n## alternative hypothesis: AR(1) errors sub random effects\npbsytest(log(gsp)~log(pcap)+log(pc)+log(emp)+unemp, data=Produc, test=\"re\")## \n##  Bera, Sosa-Escudero and Yoon locally robust test (one-sided) -\n##  balanced panel\n## \n## data:  formula\n## z = 57.914, p-value < 2.2e-16\n## alternative hypothesis: random effects sub AR(1) errors\nplmtest(inv ~ value + capital, data = Grunfeld, type = \"honda\")## \n##  Lagrange Multiplier Test - (Honda) for balanced panels\n## \n## data:  inv ~ value + capital\n## normal = 28.252, p-value < 2.2e-16\n## alternative hypothesis: significant effects\nlmtest::bgtest()\npbltest(log(gsp)~log(pcap)+log(pc)+log(emp)+unemp, \n        data=Produc, alternative=\"onesided\")## \n##  Baltagi and Li one-sided LM test\n## \n## data:  log(gsp) ~ log(pcap) + log(pc) + log(emp) + unemp\n## z = 21.69, p-value < 2.2e-16\n## alternative hypothesis: AR(1)/MA(1) errors in RE panel model\nplm::pbgtest(plm::plm(inv~value+capital, data = Grunfeld, model = \"within\"), order = 2)## \n##  Breusch-Godfrey/Wooldridge test for serial correlation in panel models\n## \n## data:  inv ~ value + capital\n## chisq = 42.587, df = 2, p-value = 5.655e-10\n## alternative hypothesis: serial correlation in idiosyncratic errors\npwartest(log(emp) ~ log(wage) + log(capital), data=EmplUK)## \n##  Wooldridge's test for serial correlation in FE panels\n## \n## data:  plm.model\n## F = 312.3, df1 = 1, df2 = 889, p-value < 2.2e-16\n## alternative hypothesis: serial correlation"},{"path":"data.html","id":"unit-rootsstationarity","chapter":"12 Data","heading":"12.4.3.5 Unit roots/stationarity","text":"Dickey-Fuller test stochastic trends.Null hypothesis: series non-stationary (unit root)want test less critical value (p<.5) evidence unit roots.","code":""},{"path":"data.html","id":"heteroskedasticity-2","chapter":"12 Data","heading":"12.4.3.6 Heteroskedasticity","text":"Breusch-Pagan testBreusch-Pagan testNull hypothesis: data homoskedasticNull hypothesis: data homoskedasticIf evidence heteroskedasticity, robust covariance matrix advised.evidence heteroskedasticity, robust covariance matrix advised.control heteroskedasticity: Robust covariance matrix estimation (Sandwich estimator)\n“white1” - general heteroskedasticity serial correlation (check serial correlation first). Recommended random effects.\n“white2” - “white1” restricted common variance within groups. Recommended random effects.\n“arellano” - heteroskedasticity serial correlation. Recommended fixed effects\ncontrol heteroskedasticity: Robust covariance matrix estimation (Sandwich estimator)“white1” - general heteroskedasticity serial correlation (check serial correlation first). Recommended random effects.“white2” - “white1” restricted common variance within groups. Recommended random effects.“arellano” - heteroskedasticity serial correlation. Recommended fixed effects","code":""},{"path":"data.html","id":"model-selection","chapter":"12 Data","heading":"12.4.4 Model Selection","text":"","code":""},{"path":"data.html","id":"pols-vs.-re","chapter":"12 Data","heading":"12.4.4.1 POLS vs. RE","text":"continuum RE (used FGLS assumption ) POLS check back section FGLSBreusch-Pagan LM testTest random effect model based OLS residualNull hypothesis: variances across entities zero. another word, panel effect.test significant, RE preferable compared POLS","code":""},{"path":"data.html","id":"fe-vs.-re","chapter":"12 Data","heading":"12.4.4.2 FE vs. RE","text":"RE require strict exogeneity consistency (feedback effect residual covariates)Hausman TestFor Hausman test run, need assume thatstrict exogeneity holdA4 hold \\(u_{}\\),Hausman test statistic: \\(H=(\\hat{\\beta}_{RE}-\\hat{\\beta}_{FE})'(V(\\hat{\\beta}_{RE})- V(\\hat{\\beta}_{FE}))(\\hat{\\beta}_{RE}-\\hat{\\beta}_{FE}) \\sim \\chi_{n(X)}^2\\) \\(n(X)\\) number parameters time-varying regressors.low p-value means reject null hypothesis prefer FEA high p-value means reject null hypothesis consider RE estimator.","code":"\ngw <- plm(inv~value+capital, data=Grunfeld, model=\"within\")\ngr <- plm(inv~value+capital, data=Grunfeld, model=\"random\")\nphtest(gw, gr)## \n##  Hausman Test\n## \n## data:  inv ~ value + capital\n## chisq = 2.3304, df = 2, p-value = 0.3119\n## alternative hypothesis: one model is inconsistent"},{"path":"data.html","id":"summary-2","chapter":"12 Data","heading":"12.4.5 Summary","text":"three estimators (POLS, RE, FE) require A1, A2, A5 (individuals) consistent. Additionally,three estimators (POLS, RE, FE) require A1, A2, A5 (individuals) consistent. Additionally,POLS consistent A3a(\\(u_{}\\)): \\(E(\\mathbf{x}_{}'u_{})=0\\), RE Assumption \\(E(\\mathbf{x}_{}'c_{})=0\\)\nA4 hold, use cluster robust SE POLS efficient\nPOLS consistent A3a(\\(u_{}\\)): \\(E(\\mathbf{x}_{}'u_{})=0\\), RE Assumption \\(E(\\mathbf{x}_{}'c_{})=0\\)A4 hold, use cluster robust SE POLS efficientRE consistent A3a(\\(u_{}\\)): \\(E(\\mathbf{x}_{}'u_{})=0\\), RE Assumption \\(E(\\mathbf{x}_{}'c_{})=0\\)\nA4 (\\(u_{}\\)) holds usual SE valid RE efficient\nA4 (\\(u_{}\\)) hold, use cluster robust SE ,RE longer efficient (still efficient POLS)\nRE consistent A3a(\\(u_{}\\)): \\(E(\\mathbf{x}_{}'u_{})=0\\), RE Assumption \\(E(\\mathbf{x}_{}'c_{})=0\\)A4 (\\(u_{}\\)) holds usual SE valid RE efficientIf A4 (\\(u_{}\\)) hold, use cluster robust SE ,RE longer efficient (still efficient POLS)FE consistent A3 \\(E((\\mathbf{x}_{}-\\bar{\\mathbf{x}}_{})'(u_{} -\\bar{u}_{}))=0\\)\nestimate effects time constant variables\nA4 generally hold \\(u_{} -\\bar{u}_{}\\) cluster robust SE needed\nFE consistent A3 \\(E((\\mathbf{x}_{}-\\bar{\\mathbf{x}}_{})'(u_{} -\\bar{u}_{}))=0\\)estimate effects time constant variablesA4 generally hold \\(u_{} -\\bar{u}_{}\\) cluster robust SE neededNote: A5 individual (time dimension) implies A5a entire data set.Based table provided Ani Katchova","code":""},{"path":"data.html","id":"application-9","chapter":"12 Data","heading":"12.4.6 Application","text":"Recommended application plm can found Yves CroissantAdvancedOther methods estimate random model:\"swar\": default (Swamy Arora 1972)\"walhus\": (Wallace Hussain 1969)\"amemiya\": (Fuller Battese 1974)\"nerlove\"\" (Nerlove 1971)effects:Individual effects: defaultTime effects: \"time\"Individual time effects: \"twoways\"Note: random two-ways effect model random.method = \"nerlove\"call estimation variance error componentsCheck unbalancedness. Closer 1 indicates balanced data (Ahrens Pincus 1981)Instrumental variable\"bvk\": default (Balestra Varadharajan-Krishnakumar 1987)\"baltagi\": (Baltagi 1981)\"\" (Amemiya MaCurdy 1986)\"bms\": (Trevor S. Breusch, Mizon, Schmidt 1989)","code":"\n#install.packages(\"plm\")\nlibrary(\"plm\")\n\nlibrary(foreign)\nPanel <- read.dta(\"http://dss.princeton.edu/training/Panel101.dta\")\n\nattach(Panel)\nY <- cbind(y)\nX <- cbind(x1, x2, x3)\n\n# Set data as panel data\npdata <- pdata.frame(Panel, index=c(\"country\",\"year\"))\n\n# Pooled OLS estimator\npooling <- plm(Y ~ X, data=pdata, model= \"pooling\")\nsummary(pooling)\n\n# Between estimator\nbetween <- plm(Y ~ X, data=pdata, model= \"between\")\nsummary(between)\n\n# First differences estimator\nfirstdiff <- plm(Y ~ X, data=pdata, model= \"fd\")\nsummary(firstdiff)\n\n# Fixed effects or within estimator\nfixed <- plm(Y ~ X, data=pdata, model= \"within\")\nsummary(fixed)\n\n# Random effects estimator\nrandom <- plm(Y ~ X, data=pdata, model= \"random\")\nsummary(random)\n\n# LM test for random effects versus OLS\n# Accept Null, then OLS, Reject Null then RE\nplmtest(pooling,effect = \"individual\", type = c(\"bp\")) # other type: \"honda\", \"kw\",\" \"ghm\"; other effect : \"time\" \"twoways\"\n\n\n# B-P/LM and Pesaran CD (cross-sectional dependence) test\npcdtest(fixed, test = c(\"lm\")) # Breusch and Pagan's original LM statistic\npcdtest(fixed, test = c(\"cd\")) # Pesaran's CD statistic\n\n# Serial Correlation\npbgtest(fixed)\n\n# stationary\nlibrary(\"tseries\")\nadf.test(pdata$y, k = 2)\n\n# LM test for fixed effects versus OLS\npFtest(fixed, pooling)\n\n# Hausman test for fixed versus random effects model\nphtest(random, fixed)\n\n# Breusch-Pagan heteroskedasticity\nlibrary(lmtest)\nbptest(y ~ x1 + factor(country), data = pdata)\n\n# If there is presence of heteroskedasticity\n## For RE model\ncoeftest(random) #orginal coef\ncoeftest(random, vcovHC) # Heteroskedasticity consistent coefficients\n\nt(sapply(c(\"HC0\", \"HC1\", \"HC2\", \"HC3\", \"HC4\"), function(x) sqrt(diag(vcovHC(random, type = x))))) #show HC SE of the coef\n# HC0 - heteroskedasticity consistent. The default.\n# HC1,HC2, HC3 – Recommended for small samples. HC3 gives less weight to influential observations.\n# HC4 - small samples with influential observations\n# HAC - heteroskedasticity and autocorrelation consistent\n\n## For FE model\ncoeftest(fixed) # Original coefficients\ncoeftest(fixed, vcovHC) # Heteroskedasticity consistent coefficients\ncoeftest(fixed, vcovHC(fixed, method = \"arellano\")) # Heteroskedasticity consistent coefficients (Arellano)\nt(sapply(c(\"HC0\", \"HC1\", \"HC2\", \"HC3\", \"HC4\"), function(x) sqrt(diag(vcovHC(fixed, type = x))))) #show HC SE of the coef\namemiya <- plm(Y ~ X, data=pdata, model= \"random\",random.method = \"amemiya\",effect = \"twoways\")\nercomp(Y~X, data=pdata, method = \"amemiya\", effect = \"twoways\")\npunbalancedness(random)\ninstr <- plm(Y ~ X | X_ins, data = pdata, random.method = \"ht\", model = \"random\", inst.method = \"baltagi\")"},{"path":"data.html","id":"other-estimators","chapter":"12 Data","heading":"12.4.7 Other Estimators","text":"","code":""},{"path":"data.html","id":"variable-coefficients-model","chapter":"12 Data","heading":"12.4.7.1 Variable Coefficients Model","text":"details can found ","code":"\nfixed_pvcm <- pvcm(Y~X, data=pdata, model=\"within\")\nrandom_pvcm <- pvcm(Y~X, data=pdata, model=\"random\")"},{"path":"data.html","id":"generalized-method-of-moments-estimator","chapter":"12 Data","heading":"12.4.7.2 Generalized Method of Moments Estimator","text":"Typically use dynamic models. Example plm package","code":"\nz2 <- pgmm(log(emp) ~ lag(log(emp), 1)+ lag(log(wage), 0:1) +\n           lag(log(capital), 0:1) | lag(log(emp), 2:99) +\n           lag(log(wage), 2:99) + lag(log(capital), 2:99),        \n           data = EmplUK, effect = \"twoways\", model = \"onestep\", \n           transformation = \"ld\")\nsummary(z2, robust = TRUE)"},{"path":"data.html","id":"general-feasible-generalized-least-squares-models","chapter":"12 Data","heading":"12.4.7.3 General Feasible Generalized Least Squares Models","text":"Assume cross-sectional correlation Robust intragroup heteroskedasticity serial correlation. Suited n much larger T (long panel) However, inefficient groupwise heteorskedasticity.","code":"\n# Random Effects\nzz <- pggls(log(emp)~log(wage)+log(capital), data=EmplUK, model=\"pooling\")\n\n# Fixed\nzz <- pggls(log(emp)~log(wage)+log(capital), data=EmplUK, model=\"within\")"},{"path":"hypothesis-testing.html","id":"hypothesis-testing","chapter":"13 Hypothesis Testing","heading":"13 Hypothesis Testing","text":"Error types:Type Error (False Positive):\nReality: nope\nDiagnosis/Analysis: yes\nType Error (False Positive):Reality: nopeDiagnosis/Analysis: yesType II Error (False Negative):\nReality: yes\nDiagnosis/Analysis: nope\nType II Error (False Negative):Reality: yesDiagnosis/Analysis: nopePower: probability rejecting null hypothesis actually falseNote:Always written terms population parameter (\\(\\beta\\)) estimator/estimate (\\(\\hat{\\beta}\\))Always written terms population parameter (\\(\\beta\\)) estimator/estimate (\\(\\hat{\\beta}\\))Sometimes, different disciplines prefer use \\(\\beta\\) (.e., standardized coefficient), \\(\\mathbf{b}\\) (.e., unstandardized coefficient)\n\\(\\beta\\) \\(\\mathbf{b}\\) similar interpretation; however, \\(\\beta\\) scale free. Hence, can see relative contribution \\(\\beta\\) dependent variable. hand, \\(\\mathbf{b}\\) can easily used policy decisions.\n\\[\n\\beta_j = \\mathbf{b} \\frac{s_{x_j}}{s_y}\n\\]\nSometimes, different disciplines prefer use \\(\\beta\\) (.e., standardized coefficient), \\(\\mathbf{b}\\) (.e., unstandardized coefficient)\\(\\beta\\) \\(\\mathbf{b}\\) similar interpretation; however, \\(\\beta\\) scale free. Hence, can see relative contribution \\(\\beta\\) dependent variable. hand, \\(\\mathbf{b}\\) can easily used policy decisions.\\(\\beta\\) \\(\\mathbf{b}\\) similar interpretation; however, \\(\\beta\\) scale free. Hence, can see relative contribution \\(\\beta\\) dependent variable. hand, \\(\\mathbf{b}\\) can easily used policy decisions.\\[\n\\beta_j = \\mathbf{b} \\frac{s_{x_j}}{s_y}\n\\]\\[\n\\beta_j = \\mathbf{b} \\frac{s_{x_j}}{s_y}\n\\]Assuming null hypothesis true, (asymptotic) distribution estimatorTwo-sided\\[\nH_0: \\beta_j = 0 \\\\\nH_1: \\beta_j \\neq 0 \n\\]null, OLS estimator following distribution\\[\nA1-A3a, A5: \\sqrt{n} \\hat{\\beta_j}  \\sim  N(0,Avar(\\sqrt{n}\\hat{\\beta}_j))\n\\]one-sided test, null set values, now choose worst case single value hardest prove derive distribution nullOne-sided\\[\nH_0: \\beta_j\\ge 0 \\\\\nH_1: \\beta_j < 0 \n\\]hardest null value prove \\(H_0: \\beta_j=0\\). specific null, OLS estimator following asymptotic distribution\\[\nA1-A3a, A5: \\sqrt{n}\\hat{\\beta_j} \\sim N(0,Avar(\\sqrt{n}\\hat{\\beta}_j))\n\\]","code":""},{"path":"hypothesis-testing.html","id":"types-of-hypothesis-testing","chapter":"13 Hypothesis Testing","heading":"13.1 Types of hypothesis testing","text":"\\(H_0 : \\theta = \\theta_0\\)\\(H_1 : \\theta \\neq \\theta_0\\)far away / extreme \\(\\theta\\) can null hypothesis trueAssume likelihood function q \\(L(q) = q^{30}(1-q)^{70}\\) Likelihood functionLog-Likelihood function(Fox 1991)typically, likelihood ratio test (Lagrange Multiplier (Score)) performs better small moderate sample sizes, Wald test requires one maximization (full model).","code":"\nq = seq(0,1,length=100)\nL= function(q){q^30 * (1-q)^70}\nplot(q,L(q),ylab=\"L(q)\",xlab=\"q\",type=\"l\")\nq = seq(0,1,length=100)\nl= function(q){30*log(q) + 70 * log(1-q)}\nplot(q,l(q)-l(0.3),ylab=\"l(q) - l(qhat)\",xlab=\"q\",type=\"l\")\nabline(v=0.2)"},{"path":"hypothesis-testing.html","id":"wald-test","chapter":"13 Hypothesis Testing","heading":"13.2 Wald test","text":"\\[\nW = (\\hat{\\theta}-\\theta_0)'[cov(\\hat{\\theta})]^{-1}(\\hat{\\theta}-\\theta_0) \\\\\nW \\sim \\chi_q^2\n\\]\\(cov(\\hat{\\theta})\\) given inverse Fisher Information matrix evaluated \\(\\hat{\\theta}\\) q rank \\(cov(\\hat{\\theta})\\), number non-redundant parameters \\(\\theta\\)Alternatively,\\[\nt_W=\\frac{(\\hat{\\theta}-\\theta_0)^2}{(\\theta_0)^{-1}} \\sim \\chi^2_{(v)}\n\\]v degree freedom.Equivalently,\\[\ns_W= \\frac{\\hat{\\theta}-\\theta_0}{\\sqrt{(\\hat{\\theta})^{-1}}} \\sim Z\n\\]far away distribution sample estimate hypothesized population parameter.null value, probability obtained realization “extreme” “worse” estimate actually obtained.Significance Level (\\(\\alpha\\)) Confidence Level (\\(1-\\alpha\\))significance level benchmark probability low reject nullThe confidence level probability sets bounds far away realization estimator reject null.Test StatisticsStandardized (transform) estimator null value test statistic always distributionTest Statistic OLS estimator single hypothesis\\[\nT = \\frac{\\sqrt{n}(\\hat{\\beta}_j-\\beta_{j0})}{\\sqrt{n}SE(\\hat{\\beta_j})} \\sim^N(0,1)\n\\]Equivalently,\\[\nT = \\frac{(\\hat{\\beta}_j-\\beta_{j0})}{SE(\\hat{\\beta_j})} \\sim^N(0,1)\n\\]test statistic another random variable function data null hypothesis.T denotes random variable test statistict denotes single realization test statisticEvaluating Test Statistic: determine whether reject fail reject null hypothesis given significance / confidence levelThree equivalent waysCritical ValueCritical ValueP-valueP-valueConfidence IntervalConfidence IntervalCritical ValueCritical ValueFor given significance level, determine critical value (c)\n* One-sided: \\(H_0: \\beta_j \\ge \\beta_{j0}\\)\\[\nP(T<c|H_0)=\\alpha\n\\]Reject null \\(t<c\\)One-sided: \\(H_0: \\beta_j \\le \\beta_{j0}\\)\\[\nP(T>c|H_0)=\\alpha\n\\]Reject null \\(t>c\\)TWo-sided: \\(H_0: \\beta_j \\neq \\beta_{j0}\\)\\[\nP(|T|>c|H_0)=\\alpha\n\\]Reject null \\(|t|>c\\)p-valueCalculate probability test statistic worse realization haveOne-sided: \\(H_0: \\beta_j \\ge \\beta_{j0}\\)\\[\n\\text{p-value} = P(T<t|H_0)\n\\]One-sided: \\(H_0: \\beta_j \\le \\beta_{j0}\\)\\[\n\\text{p-value} = P(T>t|H_0)\n\\]Two-sided: \\(H_0: \\beta_j \\neq \\beta_{j0}\\)\\[\n\\text{p-value} = P(|T|<t|H_0)\n\\]reject null p-value \\(< \\alpha\\)Confidence IntervalUsing critical value associated null hypothesis significance level, create interval\\[\nCI(\\hat{\\beta}_j)_{\\alpha} = [\\hat{\\beta}_j-(c \\times SE(\\hat{\\beta}_j)),\\hat{\\beta}_j+(c \\times SE(\\hat{\\beta}_j))]\n\\]null set lies outside interval reject null.testing whether true population value close estimate, testing given field true population value parameter, like observed estimate.Can interpreted believe \\((1-\\alpha)\\times 100 \\%\\) probability confidence interval captures true parameter value.stronger assumption (A1-A6), consider Finite Sample Properties\\[\nT = \\frac{\\hat{\\beta}_j-\\beta_{j0}}{SE(\\hat{\\beta}_j)} \\sim T(n-k)\n\\]distributional derivation strongly dependent A4 A5T student t-distribution numerator normal denominator \\(\\chi^2\\).Critical value p-values calculated student t-distribution rather standard normal distribution.\\(n \\\\infty\\), \\(T(n-k)\\) asymptotically standard normal.Rule thumbif \\(n-k>120\\): critical values p-values t-distribution (almost) critical values p-values standard normal distribution.\\(n-k>120\\): critical values p-values t-distribution (almost) critical values p-values standard normal distribution.\\(n-k<120\\)\n(A1-A6) hold t-test exact finite distribution test\n(A1-A3a, A5) hold, t-distribution asymptotically normal, computing critical values t-distribution still valid asymptotic test (.e., quite right critical values p0values, difference goes away \\(n \\\\infty\\))\n\\(n-k<120\\)(A1-A6) hold t-test exact finite distribution testif (A1-A3a, A5) hold, t-distribution asymptotically normal, computing critical values t-distribution still valid asymptotic test (.e., quite right critical values p0values, difference goes away \\(n \\\\infty\\))","code":""},{"path":"hypothesis-testing.html","id":"multiple-hypothesis","chapter":"13 Hypothesis Testing","heading":"13.2.1 Multiple Hypothesis","text":"test multiple parameters time\n\\(H_0: \\beta_1 = 0\\ \\& \\ \\beta_2 = 0\\)\n\\(H_0: \\beta_1 = 1\\ \\& \\ \\beta_2 = 0\\)\ntest multiple parameters time\\(H_0: \\beta_1 = 0\\ \\& \\ \\beta_2 = 0\\)\\(H_0: \\beta_1 = 1\\ \\& \\ \\beta_2 = 0\\)perform series simply hypothesis answer question (joint distribution vs. two marginal distributions).perform series simply hypothesis answer question (joint distribution vs. two marginal distributions).test statistic based restriction written matrix form.test statistic based restriction written matrix form.\\[\ny=\\beta_0+x_1\\beta_1 + x_2\\beta_2 + x_3\\beta_3 + \\epsilon\n\\]Null hypothesis \\(H_0: \\beta_1 = 0\\) & \\(\\beta_2=0\\) can rewritten \\(H_0: \\mathbf{R}\\beta -\\mathbf{q}=0\\) \\(\\mathbf{R}\\) m x k matrix m number restrictions k number parameters. \\(\\mathbf{q}\\) k x 1 vector\\(\\mathbf{R}\\) “picks ” relevant parameters \\(\\mathbf{q}\\) null value parameter\\[\n\\mathbf{R}= \n\\left(\n\\begin{array}{cccc}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n\\end{array}\n\\right),\n\\mathbf{q} = \n\\left(\n\\begin{array}{c}\n0 \\\\\n0 \\\\\n\\end{array}\n\\right)\n\\]Test Statistic OLS estimator multiple hypothesis\\[\nF = \\frac{(\\mathbf{R\\hat{\\beta}-q})\\hat{\\Sigma}^{-1}(\\mathbf{R\\hat{\\beta}-q})}{m} \\sim^F(m,n-k)\n\\]\\(\\hat{\\Sigma}^{-1}\\) estimator asymptotic variance-covariance matrix\nA4 holds, homoskedastic heteroskedastic versions produce valid estimator\nA4 hold, heteroskedastic version produces valid estimators.\n\\(\\hat{\\Sigma}^{-1}\\) estimator asymptotic variance-covariance matrixif A4 holds, homoskedastic heteroskedastic versions produce valid estimatorIf A4 hold, heteroskedastic version produces valid estimators.m = 1, single restriction, F-statistic t-statistic squared.m = 1, single restriction, F-statistic t-statistic squared.F distribution strictly positive, check F-Distribution details.F distribution strictly positive, check F-Distribution details.","code":""},{"path":"hypothesis-testing.html","id":"linear-combination","chapter":"13 Hypothesis Testing","heading":"13.2.2 Linear Combination","text":"Testing multiple parameters time\\[\nH_0: \\beta_1 -\\beta_2 = 0 \\\\\nH_0: \\beta_1 - \\beta_2 > 0 \\\\\nH_0: \\beta_1 - 2*\\beta_2 =0\n\\]single restriction function parameters.Null hypothesis:\\[\nH_0: \\beta_1 -\\beta_2 = 0\n\\]can rewritten \\[\nH_0: \\mathbf{R}\\beta -\\mathbf{q}=0\n\\]\\(\\mathbf{R}\\)=(0 1 -1 0 0) \\(\\mathbf{q}=0\\)","code":""},{"path":"hypothesis-testing.html","id":"estimate-difference-in-coefficients","chapter":"13 Hypothesis Testing","heading":"13.2.3 Estimate Difference in Coefficients","text":"package estimate difference two coefficients CI, simple function created Katherine Zee can used calculate difference. modifications might needed don’t use standard lm model R.","code":"\ndifftest_lm <- function(x1, x2, model) {\n    diffest <-\n        summary(model)$coef[x1, \"Estimate\"] - summary(model)$coef[x2, \"Estimate\"]\n    vardiff <- (summary(model)$coef[x1, \"Std. Error\"] ^ 2 +\n                    summary(model)$coef[x2, \"Std. Error\"] ^ 2) - (2 * (vcov(model)[x1, x2]))\n    # variance of x1 + variance of x2 - 2*covariance of x1 and x2\n    diffse <- sqrt(vardiff)\n    tdiff <- (diffest) / (diffse)\n    ptdiff <- 2 * (1 - pt(abs(tdiff), model$df, lower.tail = T))\n    upr <-\n        diffest + qt(.975, df = model$df) * diffse # will usually be very close to 1.96\n    lwr <- diffest + qt(.025, df = model$df) * diffse\n    df <- model$df\n    return(list(\n        est = round(diffest, digits = 2),\n        t = round(tdiff, digits = 2),\n        p = round(ptdiff, digits = 4),\n        lwr = round(lwr, digits = 2),\n        upr = round(upr, digits = 2),\n        df = df\n    ))\n}"},{"path":"hypothesis-testing.html","id":"application-10","chapter":"13 Hypothesis Testing","heading":"13.2.4 Application","text":"","code":"\nlibrary(\"car\")## Warning: package 'car' was built under R version 4.0.5## Loading required package: carData\n# Multiple hypothesis\nmod.davis <- lm(weight ~ repwt, data=Davis)\nlinearHypothesis(mod.davis, c(\"(Intercept) = 0\", \"repwt = 1\"),white.adjust = TRUE)## Linear hypothesis test\n## \n## Hypothesis:\n## (Intercept) = 0\n## repwt = 1\n## \n## Model 1: restricted model\n## Model 2: weight ~ repwt\n## \n## Note: Coefficient covariance matrix supplied.\n## \n##   Res.Df Df      F  Pr(>F)  \n## 1    183                    \n## 2    181  2 3.3896 0.03588 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Linear Combination\nmod.duncan <- lm(prestige ~ income + education, data=Duncan)\nlinearHypothesis(mod.duncan, \"1*income - 1*education = 0\")## Linear hypothesis test\n## \n## Hypothesis:\n## income - education = 0\n## \n## Model 1: restricted model\n## Model 2: prestige ~ income + education\n## \n##   Res.Df    RSS Df Sum of Sq      F Pr(>F)\n## 1     43 7518.9                           \n## 2     42 7506.7  1    12.195 0.0682 0.7952"},{"path":"hypothesis-testing.html","id":"nonlinear-1","chapter":"13 Hypothesis Testing","heading":"13.2.5 Nonlinear","text":"Suppose q nonlinear functions parameters\\[\n\\mathbf{h}(\\theta) = \\{ h_1 (\\theta), ..., h_q (\\theta)\\}'\n\\],n, Jacobian matrix (\\(\\mathbf{H}(\\theta)\\)), rank q \\[\n\\mathbf{H}_{q \\times p}(\\theta) = \n\\left(\n\\begin{array}\n{ccc}\n\\frac{\\partial h_1(\\theta)}{\\partial \\theta_1} & ... & \\frac{\\partial h_1(\\theta)}{\\partial \\theta_p} \\\\\n. & . & . \\\\\n\\frac{\\partial h_q(\\theta)}{\\partial \\theta_1} & ... & \\frac{\\partial h_q(\\theta)}{\\partial \\theta_p}\n\\end{array}\n\\right)\n\\]null hypothesis \\(H_0: \\mathbf{h} (\\theta) = 0\\) can tested agiasnt 2-sided alternative Wald statistic\\[\nW = \\frac{\\mathbf{h(\\hat{\\theta})'\\{H(\\hat{\\theta})[F(\\hat{\\theta})'F(\\hat{\\theta})]^{-1}H(\\hat{\\theta})'\\}^{-1}h(\\hat{\\theta})}}{s^2q} \\sim F_{q,n-p}\n\\]","code":""},{"path":"hypothesis-testing.html","id":"the-likelihood-ratio-test","chapter":"13 Hypothesis Testing","heading":"13.3 The likelihood ratio test","text":"\\[\nt_{LR} = 2[l(\\hat{\\theta})-l(\\theta_0)] \\sim \\chi^2_v\n\\]v degree freedom.Compare height log-likelihood sample estimate relation height log-likelihood hypothesized population parameterAlternatively,test considers ratio two maximizations,\\[\nL_r = \\text{maximized value likelihood $H_0$ (reduced model)} \\\\\nL_f = \\text{maximized value likelihood $H_0 \\cup H_a$ (full model)}\n\\], likelihood ratio :\\[\n\\Lambda = \\frac{L_r}{L_f}\n\\]can’t exceed 1 (since \\(L_f\\) always least large \\(L-r\\) \\(L_r\\) result maximization restricted set parameter values).likelihood ratio statistic :\\[\n-2ln(\\Lambda) = -2ln(L_r/L_f) = -2(l_r - l_f) \\\\\n\\lim_{n \\\\infty}(-2ln(\\Lambda)) \\sim \\chi^2_v\n\\]v number parameters full model minus number parameters reduced model.\\(L_r\\) much smaller \\(L_f\\) (likelihood ratio exceeds \\(\\chi_{\\alpha,v}^2\\)), reject reduced model accept full model \\(\\alpha \\times 100 \\%\\) significance level","code":""},{"path":"hypothesis-testing.html","id":"lagrange-multiplier-score","chapter":"13 Hypothesis Testing","heading":"13.4 Lagrange Multiplier (Score)","text":"\\[\nt_S= \\frac{S(\\theta_0)^2}{(\\theta_0)} \\sim \\chi^2_v\n\\]v degree freedom.Compare slope log-likelihood sample estimate relation slope log-likelihood hypothesized population parameter","code":""},{"path":"prediction-and-estimation.html","id":"prediction-and-estimation","chapter":"14 Prediction and Estimation","heading":"14 Prediction and Estimation","text":"Prediction Estimation (Inference) two fundamental pillars statistics.. can either high prediction high estimation.\nprediction, minimize loss function.\nestimation, try best fit data. goal estimation best fit data, always run risk predicting well.\n. can either high prediction high estimation.prediction, minimize loss function.prediction, minimize loss function.estimation, try best fit data. goal estimation best fit data, always run risk predicting well.estimation, try best fit data. goal estimation best fit data, always run risk predicting well.high dimension, always weak strong collinearity. Hence, estimation can undesirable. can’t pick one variable stay model, troubles affect prediction. Plateau problemIf two functions similar output space, can still prediction, can’t estimation exploded standard errors.(SICSS 2018 - Sendhil Mullainathan’s presentation slide)Selective Labels Problem (Selective Labels Problem: Evaluating Algorithmic Predictions Presence Unobservables)Recall Linear Regression 5 OLS estimates\\[\n\\begin{aligned}\n\\hat{\\beta}_{OLS} &= (\\mathbf{X}'\\mathbf{X})^{-1}(\\mathbf{X}'\\mathbf{Y}) \\\\\n&= (\\mathbf{X}'\\mathbf{X})^{-1}(\\mathbf{X}'(\\mathbf{X \\beta}+ \\epsilon)) \\\\\n&= (\\mathbf{X}'\\mathbf{X})^{-1} (\\mathbf{X}'\\mathbf{X}) \\beta + (\\mathbf{X}'\\mathbf{X})^{-1}(\\mathbf{X}'\\epsilon) \\\\\n\\hat{\\beta}_{OLS} & \\\\beta + (\\mathbf{X}'\\mathbf{X})^{-1}(\\mathbf{X}'\\epsilon)\n\\end{aligned}\n\\]Hence, OLS estimates unbiased (.e., get rid \\((\\mathbf{X}'\\mathbf{X})^{-1}(\\mathbf{X}'\\epsilon)\\)) following 2 conditions:\\(E(\\epsilon|\\mathbf{X}) = 0\\) intercept, can usually solve problem\\(Cov(\\mathbf{X}, \\epsilon) = 0\\)Problem estimation usually stems second condition.Tools combat problem can found causal inference 18","code":""},{"path":"sampling.html","id":"sampling","chapter":"15 Sampling","heading":"15 Sampling","text":"","code":""},{"path":"sampling.html","id":"simple-sampling","chapter":"15 Sampling","heading":"15.1 Simple Sampling","text":"Simple (random) SamplingIdentify missing points sample collected data","code":"\nlibrary(dplyr)## Warning: package 'dplyr' was built under R version 4.0.5## \n## Attaching package: 'dplyr'## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\niris_df <- iris\nset.seed(1)\nsample_n(iris_df, 10)##    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n## 1           5.8         2.7          4.1         1.0 versicolor\n## 2           6.4         2.8          5.6         2.1  virginica\n## 3           4.4         3.2          1.3         0.2     setosa\n## 4           4.3         3.0          1.1         0.1     setosa\n## 5           7.0         3.2          4.7         1.4 versicolor\n## 6           5.4         3.0          4.5         1.5 versicolor\n## 7           5.4         3.4          1.7         0.2     setosa\n## 8           7.6         3.0          6.6         2.1  virginica\n## 9           6.1         2.8          4.7         1.2 versicolor\n## 10          4.6         3.4          1.4         0.3     setosa\nlibrary(sampling)## Warning: package 'sampling' was built under R version 4.0.5\n# set unique id number for each row \niris_df$id = 1:nrow(iris_df)\n\n# Simple random sampling with replacement\nsrswor(10, length(iris_df$id))##   [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1\n##  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n##  [75] 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0\n## [112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n## [149] 0 0\n# Simple random sampling without replacement (sequential method)\nsrswor1(10, length(iris_df$id))##   [1] 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n##  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n##  [75] 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n## [112] 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n## [149] 0 0\n# Simple random sampling with replacement\nsrswr(10, length(iris_df$id))##   [1] 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0\n##  [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n##  [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n## [112] 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n## [149] 0 0\nlibrary(survey)## Warning: package 'survey' was built under R version 4.0.5## Loading required package: grid## Loading required package: Matrix## Warning: package 'Matrix' was built under R version 4.0.5## Loading required package: survival## Warning: package 'survival' was built under R version 4.0.5## \n## Attaching package: 'survival'## The following objects are masked from 'package:sampling':\n## \n##     cluster, strata## \n## Attaching package: 'survey'## The following object is masked from 'package:graphics':\n## \n##     dotchart\ndata(\"api\")\nsrs_design <- svydesign(data = apistrat,\n                        weights = ~pw, \n                        fpc = ~fpc, \n                        id = ~1)## Warning in as.fpc(fpc, strata, ids, pps = pps): `fpc' varies within strata:\n## stratum 1 at stage 1\nlibrary(sampler)\nrsamp(albania,\n      n = 260,\n      over = 0.1, # desired oversampling proportion\n      rep = F)\nalsample <- rsamp(df = albania, 544)\nalreceived <- rsamp(df = alsample, 390)\nrmissing(sampdf = alsample,\n         colldf = alreceived,\n         col_name = qvKod)"},{"path":"sampling.html","id":"stratified-sampling","chapter":"15 Sampling","heading":"15.2 Stratified Sampling","text":"stratum subset population least one common characteristic.Steps:Identify relevant stratums representation population.Randomly sample select sufficient number subjects stratum.Stratified sampling reduces sampling error.Identify number missing points strata sample collected data","code":"\nlibrary(dplyr)\n# by number of rows\nsample_iris <- iris %>%\n    group_by(Species) %>%\n    sample_n(5)\nsample_iris## # A tibble: 15 x 5\n## # Groups:   Species [3]\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n##           <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n##  1          4.4         3            1.3         0.2 setosa    \n##  2          5.2         3.5          1.5         0.2 setosa    \n##  3          5.1         3.8          1.5         0.3 setosa    \n##  4          5.2         3.4          1.4         0.2 setosa    \n##  5          4.5         2.3          1.3         0.3 setosa    \n##  6          5.5         2.5          4           1.3 versicolor\n##  7          7           3.2          4.7         1.4 versicolor\n##  8          6.7         3            5           1.7 versicolor\n##  9          6.1         2.9          4.7         1.4 versicolor\n## 10          5.5         2.4          3.8         1.1 versicolor\n## 11          6.4         2.7          5.3         1.9 virginica \n## 12          6.4         2.8          5.6         2.1 virginica \n## 13          6.4         3.2          5.3         2.3 virginica \n## 14          6.8         3.2          5.9         2.3 virginica \n## 15          7.2         3.6          6.1         2.5 virginica\n# by fraction\nsample_iris <- iris %>%\n    group_by(Species) %>%\n    sample_frac(size = .15)\nsample_iris## # A tibble: 24 x 5\n## # Groups:   Species [3]\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n##           <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n##  1          5.5         4.2          1.4         0.2 setosa    \n##  2          5           3            1.6         0.2 setosa    \n##  3          5.2         4.1          1.5         0.1 setosa    \n##  4          4.6         3.1          1.5         0.2 setosa    \n##  5          5.1         3.7          1.5         0.4 setosa    \n##  6          4.8         3.4          1.9         0.2 setosa    \n##  7          5.1         3.3          1.7         0.5 setosa    \n##  8          5.5         3.5          1.3         0.2 setosa    \n##  9          5           2.3          3.3         1   versicolor\n## 10          5.6         2.9          3.6         1.3 versicolor\n## # ... with 14 more rows\nlibrary(sampler)## Warning: package 'sampler' was built under R version 4.0.5\n# Stratified sample using proportional allocation without replacement\nssamp(df=albania, n=360, strata=qarku, over=0.1)## # A tibble: 395 x 45\n##    qarku  Q_ID bashkia       BAS_ID zaz   njesiaAdministr~ COM_ID qvKod zgjedhes\n##    <fct> <int> <fct>          <int> <fct> <fct>             <int> <fct>    <int>\n##  1 Berat     1 Berat             11 ZAZ ~ \"Berat \"           1101 \"\\\"3~      558\n##  2 Berat     1 Berat             11 ZAZ ~ \"Berat \"           1101 \"\\\"3~      815\n##  3 Berat     1 Berat             11 ZAZ ~ \"Sinje\"            1108 \"\\\"3~      419\n##  4 Berat     1 Kucove            13 ZAZ ~ \"Lumas\"            1104 \"\\\"3~      237\n##  5 Berat     1 Kucove            13 ZAZ ~ \"Kucove\"           1201 \"\\\"3~      562\n##  6 Berat     1 Skrapar           17 ZAZ ~ \"Corovode\"         1303 \"\\\"3~      829\n##  7 Berat     1 Berat             11 ZAZ ~ \"Roshnik\"          1107 \"\\\"3~      410\n##  8 Berat     1 Ura Vajgurore     19 ZAZ ~ \"Ura Vajgurore\"    1110 \"\\\"3~      708\n##  9 Berat     1 Kucove            13 ZAZ ~ \"Perondi\"          1203 \"\\\"3~      835\n## 10 Berat     1 Kucove            13 ZAZ ~ \"Kucove\"           1201 \"\\\"3~      907\n## # ... with 385 more rows, and 36 more variables: meshkuj <int>, femra <int>,\n## #   totalSeats <int>, vendndodhja <fct>, ambienti <fct>, totalVoters <int>,\n## #   femVoters <int>, maleVoters <int>, unusedBallots <int>,\n## #   damagedBallots <int>, ballotsCast <int>, invalidVotes <int>,\n## #   validVotes <int>, lsi <int>, ps <int>, pkd <int>, sfida <int>, pr <int>,\n## #   pd <int>, pbdksh <int>, adk <int>, psd <int>, ad <int>, frd <int>,\n## #   pds <int>, pdiu <int>, aak <int>, mega <int>, pksh <int>, apd <int>, ...\nalsample <- rsamp(df = albania, 544)\nalreceived <- rsamp(df = alsample, 390)\nsmissing(\n    sampdf = alsample,\n    colldf = alreceived,\n    strata = qarku,\n    col_name = qvKod\n)"},{"path":"sampling.html","id":"unequal-probability-sampling","chapter":"15 Sampling","heading":"15.3 Unequal Probability Sampling","text":"","code":"\nUPbrewer()\nUPmaxentropy()\nUPmidzuno()\nUPmidzunopi2()\nUPmultinomial()\nUPpivotal()\nUPrandompivotal()\nUPpoisson()\nUPsampford()\nUPsystematic()\nUPrandomsystematic()\nUPsystematicpi2()\nUPtille()\nUPtillepi2()"},{"path":"sampling.html","id":"balanced-sampling","chapter":"15 Sampling","heading":"15.4 Balanced Sampling","text":"Purpose: get means population sample auxiliary variablesPurpose: get means population sample auxiliary variablesBalanced sampling different purposive selectionBalanced sampling different purposive selectionBalancing equations\\[\n\\sum_{k \\S} \\frac{\\mathbf{x}_k}{\\pi_k} = \\sum_{k \\U} \\mathbf{x}_k\n\\]\\(\\mathbf{x}_k\\) vector auxiliary variables","code":""},{"path":"sampling.html","id":"cube","chapter":"15 Sampling","heading":"15.4.1 Cube","text":"flight phaseflight phaselanding phaselanding phase","code":"\nsamplecube()\nfastflightcube()\nlandingcube()"},{"path":"sampling.html","id":"stratification","chapter":"15 Sampling","heading":"15.4.2 Stratification","text":"Try replicate population based original multivariate histogram","code":"\nlibrary(survey)\ndata(\"api\")\nsrs_design <- svydesign(data = apistrat,\n                        weights = ~pw, \n                        fpc = ~fpc, \n                        strata = ~stype,\n                        id = ~1)\nbalancedstratification()"},{"path":"sampling.html","id":"cluster-1","chapter":"15 Sampling","heading":"15.4.3 Cluster","text":"","code":"\nlibrary(survey)\ndata(\"api\")\nsrs_design <- svydesign(data = apiclus1,\n                        weights = ~pw, \n                        fpc = ~fpc, \n                        id = ~dnum)\nbalancedcluster()"},{"path":"sampling.html","id":"two-stage","chapter":"15 Sampling","heading":"15.4.4 Two-stage","text":"","code":"\nlibrary(survey)\ndata(\"api\")\nsrs_design <- svydesign(data = apiclus2, \n                        fpc = ~fpc1 + fpc2, \n                        id = ~ dnum + snum)\nbalancedtwostage()"},{"path":"analysis-of-variance-anova.html","id":"analysis-of-variance-anova","chapter":"16 Analysis of Variance (ANOVA)","heading":"16 Analysis of Variance (ANOVA)","text":"ANOVA using underlying mechanism linear regression. However, angle ANOVA chooses look slightly different traditional linear regression. can useful case qualitative variables designed experiments.Experimental DesignFactor: explanatory predictor variable studied investigationTreatment (Factor Level): “value” factor applied experimental unitExperimental Unit: person, animal, piece material, etc. subjected treatment(s) provides responseSingle Factor Experiment: one explanatory variable consideredMultifactor Experiment: one explanatory variableClassification Factor: factor control experimenter (observational data)Experimental Factor: assigned experimenterBasics experimental design:Choices statistician make:\nset treatments\nset experimental units\ntreatment assignment (selection bias)\nmeasurement (measurement bias, blind experiments)\nChoices statistician make:set treatmentsset experimental unitstreatment assignment (selection bias)measurement (measurement bias, blind experiments)Advancements experimental design:\nFactorial Experiments:\nconsider multiple factors time (interaction)\nReplication: repetition experiment\nassess mean squared error\ncontrol precision experiment (power)\n\nRandomization\nR.. Fisher (1900s), treatments assigned systematically subjectively\nrandomization: assign treatments experimental units random, averages systematic effects control investigator\n\nLocal control: Blocking Stratification\nReduce experimental errors increase power placing restrictions randomization treatments experimental units.\n\nAdvancements experimental design:Factorial Experiments:\nconsider multiple factors time (interaction)Factorial Experiments:\nconsider multiple factors time (interaction)Replication: repetition experiment\nassess mean squared error\ncontrol precision experiment (power)\nReplication: repetition experimentassess mean squared errorcontrol precision experiment (power)Randomization\nR.. Fisher (1900s), treatments assigned systematically subjectively\nrandomization: assign treatments experimental units random, averages systematic effects control investigator\nRandomizationBefore R.. Fisher (1900s), treatments assigned systematically subjectivelyrandomization: assign treatments experimental units random, averages systematic effects control investigatorLocal control: Blocking Stratification\nReduce experimental errors increase power placing restrictions randomization treatments experimental units.\nLocal control: Blocking StratificationReduce experimental errors increase power placing restrictions randomization treatments experimental units.Randomization may also eliminate correlations due time space.","code":""},{"path":"analysis-of-variance-anova.html","id":"completely-randomized-design-crd","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1 Completely Randomized Design (CRD)","text":"Treatment factor \\(\\ge2\\) treatments levels. Experimental units randomly assinged treatment. number experiemntal units group can beequal (balanced): nunequal (unbalanced): \\(n_i\\) -th group (= 1,…,).total sample size \\(N=\\sum_{=1}^{}n_i\\)Possible assignments units treatments \\(k=\\frac{N!}{n_1!n_2!...n_a!}\\)probability 1/k selected. experimental unit measured response \\(Y_{ij}\\), j denotes unit denotes treatment.Treatmentwhere \\(\\bar{Y_{.}}=\\frac{1}{n_i}\\sum_{j=1}^{n_i}Y_{ij}\\)\\(s_i^2=\\frac{1}{n_i-1}\\sum_{j=1}^{n_i}(Y_{ij}-\\bar{Y_i})^2\\)grand mean \\(\\bar{Y_{..}}=\\frac{1}{N}\\sum_{}\\sum_{j}Y_{ij}\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"single-factor-fixed-effects-model","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.1 Single Factor Fixed Effects Model","text":"also known Single Factor (One-Way) ANOVA ANOVA Type model.Partitioning VarianceThe total variability \\(Y_{ij}\\) observation can measured deviation \\(Y_{ij}\\) around overall mean \\(\\bar{Y_{..}}\\): \\(Y_{ij} - \\bar{Y_{..}}\\)can rewritten : \\[\n\\begin{split}\nY_{ij} - \\bar{Y_{..}}&=Y_{ij} - \\bar{Y_{..}} + \\bar{Y_{.}} - \\bar{Y_{.}} \\\\\n&= (\\bar{Y_{.}}-\\bar{Y_{..}})+(Y_{ij}-\\bar{Y_{.}})\n\\end{split}\n\\] wherethe first term treatment differences (.e., deviation treatment mean overall mean)second term within treatment differences (.e., deviation observation around treatment mean)\\[\n\\begin{split}\n\\sum_{}\\sum_{j}(Y_{ij} - \\bar{Y_{..}})^2 &=  \\sum_{}n_i(\\bar{Y_{.}}-\\bar{Y_{..}})^2+\\sum_{}\\sum_{j}(Y_{ij}-\\bar{Y_{.}})^2 \\\\\nSSTO &= SSTR + SSE \\\\\ntotal~SS &= treatment~SS + error~SS \\\\\n(N-1)~d.f. &= (-1)~d.f. + (N - ) ~ d.f.\n\\end{split}\n\\]lose d.f. total corrected SSTO estimation mean (\\(\\sum_{}\\sum_{j}(Y_{ij} - \\bar{Y_{..}})=0\\))\n, SSTR \\(\\sum_{}n_i(\\bar{Y_{.}}-\\bar{Y_{..}})=0\\)Accordingly, \\(MSTR= \\frac{SST}{-1}\\) \\(MSR=\\frac{SSE}{N-}\\)ANOVA TableLinear Model Explanation ANOVA","code":""},{"path":"analysis-of-variance-anova.html","id":"cell-means-model","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.1.1 Cell means model","text":"\\[\nY_{ij}=\\mu_i+\\epsilon\\_{ij}\n\\]\\(Y_{ij}\\) response variable j-th subject -th treatment\\(Y_{ij}\\) response variable j-th subject -th treatment\\(\\mu_i\\): parameters (fixed) representing unknown population mean -th treatment\\(\\mu_i\\): parameters (fixed) representing unknown population mean -th treatment\\(\\epsilon_{ij}\\) independent \\(N(0,\\sigma^2)\\) errors\\(\\epsilon_{ij}\\) independent \\(N(0,\\sigma^2)\\) errors\\(E(Y_{ij})=\\mu_i\\) \\(var(Y_{ij})=var(\\epsilon_{ij})=\\sigma^2\\)\\(E(Y_{ij})=\\mu_i\\) \\(var(Y_{ij})=var(\\epsilon_{ij})=\\sigma^2\\)observations varianceAll observations varianceExample:= 3 (3 treatments) \\(n_1=n_2=n_3=2\\)\\[\n\\begin{split}\n\\left(\\begin{array}{c} \nY_{11}\\\\\nY_{12}\\\\\nY_{21}\\\\\nY_{22}\\\\\nY_{31}\\\\\nY_{32}\\\\\n\\end{array}\\right) &= \n\\left(\\begin{array}{ccc} \n1 & 0 & 0 \\\\ \n1 & 0 & 0 \\\\ \n0 & 1 & 0 \\\\ \n0 & 1 & 0 \\\\ \n0 & 0 & 1 \\\\ \n0 & 0 & 1 \\\\ \n\\end{array}\\right)\n\\left(\\begin{array}{c}\n\\mu_1 \\\\\n\\mu_2 \\\\\n\\mu_3 \\\\\n\\end{array}\\right) + \\left(\\begin{array}{c}\n\\epsilon_{11} \\\\\n\\epsilon_{12} \\\\\n\\epsilon_{21} \\\\\n\\epsilon_{22} \\\\\n\\epsilon_{31} \\\\\n\\epsilon_{32} \\\\\n\\end{array}\\right)\\\\\n\\mathbf{y} &= \\mathbf{X\\beta} +\\mathbf{\\epsilon}\n\\end{split}\n\\]\\(X_{k,ij}=1\\) k-th treatment used\\(X_{k,ij}=0\\) OtherwiseNote: intercept term.BLUE (best linear unbiased estimator) \\(\\beta=[\\mu_1 \\mu_2\\mu_3]'\\)\\[\nE(\\mathbf{b})=\\beta\n\\]\\[\nvar(\\mathbf{b})=\\sigma^2(\\mathbf{X'X})^{-1}=\\sigma^2\n\\left[\\begin{array}{ccc}\n1/n_1 & 0 & 0\\\\\n0 & 1/n_2 & 0\\\\\n0 & 0 & 1/n_3\\\\\n\\end{array}\\right]\n\\]\\(var(b_i)=var(\\hat{\\mu_i})=\\sigma^2/n_i\\) \\(\\mathbf{b} \\sim N(\\beta,\\sigma^2(\\mathbf{X'X})^{-1})\\)\\[\n\\begin{split}\nMSE &= \\frac{1}{N-} \\sum_{}\\sum_{j}(Y_{ij}-\\bar{Y_{.}})^2 \\\\\n    &= \\frac{1}{N-} \\sum_{}[(n_i-1)\\frac{\\sum_{}(Y_{ij}-\\bar{Y_{.}})^2}{n_i-1}] \\\\\n    &= \\frac{1}{N-} \\sum_{}(n_i-1)s_1^2\n\\end{split}\n\\]\\(E(s_i^2)=\\sigma^2\\)\\(E(MSE)=\\frac{1}{N-}\\sum_{}(n_i-1)\\sigma^2=\\sigma^2\\)Hence, MSE unbiased estimator \\(\\sigma^2\\), regardless whether treatment means equal .\\(E(MSTR)=\\sigma^2+\\frac{\\sum_{}n_i(\\mu_i-\\mu_.)^2}{-1}\\)\n\\(\\mu_.=\\frac{\\sum_{=1}^{}n_i\\mu_i}{\\sum_{=1}^{}n_i}\\)\ntreatment means equals (=\\(\\mu_.\\)), \\(E(MSTR)=\\sigma^2\\).can use F-test teh equality treatment means:\\[H_0:\\mu_1=\\mu_2=..=\\mu_a\\]\\[H_a: ~al l~ \\mu_i ~ ~ equal \\]\\(F=\\frac{MSTR}{MSE}\\)\nlarge values F support \\(H_a\\) (since MSTR tend exceed MSE \\(H_a\\) holds)\nF near 1 support \\(H_0\\) (upper tail test)Equivalently, \\(H_0\\) true, \\(F \\sim f_{(-1,N-)}\\)\\(F \\leq f_{(-1,N-;1-\\alpha)}\\), reject \\(H_0\\)\\(F \\geq f_{(-1,N-;1-\\alpha)}\\), reject \\(H_0\\)Note: = 2 (2 treatments), F-test = two sample t-test","code":""},{"path":"analysis-of-variance-anova.html","id":"treatment-effects-factor-effects","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.1.2 Treatment Effects (Factor Effects)","text":"Besides Cell means model, another way formalize one-way ANOVA: \\[Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\] \\(Y_{ij}\\) j-th response -th treatment\\(\\tau_i\\) -th treatment effect\\(\\mu\\) constant component, common observations\\(\\epsilon_{ij}\\) independent random errors ~ \\(N(0,\\sigma^2)\\)example, = 3, \\(n_1=n_2=n_3=2\\)However,\\[\n\\mathbf{X'X} = \n\\left(\n\\begin{array}\n{cccc}\n\\sum_{}n_i & n_1 & n_2 & n_3 \\\\\nn_1 & n_1 & 0 & 0 \\\\\nn_2 & 0 & n_2 & 0 \\\\\nn_3 & 0 & 0 & n_3 \\\\\n\\end{array}\n\\right)\n\\]singular thus exist, \\(\\mathbf{b}\\) insolvable (infinite solutions)Hence, impose restrictions parameters model matrix \\(\\mathbf{X}\\) full rank.Whatever restriction use, still :\\(E(Y_{ij})=\\mu + \\tau_i = \\mu_i = mean ~ response ~ ~ -th ~ treatment\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"restriction-on-sum-of-tau","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.1.2.1 Restriction on sum of tau","text":"\\(\\sum_{=1}^{}\\tau_i=0\\)implies\\[\n\\mu= \\mu +\\frac{1}{}\\sum_{=1}^{}(\\mu+\\tau_i)\n\\]average treatment mean (grand mean) (overall mean)\\[\n\\begin{split}\n\\tau_i  &=(\\mu+\\tau_i) -\\mu = \\mu_i-\\mu \\\\\n        &= \\text{treatment  mean} - \\text{grand~mean} \\\\\n        &= \\text{treatment  effect}\n\\end{split}\n\\]\\[\n\\tau_a=-\\tau_1-\\tau_2-...-\\tau_{-1}\n\\]Hence, mean -th treatment \\[\n\\mu_a=\\mu+\\tau_a=\\mu-\\tau_1-\\tau_2-...-\\tau_{-1}\n\\]Hence, model need “” parameters:\\[\n\\mu,\\tau_1,\\tau_2,..,\\tau_{-1}\n\\]Equation (16.2) becomeswhere \\(\\beta\\equiv[\\mu,\\tau_1,\\tau_2]'\\)Equation (16.1) \\(\\sum_{}\\tau_i=0\\) becomes\\[\n\\begin{split}\n\\mathbf{b}= \\left[\\begin{array}{c}\n\\hat{\\mu} \\\\\n\\hat{\\tau_1} \\\\\n\\hat{\\tau_2} \\\\\n\\end{array}\\right] &= \n(\\mathbf{x}'\\mathbf{x})^{-1}\\mathbf{x}'\\mathbf{y} \\\\\n& = \n\\left[\\begin{array}{ccc}\n\\sum_{}n_i & n_1-n_3 & n_2-n_3\\\\\nn_1-n_3 & n_1+n_3 & n_3\\\\\nn_2-n_3 & n_3 & n_2-n_3 \\\\\n\\end{array}\\right]^{-1}\n\\left[\\begin{array}{c}\nY_{..}\\\\\nY_{1.}-Y_{3.}\\\\\nY_{2.}-Y_{3.}\\\\\n\\end{array}\\right] \\\\\n& =\n\\left[\\begin{array}{c}\n\\frac{1}{3}\\sum_{=1}^{3}\\bar{Y_{.}}\\\\\n\\bar{Y_{1.}}-\\frac{1}{3}\\sum_{=1}^{3}\\bar{Y_{.}}\\\\\n\\bar{Y_{2.}}-\\frac{1}{3}\\sum_{=1}^{3}\\bar{Y_{.}}\\\\\n\\end{array}\\right]\\\\\n& = \n\\left[\\begin{array}{c}\n\\hat{\\mu}\\\\\n\\hat{\\tau_1}\\\\\n\\hat{\\tau_2}\\\\\n\\end{array}\\right]\n\\end{split}\n\\]\\(\\hat{\\tau_3}=-\\hat{\\tau_1}-\\hat{\\tau_2}=\\bar{Y_3}-\\frac{1}{3} \\sum_{}\\bar{Y_{.}}\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"restriction-on-first-tau","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.1.2.2 Restriction on first tau","text":"R, lm() uses restriction \\(\\tau_1=0\\)previous example, \\(n_1=n_2=n_3=2\\), \\(\\tau_1=0\\). treatment means can written :\\[\n\\mu_1= \\mu + \\tau_1 = \\mu + 0 = \\mu  \\\\\n\\mu_2= \\mu + \\tau_2 \\\\\n\\mu_3 = \\mu + \\tau_3\n\\]Hence, \\(\\mu\\) mean response first treatmentIn matrix form,\\[\n\\begin{split}\n\\left(\\begin{array}{c} \nY_{11}\\\\\nY_{12}\\\\\nY_{21}\\\\\nY_{22}\\\\\nY_{31}\\\\\nY_{32}\\\\\n\\end{array}\\right) &= \n\\left(\\begin{array}{ccc} \n1 & 0 & 0 \\\\ \n1 & 0 & 0 \\\\ \n1 & 1 & 0 \\\\ \n1 & 1 & 0 \\\\ \n1 & 0 & 1 \\\\ \n1 & 0 & 1 \\\\ \n\\end{array}\\right)\n\\left(\\begin{array}{c}\n\\mu \\\\\n\\tau_2 \\\\\n\\tau_3 \\\\\n\\end{array}\\right) + \\left(\\begin{array}{c}\n\\epsilon_{11} \\\\\n\\epsilon_{12} \\\\\n\\epsilon_{21} \\\\\n\\epsilon_{22} \\\\\n\\epsilon_{31} \\\\\n\\epsilon_{32} \\\\\n\\end{array}\\right)\\\\\n\\mathbf{y} &= \\mathbf{X\\beta} +\\mathbf{\\epsilon}\n\\end{split}\n\\]\\(\\beta = [\\mu,\\tau_2,\\tau_3]'\\)\\[\n\\begin{split}\n\\mathbf{b}= \\left[\\begin{array}{c}\n\\hat{\\mu} \\\\\n\\hat{\\tau_2} \\\\\n\\hat{\\tau_3} \\\\\n\\end{array}\\right] &= \n(\\mathbf{x}'\\mathbf{x})^{-1}\\mathbf{x}'\\mathbf{y} \\\\\n& = \n\\left[\\begin{array}{ccc}\n\\sum_{}n_i & n_2 & n_3\\\\\nn_2 & n_2 & 0\\\\\nn_3 & 0 & n_3 \\\\\n\\end{array}\\right]^{-1}\n\\left[\\begin{array}{c}\nY_{..}\\\\\nY_{2.}\\\\\nY_{3.}\\\\\n\\end{array}\\right] \\\\\n& = \n\\left[\n\\begin{array}{c}\n\\bar{Y_{1.}} \\\\\n\\bar{Y_{2.}} - \\bar{Y_{1.}} \\\\\n\\bar{Y_{3.}} - \\bar{Y_{1.}}\\\\\n\\end{array}\\right]\n\\end{split}\n\\]\\[\nE(\\mathbf{b})= \\beta = \n\\left[\\begin{array}{c}\n{\\mu}\\\\\n{\\tau_2}\\\\\n{\\tau_3}\\\\\n\\end{array}\\right]\n=\n\\left[\\begin{array}{c}\n\\mu_1\\\\\n\\mu_2-\\mu_1\\\\\n\\mu_3-\\mu_1\\\\\n\\end{array}\\right]\n\\]\\[\nvar(\\mathbf{b}) = \\sigma^2(\\mathbf{X'X})^{-1} \\\\\nvar(\\hat{\\mu}) = var(\\bar{Y_{1.}})=\\sigma^2/n_1 \\\\\nvar(\\hat{\\tau_2}) = var(\\bar{Y_{2.}}-\\bar{Y_{1.}}) = \\sigma^2/n_2 + \\sigma^2/n_1 \\\\\nvar(\\hat{\\tau_3}) = var(\\bar{Y_{3.}}-\\bar{Y_{1.}}) = \\sigma^2/n_3 + \\sigma^2/n_1\n\\]Note three parameterization, ANOVA table sameModel 1: \\(Y_{ij} = \\mu_i + \\epsilon_{ij}\\)Model 2: \\(Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\) \\(\\sum_{} \\tau_i=0\\)Model 3: \\(Y_{ij}= \\mu + \\tau_i + \\epsilon_{ij}\\) \\(\\tau_1=0\\)models calculation \\(\\hat{Y}\\) \\[\n\\mathbf{\\hat{Y} = X(X'X)^{-1}X'Y=PY = Xb}\n\\]ANOVA TableError(within treatments)\\(\\mathbf{P_1} = \\frac{1}{n}\\mathbf{J}\\)F-statistic (-1,N-) degrees freedom, gives value three parameterization, hypothesis test written bit different:\\[\nH_0 : \\mu_1 = \\mu_2 = ... = \\mu_a \\\\\nH_0 : \\mu + \\tau_1 = \\mu + tau_2 = ... = \\mu + \\tau_a \\\\\nH_0 : \\tau_1 = \\tau_2 = ...= \\tau_a \n\\]F-test serves preliminary analysis, see difference different factors. -depth analysis, consider different testing treatment effects.","code":""},{"path":"analysis-of-variance-anova.html","id":"testing-of-treatment-effects","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.1.3 Testing of Treatment Effects","text":"Single Treatment Mean \\(\\mu_i\\)Differences Treatment MeansA Contrast Among Treatment MeansA Linear Combination Treatment Means","code":""},{"path":"analysis-of-variance-anova.html","id":"single-treatment-mean","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.1.3.1 Single Treatment Mean","text":"\\(\\hat{\\mu_i}=\\bar{Y_{.}}\\) \\(E(\\bar{Y_{.}})=\\mu_i\\)\\(var(\\bar{Y_{}})=\\sigma^2/n_i\\) estimated \\(s^2(\\bar{Y_{.}})=MSE / n_i\\)Since \\(\\frac{\\bar{Y_{.}}-\\mu_i}{s(\\bar{Y_{.}})} \\sim t_{N-}\\) confidence interval \\(\\mu_i\\) \\(\\bar{Y_{.}} \\pm t_{1-\\alpha/2;N-}s(\\bar{Y_{.}})\\),\ncan t-test means difference constant c\\[\nH_0: \\mu_i = c \\\\\nH_1: \\mu_i \\neq c\n\\]\\[\nT =\\frac{\\bar{Y_{.}}-c}{s(\\bar{Y_{.}})}\n\\]follows \\(t_{N-}\\) \\(H_0\\) true.\n\\(|T| > t_{1-\\alpha/2;N-}\\), can reject \\(H_0\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"differences-between-treatment-means","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.1.3.2 Differences Between Treatment Means","text":"Let \\(D=\\mu_i - \\mu_i'\\), also known pairwise comparison\\(D\\) can estimated \\(\\hat{D}=\\bar{Y_{}}-\\bar{Y_{}}'\\) unbiased (\\(E(\\hat{D})=\\mu_i-\\mu_i'\\))Since \\(\\bar{Y_{}}\\) \\(\\bar{Y_{}}'\\) independent, \\[\nvar(\\hat{D})=var(\\bar{Y_{}}) + var(\\bar{Y_{'}}) = \\sigma^2(1/n_i + 1/n_i')\n\\]can estimated \\[\ns^2(\\hat{D}) = MSE(1/n_i + 1/n_i')\n\\]single treatment inference,\\[\n\\frac{\\hat{D}-D}{s(\\hat{D})} \\sim t_{N-}\n\\]hence,\\[\n\\hat{D} \\pm t_{(1-\\alpha/2;N-)}s(\\hat{D})\n\\]Hypothesis tests:\\[\nH_0: \\mu_i = \\mu_i' \\\\\nH_a: \\mu_i \\neq \\mu_i'\n\\]can tested following statistic\\[\nT = \\frac{\\hat{D}}{s(\\hat{D})} \\sim t_{1-\\alpha/2;N-}\n\\]reject \\(H_0\\) \\(|T| > t_{1-\\alpha/2;N-}\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"contrast-among-treatment-means","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.1.3.3 Contrast Among Treatment Means","text":"generalize comparison two means, contrastsA contrast linear combination treatment means:\\[\nL = \\sum_{=1}^{}c_i \\mu_i\n\\]\\(c_i\\) non-random constant sum 0:\\[\n\\sum_{=1}^{} c_i = 0\n\\]unbiased estimator contrast L \\[\n\\hat{L} = \\sum_{=1}^{}c_i \\bar{Y}_{.}\n\\]\\(E(\\hat{L}) = L\\). Since \\(\\bar{Y}_{.}\\), = 1,…, independent.\\[\nvar(\\hat{L}) = var(\\sum_{=1}^c_i \\bar{Y}_{.}) = \\sum_{=1}^var(c_i \\bar{Y}_i)  \\\\\n= \\sum_{=1}^c_i^2 var(\\bar{Y}_i) = \\sum_{=1}^c_i^2 \\sigma^2 /n_i \\\\\n= \\sigma^2 \\sum_{=1}^{} c_i^2 /n_i\n\\]Estimation variance:\\[\ns^2(\\hat{L}) = MSE \\sum_{=1}^{} \\frac{c_i^2}{n_i}\n\\]\\(\\hat{L}\\) normally distributed (since linear combination independent normal random variables)., since \\(SSE/\\sigma^2\\) \\(\\chi_{N-}^2\\)\\[\n\\frac{\\hat{L}-L}{s(\\hat{L})} \\sim t_{N-}\n\\]\\(1-\\alpha\\) confidence limits given \\[\n\\hat{L} \\pm t_{1-\\alpha/2; N-}s(\\hat{L})\n\\]Hypothesis testing\\[\nH_0: L = 0 \\\\\nH_a: L \\neq 0\n\\]\\[\nT = \\frac{\\hat{L}}{s(\\hat{L})}\n\\]reject H_0 \\(|T| > t_{1-\\alpha/2;N-}\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"linear-combination-of-treatment-means","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.1.3.4 Linear Combination of Treatment Means","text":"just like contrast \\(L = \\sum_{=1}^c_i \\mu_i\\) restrictions \\(c_i\\) coefficients.Tests og single treatment mean, two treatment means, contrasts can considered form perspective.\\[\nH_0: \\sum c_i \\mu_i = c \\\\\nH_a: \\sum c_i \\mu_i \\neq c \n\\]test statistics (t-stat) can considered equivalently F-tests; \\(F = (T)^2\\) \\(F \\sim F_{1,N-}\\). Since numerator degrees freedom always 1 cases, refer single-degree--freedom tests.Multiple ContrastsTo test simultaneously \\(k \\ge 2\\) contrasts, let \\(T_1,...,T_k\\) t-stat. joint distribution random variables multivariate t-distribution (tests dependent since re based data).Limitations comparing multiple contrasts:confidence coefficient \\(1-\\alpha\\) applies particular estimate, series estimates; similarly, Type error rate, \\(\\alpha\\), applies particular test, series tests. Example: 3 t-tests \\(\\alpha = 0.05\\), tests independent (), \\(0.95^3 = 0.857\\) (thus \\(\\alpha - 0.143\\) 0.05)confidence coefficient \\(1-\\alpha\\) applies particular estimate, series estimates; similarly, Type error rate, \\(\\alpha\\), applies particular test, series tests. Example: 3 t-tests \\(\\alpha = 0.05\\), tests independent (), \\(0.95^3 = 0.857\\) (thus \\(\\alpha - 0.143\\) 0.05)confidence coefficient \\(1-\\alpha\\) significance level \\(\\alpha\\) appropriate test suggest data.\noften, results experiment suggest important (ie..g, potential significant) relationships.\nprocess studying effects suggests data called data snooping\nconfidence coefficient \\(1-\\alpha\\) significance level \\(\\alpha\\) appropriate test suggest data.often, results experiment suggest important (ie..g, potential significant) relationships.process studying effects suggests data called data snoopingMultiple Comparison Procedures:TukeyScheffeBonferroni","code":""},{},{},{},{},{},{"path":"analysis-of-variance-anova.html","id":"multiple-comparisons-with-a-control","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.1.3.5 Multiple comparisons with a control","text":"","code":""},{},{"path":"analysis-of-variance-anova.html","id":"summary-3","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.1.3.6 Summary","text":"choosing multiple contrast method:Pairwise\nEqual groups sizes: Tukey\nUnequal groups sizes: Tukey, Scheffe\nPairwiseEqual groups sizes: TukeyUnequal groups sizes: Tukey, ScheffeNot pairwise\ncontrol: Dunnett\ngeneral: Bonferroni, Scheffe\npairwisewith control: Dunnettgeneral: Bonferroni, Scheffe","code":""},{"path":"analysis-of-variance-anova.html","id":"single-factor-random-effects-model","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.2 Single Factor Random Effects Model","text":"Also known ANOVA Type II models.Treatments chosen larger population. extend inference treatments population restrict inference treatments happened selected study.","code":""},{"path":"analysis-of-variance-anova.html","id":"random-cell-means","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.2.1 Random Cell Means","text":"\\[\nY_{ij} = \\mu_i + \\epsilon_{ij}\n\\]\\(\\mu_i \\sim N(\\mu, \\sigma^2_{\\mu})\\) independent\\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) independent\\(\\mu_i\\) \\(\\epsilon_{ij}\\) mutually independent \\(=1,...,; j = 1,...,n\\)treatment sample sizes equal\\[\nE(Y_{ij}) = E(\\mu_i) = \\mu \\\\\nvar(Y_{ij}) = var(\\mu_i) + var(\\epsilon_i) = \\sigma^2_{\\mu} + \\sigma^2\n\\]Since \\(Y_{ij}\\) independent\\[\n\\begin{aligned}\ncov(Y_{ij},Y_{ij'}) &= E(Y_{ij}Y_{ij'}) - E(Y_{ij})E(Y_{ij'})  \\\\\n&= E(\\mu_i^2 + \\mu_i \\epsilon_{ij'} + \\mu_i \\epsilon_{ij} + \\epsilon_{ij}\\epsilon_{ij'}) - \\mu^2 \\\\\n&= \\sigma^2_{\\mu} + \\mu^2 - \\mu^2 & \\text{} j \\neq j' \\\\\n&= \\sigma^2_{\\mu} & \\text{} j \\neq j' \n\\end{aligned}\n\\]\\[\n\\begin{aligned}\ncov(Y_{ij},Y_{'j'}) &= E(\\mu_i \\mu_{'} + \\mu_i \\epsilon_{'j'}+ \\mu_{'}\\epsilon_{ij}+ \\epsilon_{ij}\\epsilon_{'j'}) - \\mu^2 \\\\\n&= \\mu^2 - \\mu^2 & \\text{} \\neq ' \\\\\n&= 0 \\\\\n\\end{aligned}\n\\]Hence,observations varianceany two observations treatment covariance \\(\\sigma^2_{\\mu}\\)correlation two responses treatment:\\[\n\\begin{aligned}\n\\rho(Y_{ij},Y_{ij'}) &= \\frac{\\sigma^2_{\\mu}}{\\sigma^2_{\\mu}+ \\sigma^2} && \\text{$j \\neq j'$}\n\\end{aligned}\n\\]InferenceIntraclass Correlation Coefficient\\[\n\\frac{\\sigma^2_{\\mu}}{\\sigma^2 + \\sigma^2_{\\mu}}\n\\]measures proportion total variability \\(Y_{ij}\\) accounted variance \\(\\mu_i\\)\\[\nH_0: \\sigma_{\\mu}^2 = 0 \\\\\nH_a: \\sigma_{\\mu}^2 \\neq 0\n\\]\\(H_0\\) implies \\(\\mu_i = \\mu\\) , can tested F-test ANOVA.understandings Single Factor Fixed Effects Model Single Factor Random Effects Model different, ANOVA one factor model. difference expected mean squaresIf \\(\\sigma^2_\\mu\\), MSE MSTR expectation (\\(\\sigma^2\\)). Otherwise, \\(E(MSTR) >E(MSE)\\). Large values statistic\\[\nF = \\frac{MSTR}{MSE}\n\\]suggest reject \\(H_0\\).Since \\(F \\sim F_{(-1,(n-1))}\\) \\(H_0\\) holds. \\(F > f_{(1-\\alpha;-1,(n-1))}\\) reject \\(H_0\\). sample sizes equal, F-test can still used, df \\(-1\\) \\(N-\\).","code":""},{"path":"analysis-of-variance-anova.html","id":"estimation-of-mu","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.2.1.1 Estimation of \\(\\mu\\)","text":"unbiased estimator \\(E(Y_{ij})=\\mu\\) grand mean: \\(\\hat{\\mu} = \\hat{Y}_{..}\\)variance estimator \\[\n\\begin{aligned}\nvar(\\bar{Y}_{..}) &= var(\\sum_i \\bar{Y}_{.}/) \\\\\n&= \\frac{1}{^2}\\sum_ivar(\\bar{Y}_{.}) \\\\\n&= \\frac{1}{^2}\\sum_i(\\sigma^2_\\mu+\\sigma^2/n) \\\\\n&= \\frac{1}{^2}(\\sigma^2_{\\mu}+\\sigma^2/n) \\\\\n&= \\frac{n\\sigma^2_{\\mu}+ \\sigma^2}{}\n\\end{aligned}\n\\]unbiased estimator variance \\(s^2(\\bar{Y})=\\frac{MSTR}{}\\). Thus \\(\\frac{\\bar{Y}_{..}-\\mu}{s(\\bar{Y}_{..})} \\sim t_{-1}\\)\\(1-\\alpha\\) confidence interval \\(\\bar{Y}_{..} \\pm t_{(1-\\alpha/2;-1)}s(\\bar{Y}_{..})\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"estimation-of-sigma2_musigma2_musigma2","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.2.1.2 Estimation of \\(\\sigma^2_\\mu/(\\sigma^2_{\\mu}+\\sigma^2)\\)","text":"random fixed effects model, MSTR MSE independent. sample sizes equal (\\(n_i = n\\) ),\\[\n\\frac{\\frac{MSTR}{n\\sigma^2_\\mu+ \\sigma^2}}{\\frac{MSE}{\\sigma^2}} \\sim f_{(-1,(n-1))}\n\\]\\[\nP(f_{(\\alpha/2;-1,(n-1))}\\le \\frac{\\frac{MSTR}{n\\sigma^2_\\mu+ \\sigma^2}}{\\frac{MSE}{\\sigma^2}} \\le f_{(1-\\alpha/2;-1,(n-1))}) = 1-\\alpha\n\\]\\[\nL = \\frac{1}{n}(\\frac{MSTR}{MSE}(\\frac{1}{f_{(1-\\alpha/2;-1,(n-1))}})-1) \\\\\nU = \\frac{1}{n}(\\frac{MSTR}{MSE}(\\frac{1}{f_{(\\alpha/2;-1,(n-1))}})-1)\n\\]lower upper \\((L^*,U^*)\\) confidence limits \\(\\frac{\\sigma^2_\\mu}{\\sigma^2_\\mu + \\sigma^2}\\)\\[\nL^* = \\frac{L}{1+L} \\\\\nU^* = \\frac{U}{1+U}\n\\]lower limit \\(\\frac{\\sigma^2_\\mu}{\\sigma^2}\\) negative, customary set L = 0.","code":""},{"path":"analysis-of-variance-anova.html","id":"estimation-of-sigma2","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.2.1.3 Estimation of \\(\\sigma^2\\)","text":"\\((n-1)MSE/\\sigma^2 \\sim \\chi^2_{(n-1)}\\), \\((1-\\alpha)\\) confidence interval \\(\\sigma^2\\):\\[\n\\frac{(n-1)MSE}{\\chi^2_{1-\\alpha/2;(n-1)}} \\le \\sigma^2 \\le \\frac{(n-1)MSE}{\\chi^2_{\\alpha/2;(n-1)}}\n\\]can also used case sample sizes equal - df N-.","code":""},{"path":"analysis-of-variance-anova.html","id":"estimation-of-sigma2_mu","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.2.1.4 Estimation of \\(\\sigma^2_\\mu\\)","text":"\\(E(MSE) = \\sigma^2\\) \\(E(MSTR) = \\sigma^2 + n\\sigma^2_\\mu\\). Hence,\\[\n\\sigma^2_{\\mu} = \\frac{E(MSTR)- E(MSE)}{n}\n\\]unbiased estimator \\(\\sigma^2_\\mu\\) given \\[\ns^2_\\mu =\\frac{MSTR-MSE}{n}\n\\]\\(s^2_\\mu < 0\\), set \\(s^2_\\mu = 0\\)sample sizes equal,\\[\ns^2_\\mu = \\frac{MSTR - MSE}{n'}\n\\]\\(n' = \\frac{1}{-1}(\\sum_i n_i- \\frac{\\sum_i n^2_i}{\\sum_i n_i})\\)exact confidence intervals \\(\\sigma^2_\\mu\\), can approximate intervals.Satterthewaite Procedure can used construct approximate confidence intervals linear combination expected mean squares\nlinear combination:\\[\n\\sigma^2_\\mu = \\frac{1}{n} E(MSTR) + (-\\frac{1}{n}) E(MSE)\n\\]\\[\nS = d_1 E(MS_1) + ..+ d_h E(MS_h)\n\\]\\(d_i\\) coefficients.unbiased estimator S \\[\n\\hat{S} = d_1 MS_1 + ...+ d_h  MS_h \n\\]Let \\(df_i\\) degrees freedom associated teh mean square \\(MS_i\\). Satterthwaite approximation:\\[\n\\frac{(df)\\hat{S}}{S} \\sim \\chi^2_{df}\n\\]\\[\ndf = \\frac{(d_1MS_1+...+d_hMS_h)^2}{(d_1MS_1)^2/df_1 + ...+ (d_hMS_h)^2/df_h}\n\\]approximate \\(1-\\alpha\\) confidence interval S:\\[\n\\frac{(df)\\hat{S}}{\\chi^2_{1-\\alpha/2;df}} \\le S \\le \\frac{(df)\\hat{S}}{\\chi^2_{\\alpha/2;df}}\n\\]single factor random effects model\\[\n\\frac{(df)s^2_\\mu}{\\chi^2_{1-\\alpha/2;df}} \\le \\sigma^2_\\mu \\le \\frac{(df)s^2_\\mu}{\\chi^2_{\\alpha/2;df}}\n\\]\\[\ndf = \\frac{(sn^2_\\mu)^2}{\\frac{(MSTR)^2}{-1}+ \\frac{(MSE)^2}{(n-1)}}\n\\]","code":""},{"path":"analysis-of-variance-anova.html","id":"random-treatment-effects-model","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.2.2 Random Treatment Effects Model","text":"\\[\n\\tau_i = \\mu_i - E(\\mu_i) = \\mu_i - \\mu\n\\]\\(\\mu_i = \\mu + \\tau_i\\) \\[\nY_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\n\\]\\(\\mu\\) = constant, common observations\\(\\tau_i \\sim N(0,\\sigma^2_\\tau)\\) independent (random variables)\\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) independent.\\(\\tau_{}, \\epsilon_{ij}\\) independent (=1,…,; j =1,..,n)model concerned balanced single factor ANOVA.Diagnostics MeasuresNon-constant error variance (plots, Levene test, Hartley test).Non-independence errors (plots, Durban-Watson test).Outliers (plots, regression methods).Non-normality error terms (plots, Shapiro-Wilk, Anderson-Darling).Omitted Variable Bias (plots)RemedialWeighted Least SquaresTransformationsNon-parametric Procedures.NoteFixed effect ANOVA relatively robust \nnon-normality\nunequal variances sample sizes approximately equal; least F-test multiple comparisons. However, single comparisons treatment means sensitive unequal variances.\nFixed effect ANOVA relatively robust tonon-normalityunequal variances sample sizes approximately equal; least F-test multiple comparisons. However, single comparisons treatment means sensitive unequal variances.Lack independence can seriously affect fixed random effect ANVOA.Lack independence can seriously affect fixed random effect ANVOA.","code":""},{"path":"analysis-of-variance-anova.html","id":"two-factor-fixed-effect-anova","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.3 Two Factor Fixed Effect ANOVA","text":"multi-factor experiment ismore efficientprovides infogives validity findings.","code":""},{"path":"analysis-of-variance-anova.html","id":"balanced","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.3.1 Balanced","text":"Assumption:treatment sample sizes equalAll treatment means equal importanceAssume:Factor levels Factor B b levels. x b factor levels considered.number treatments level n. \\(N = abn\\) observations study.","code":""},{"path":"analysis-of-variance-anova.html","id":"cell-means-model-1","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.3.1.1 Cell Means Model","text":"\\[\nY_{ijk} = \\mu_{ij} + \\epsilon_{ijk}\n\\]\\(\\mu_{ij}\\) fixed parameters (cell means)\\(= 1,...,\\) = levels Factor \\(j = 1,...,b\\) = levels Factor B.\\(\\epsilon_{ijk} \\sim \\text{indep } N(0,\\sigma^2)\\) \\(= 1,...,\\), \\(j = 1,..,b\\) \\(k = 1,..,n\\)\\[\nE(Y_{ijk}) = \\mu_{ij} \\\\\nvar(Y_{ijk}) = var(\\epsilon_{ijk}) = \\sigma^2\n\\]Hence,\\[\nY_{ijk} \\sim \\text{indep } N(\\mu_{ij},\\sigma^2)\n\\]model \\[\n\\mathbf{Y} = \\mathbf{X} \\beta + \\epsilon\n\\]Thus,\\[\nE(\\mathbf{Y}) = \\mathbf{X}\\beta \\\\\nvar(\\mathbf{Y}) = \\sigma^2 \\mathbf{}\n\\]Interaction\\[\n(\\alpha \\beta)_{ij} = \\mu_{ij} - (\\mu_{..}+ \\alpha_i + \\beta_j)\n\\]\\(\\mu_{..} = \\sum_i \\sum_j \\mu_{ij}/ab\\) grand mean\\(\\alpha_i = \\mu_{.}-\\mu_{..}\\) main effect factor -th level\\(\\beta_j = \\mu_{.j} - \\mu_{..}\\) main effect factor B j-th level\\((\\alpha \\beta)_{ij}\\) interaction effect factor -th level factor B j-th level.\\((\\alpha \\beta)_{ij} = \\mu_{ij} - \\mu_{.}-\\mu_{.j}+ \\mu_{..}\\)Examine interactions:Examine whether \\(\\mu_{ij}\\) can expressed sums \\(\\mu_{..} + \\alpha_i + \\beta_j\\)Examine whether difference mean responses two levels factor B levels factor .Examine whether difference mean response two levels factor levels factor BExamine whether treatment mean curves different factor levels treatment plot parallel.\\(j = 1,...,b\\)\\[\n\\begin{aligned}\n\\sum_i(\\alpha \\beta)_{ij} &= \\sum_i (\\mu_{ij} - \\mu_{..} - \\alpha_i - \\beta_j) \\\\\n&= \\sum_i \\mu_{ij} - \\mu_{..} - \\sum_i \\alpha_i - \\beta_j \\\\\n&= \\mu_{.j} - \\mu_{..}- \\sum_i (\\mu_{.} - \\mu_{..}) - (\\mu_{.j}-\\mu_{..}) \\\\\n&= \\mu_{.j} - \\mu_{..} - \\mu_{..}+ \\mu_{..} - (\\mu_{.j} - \\mu_{..}) \\\\\n&= 0\n\\end{aligned}\n\\]Similarly, \\(\\sum_j (\\alpha \\beta) = 0, = 1,...,\\) \\(\\sum_i \\sum_j (\\alpha \\beta)_{ij} =0\\), \\(\\sum_i \\alpha_i = 0\\), \\(\\sum_j \\beta_j = 0\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"factor-effects-model","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.3.1.2 Factor Effects Model","text":"\\[\n\\mu_{ij} = \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} \\\\\nY_{ijk} = \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}\n\\]\\(\\mu_{..}\\) constant\\(\\alpha_i\\) constants subject restriction \\(\\sum_i \\alpha_i=0\\)\\(\\beta_j\\) constants subject restriction \\(\\sum_j \\beta_j = 0\\)\\((\\alpha \\beta)_{ij}\\) constants subject restriction \\(\\sum_i(\\alpha \\beta)_{ij} = 0\\) \\(j=1,...,b\\) \\(\\sum_j(\\alpha \\beta)_{ij} = 0\\) \\(= 1,...,\\)\\(\\epsilon_{ijk} \\sim \\text{indep } N(0,\\sigma^2)\\) \\(k = 1,..,n\\)\\[\nE(Y_{ijk}) = \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij}\\\\\nvar(Y_{ijk}) = \\sigma^2 \\\\\nY_{ijk} \\sim N (\\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij}, \\sigma^2)\n\\]\\(1++b+ab\\) parameters. \\(ab\\) parameters Cell Means Model. Factor Effects Model, restrictions limit number parameters can estimated:\\[\n1 \\text{ } \\mu_{..} \\\\\n(-1) \\text{ } \\alpha_i \\\\\n(b-1) \\text{ } \\beta_j \\\\\n(-1)(b-1) \\text{ } (\\alpha \\beta)_{ij}\n\\]Hence, \\[\n1 + - 1 + b - 1 + ab - - b + 1 = ab\n\\]parameters model.can several restrictions considering model form \\(\\mathbf{Y} = \\mathbf{X} \\beta + \\epsilon\\)One way:\\[\n\\alpha_a  = \\alpha_1 - \\alpha_2 - ... - \\alpha_{-1} \\\\\n\\beta_b = -\\beta_1 - \\beta_2 - ... - \\beta_{b-1} \\\\\n(\\alpha \\beta)_{ib} = -(\\alpha \\beta)_{i1} -(\\alpha \\beta)_{i2} -...-(\\alpha \\beta)_{,b-1} ; = 1,..,\\\\\n(\\alpha \\beta)_{aj} = -(\\alpha \\beta)_{1j}-(\\alpha \\beta)_{2j} - ... -(\\alpha \\beta)_{-1,j}; j = 1,..,b\n\\]can fit model least squares maximum likelihoodCell Means Model\nminimize\\[\nQ = \\sum_i \\sum_j \\sum_k (Y_{ijk}-\\mu_{ij})^2\n\\]estimators\\[\n\\hat{\\mu}_{ij}= \\bar{Y}_{ij} \\\\\n\\hat{Y}_{ijk} = \\bar{Y}_{ij} \\\\\ne_{ijk} = Y_{ijk} - \\hat{Y}_{ijk} = Y_{ijk} - \\bar{Y}_{ij}\n\\]Factor Effects Model\\[\nQ = \\sum_i \\sum_j \\sum_k (Y_{ijk} - \\mu_{..}-\\alpha_i = \\beta_j - (\\alpha \\beta)_{ij})^2\n\\]subject restrictions\\[\n\\sum_i \\alpha_i = 0 \\\\\n\\sum_j \\beta_j = 0 \\\\\n\\sum_i (\\alpha \\beta)_{ij} = 0 \\\\\n\\sum_j (\\alpha \\beta)_{ij} = 0\n\\]estimators\\[\n\\hat{\\mu}_{..} = \\bar{Y}_{...} \\\\\n\\hat{\\alpha}_i = \\bar{Y}_{..} - \\bar{Y}_{...} \\\\\n\\hat{\\beta}_j = \\bar{Y}_{.j.}-\\bar{Y}_{...} \\\\\n(\\hat{\\alpha \\beta})_{ij} = \\bar{Y}_{ij.} - \\bar{Y}_{..} - \\bar{Y}_{.j.}+ \\bar{Y}_{...}\n\\]fitted values\\[\n\\hat{Y}_{ijk} = \\bar{Y}_{...}+ (\\bar{Y}_{..}- \\bar{Y}_{...})+ (\\bar{Y}_{.j.}- \\bar{Y}_{...}) + (\\bar{Y}_{ij.} - \\bar{Y}_{..}-\\bar{Y}_{.j.}+\\bar{Y}_{...}) = \\bar{Y}_{ij.}\n\\]\\[\ne_{ijk} = Y_{ijk} - \\bar{Y}_{ij.} \\\\\ne_{ijk} \\sim \\text{ indep } (0,\\sigma^2)\n\\]\\[\ns^2_{\\hat{\\mu}..} = \\frac{MSE}{nab} \\\\\ns^2_{\\hat{\\alpha}_i} = MSE(\\frac{1}{nb} - \\frac{1}{nab}) \\\\\ns^2_{\\hat{\\beta}_j} = MSE(\\frac{1}{na} - \\frac{1}{nab}) \\\\\ns^2_{(\\hat{\\alpha\\beta})_{ij}} = MSE (\\frac{1}{n} - \\frac{1}{na}- \\frac{1}{nb} + \\frac{1}{nab})\n\\]","code":""},{},{},{},{},{"path":"analysis-of-variance-anova.html","id":"unbalanced","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.3.2 Unbalanced","text":"unequal numbers replications treatment combinations:observational studiesdropouts designed studieslarger sample sizes inexpensive treatmentsSample sizes match population makeup.Assume factor combination least 1 observation (empty cells)Consider model :\\[\nY_{ijk} = \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}\n\\]sample sizes : \\(n_{ij}\\):\\[\nn_{.} = \\sum_j n_{ij} \\\\\nn_{.j} = \\sum_i n_{ij} \\\\\nn_T = \\sum_i \\sum_j n_{ij}\n\\]Problem \\[\nSSTO \\neq SSA + SSB + SSAB + SSE\n\\](design non-orthogonal)\\(= 1,...,-1,\\)\n\\begin{equation} u_i = \\begin{cases} +1 & \\text{obs -th level Factor 1} \\\\ -1 & \\text{obs -th level Factor 1} \\\\ 0 & \\text{otherwise} \\\\ \\end{cases} \\end{equation}\\(j=1,...,b-1\\)\n\\begin{equation} v_i = \\begin{cases} +1 & \\text{obs j-th level Factor 1} \\\\ -1 & \\text{obs b-th level Factor 1} \\\\ 0 & \\text{otherwise} \\\\ \\end{cases} \\end{equation}can use indicator variables predictor variables \\(\\mu_{..}, \\alpha_i ,\\beta_j, (\\alpha \\beta)_{ij}\\) unknown parameters.\\[\nY = \\mu_{..} + \\sum_{=1}^{-1} \\alpha_i u_i + \\sum_{j=1}^{b-1} \\beta_j v_j + \\sum_{=1}^{-1} \\sum_{j=1}^{b-1}(\\alpha \\beta)_{ij} u_i v_j + \\epsilon\n\\]test hypotheses, use extra sum squares idea.interaction effects\\[\nH_0: (\\alpha \\beta)_{ij} = 0 \\\\\nH_a: \\text{}(\\alpha \\beta)_{ij} =0\n\\]test\\[\nH_0: \\beta_1 = \\beta_2 = \\beta_3 = 0 \\\\\nH_a: \\text{} \\beta_j = 0\n\\]Analysis Factor Means(e.g., contrasts) analogous balanced case, modifications formulas means standard errors account unequal sample sizes., can fit cell means model consider regression perspectiveIf empty cells (.e., factor combinations observation), equivalent regression approach can’t used. can still partial analyses","code":""},{"path":"analysis-of-variance-anova.html","id":"two-way-random-effects-anova","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.4 Two-Way Random Effects ANOVA","text":"\\[\nY_{ijk} = \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ij}\n\\]\\(\\mu_{..}\\): constant\\(\\alpha_i \\sim N(0,\\sigma^2_{\\alpha}), = 1,..,\\) (independent)\\(\\beta_j \\sim N(0,\\sigma^2_{\\beta}), j = 1,..,b\\) (independent)\\((\\alpha \\beta)_{ij} \\sim N(0,\\sigma^2_{\\alpha \\beta}),=1,...,,j=1,..,b\\) (independent)\\(\\epsilon_{ijk} \\sim N(0,\\sigma^2)\\) (independent)\\(\\alpha_i, \\beta_j, (\\alpha \\beta)_{ij}\\) pairwise independentTheoretical means, variances, covariances \\[\nE(Y_{ijk}) = \\mu_{..} \\\\\nvar(Y_{ijk}) = \\sigma^2_Y= \\sigma^2_\\alpha + \\sigma^2_\\beta +  \\sigma^2_{\\alpha \\beta} + \\sigma^2 \n\\]\\(Y_{ijk} \\sim N(\\mu_{..},\\sigma^2_\\alpha + \\sigma^2_\\beta + \\sigma^2_{\\alpha \\beta} + \\sigma^2)\\)\\[\ncov(Y_{ijk},Y_{ij'k'}) = \\sigma^2_{\\alpha}, j \\neq j' \\\\\ncov(Y_{ijk},Y_{'jk'}) = \\sigma^2_{\\beta}, \\neq '\\\\\ncov(Y_{ijk},Y_{ijk'}) = \\sigma^2_\\alpha + \\sigma^2_{\\beta} + \\sigma^2_{\\alpha \\beta}, k \\neq k' \\\\\ncov(Y_{ijk},Y_{'j'k'}) = , \\neq ', j \\neq j'\n\\]","code":""},{"path":"analysis-of-variance-anova.html","id":"two-way-mixed-effects-anova","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.5 Two-Way Mixed Effects ANOVA","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"balanced-1","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.5.1 Balanced","text":"One fixed factor, random treatment levels, mixed effects model mixed modelRestricted mixed model 2-way ANOVA:\\[\nY_{ijk} = \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk}\n\\]\\(\\mu_{..}\\): constant\\(\\alpha_i\\): fixed effects constraints subject restriction \\(\\sum \\alpha_i = 0\\)\\(\\beta_j \\sim indep N(0,\\sigma^2_\\beta)\\)\\((\\alpha \\beta)_{ij} \\sim N(0,\\frac{-1}{}\\sigma^2_{\\alpha \\beta})\\) subject restriction \\(\\sum_i (\\alpha \\beta)_{ij} = 0\\) j, variance written proportion convenience; makes expected mean squares simpler (assumed \\(var((\\alpha \\beta)_{ij}= \\sigma^2_{\\alpha \\beta}\\))\\(cov((\\alpha \\beta)_{ij},(\\alpha \\beta)_{'j'}) = - \\frac{1}{} \\sigma^2_{\\alpha \\beta}, \\neq '\\)\\(\\epsilon_{ijk}\\sim indepN(0,\\sigma^2)\\)\\(\\beta_j, (\\alpha \\beta)_{ij}, \\epsilon_{ijk}\\) pairwise independentTwo-way mixed models written “unrestricted” form, restrictions interaction effects \\((\\alpha \\beta)_{ij}\\), pairwise independent. Let \\(\\beta^*, (\\alpha \\beta)^*_{ij}\\) unrestricted random effects, \\((\\bar{\\alpha \\beta})_{ij}^*\\) means averaged fixed factor level random factor B.\\[\n\\beta_j = \\beta_j^* + (\\bar{\\alpha \\beta})_{ij}^* \\\\\n(\\alpha \\beta)_{ij} = (\\alpha \\beta)_{ij}^* - (\\bar{\\alpha \\beta})_{ij}^*\n\\]consider restricted model general. consider restricted form.\\[\nE(Y_{ijk}) = \\mu_{..} + \\alpha_i \\\\\nvar(Y_{ijk}) = \\sigma^2_\\beta + \\frac{-1}{} \\sigma^2_{\\alpha \\beta} + \\sigma^2\n\\]Responses random factor (B) level correlated\\[\ncov(Y_{ijk},Y_{ijk'}) = E(Y_{ijk}Y_{ijk'}) - E(Y_{ijk})E(Y_{ijk'}) \\\\\n= \\sigma^2_\\beta + \\frac{-1}{} \\sigma^2_{\\alpha \\beta} , k \\neq k'\n\\]Similarly,\\[\ncov(Y_{ijk},Y_{'jk'}) = \\sigma^2_\\beta - \\frac{1}{} \\sigma^2_{\\alpha\\ \\beta}, \\neq ' \\\\\ncov(Y_{ijk},Y_{'j'k'}) = 0,  j \\neq j'\n\\]Hence, can see way don’t dependence Y don’t share random effect.advantage restricted mixed model 2 observations random factor b level can positively negatively correlated. unrestricted model, can positively correlated.Fixed ANOVARandom ANOVAMixed ANVOAFor fixed, random, mixed models (balanced), ANOVA table sums squares calculations identical. (also true df mean squares). difference expected mean squares, thus test statistics.Random ANOVA, test\\[\nH_0: \\sigma^2 = 0 \\\\\nH_a: \\sigma^2 > 0 \n\\]considering \\(F= \\frac{MSA}{MSAB} \\sim F_{-1;(-1)(b-1)}\\)test statistic used mixed models, case testing null hypothesis \\(\\alpha_i = 0\\)test statistic different null hypothesis fixed effects model.Fixed ANOVARandom ANOVAMixed ANOVAEstimation Variance ComponentsIn random mixed effects models, interested estimating variance components\nVariance component \\(\\sigma^2_\\beta\\) mixed ANOVA.\\[\nE(\\sigma^2_\\beta) = \\frac{E(MSB)-E(MSE)}{na} = \\frac{\\sigma^2 + na \\sigma^2_\\beta - \\sigma^2}{na} = \\sigma^2_\\beta\n\\]can estimated \\[\n\\hat{\\sigma}^2_\\beta = \\frac{MSB - MSE}{na}\n\\]Confidence intervals variance components can constructed (approximately) using Satterthwaite procedure MLS procedure (like 1-way random effects)Estimation Fixed Effects Mixed Models\\[\n\\hat{\\alpha}_i = \\bar{Y}_{..} - \\bar{Y}_{...} \\\\\n\\hat{\\mu}_{.} = \\bar{Y}_{...} + (\\bar{Y}_{..}- \\bar{Y}_{...}) = \\bar{Y}_{..}  \\\\\n\\sigma^2(\\hat{\\alpha}_i) = \\frac{\\sigma^2 + n \\sigma^2_{\\alpha \\beta}}{bn} = \\frac{E(MSAB)}{bn} \\\\\ns^2(\\hat{\\alpha}_i) = \\frac{MSAB}{bn}\n\\]Contrasts Fixed Effects\\[\nL = \\sum c_i \\alpha_i \\\\\n\\sum c_i = 0 \\\\\n\\hat{L} = \\sum c_i \\hat{\\alpha}_i \\\\\n\\sigma^2(\\hat{L}) = \\sum c^2_i \\sigma^2 (\\hat{\\alpha}_i) \\\\\ns^2(\\hat{L}) = \\frac{MSAB}{bn} \\sum c^2_i\n\\]Confidence intervals tests can constructed usual","code":""},{"path":"analysis-of-variance-anova.html","id":"unbalanced-1","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.1.5.2 Unbalanced","text":"mixed model = 2, b = 4\\[\nY_{ijk} = \\mu_{..} + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\epsilon_{ijk} \\\\\nvar(\\beta_j)= \\sigma^2_\\beta \\\\\nvar((\\alpha \\beta)_{ij})= \\frac{2-1}{2}\\sigma^2_{\\alpha \\beta} = \\frac{\\sigma^2_{\\alpha \\beta}}{2} \\\\\nvar(\\epsilon_{ijk}) = \\sigma^2 \\\\\nE(Y_{ijk}) = \\mu_{..} + \\alpha_i \\\\\nvar(Y_{ijk}) = \\sigma^2_{\\beta} + \\frac{\\sigma^2_{\\alpha \\beta}}{2} + \\sigma^2 \\\\\ncov(Y_{ijk},Y_{ijk'}) = \\sigma^2 + \\frac{\\sigma^2_{\\alpha \\beta}}{2}, k \\neq k' \\\\\ncov(Y_{ijk},Y_{'jk'}) = \\sigma^2_{\\beta} - \\frac{\\sigma^2_{\\alpha \\beta}}{2}, \\neq ' \\\\\ncov(Y_{ijk},Y_{'j'k'}) = 0, j \\neq j' \n\\]assume\\[\n\\mathbf{Y} \\sim N(\\mathbf{X}\\beta, M)\n\\]\\(M\\) block diagonaldensity function\\[\nf(\\mathbf{Y}) = \\frac{1}{(2\\pi)^{N/2}|M|^{1/2}}exp(-\\frac{1}{2}\\mathbf{(Y - X \\beta)' M^{-1}(Y-X\\beta)})\n\\]knew variance components, use GLS:\\[\n\\hat{\\beta}_{GLS} = \\mathbf{(X'M^{-1}X)^{-1}X'M^{-1}Y}\n\\]usually don’t know variance components \\(\\sigma^2, \\sigma^2_\\beta, \\sigma^2_{\\alpha \\beta}\\) make \\(M\\)\nAnother way get estimates Maximum likelihood estimationwe try maximize log\\[\n\\ln L = - \\frac{N}{2} \\ln (2\\pi) - \\frac{1}{2}\\ln|M| - \\frac{1}{2} \\mathbf{(Y-X \\beta)'\\Sigma^{-1}(Y-X\\beta)}\n\\]","code":""},{"path":"analysis-of-variance-anova.html","id":"nonparametric-anova","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.2 Nonparametric ANOVA","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"kruskal-wallis","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.2.1 Kruskal-Wallis","text":"Generalization independent samples Wilcoxon Rank sum test 2 independent samples (like F-test one-way ANOVA generalization several independent maples two sample t-test)Consider one-way case:\\(\\ge2\\) treatments\\(n_i\\) sample size ith treatment\\(Y_{ij}\\) j-th observation ith treatment.make assumption normalityWe assume observations ith treatment random sample continuous CDF \\(F_i\\), = 1,..,n, mutually independent.\\[\nH_0: F_1 = F_2 = ... = F_a \\\\\nH_a: F_i < F_j \\text{ } \\neq j\n\\]distribution location-scale family, \\(H_0: \\theta_1 = \\theta_2 = ... = \\theta_a\\))ProcedureRank \\(N = \\sum_{=1}^n_i\\) observations ascending order. Let \\(r_{ij} = rank(Y_{ij})\\), note \\(\\sum_i \\sum_j r_{ij} = 1 + 2 .. + N = \\frac{N(N+1)}{2}\\)Calculate rank sums averages:\\[\nr_{.} = \\sum_{j=1}^{n_i} r_{ij}\n\\] \\[\n\\bar{r}_{.} = \\frac{r_{.}}{n_i}, = 1,..,\n\\]Calculate test statistic ranks: \\[\n\\chi_{KW}^2 = \\frac{SSTR}{\\frac{SSTO}{N-1}}\n\\] \\(SSTR = \\sum n_i (\\bar{r}_{.}- \\bar{r}_{..})^2\\) \\(SSTO = \\sum \\sum (\\bar{r}_{ij}- \\bar{r}_{..})^2\\)large \\(n_i\\) (\\(\\ge 5\\) observations) Kruskal-Wallis statistic approximated \\(\\chi^2_{-1}\\) distribution treatment means equal. Hence, reject \\(H_0\\) \\(\\chi^2_{KW} > \\chi^2_{(1-\\alpha;-1)}\\).sample sizes small, one can exhaustively work possible distinct ways assigning N ranks observations treatments calculate value KW statistic case (\\(\\frac{N!}{n_1!..n_a!}\\) possible combinations). \\(H_0\\) assignments equally likely.","code":""},{"path":"analysis-of-variance-anova.html","id":"friedman-test","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.2.2 Friedman Test","text":"responses \\(Y_{ij} = 1,..,n, j = 1,..,r\\) randomized complete block design normally distributed (constant variance), nonparametric test helpful.distribution-free rank-based test comparing treatments setting Friedman test. Let \\(F_{ij}\\) CDF random \\(Y_{ij}\\), corresponding observed value \\(y_{ij}\\)null hypothesis, \\(F_{ij}\\) identical treatments j separately block .\\[\nH_0: F_{i1} = F_{i2} = ... = F_{ir}  \\text{ } \\\\\nH_a: F_{ij} < F_{ij'} \\text{ } j \\neq j' \\text{ } \n\\]location parameter distributions, treatment effects can tested:\\[\nH_0: \\tau_1 = \\tau_2 = ... = \\tau_r \\\\\nH_a: \\tau_j > \\tau_{j'} \\text{ } j \\neq j'\n\\]ProcedureRank observations r treatments separately within block (ascending order; ties, tied observation given mean ranks involved). Let ranks called \\(r_{ij}\\)Calculate Friedman test statistic\\[\n\\chi^2_F = \\frac{SSTR}{\\frac{SSTR + SSE}{n(r-1)}}\n\\] \\[\nSSTR = n \\sum (\\bar{r}_{.j}-\\bar{r}_{..})^2 \\\\\nSSE = \\sum \\sum (r_{ij} - \\bar{r}_{.j})^2 \\\\\n\\bar{r}_{.j} = \\frac{\\sum_i r_{ij}}{n}\\\\\n\\bar{r}_{..} = \\frac{r+1}{2}\n\\]ties, can rewritten \\[\n\\chi^2_{F} = [\\frac{12}{nr(n+1)}\\sum_j r_{.j}^2] - 3n(r+1)\n\\]large number blocks, \\(\\chi^2_F\\) approximately \\(\\chi^2_{r-1}\\) \\(H_0\\). Hence, reject \\(H_0\\) \\(\\chi^2_F > \\chi^2_{(1-\\alpha;r-1)}\\)\nexact null distribution \\(\\chi^2_F\\) can derived since r! possible ways assigning ranks 1,2,…,r r observations within block. n blocks thus \\((r!)^n\\) possible assignments ranks, equally likely \\(H_0\\) true.","code":""},{"path":"analysis-of-variance-anova.html","id":"sample-size-planning-for-anova","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.3 Sample Size Planning for ANOVA","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"balanced-designs","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.3.1 Balanced Designs","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"single-factor-studies","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.3.1.1 Single Factor Studies","text":"","code":""},{"path":"analysis-of-variance-anova.html","id":"fixed-cell-means","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.3.1.1.1 Fixed cell means","text":"\\[\nP(F>f_{(1-\\alpha;-1,N-)}|\\phi) = 1 - \\beta\n\\]\\(\\phi\\) noncentrality parameter (measures unequal treatment means \\(\\mu_i\\) )\\[\n\\phi = \\frac{1}{\\sigma}\\sqrt{\\frac{n}{}\\sum_i (\\mu_i - \\mu_.)^2} , (n_i \\equiv n)\n\\]\\[\n\\mu_. = \\frac{\\sum \\mu_i}{}\n\\]decide power probabilities use noncetral F distribution.use power table directly effects fixed design balanced using minimum range factor level means desired differences\\[\n\\Delta = \\max(\\mu_i) - \\min(\\mu_i)\n\\]Hence, need\\(\\alpha\\) level\\(\\Delta\\)\\(\\sigma\\)\\(\\beta\\)Notes:\\(\\Delta/\\sigma\\) small greatly affects sample size, \\(\\Delta/\\sigma\\) large.Reducing \\(\\alpha\\) \\(\\beta\\) increases required sample sizes.Error estimating \\(\\sigma\\) can make large difference.","code":""},{"path":"analysis-of-variance-anova.html","id":"multi-factor-studies","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.3.1.2 Multi-factor Studies","text":"noncentral F tables can used hereFor two-factor fixed effect modelTest interactions:\\[\n\\phi = \\frac{1}{\\sigma} \\sqrt{\\frac{n \\sum \\sum (\\alpha \\beta_{ij})^2}{(-1)(b-1)+1}} = \\frac{1}{\\sigma} \\sqrt{\\frac{n \\sum \\sum (\\mu_{ij}- \\mu_{.} - \\mu_{.j} + \\mu_{..})^2}{(-1)(b-1)+1}} \\\\\n\\upsilon_1 = (-1)(b-1) \\\\\n\\upsilon_2 = ab(n-1)\n\\]Test Factor main effects:\\[\n\\phi = \\frac{1}{\\sigma} \\sqrt{\\frac{nb \\sum \\alpha_i^2}{}} = \\frac{1}{\\sigma}\\sqrt{\\frac{nb \\sum (\\mu_{.}- \\mu_{..})^2}{}} \\\\\n\\upsilon_1 = -1 \\\\\n\\upsilon_2 = ab(n-1)\n\\]Test Factor B main effects:\\[\n\\phi = \\frac{1}{\\sigma} \\sqrt{\\frac{na \\sum \\beta_j^2}{b}} = \\frac{1}{\\sigma}\\sqrt{\\frac{na \\sum (\\mu_{.j}- \\mu_{..})^2}{b}} \\\\\n\\upsilon_1 = b-1 \\\\\n\\upsilon_2 = ab(n-1)\n\\]Procedure:Specify minimu range Factor meansObtain sample sizes r = . resulting sample size bn, n can obtained.Repeat first 2 steps Factor B minimum range.Choose greater number sample size B.","code":""},{"path":"analysis-of-variance-anova.html","id":"randomized-block-experiments","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.3.2 Randomized Block Experiments","text":"Analogous completely randomized designs . power F-test treatment effects randomized block design uses non-centrality parameter completely randomized design:\\[\n\\phi = \\frac{1}{\\sigma} \\sqrt{\\frac{n}{r} \\sum (\\mu_i - \\mu_.)^2}\n\\]However, power level different randomized block design becauseerror variance \\(\\sigma^2\\) differentdf(MSE) different.","code":""},{"path":"analysis-of-variance-anova.html","id":"randomized-block-designs","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.4 Randomized Block Designs","text":"improve precision treatment comparisons, can reduce variability among experimental units. can group experimental units blocks block contains relatively homogeneous units.Within block, random assignment treatments units (separate random assignment block)number units per block multiple number factor combinations.Commonly, use treatment block.Benefits BlockingReduction variability estimators treatment means\nImproved power t-tests F-tests\nNarrower confidence intervals\nSmaller MSE\nReduction variability estimators treatment meansImproved power t-tests F-testsNarrower confidence intervalsSmaller MSECompare treatments different conditions (related different blocks).Compare treatments different conditions (related different blocks).Loss Blocking (little lose)don’t blocking well, waste df negligible block effects used estimate \\(\\sigma^2\\)hence, df t-tests denominator df F-tests reduced without reducing MSE small loss power tests.Consider\\[\nY_{ij} = \\mu_{..} + \\rho_i + \\tau_j + \\epsilon_{ij} \\\\\n= 1,2,...,n \\\\\nj = 1,2,..,r\n\\]\\(\\mu_{..}\\): overall mean response, averaging across blocks treatments\\(\\rho_i\\): block effect, average difference response -th block (\\(\\sum \\rho_i =0\\))\\(\\tau_j\\) treatment effect, average across blocks (\\(\\sum \\tau_j = 0\\))\\(\\epsilon_{ij} \\sim iid N(0,\\sigma^2)\\): random experimental error., assume block treatment effects additive. difference average response pair treatments within block\\[\n(\\mu_{..} +  \\rho_i + \\tau_j) - (\\mu_{..} + \\rho_i + \\tau_j') = \\tau_j - \\tau_j'\n\\]\\(=1,..,n\\) blocks\\[\n\\hat{\\mu} = \\bar{Y}_{..} \\\\\n\\hat{\\rho}_i = \\bar{Y}_{.} - \\bar{Y}_{..} \\\\\n\\hat{\\tau}_j = \\bar{Y}_{.j} - \\bar{Y}_{..}\n\\]Hence,\\[\n\\hat{Y}_{ij} = \\bar{Y}_{..} + (\\bar{Y}_{.} - \\bar{Y}_{..}) + (\\bar{Y}_{.j}- \\bar{Y}_{..}) = \\bar{Y}_{.} + \\bar{Y}_{.j} - \\bar{Y}_{..} \\\\\ne_{ij} = Y_{ij} - \\hat{Y}_{ij} = Y_{ij}- \\bar{Y}_{.} - \\bar{Y}_{.j} + \\bar{Y}_{..}\n\\]ANOVA tableF-tests\\[\n\\begin{aligned}\nH_0: \\tau_1 = \\tau_2 = ... = \\tau_r = 0 && \\text{Fixed Treatment Effects} \\\\\nH_a: \\text{} \\tau_j = 0 \\\\\n\\\\\nH_0: \\sigma^2_{\\tau} = 0 && \\text{Random Treatment Effects} \\\\\nH_a: \\sigma^2_{\\tau} \\neq 0 \n\\end{aligned}\n\\]cases \\(F = \\frac{MSTR}{MSE}\\), reject \\(H_0\\) \\(F > f_{(1-\\alpha; r-1,(n-1)(r-1))}\\)don’t use F-test compare blocks, becauseWe priori blocs differentRandomization done “within” block.estimate efficiency gained blocking (relative completely randomized design).\\[\n\\begin{aligned}\n\\hat{\\sigma}^2_{CR} &= \\frac{(n-1)MSBL + n(r-1)MSE}{nr-1} \\\\\n\\hat{\\sigma}^2_{RB} &= MSE \\\\\n\\frac{\\hat{\\sigma}^2_{CR}}{\\hat{\\sigma}^2_{RB}} &= \\text{1} \\\\\n\\end{aligned}\n\\]completely randomized experiment \\[\n(\\frac{\\hat{\\sigma}^2_{CR}}{\\hat{\\sigma}^2_{RB}}-1)\\%%\n\\]observations randomized block design get MSEIf batches randomly selected random effects. , experiment repeated, new sample batches selected,d yielding new values \\(\\rho_1, \\rho_2,...,\\rho_i\\) .\\[\n\\rho_1, \\rho_2,...,\\rho_j \\sim N(0,\\sigma^2_\\rho)\n\\],\\[\nY_{ij} = \\mu_{..} + \\rho_i + \\tau_j + \\epsilon_{ij}\n\\]\\(\\mu_{..}\\) fixed\\(\\rho_i\\): random iid \\(N(0,\\sigma^2_p)\\)\\(\\tau_j\\) fixed (random) \\(\\sum \\tau_j = 0\\)\\(\\epsilon_{ij} \\sim iid N(0,\\sigma^2)\\)**Fixed Treatment&&\\[\nE(Y_{ij}) = \\mu_{..} + \\tau_j \\\\\nvar(Y_{ij}) = \\sigma^2_{\\rho} + \\sigma^2 \\\\\ncov(Y_{ij},Y_{ij'}) = \\sigma^2 , j \\neq j' \\text{ treatments within block correlated} \\\\\ncov(Y_{ij},Y_{'j'}) = 0 , \\neq ' , j \\neq j'\n\\]Correlation 2 observations teh block\\[\n\\frac{\\sigma^2_{\\rho}}{\\sigma^2 + \\sigma^2_{\\rho}}\n\\]expected MS additive fixed treatment effect, random block effect isInteractions Blocks\nwithout replications within block treatment, can’t consider interaction block treatment block effect fixed. Hence, random block effect, \\[\nY_{ij} = \\mu_{..} + \\rho_i + \\tau_j + (\\rho \\tau)_{ij} + \\epsilon_{ij}\n\\]\\(\\mu_{..}\\) constant\\(\\rho_i \\sim idd N(0,\\sigma^2_{\\rho})\\) random\\(\\tau_j\\) fixed (\\(\\sum \\tau_j = 0\\))\\((\\rho \\tau)_{ij} \\sim N(0,\\frac{r-1}{r}\\sigma^2_{\\rho \\tau})\\) \\(\\sum_j (\\rho \\tau)_{ij}=0\\) \\(cov((\\rho \\tau)_{ij},(\\rho \\tau)_{ij'})= -\\frac{1}{r} \\sigma^2_{\\rho \\tau}\\) \\(j \\neq j'\\)\\(\\epsilon_{ij} \\sim iid N(0,\\sigma^2)\\) randomNote: special case mixed 2-factor model 1 observation per “cell”\\[\nE(Y_{ij}) = \\mu_{..} + \\tau_j \\\\\nvar(Y_{ij}) = \\sigma^2_\\rho + \\frac{r-1}{r} \\sigma^2_{\\rho \\tau} + \\sigma^2 \\\\\ncov(Y_{ij},Y_{ij'}) = \\sigma^2_\\rho - \\frac{1}{r} \\sigma^2_{\\rho \\tau}, j \\neq j' \\text{ obs block correlated} \\\\\ncov(Y_{ij},Y_{'j'}) = 0, \\neq ', j \\neq j' \\text{ obs different blocks independent}\n\\]sum squares degrees freedom interaction model additive model. difference exists expected mean squaresNo exact test possible block effects interaction present (important blocks used primarily reduce experimental error variability)\\(E(MSE) = \\sigma^2 + \\sigma^2_{\\rho \\tau}\\) error term variance interaction variance \\(\\sigma^2_{\\rho \\tau}\\). can’t estimate components separately model. two confounded.1 observation per treatment block combination, one can consider interaction fixed block effects, called generalized randomized block designs (multifactor analysis).","code":""},{"path":"analysis-of-variance-anova.html","id":"tukey-test-of-additivity","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.4.1 Tukey Test of Additivity","text":"(Tukey’s 1 df test additivity)formal test interaction effects blocks treatments randomized block design. can also considered testing additivity 2-way analyses one observation per cell.consider less restricted interaction term\\[\n(\\rho \\tau)_{ij} = D\\rho_i \\tau_j \\text{(D: Constant)}\n\\],\\[\nY_{ij} = \\mu_{..} + \\rho_i + \\tau_j + D\\rho_i \\tau_j + \\epsilon_{ij}\n\\]least square estimate MLE D\\[\n\\hat{D} = \\frac{\\sum_i \\sum_j \\rho_i \\tau_j Y_{ij}}{\\sum_i \\rho_i^2 \\sum_j \\tau^2_j}\n\\]replacing parameters estimates\\[\n\\hat{D} = \\frac{\\sum_i \\sum_j (\\bar{Y}_{.}- \\bar{Y}_{..})(\\bar{Y}_{.j}- \\bar{Y}_{..})Y_{ij}}{\\sum_i (\\bar{Y}_{.}- \\bar{Y}_{..})^2 \\sum_j(\\bar{Y}_{.j}- \\bar{Y}_{..})^2}\n\\]Thus, interaction sum squares\\[\nSSint = \\sum_i \\sum_j \\hat{D}^2(\\bar{Y}_{.}- \\bar{Y}_{..})^2(\\bar{Y}_{.j}- \\bar{Y}_{..})^2\n\\]ANOVA decomposition\\[\nSSTO = SSBL + SSTR + SSint + SSRem\n\\]\\(SSRem\\): remainder sum squares\\[\nSSRem = SSTO - SSBL - SSTR - SSint\n\\]D = 0 (.e., interactions type \\(D \\rho_i \\tau_j\\)). SSint SSRem independent \\(\\chi^2_{1,rn-r-n}\\).D = 0,\\[\nF = \\frac{SSint/1}{SSRem/(rn-r-n)} \\sim f_{(1-\\alpha;rn-r-n)}\n\\]\\[\nH_0: D = 0 \\text{ interaction present} \\\\\nH_a: D \\neq 0 \\text{ interaction form $D \\rho_i \\tau_j$ present}\n\\]reject \\(H_0\\) \\(F > f_{(1-\\alpha;1,nr-r-n)}\\)","code":""},{"path":"analysis-of-variance-anova.html","id":"nested-designs","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.5 Nested Designs","text":"Let \\(\\mu_{ij}\\) mean response factor -th level factor B j-th level.\nfactors crossed, jth level B levels .\nfactor B nested within , j-th level B level 1 nothing common j-th level B level 2.Factors can’t manipulated designated classification factors, opposed experimental factors (.e., assign experimental units).","code":""},{"path":"analysis-of-variance-anova.html","id":"two-factor-nested-designs","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.5.1 Two-Factor Nested Designs","text":"Consider B nested within .factors fixedAll treatment means equally important.Mean responses\\[\n\\mu_{.} = \\sum_j \\mu_{ij}/b\n\\]Main effect factor \\[\n\\alpha_i = \\mu_{.} - \\mu_{..}\n\\]\\(\\mu_{..} = \\frac{\\mu_{ij}}{ab} = \\frac{\\sum_i \\mu_{.}}{}\\) \\(\\sum_i \\alpha_i = 0\\)Individual effects B denoted \\(\\beta_{j()}\\) \\(j()\\) indicates j-th level factor B nested within -h level factor \\[\n\\beta_{j()} = \\mu_{ij} - \\mu_{.} \\\\\n= \\mu_{ij} - \\alpha_i - \\mu_{..} \\\\\n\\sum_j \\beta_{j()}=0 , = 1,...,\n\\]\\(\\beta_{j()}\\) specific effect jth level factor B nested within ith level factor . Hence,\\[\n\\mu_{ij} \\equiv \\mu_{..} + \\alpha_i + \\beta_{j()} \\equiv \\mu_{..} + (\\mu_{.} - \\mu_{..}) + (\\mu_{ij} - \\mu_{.})\n\\]Model\\[\nY_{ijk} = \\mu_{..} + \\alpha_i + \\beta_{j()} + \\epsilon_{ijk}\n\\]\\(Y_{ijk}\\) response kth treatment factor -th level factor B hte jth level (= 1,..,; j = 1,..,b; k = 1,..n)\\(\\mu_{..}\\) constant\\(\\alpha_i\\) constants subject restriction \\(\\sum_i \\alpha_i = 0\\)\\(\\beta_{j()}\\) constants subject restriction \\(\\sum_j \\beta_{j()} = 0\\) \\(\\epsilon_{ijk} \\sim iid N(0,\\sigma^2)\\)\\[\nE(Y_{ijk}) = \\mu_{..} + \\alpha_i + \\beta_{j()} \\\\\nvar(Y_{ijk}) = \\sigma^2\n\\]interaction term nested modelANOVA Two-Factor Nested DesignsLeast Squares MLE estimatesresidual \\(e_{ijk} = Y_{ijk} - \\bar{Y}_{ijk}\\)\\[\n\\begin{aligned}\nSSTO &= SSA + SSB() + SSE \\\\\n\\sum_i \\sum_j \\sum_k (Y_{ijk}- \\bar{Y}_{...})^2 &= bn \\sum_i (\\bar{Y}_{..}- \\bar{Y}_{...})^2 + n \\sum_i \\sum_j (\\bar{Y}_{ij.}- \\bar{Y}_{..})^2 + \\sum_i \\sum_j \\sum_k (Y_{ijk} -\\bar{Y}_{ij.})^2\n\\end{aligned}\n\\]ANOVA TableTests Factor Effects\\[\nH_0: \\text{ } \\alpha_i =0 \\\\\nH_a: \\text{ } \\alpha_i = 0\n\\]\\(F = \\frac{MSA}{MSE} \\sim f_{(1-\\alpha;-1,(n-1)ab)}\\) reject \\(F > f\\)\\[\nH_0: \\text{ } \\beta_{j()} =0 \\\\\nH_a: \\text{ } \\beta_{j()} = 0\n\\]\\(F = \\frac{MSB()}{MSE} \\sim f_{(1-\\alpha;(b-1),(n-1)ab)}\\) reject \\(F>f\\)Testing Factor Effect Contrasts\\(L = \\sum c_i \\mu_i\\) \\(\\sum c_i =0\\)\\[\n\\hat{L} = \\sum c_i \\bar{Y}_{..} \\\\\n\\hat{L} \\pm t_{(1-\\alpha/2;df)}s(\\hat{L})\n\\]\\(s^2(\\hat{L}) = \\sum c_i^2 s^2(\\bar{Y}_{..})\\), \\(s^2(\\bar{Y}_{..}) = \\frac{MSE}{bn}, df = ab(n-1)\\)Testing Treatment Means\\(L = \\sum c_i \\mu_{.j}\\) estimated \\(\\hat{L} = \\sum c_i \\bar{Y}_{ij}\\) confidence limits:\\[\n\\hat{L} \\pm t_{(1-\\alpha/2;(n-1)ab)}s(\\hat{L})\n\\]\\[\ns^2(\\hat{L}) = \\frac{MSE}{n}\\sum c^2_i\n\\]Unbalanced Nested Two-Factor DesignsIf different number levels factor B different levels factor , design called unbalancedThe model\\[\nY_{ijk} = \\mu_{..} + \\alpha_i + \\beta_{j()} + \\epsilon_{ijk} \\\\\n= 1,2;j =1,..,b_i;k=1,..,n_{ij} \\\\\nb_1 = 3, b_2= 2, n_{11} =  n_{13} =2, n_{12}=1,n_{21} = n_{22} = 2\\\\\n\\sum_{=1}^2 \\alpha_i =0 \\\\\n\\sum_{j=1}^3 \\beta_{j(1)} = 0 \\\\\n\\sum_{j=1}^2 \\beta_{j(2)}=0\n\\]\\(\\alpha_1,\\beta_{1(1)}, \\beta_{2(1)}, \\beta_{1(2)}\\) parameters. constraints: \\(\\alpha_2 = - \\alpha_1, \\beta_{3(1)}= - \\beta_{1(1)}-\\beta_{2(1)}, \\beta_{2(2)}=-\\beta_{1(2)}\\)4 indicator variables\\[\\begin{equation}\nX_1 = \n\\begin{cases}\n1&\\text{obs school 1}\\\\\n-1&\\text{obs school 2}\\\\\n\\end{cases}\n\\end{equation}\\]\\[\\begin{equation}\nX_2 = \n\\begin{cases}\n1&\\text{obs instructor 1 school 1}\\\\\n-1&\\text{obs instructor 3 school 1}\\\\\n0&\\text{otherwise}\\\\\n\\end{cases}\n\\end{equation}\\]\\[\\begin{equation}\nX_3 = \n\\begin{cases}\n1&\\text{obs instructor 2 school 1}\\\\\n-1&\\text{obs instructor 3 school 1}\\\\\n0&\\text{otherwise}\\\\\n\\end{cases}\n\\end{equation}\\]\\[\\begin{equation}\nX_4 = \n\\begin{cases}\n1&\\text{obs instructor 1 school 1}\\\\\n-1&\\text{obs instructor 2 school 1}\\\\\n0&\\text{otherwise}\\\\\n\\end{cases}\n\\end{equation}\\]Regression Full Model\\[\nY_{ijk} = \\mu_{..} + \\alpha_1 X_{ijk1} + \\beta_{1(1)}X_{ijk2} + \\beta_{2(1)}X_{ijk3} + \\beta_{1(2)}X_{ijk4} + \\epsilon_{ijk}\n\\]Random Factor EffectsIf\\[\n\\alpha_1 \\sim iid N(0,\\sigma^2_\\alpha) \\\\\n\\beta_{j()} \\sim iid N(0,\\sigma^2_\\beta)\n\\]Test StatsticsAnother way increase precision treatment comparisons reducing variability use regression models adjust differences among experimental units (also known analysis covariance).","code":""},{"path":"analysis-of-variance-anova.html","id":"single-factor-covariance-model","chapter":"16 Analysis of Variance (ANOVA)","heading":"16.6 Single Factor Covariance Model","text":"\\[\nY_{ij} = \\mu_{.} + \\tau_i + \\gamma(X_{ij} - \\bar{X}_{..}) + \\epsilon_{ij} \n\\]\\(= 1,...,r;j=1,..,n_i\\)\\(\\mu_.\\) overall mean\\(\\tau_i\\): fixed treatment effects (\\(\\sum \\tau_i =0\\))\\(\\gamma\\): fixed regression coefficient effect X Y\\(X_{ij}\\) covariate (random)\\(\\epsilon_{ij} \\sim iid N(0,\\sigma^2)\\): random errorsIf just use \\(\\gamma X_{ij}\\) regression term (rather \\(\\gamma(X_{ij}-\\bar{X}_{..})\\)), \\(\\mu_.\\) longer overall mean; thus need centered mean.\\[\nE(Y_{ij}) = \\mu_. + \\tau_i + \\gamma(X_{ij}-\\bar{X}_{..}) \\\\\nvar(Y_{ij}) = \\sigma^2\n\\]\\(Y_{ij} \\sim N(\\mu_{ij},\\sigma^2)\\), \\[\n\\mu_{ij} = \\mu_. + \\tau_i + \\gamma(X_{ij} - \\bar{X}_{..}) \\\\\n\\sum \\tau_i =0 \n\\]Thus, mean response (\\(\\mu_{ij}\\)) regression line intercept \\(\\mu_. + \\tau_i\\) slope \\(\\gamma\\) treatment .Assumption:treatment regression lines slopewhen treatment interact covariate X (non-parallel slopes), covariance analysis appropriate. case use separate regression lines.complicated regression features (e.g., quadratic, cubic) additional covariates e.g.,\\[\nY_{ij} = \\mu_. + \\tau_i + \\gamma_1(X_{ij1}-\\bar{X}_{..2}) + \\gamma_2(X_{ij2}-\\bar{X}_{..2}) + \\epsilon_{ij}\n\\]Regression FormulationWe can use indicator variables treatments\\[\nl_1 =\n\\begin{cases}\n1 & \\text{case treatment 1}\\\\\n-1 & \\text{case treatment r}\\\\\n0 &\\text{otherwise}\\\\\n\\end{cases}\n. \\\\\n. \\\\\n. \\\\\nl_{r-1} =\n\\begin{cases}\n1 & \\text{case treatment r-1}\\\\\n-1 & \\text{case treatment r}\\\\\n0 &\\text{otherwise}\\\\\n\\end{cases}\n\\]Let \\(x_{ij} = X_{ij}- \\bar{X}_{..}\\). regression model \\[\nY_{ij} = \\mu_. + \\tau_1l_{ij,1} + .. + \\tau_{r-1}l_{ij,r-1} + \\gamma x_{ij}+\\epsilon_{ij}\n\\]\\(I_{ij,1}\\) indicator variable \\(l_1\\) j-th case treatment . treatment effect \\(\\tau_1,..\\tau_{r-1}\\) just regression coefficients indicator variables.use diagnostic tools case.InferenceTreatment effects\\[\nH_0: \\tau_1 = \\tau_2 = ...= 0 \\\\\nH_a: \\text{} \\tau_i =0\n\\]\\[\n\\text{Full Model}: Y_{ij} = \\mu_. + \\tau_i + \\gamma X_{ij} +\\epsilon_{ij}  \\\\\n\\text{Reduced Model}: Y_{ij} = \\mu_. + \\gamma X_{ij} + \\epsilon_{ij}\n\\]\\[\nF = \\frac{SSE(R) - SSE(F)}{(N-2)-(N-(r+1))} / \\frac{SSE(F)}{N-(r+1)} \\sim F_{(r-1,N-(r+1))}\n\\]interested comparisons treatment effects.\nexample, r - 3. estimate \\(\\tau_1,\\tau_2, \\tau_3 = -\\tau_1 - \\tau_2\\)Testing Parallel SlopesExample:r = 3\\[\nY_{ij} = \\mu_{.} + \\tau_1 I_{ij,1} + \\tau_2 I_{ij,2} + \\gamma X_{ij} + \\beta_1 I_{ij,1}X_{ij} + \\beta_2 I_{ij,2}X_{ij} + \\epsilon_{ij}\n\\]\\(\\beta_1,\\beta_2\\): interaction coefficients.\\[\nH_0: \\beta_1 = \\beta_2 = 0 \\\\\nH_a: \\text{least one} \\beta \\neq 0 \n\\]can’t reject \\(H_0\\) using F-test evidence slopes parallelAdjusted MeansThe means response adjusting covariate effect\\[\nY_{.}(adj) = \\bar{Y}_{.} - \\hat{\\gamma}(\\bar{X}_{.} - \\bar{X}_{..})\n\\]","code":""},{"path":"multivariate-methods.html","id":"multivariate-methods","chapter":"17 Multivariate Methods","heading":"17 Multivariate Methods","text":"\\(y_1,...,y_p\\) possibly correlated random variables means \\(\\mu_1,...,\\mu_p\\)\\[\n\\mathbf{y} = \n\\left(\n\\begin{array}\n{c}\ny_1 \\\\\n. \\\\\ny_p \\\\\n\\end{array}\n\\right)\n\\]\\[\nE(\\mathbf{y}) = \n\\left(\n\\begin{array}\n{c}\n\\mu_1 \\\\\n. \\\\\n\\mu_p \\\\\n\\end{array}\n\\right)\n\\]Let \\(\\sigma_{ij} = cov(y_i, y_j)\\) \\(,j = 1,…,p\\)\\[\n\\mathbf{\\Sigma} = (\\sigma_{ij}) = \n\\left(\n\\begin{array}\n{cccc}\n\\sigma_{11} & \\sigma_{22} & ... &  \\sigma_{1p} \\\\\n\\sigma_{21} & \\sigma_{22} & ... & \\sigma_{2p} \\\\\n. & . & . & . \\\\\n\\sigma_{p1} & \\sigma_{p2} & ... & \\sigma_{pp}\n\\end{array}\n\\right)\n\\]\\(\\mathbf{\\Sigma}\\) (symmetric) variance-covariance dispersion matrixLet \\(\\mathbf{u}_{p \\times 1}\\) \\(\\mathbf{v}_{q \\times 1}\\) random vectors means \\(\\mu_u\\) \\(\\mu_v\\) . \\[\n\\mathbf{\\Sigma}_{uv} = cov(\\mathbf{u,v}) = E[(\\mathbf{u} - \\mu_u)(\\mathbf{v} - \\mu_v)']\n\\]\\(\\mathbf{\\Sigma}_{uv} \\neq \\mathbf{\\Sigma}_{vu}\\) \\(\\mathbf{\\Sigma}_{uv} = \\mathbf{\\Sigma}_{vu}'\\)Properties Covariance MatricesSymmetric \\(\\mathbf{\\Sigma}' = \\mathbf{\\Sigma}\\)Non-negative definite \\(\\mathbf{'\\Sigma } \\ge 0\\) \\(\\mathbf{} \\R^p\\), equivalent eigenvalues \\(\\mathbf{\\Sigma}\\), \\(\\lambda_1 \\ge \\lambda_2 \\ge ... \\ge \\lambda_p \\ge 0\\)\\(|\\mathbf{\\Sigma}| = \\lambda_1 \\lambda_2 ... \\lambda_p \\ge 0\\) (generalized variance) (bigger number , variation \\(trace(\\mathbf{\\Sigma}) = tr(\\mathbf{\\Sigma}) = \\lambda_1 + ... + \\lambda_p = \\sigma_{11} + ... + \\sigma_{pp} =\\) sum variance (total variance)Note:\\(\\mathbf{\\Sigma}\\) typically required positive definite, means eigenvalues positive, \\(\\mathbf{\\Sigma}\\) inverse \\(\\mathbf{\\Sigma}^{-1}\\) \\(\\mathbf{\\Sigma}^{-1}\\mathbf{\\Sigma} = \\mathbf{}_{p \\times p} = \\mathbf{\\Sigma \\Sigma}^{-1}\\)Correlation Matrices\\[\n\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii} \\sigma_{jj}}}\n\\]\\[\n\\mathbf{R} = \n\\left(\n\\begin{array}\n{cccc}\n\\rho_{11} & \\rho_{12} & ... & \\rho_{1p} \\\\\n\\rho_{21} & \\rho_{22} & ... & \\rho_{2p} \\\\\n. & . & . &. \\\\\n\\rho_{p1} & \\rho_{p2} & ... & \\rho_{pp} \\\\\n\\end{array}\n\\right)\n\\]\\(\\rho_{ij}\\) correlation, \\(\\rho_{ii} = 1\\) iAlternatively,\\[\n\\mathbf{R} = [diag(\\mathbf{\\Sigma})]^{-1/2}\\mathbf{\\Sigma}[diag(\\mathbf{\\Sigma})]^{-1/2}\n\\]\\(diag(\\mathbf{\\Sigma})\\) matrix \\(\\sigma_{ii}\\)’s diagonal 0’s elsewhereand \\(\\mathbf{}^{1/2}\\) (square root symmetric matrix) symmetric matrix \\(\\mathbf{} = \\mathbf{}^{1/2}\\mathbf{}^{1/2}\\)EqualitiesLet\\(\\mathbf{x}\\) \\(\\mathbf{y}\\) random vectors means \\(\\mu_x\\) \\(\\mu_y\\) variance -variance matrices \\(\\mathbf{\\Sigma}_x\\) \\(\\mathbf{\\Sigma}_y\\).\\(\\mathbf{x}\\) \\(\\mathbf{y}\\) random vectors means \\(\\mu_x\\) \\(\\mu_y\\) variance -variance matrices \\(\\mathbf{\\Sigma}_x\\) \\(\\mathbf{\\Sigma}_y\\).\\(\\mathbf{}\\) \\(\\mathbf{B}\\) matrices constants \\(\\mathbf{c}\\) \\(\\mathbf{d}\\) vectors constants\\(\\mathbf{}\\) \\(\\mathbf{B}\\) matrices constants \\(\\mathbf{c}\\) \\(\\mathbf{d}\\) vectors constantsThen\\(E(\\mathbf{Ay + c} ) = \\mathbf{} \\mu_y + c\\)\\(E(\\mathbf{Ay + c} ) = \\mathbf{} \\mu_y + c\\)\\(var(\\mathbf{Ay + c}) = \\mathbf{} var(\\mathbf{y})\\mathbf{}' = \\mathbf{\\Sigma_y }'\\)\\(var(\\mathbf{Ay + c}) = \\mathbf{} var(\\mathbf{y})\\mathbf{}' = \\mathbf{\\Sigma_y }'\\)\\(cov(\\mathbf{Ay + c, + d}) = \\mathbf{\\Sigma_y B}'\\)\\(cov(\\mathbf{Ay + c, + d}) = \\mathbf{\\Sigma_y B}'\\)\\(E(\\mathbf{Ay + Bx + c}) = \\mathbf{\\mu_y + B \\mu_x + c}\\)\\(E(\\mathbf{Ay + Bx + c}) = \\mathbf{\\mu_y + B \\mu_x + c}\\)\\(var(\\mathbf{Ay + Bx + c}) = \\mathbf{\\Sigma_y ' + B \\Sigma_x B' + \\Sigma_{yx}B' + B\\Sigma'_{yx}'}\\)\\(var(\\mathbf{Ay + Bx + c}) = \\mathbf{\\Sigma_y ' + B \\Sigma_x B' + \\Sigma_{yx}B' + B\\Sigma'_{yx}'}\\)Multivariate Normal DistributionLet \\(\\mathbf{y}\\) multivariate normal (MVN) random variable mean \\(\\mu\\) variance \\(\\mathbf{\\Sigma}\\). density \\(\\mathbf{y}\\) \\[\nf(\\mathbf{y}) = \\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}|^{1/2}} \\exp(-\\frac{1}{2} \\mathbf{(y-\\mu)'\\Sigma^{-1}(y-\\mu)} )\n\\]\\(\\mathbf{y} \\sim N_p(\\mu, \\mathbf{\\Sigma})\\)","code":""},{"path":"multivariate-methods.html","id":"properties-of-mvn","chapter":"17 Multivariate Methods","heading":"17.0.1 Properties of MVN","text":"Let \\(\\mathbf{}_{r \\times p}\\) fixed matrix. \\(\\mathbf{Ay} \\sim N_r (\\mathbf{\\mu, \\Sigma '})\\) . \\(r \\le p\\) rows \\(\\mathbf{}\\) must linearly independent guarantee \\(\\mathbf{\\Sigma }'\\) non-singular.Let \\(\\mathbf{}_{r \\times p}\\) fixed matrix. \\(\\mathbf{Ay} \\sim N_r (\\mathbf{\\mu, \\Sigma '})\\) . \\(r \\le p\\) rows \\(\\mathbf{}\\) must linearly independent guarantee \\(\\mathbf{\\Sigma }'\\) non-singular.Let \\(\\mathbf{G}\\) matrix \\(\\mathbf{\\Sigma}^{-1} = \\mathbf{GG}'\\). \\(\\mathbf{G'y} \\sim N_p(\\mathbf{G' \\mu, })\\) \\(\\mathbf{G'(y-\\mu)} \\sim N_p (0,\\mathbf{})\\)Let \\(\\mathbf{G}\\) matrix \\(\\mathbf{\\Sigma}^{-1} = \\mathbf{GG}'\\). \\(\\mathbf{G'y} \\sim N_p(\\mathbf{G' \\mu, })\\) \\(\\mathbf{G'(y-\\mu)} \\sim N_p (0,\\mathbf{})\\)fixed linear combination \\(y_1,...,y_p\\) (say \\(\\mathbf{c'y}\\)) follows \\(\\mathbf{c'y} \\sim N_1 (\\mathbf{c' \\mu, c' \\Sigma c})\\)fixed linear combination \\(y_1,...,y_p\\) (say \\(\\mathbf{c'y}\\)) follows \\(\\mathbf{c'y} \\sim N_1 (\\mathbf{c' \\mu, c' \\Sigma c})\\)Define partition, \\([\\mathbf{y}'_1,\\mathbf{y}_2']'\\) \n\\(\\mathbf{y}_1\\) \\(p_1 \\times 1\\)\n\\(\\mathbf{y}_2\\) \\(p_2 \\times 1\\),\n\\(p_1 + p_2 = p\\)\n\\(p_1,p_2 \\ge 1\\) \nDefine partition, \\([\\mathbf{y}'_1,\\mathbf{y}_2']'\\) \\(\\mathbf{y}_1\\) \\(p_1 \\times 1\\)\\(\\mathbf{y}_1\\) \\(p_1 \\times 1\\)\\(\\mathbf{y}_2\\) \\(p_2 \\times 1\\),\\(\\mathbf{y}_2\\) \\(p_2 \\times 1\\),\\(p_1 + p_2 = p\\)\\(p_1 + p_2 = p\\)\\(p_1,p_2 \\ge 1\\) \\(p_1,p_2 \\ge 1\\) \\[\n\\left(\n\\begin{array}\n{c}\n\\mathbf{y}_1 \\\\\n\\mathbf{y}_2 \\\\\n\\end{array}\n\\right)\n\\sim\nN\n\\left(\n\\left(\n\\begin{array}\n{c}\n\\mu_1 \\\\\n\\mu_2 \\\\\n\\end{array}\n\\right),\n\\left(\n\\begin{array}\n{cc}\n\\mathbf{\\Sigma}_{11} & \\mathbf{\\Sigma}_{12} \\\\\n\\mathbf{\\Sigma}_{21} & \\mathbf{\\Sigma}_{22}\\\\\n\\end{array}\n\\right)\n\\right)\n\\]marginal distributions \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) \\(\\mathbf{y}_1 \\sim N_{p1}(\\mathbf{\\mu_1, \\Sigma_{11}})\\) \\(\\mathbf{y}_2 \\sim N_{p2}(\\mathbf{\\mu_2, \\Sigma_{22}})\\)marginal distributions \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) \\(\\mathbf{y}_1 \\sim N_{p1}(\\mathbf{\\mu_1, \\Sigma_{11}})\\) \\(\\mathbf{y}_2 \\sim N_{p2}(\\mathbf{\\mu_2, \\Sigma_{22}})\\)Individual components \\(y_1,...,y_p\\) normally distributed \\(y_i \\sim N_1(\\mu_i, \\sigma_{ii})\\)Individual components \\(y_1,...,y_p\\) normally distributed \\(y_i \\sim N_1(\\mu_i, \\sigma_{ii})\\)conditional distribution \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) normal\n\\(\\mathbf{y}_1 | \\mathbf{y}_2 \\sim N_{p1}(\\mathbf{\\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1}(y_2 - \\mu_2),\\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\sigma_{21}})\\)\nformula, see know (info ) \\(\\mathbf{y}_2\\), can re-weight \\(\\mathbf{y}_1\\) ’s mean, variance reduced know \\(\\mathbf{y}_1\\) know \\(\\mathbf{y}_2\\)\n\nanalogous \\(\\mathbf{y}_2 | \\mathbf{y}_1\\). \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) independently distrusted \\(\\mathbf{\\Sigma}_{12} = 0\\)\nconditional distribution \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) normal\\(\\mathbf{y}_1 | \\mathbf{y}_2 \\sim N_{p1}(\\mathbf{\\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1}(y_2 - \\mu_2),\\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\sigma_{21}})\\)\nformula, see know (info ) \\(\\mathbf{y}_2\\), can re-weight \\(\\mathbf{y}_1\\) ’s mean, variance reduced know \\(\\mathbf{y}_1\\) know \\(\\mathbf{y}_2\\)\n\\(\\mathbf{y}_1 | \\mathbf{y}_2 \\sim N_{p1}(\\mathbf{\\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1}(y_2 - \\mu_2),\\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\sigma_{21}})\\)formula, see know (info ) \\(\\mathbf{y}_2\\), can re-weight \\(\\mathbf{y}_1\\) ’s mean, variance reduced know \\(\\mathbf{y}_1\\) know \\(\\mathbf{y}_2\\)analogous \\(\\mathbf{y}_2 | \\mathbf{y}_1\\). \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) independently distrusted \\(\\mathbf{\\Sigma}_{12} = 0\\)analogous \\(\\mathbf{y}_2 | \\mathbf{y}_1\\). \\(\\mathbf{y}_1\\) \\(\\mathbf{y}_2\\) independently distrusted \\(\\mathbf{\\Sigma}_{12} = 0\\)\\(\\mathbf{y} \\sim N(\\mathbf{\\mu, \\Sigma})\\) \\(\\mathbf{\\Sigma}\\) positive definite, \\(\\mathbf{(y-\\mu)' \\Sigma^{-1} (y - \\mu)} \\sim \\chi^2_{(p)}\\)\\(\\mathbf{y} \\sim N(\\mathbf{\\mu, \\Sigma})\\) \\(\\mathbf{\\Sigma}\\) positive definite, \\(\\mathbf{(y-\\mu)' \\Sigma^{-1} (y - \\mu)} \\sim \\chi^2_{(p)}\\)\\(\\mathbf{y}_i\\) independent \\(N_p (\\mathbf{\\mu}_i , \\mathbf{\\Sigma}_i)\\) random variables, fixed matrices \\(\\mathbf{}_{(m \\times p)}\\), \\(\\sum_{=1}^k \\mathbf{}_i \\mathbf{y}_i \\sim N_m (\\sum_{=1}^{k} \\mathbf{}_i \\mathbf{\\mu}_i, \\sum_{=1}^k \\mathbf{}_i \\mathbf{\\Sigma}_i \\mathbf{}_i)\\)\\(\\mathbf{y}_i\\) independent \\(N_p (\\mathbf{\\mu}_i , \\mathbf{\\Sigma}_i)\\) random variables, fixed matrices \\(\\mathbf{}_{(m \\times p)}\\), \\(\\sum_{=1}^k \\mathbf{}_i \\mathbf{y}_i \\sim N_m (\\sum_{=1}^{k} \\mathbf{}_i \\mathbf{\\mu}_i, \\sum_{=1}^k \\mathbf{}_i \\mathbf{\\Sigma}_i \\mathbf{}_i)\\)Multiple Regression\\[\n\\left(\n\\begin{array}\n{c}\nY \\\\\n\\mathbf{x}\n\\end{array}\n\\right)\n\\sim \nN_{p+1}\n\\left(\n\\left[\n\\begin{array}\n{c}\n\\mu_y \\\\\n\\mathbf{\\mu}_x\n\\end{array}\n\\right]\n,\n\\left[\n\\begin{array}\n{cc}\n\\sigma^2_Y & \\mathbf{\\Sigma}_{yx} \\\\\n\\mathbf{\\Sigma}_{yx} & \\mathbf{\\Sigma}_{xx}\n\\end{array}\n\\right]\n\\right)\n\\]conditional distribution Y given x follows univariate normal distribution \\[\n\\begin{aligned}\nE(Y| \\mathbf{x}) &= \\mu_y + \\mathbf{\\Sigma}_{yx} \\Sigma_{xx}^{-1} (\\mathbf{x}- \\mu_x) \\\\\n&= \\mu_y - \\Sigma_{yx} \\Sigma_{xx}^{-1}\\mu_x + \\Sigma_{yx} \\Sigma_{xx}^{-1}\\mathbf{x} \\\\\n&= \\beta_0 + \\mathbf{\\beta'x}\n\\end{aligned} \n\\]\\(\\beta = (\\beta_1,...,\\beta_p)' = \\mathbf{\\Sigma}_{xx}^{-1} \\mathbf{\\Sigma}_{yx}'\\) (e.g., analogous \\(\\mathbf{(x'x)^{-1}x'y}\\) consider \\(Y_i\\) \\(\\mathbf{x}_i\\), \\(= 1,..,n\\) use empirical covariance formula: \\(var(Y|\\mathbf{x}) = \\sigma^2_Y - \\mathbf{\\Sigma_{yx}\\Sigma^{-1}_{xx} \\Sigma'_{yx}}\\))Samples Multivariate Normal PopulationsA random sample size n, \\(\\mathbf{y}_1,.., \\mathbf{y}_n\\) \\(N_p (\\mathbf{\\mu}, \\mathbf{\\Sigma})\\). ThenSince \\(\\mathbf{y}_1,..., \\mathbf{y}_n\\) iid, sample mean, \\(\\bar{\\mathbf{y}} = \\sum_{=1}^n \\mathbf{y}_i/n \\sim N_p (\\mathbf{\\mu}, \\mathbf{\\Sigma}/n)\\). , \\(\\bar{\\mathbf{y}}\\) unbiased estimator \\(\\mathbf{\\mu}\\)Since \\(\\mathbf{y}_1,..., \\mathbf{y}_n\\) iid, sample mean, \\(\\bar{\\mathbf{y}} = \\sum_{=1}^n \\mathbf{y}_i/n \\sim N_p (\\mathbf{\\mu}, \\mathbf{\\Sigma}/n)\\). , \\(\\bar{\\mathbf{y}}\\) unbiased estimator \\(\\mathbf{\\mu}\\)\\(p \\times p\\) sample variance-covariance matrix, \\(\\mathbf{S}\\) \\(\\mathbf{S} = \\frac{1}{n-1}\\sum_{=1}^n (\\mathbf{y}_i - \\bar{\\mathbf{y}})(\\mathbf{y}_i - \\bar{\\mathbf{y}})' = \\frac{1}{n-1} (\\sum_{=1}^n \\mathbf{y}_i \\mathbf{y}_i' - n \\bar{\\mathbf{y}}\\bar{\\mathbf{y}}')\\)\n\\(\\mathbf{S}\\) symmetric, unbiased estimator \\(\\mathbf{\\Sigma}\\) \\(p(p+1)/2\\) random variables.\n\\(p \\times p\\) sample variance-covariance matrix, \\(\\mathbf{S}\\) \\(\\mathbf{S} = \\frac{1}{n-1}\\sum_{=1}^n (\\mathbf{y}_i - \\bar{\\mathbf{y}})(\\mathbf{y}_i - \\bar{\\mathbf{y}})' = \\frac{1}{n-1} (\\sum_{=1}^n \\mathbf{y}_i \\mathbf{y}_i' - n \\bar{\\mathbf{y}}\\bar{\\mathbf{y}}')\\)\\(\\mathbf{S}\\) symmetric, unbiased estimator \\(\\mathbf{\\Sigma}\\) \\(p(p+1)/2\\) random variables.\\((n-1)\\mathbf{S} \\sim W_p (n-1, \\mathbf{\\Sigma})\\) Wishart distribution n-1 degrees freedom expectation \\((n-1) \\mathbf{\\Sigma}\\). Wishart distribution multivariate extension Chi-squared distribution.\\((n-1)\\mathbf{S} \\sim W_p (n-1, \\mathbf{\\Sigma})\\) Wishart distribution n-1 degrees freedom expectation \\((n-1) \\mathbf{\\Sigma}\\). Wishart distribution multivariate extension Chi-squared distribution.\\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\) independent\\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\) independent\\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\) sufficient statistics. (info data \\(\\mathbf{\\mu}\\) \\(\\mathbf{\\Sigma}\\) contained \\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\) , regardless sample size).\\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\) sufficient statistics. (info data \\(\\mathbf{\\mu}\\) \\(\\mathbf{\\Sigma}\\) contained \\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{S}\\) , regardless sample size).Large Sample Properties\\(\\mathbf{y}_1,..., \\mathbf{y}_n\\) random sample population mean \\(\\mathbf{\\mu}\\) variance-covariance matrix \\(\\mathbf{\\Sigma}\\)\\(\\bar{\\mathbf{y}}\\) consistent estimator \\(\\mu\\)\\(\\bar{\\mathbf{y}}\\) consistent estimator \\(\\mu\\)\\(\\mathbf{S}\\) consistent estimator \\(\\mathbf{\\Sigma}\\)\\(\\mathbf{S}\\) consistent estimator \\(\\mathbf{\\Sigma}\\)Multivariate Central Limit Theorem: Similar univariate case, \\(\\sqrt{n}(\\bar{\\mathbf{y}} - \\mu) \\dot{\\sim} N_p (\\mathbf{0,\\Sigma})\\) n large relative p (\\(n \\ge 25p\\)), equivalent \\(\\bar{\\mathbf{y}} \\dot{\\sim} N_p (\\mu, \\mathbf{\\Sigma}/n)\\)Multivariate Central Limit Theorem: Similar univariate case, \\(\\sqrt{n}(\\bar{\\mathbf{y}} - \\mu) \\dot{\\sim} N_p (\\mathbf{0,\\Sigma})\\) n large relative p (\\(n \\ge 25p\\)), equivalent \\(\\bar{\\mathbf{y}} \\dot{\\sim} N_p (\\mu, \\mathbf{\\Sigma}/n)\\)Wald’s Theorem: \\(n(\\bar{\\mathbf{y}} - \\mu)' \\mathbf{S}^{-1} (\\bar{\\mathbf{y}} - \\mu)\\) n large relative p.Wald’s Theorem: \\(n(\\bar{\\mathbf{y}} - \\mu)' \\mathbf{S}^{-1} (\\bar{\\mathbf{y}} - \\mu)\\) n large relative p.Maximum Likelihood Estimation MVNSuppose iid \\(\\mathbf{y}_1 ,... \\mathbf{y}_n \\sim N_p (\\mu, \\mathbf{\\Sigma})\\), likelihood function data \\[\n\\begin{aligned}\nL(\\mu, \\mathbf{\\Sigma}) &= \\prod_{j=1}^n (\\frac{1}{(2\\pi)^{p/2}|\\mathbf{\\Sigma}|^{1/2}} \\exp(-\\frac{1}{2}(\\mathbf{y}_j -\\mu)'\\mathbf{\\Sigma}^{-1})(\\mathbf{y}_j -\\mu)) \\\\\n&= \\frac{1}{(2\\pi)^{np/2}|\\mathbf{\\Sigma}|^{n/2}} \\exp(-\\frac{1}{2} \\sum_{j=1}^n(\\mathbf{y}_j -\\mu)'\\mathbf{\\Sigma}^{-1})(\\mathbf{y}_j -\\mu)\n\\end{aligned}\n\\], MLEs \\[\n\\hat{\\mu} = \\bar{\\mathbf{y}}\n\\]\\[\n\\hat{\\mathbf{\\Sigma}} = \\frac{n-1}{n} \\mathbf{S}\n\\]using derivatives log likelihood function respect \\(\\mu\\) \\(\\mathbf{\\Sigma}\\)Properties MLEsInvariance: \\(\\hat{\\theta}\\) MLE \\(\\theta\\), MLE \\(h(\\theta)\\) \\(h(\\hat{\\theta})\\) function h(.)Invariance: \\(\\hat{\\theta}\\) MLE \\(\\theta\\), MLE \\(h(\\theta)\\) \\(h(\\hat{\\theta})\\) function h(.)Consistency: MLEs consistent estimators, usually biasedConsistency: MLEs consistent estimators, usually biasedEfficiency: MLEs efficient estimators (estimator smaller variance large samples)Efficiency: MLEs efficient estimators (estimator smaller variance large samples)Asymptotic normality: Suppose \\(\\hat{\\theta}_n\\) MLE \\(\\theta\\) based upon n independent observations. \\(\\hat{\\theta}_n \\dot{\\sim} N(\\theta, \\mathbf{H}^{-1})\\)\n\\(\\mathbf{H}\\) Fisher Information Matrix, contains expected values second partial derivatives fo log-likelihood function. (,j)th element \\(\\mathbf{H}\\) \\(-E(\\frac{\\partial^2 l(\\mathbf{\\theta})}{\\partial \\theta_i \\partial \\theta_j})\\)\ncan estimate \\(\\mathbf{H}\\) finding form determined , evaluate \\(\\theta = \\hat{\\theta}_n\\)\nAsymptotic normality: Suppose \\(\\hat{\\theta}_n\\) MLE \\(\\theta\\) based upon n independent observations. \\(\\hat{\\theta}_n \\dot{\\sim} N(\\theta, \\mathbf{H}^{-1})\\)\\(\\mathbf{H}\\) Fisher Information Matrix, contains expected values second partial derivatives fo log-likelihood function. (,j)th element \\(\\mathbf{H}\\) \\(-E(\\frac{\\partial^2 l(\\mathbf{\\theta})}{\\partial \\theta_i \\partial \\theta_j})\\)\\(\\mathbf{H}\\) Fisher Information Matrix, contains expected values second partial derivatives fo log-likelihood function. (,j)th element \\(\\mathbf{H}\\) \\(-E(\\frac{\\partial^2 l(\\mathbf{\\theta})}{\\partial \\theta_i \\partial \\theta_j})\\)can estimate \\(\\mathbf{H}\\) finding form determined , evaluate \\(\\theta = \\hat{\\theta}_n\\)can estimate \\(\\mathbf{H}\\) finding form determined , evaluate \\(\\theta = \\hat{\\theta}_n\\)Likelihood ratio testing: null hypothesis, \\(H_0\\) can form likelihood ratio test\nstatistic : \\(\\Lambda = \\frac{\\max_{H_0}l(\\mathbf{\\mu}, \\mathbf{\\Sigma|Y})}{\\max l(\\mu, \\mathbf{\\Sigma | Y})}\\)\nlarge n, \\(-2 \\log \\Lambda \\sim \\chi^2_{(v)}\\) v number parameters unrestricted space minus number parameters \\(H_0\\)\nLikelihood ratio testing: null hypothesis, \\(H_0\\) can form likelihood ratio testThe statistic : \\(\\Lambda = \\frac{\\max_{H_0}l(\\mathbf{\\mu}, \\mathbf{\\Sigma|Y})}{\\max l(\\mu, \\mathbf{\\Sigma | Y})}\\)statistic : \\(\\Lambda = \\frac{\\max_{H_0}l(\\mathbf{\\mu}, \\mathbf{\\Sigma|Y})}{\\max l(\\mu, \\mathbf{\\Sigma | Y})}\\)large n, \\(-2 \\log \\Lambda \\sim \\chi^2_{(v)}\\) v number parameters unrestricted space minus number parameters \\(H_0\\)large n, \\(-2 \\log \\Lambda \\sim \\chi^2_{(v)}\\) v number parameters unrestricted space minus number parameters \\(H_0\\)Test Multivariate NormalityCheck univariate normality trait (X) separately\nCan check Normality Assessment\ngood thing univariate trait normal, joint distribution normal (see Properties MVN). joint multivariate distribution normal, marginal distribution normal.\nHowever, marginal normality traits imply joint MVN\nEasily rule multivariate normality, easy prove \nCheck univariate normality trait (X) separatelyCan check Normality AssessmentCan check Normality AssessmentThe good thing univariate trait normal, joint distribution normal (see Properties MVN). joint multivariate distribution normal, marginal distribution normal.good thing univariate trait normal, joint distribution normal (see Properties MVN). joint multivariate distribution normal, marginal distribution normal.However, marginal normality traits imply joint MVNHowever, marginal normality traits imply joint MVNEasily rule multivariate normality, easy prove itEasily rule multivariate normality, easy prove itMardia’s tests multivariate normality\nMultivariate skewness \\[\n\\beta_{1,p} = E[(\\mathbf{y}- \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^3\n\\]\n\\(\\mathbf{x}\\) \\(\\mathbf{y}\\) independent, distribution (note: \\(\\beta\\) regression coefficient)\nMultivariate kurtosis defined \n\\[\n\\beta_{2,p} - E[(\\mathbf{y}- \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^2\n\\]\nMVN distribution, \\(\\beta_{1,p} = 0\\) \\(\\beta_{2,p} = p(p+2)\\)\nsample size n, can estimate\n\\[\n\\hat{\\beta}_{1,p} = \\frac{1}{n^2}\\sum_{=1}^n \\sum_{j=1}^n g^2_{ij}\n\\]\n\\[\n\\hat{\\beta}_{2,p} = \\frac{1}{n} \\sum_{=1}^n g^2_{ii}\n\\]\n\\(g_{ij} = (\\mathbf{y}_i - \\bar{\\mathbf{y}})' \\mathbf{S}^{-1} (\\mathbf{y}_j - \\bar{\\mathbf{y}})\\). Note: \\(g_{ii} = d^2_i\\) \\(d^2_i\\) Mahalanobis distance\n\n(MARDIA 1970) shows large n\n\\[\n\\kappa_1 = \\frac{n \\hat{\\beta}_{1,p}}{6} \\dot{\\sim} \\chi^2_{p(p+1)(p+2)/6}\n\\]\n\\[\n\\kappa_2 = \\frac{\\hat{\\beta}_{2,p} - p(p+2)}{\\sqrt{8p(p+2)/n}} \\sim N(0,1)\n\\]\nHence, can use \\(\\kappa_1\\) \\(\\kappa_2\\) test null hypothesis MVN.\ndata non-normal, normal theory tests mean sensitive \\(\\beta_{1,p}\\) , tests covariance sensitive \\(\\beta_{2,p}\\)\n\nMardia’s tests multivariate normalityMultivariate skewness \\[\n\\beta_{1,p} = E[(\\mathbf{y}- \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^3\n\\]Multivariate skewness \\[\n\\beta_{1,p} = E[(\\mathbf{y}- \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^3\n\\]\\(\\mathbf{x}\\) \\(\\mathbf{y}\\) independent, distribution (note: \\(\\beta\\) regression coefficient)\\(\\mathbf{x}\\) \\(\\mathbf{y}\\) independent, distribution (note: \\(\\beta\\) regression coefficient)Multivariate kurtosis defined asMultivariate kurtosis defined \\[\n\\beta_{2,p} - E[(\\mathbf{y}- \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^2\n\\]\\[\n\\beta_{2,p} - E[(\\mathbf{y}- \\mathbf{\\mu})' \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu})]^2\n\\]MVN distribution, \\(\\beta_{1,p} = 0\\) \\(\\beta_{2,p} = p(p+2)\\)MVN distribution, \\(\\beta_{1,p} = 0\\) \\(\\beta_{2,p} = p(p+2)\\)sample size n, can estimate\n\\[\n\\hat{\\beta}_{1,p} = \\frac{1}{n^2}\\sum_{=1}^n \\sum_{j=1}^n g^2_{ij}\n\\]\n\\[\n\\hat{\\beta}_{2,p} = \\frac{1}{n} \\sum_{=1}^n g^2_{ii}\n\\]\n\\(g_{ij} = (\\mathbf{y}_i - \\bar{\\mathbf{y}})' \\mathbf{S}^{-1} (\\mathbf{y}_j - \\bar{\\mathbf{y}})\\). Note: \\(g_{ii} = d^2_i\\) \\(d^2_i\\) Mahalanobis distance\nsample size n, can estimate\\[\n\\hat{\\beta}_{1,p} = \\frac{1}{n^2}\\sum_{=1}^n \\sum_{j=1}^n g^2_{ij}\n\\]\\[\n\\hat{\\beta}_{2,p} = \\frac{1}{n} \\sum_{=1}^n g^2_{ii}\n\\]\\(g_{ij} = (\\mathbf{y}_i - \\bar{\\mathbf{y}})' \\mathbf{S}^{-1} (\\mathbf{y}_j - \\bar{\\mathbf{y}})\\). Note: \\(g_{ii} = d^2_i\\) \\(d^2_i\\) Mahalanobis distance(MARDIA 1970) shows large n\n\\[\n\\kappa_1 = \\frac{n \\hat{\\beta}_{1,p}}{6} \\dot{\\sim} \\chi^2_{p(p+1)(p+2)/6}\n\\]\n\\[\n\\kappa_2 = \\frac{\\hat{\\beta}_{2,p} - p(p+2)}{\\sqrt{8p(p+2)/n}} \\sim N(0,1)\n\\]\nHence, can use \\(\\kappa_1\\) \\(\\kappa_2\\) test null hypothesis MVN.\ndata non-normal, normal theory tests mean sensitive \\(\\beta_{1,p}\\) , tests covariance sensitive \\(\\beta_{2,p}\\)\n(MARDIA 1970) shows large n\\[\n\\kappa_1 = \\frac{n \\hat{\\beta}_{1,p}}{6} \\dot{\\sim} \\chi^2_{p(p+1)(p+2)/6}\n\\]\\[\n\\kappa_2 = \\frac{\\hat{\\beta}_{2,p} - p(p+2)}{\\sqrt{8p(p+2)/n}} \\sim N(0,1)\n\\]Hence, can use \\(\\kappa_1\\) \\(\\kappa_2\\) test null hypothesis MVN.Hence, can use \\(\\kappa_1\\) \\(\\kappa_2\\) test null hypothesis MVN.data non-normal, normal theory tests mean sensitive \\(\\beta_{1,p}\\) , tests covariance sensitive \\(\\beta_{2,p}\\)data non-normal, normal theory tests mean sensitive \\(\\beta_{1,p}\\) , tests covariance sensitive \\(\\beta_{2,p}\\)Chi-square Q-Q plot\nLet \\(\\mathbf{y}_i, = 1,...,n\\) random sample sample \\(N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\)\n\\(\\mathbf{z}_i = \\mathbf{\\Sigma}^{-1/2}(\\mathbf{y}_i - \\mathbf{\\mu}), = 1,...,n\\) iid \\(N_p (\\mathbf{0}, \\mathbf{})\\). Thus, \\(d_i^2 = \\mathbf{z}_i' \\mathbf{z}_i \\sim \\chi^2_p , = 1,...,n\\)\nplot ordered \\(d_i^2\\) values qualities \\(\\chi^2_p\\) distribution. normality holds, plot approximately resemble straight lien passing origin 45 degree\nrequires large sample size (.e., sensitive sample size). Even generate data MVN, tail Chi-square Q-Q plot can still line.\nChi-square Q-Q plotLet \\(\\mathbf{y}_i, = 1,...,n\\) random sample sample \\(N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\)Let \\(\\mathbf{y}_i, = 1,...,n\\) random sample sample \\(N_p(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\)\\(\\mathbf{z}_i = \\mathbf{\\Sigma}^{-1/2}(\\mathbf{y}_i - \\mathbf{\\mu}), = 1,...,n\\) iid \\(N_p (\\mathbf{0}, \\mathbf{})\\). Thus, \\(d_i^2 = \\mathbf{z}_i' \\mathbf{z}_i \\sim \\chi^2_p , = 1,...,n\\)\\(\\mathbf{z}_i = \\mathbf{\\Sigma}^{-1/2}(\\mathbf{y}_i - \\mathbf{\\mu}), = 1,...,n\\) iid \\(N_p (\\mathbf{0}, \\mathbf{})\\). Thus, \\(d_i^2 = \\mathbf{z}_i' \\mathbf{z}_i \\sim \\chi^2_p , = 1,...,n\\)plot ordered \\(d_i^2\\) values qualities \\(\\chi^2_p\\) distribution. normality holds, plot approximately resemble straight lien passing origin 45 degreeplot ordered \\(d_i^2\\) values qualities \\(\\chi^2_p\\) distribution. normality holds, plot approximately resemble straight lien passing origin 45 degreeit requires large sample size (.e., sensitive sample size). Even generate data MVN, tail Chi-square Q-Q plot can still line.requires large sample size (.e., sensitive sample size). Even generate data MVN, tail Chi-square Q-Q plot can still line.data normal, can\nignore \nuse nonparametric methods\nuse models based upon approximate distirubiton (e.g., GLMM)\ntry performing transformation\ndata normal, canignore itignore ituse nonparametric methodsuse nonparametric methodsuse models based upon approximate distirubiton (e.g., GLMM)use models based upon approximate distirubiton (e.g., GLMM)try performing transformationtry performing transformation","code":"\nlibrary(heplots)## Warning: package 'heplots' was built under R version 4.0.5## Warning: package 'car' was built under R version 4.0.5\nlibrary(ICSNP)## Warning: package 'ICSNP' was built under R version 4.0.5## Warning: package 'mvtnorm' was built under R version 4.0.5## Warning: package 'ICS' was built under R version 4.0.5\nlibrary(MVN)## Warning: package 'MVN' was built under R version 4.0.5\nlibrary(tidyverse)## Warning: package 'tidyverse' was built under R version 4.0.5## Warning: package 'ggplot2' was built under R version 4.0.5## Warning: package 'tibble' was built under R version 4.0.5## Warning: package 'readr' was built under R version 4.0.5## Warning: package 'dplyr' was built under R version 4.0.5\ntrees = read.table(\"images/trees.dat\")\nnames(trees) <- c(\"Nitrogen\",\"Phosphorous\",\"Potassium\",\"Ash\",\"Height\")\nstr(trees)## 'data.frame':    26 obs. of  5 variables:\n##  $ Nitrogen   : num  2.2 2.1 1.52 2.88 2.18 1.87 1.52 2.37 2.06 1.84 ...\n##  $ Phosphorous: num  0.417 0.354 0.208 0.335 0.314 0.271 0.164 0.302 0.373 0.265 ...\n##  $ Potassium  : num  1.35 0.9 0.71 0.9 1.26 1.15 0.83 0.89 0.79 0.72 ...\n##  $ Ash        : num  1.79 1.08 0.47 1.48 1.09 0.99 0.85 0.94 0.8 0.77 ...\n##  $ Height     : int  351 249 171 373 321 191 225 291 284 213 ...\nsummary(trees)##     Nitrogen      Phosphorous       Potassium           Ash        \n##  Min.   :1.130   Min.   :0.1570   Min.   :0.3800   Min.   :0.4500  \n##  1st Qu.:1.532   1st Qu.:0.1963   1st Qu.:0.6050   1st Qu.:0.6375  \n##  Median :1.855   Median :0.2250   Median :0.7150   Median :0.9300  \n##  Mean   :1.896   Mean   :0.2506   Mean   :0.7619   Mean   :0.8873  \n##  3rd Qu.:2.160   3rd Qu.:0.2975   3rd Qu.:0.8975   3rd Qu.:0.9825  \n##  Max.   :2.880   Max.   :0.4170   Max.   :1.3500   Max.   :1.7900  \n##      Height     \n##  Min.   : 65.0  \n##  1st Qu.:122.5  \n##  Median :181.0  \n##  Mean   :196.6  \n##  3rd Qu.:276.0  \n##  Max.   :373.0\ncor(trees, method = \"pearson\") # correlation matrix##              Nitrogen Phosphorous Potassium       Ash    Height\n## Nitrogen    1.0000000   0.6023902 0.5462456 0.6509771 0.8181641\n## Phosphorous 0.6023902   1.0000000 0.7037469 0.6707871 0.7739656\n## Potassium   0.5462456   0.7037469 1.0000000 0.6710548 0.7915683\n## Ash         0.6509771   0.6707871 0.6710548 1.0000000 0.7676771\n## Height      0.8181641   0.7739656 0.7915683 0.7676771 1.0000000\n# qq-plot \ngg <- trees %>%\n    pivot_longer(everything(), names_to = \"Var\", values_to = \"Value\") %>%\n    ggplot(aes(sample = Value)) +\n    geom_qq() +\n    geom_qq_line() +\n    facet_wrap(\"Var\", scales = \"free\")\ngg\n# Univariate normality\nsw_tests <- apply(trees, MARGIN = 2, FUN = shapiro.test)\nsw_tests## $Nitrogen\n## \n##  Shapiro-Wilk normality test\n## \n## data:  newX[, i]\n## W = 0.96829, p-value = 0.5794\n## \n## \n## $Phosphorous\n## \n##  Shapiro-Wilk normality test\n## \n## data:  newX[, i]\n## W = 0.93644, p-value = 0.1104\n## \n## \n## $Potassium\n## \n##  Shapiro-Wilk normality test\n## \n## data:  newX[, i]\n## W = 0.95709, p-value = 0.3375\n## \n## \n## $Ash\n## \n##  Shapiro-Wilk normality test\n## \n## data:  newX[, i]\n## W = 0.92071, p-value = 0.04671\n## \n## \n## $Height\n## \n##  Shapiro-Wilk normality test\n## \n## data:  newX[, i]\n## W = 0.94107, p-value = 0.1424\n# Kolmogorov-Smirnov test \nks_tests <- map(trees, ~ ks.test(scale(.x),\"pnorm\"))## Warning in ks.test(scale(.x), \"pnorm\"): ties should not be present for the\n## Kolmogorov-Smirnov test## Warning in ks.test(scale(.x), \"pnorm\"): ties should not be present for the\n## Kolmogorov-Smirnov test\n\n## Warning in ks.test(scale(.x), \"pnorm\"): ties should not be present for the\n## Kolmogorov-Smirnov test\n\n## Warning in ks.test(scale(.x), \"pnorm\"): ties should not be present for the\n## Kolmogorov-Smirnov test\n\n## Warning in ks.test(scale(.x), \"pnorm\"): ties should not be present for the\n## Kolmogorov-Smirnov test\nks_tests## $Nitrogen\n## \n##  One-sample Kolmogorov-Smirnov test\n## \n## data:  scale(.x)\n## D = 0.12182, p-value = 0.8351\n## alternative hypothesis: two-sided\n## \n## \n## $Phosphorous\n## \n##  One-sample Kolmogorov-Smirnov test\n## \n## data:  scale(.x)\n## D = 0.17627, p-value = 0.3944\n## alternative hypothesis: two-sided\n## \n## \n## $Potassium\n## \n##  One-sample Kolmogorov-Smirnov test\n## \n## data:  scale(.x)\n## D = 0.10542, p-value = 0.9348\n## alternative hypothesis: two-sided\n## \n## \n## $Ash\n## \n##  One-sample Kolmogorov-Smirnov test\n## \n## data:  scale(.x)\n## D = 0.14503, p-value = 0.6449\n## alternative hypothesis: two-sided\n## \n## \n## $Height\n## \n##  One-sample Kolmogorov-Smirnov test\n## \n## data:  scale(.x)\n## D = 0.1107, p-value = 0.9076\n## alternative hypothesis: two-sided\n# Mardia's test, need large sample size for power\nmardia_test <-\n    mvn(\n        trees,\n        mvnTest = \"mardia\",\n        covariance = FALSE,\n        multivariatePlot = \"qq\"\n    )\nmardia_test$multivariateNormality##              Test         Statistic            p value Result\n## 1 Mardia Skewness  29.7248528871795   0.72054426745778    YES\n## 2 Mardia Kurtosis -1.67743173185383 0.0934580886477281    YES\n## 3             MVN              <NA>               <NA>    YES"},{"path":"multivariate-methods.html","id":"mean-vector-inference","chapter":"17 Multivariate Methods","heading":"17.0.2 Mean Vector Inference","text":"univariate normal distribution, test \\(H_0: \\mu =\\mu_0\\) using\\[\nT = \\frac{\\bar{y}- \\mu_0}{s/\\sqrt{n}} \\sim t_{n-1}\n\\]null hypothesis. reject null \\(|T|\\) large relative \\(t_{(1-\\alpha/2,n-1)}\\) means seeing value large observed rare null trueEquivalently,\\[\nT^2 = \\frac{(\\bar{y}- \\mu_0)^2}{s^2/n} = n(\\bar{y}- \\mu_0)(s^2)^{-1}(\\bar{y}- \\mu_0) \\sim f_{(1,n-1)}\n\\]","code":""},{"path":"multivariate-methods.html","id":"natural-multivariate-generalization","chapter":"17 Multivariate Methods","heading":"17.0.2.1 Natural Multivariate Generalization","text":"\\[\nH_0: \\mathbf{\\mu} = \\mathbf{\\mu}_0 \\\\\nH_a: \\mathbf{\\mu} \\neq \\mathbf{\\mu}_0\n\\]Define Hotelling’s \\(T^2\\) \\[\nT^2 = n(\\bar{\\mathbf{y}} - \\mathbf{\\mu}_0)'\\mathbf{S}^{-1}(\\bar{\\mathbf{y}} - \\mathbf{\\mu}_0)\n\\]can viewed generalized distance \\(\\bar{\\mathbf{y}}\\) \\(\\mathbf{\\mu}_0\\)assumption normality,\\[\nF = \\frac{n-p}{(n-1)p} T^2 \\sim f_{(p,n-p)}\n\\]reject null hypothesis \\(F > f_{(1-\\alpha, p, n-p)}\\)\\(T^2\\) test invariant changes measurement units.\n\\(\\mathbf{z = Cy + d}\\) \\(\\mathbf{C}\\) \\(\\mathbf{d}\\) depend \\(\\mathbf{y}\\), \\(T^2(\\mathbf{z}) - T^2(\\mathbf{y})\\)\n\\(T^2\\) test invariant changes measurement units.\\(\\mathbf{z = Cy + d}\\) \\(\\mathbf{C}\\) \\(\\mathbf{d}\\) depend \\(\\mathbf{y}\\), \\(T^2(\\mathbf{z}) - T^2(\\mathbf{y})\\)\\(T^2\\) test can derived likelihood ratio test \\(H_0: \\mu = \\mu_0\\)\\(T^2\\) test can derived likelihood ratio test \\(H_0: \\mu = \\mu_0\\)","code":""},{"path":"multivariate-methods.html","id":"confidence-intervals","chapter":"17 Multivariate Methods","heading":"17.0.2.2 Confidence Intervals","text":"","code":""},{"path":"multivariate-methods.html","id":"confidence-region","chapter":"17 Multivariate Methods","heading":"17.0.2.2.1 Confidence Region","text":"“exact” \\(100(1-\\alpha)\\%\\) confidence region \\(\\mathbf{\\mu}\\) set vectors, \\(\\mathbf{v}\\), “close enough” observed mean vector, \\(\\bar{\\mathbf{y}}\\) satisfy\\[\nn(\\bar{\\mathbf{y}} - \\mathbf{\\mu}_0)'\\mathbf{S}^{-1}(\\bar{\\mathbf{y}} - \\mathbf{\\mu}_0) \\le \\frac{(n-1)p}{n-p} f_{(1-\\alpha, p, n-p)}\n\\]\\(\\mathbf{v}\\) just mean vectors rejected \\(T^2\\) test \\(\\mathbf{\\bar{y}}\\) observed.case 2 parameters, confidence region “hyper-ellipsoid.”region, consists \\(\\mathbf{\\mu}_0\\) vectors \\(T^2\\) test reject \\(H_0\\) significance level \\(\\alpha\\)Even though confidence region better assesses joint knowledge concerning plausible values \\(\\mathbf{\\mu}\\) , people typically include confidence statement individual component means. ’d like separate confidence statements hold simultaneously specified high probability. Simultaneous confidence intervals: intervals statement incorrect","code":""},{},{},{"path":"multivariate-methods.html","id":"general-hypothesis-testing","chapter":"17 Multivariate Methods","heading":"17.0.3 General Hypothesis Testing","text":"","code":""},{"path":"multivariate-methods.html","id":"one-sample-tests","chapter":"17 Multivariate Methods","heading":"17.0.3.1 One-sample Tests","text":"\\[\nH_0: \\mathbf{C \\mu= 0} \n\\]\\(\\mathbf{C}\\) \\(c \\times p\\) matrix rank c \\(c \\le p\\)can test hypothesis using following statistic\\[\nF = \\frac{n - c}{(n-1)c} T^2\n\\]\\(T^2 = n(\\mathbf{C\\bar{y}})' (\\mathbf{CSC'})^{-1} (\\mathbf{C\\bar{y}})\\)Example:\\[\nH_0: \\mu_1 = \\mu_2 = ... = \\mu_p\n\\]Equivalently,\\[\n\\mu_1 - \\mu_2 = 0 \\\\\n\\vdots \\\\\n\\mu_{p-1} - \\mu_p = 0\n\\]total \\(p-1\\) tests. Hence, \\(\\mathbf{C}\\) \\(p - 1 \\times p\\) matrix\\[\n\\mathbf{C} = \n\\left(\n\\begin{array}\n{ccccc}\n1 & -1 & 0 & \\ldots & 0 \\\\\n0 & 1 & -1 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\ldots & 1 & -1 \n\\end{array}\n\\right)\n\\]number rows = \\(c = p -1\\)Equivalently, can also compare means first mean. , test \\(\\mu_1 - \\mu_2 = 0, \\mu_1 - \\mu_3 = 0,..., \\mu_1 - \\mu_p = 0\\), \\((p-1) \\times p\\) matrix \\(\\mathbf{C}\\) \\[\n\\mathbf{C} = \n\\left(\n\\begin{array}\n{ccccc}\n-1 & 1 & 0 & \\ldots & 0 \\\\\n-1 & 0 & 1 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n-1 & 0 & \\ldots & 0 & 1 \n\\end{array}\n\\right)\n\\]value \\(T^2\\) invariant equivalent choices \\(\\mathbf{C}\\)often used repeated measures designs, subject receives treatment successive periods time (treatments administered unit).Example:Let \\(y_{ij}\\) response subject time j \\(= 1,..,n, j = 1,...,T\\). case, \\(\\mathbf{y}_i = (y_{i1}, ..., y_{})', = 1,...,n\\) random sample \\(N_T (\\mathbf{\\mu}, \\mathbf{\\Sigma})\\)Let \\(n=8\\) subjects, \\(T = 6\\). interested \\(\\mu_1, .., \\mu_6\\)\\[\nH_0: \\mu_1 = \\mu_2 = ... = \\mu_6\n\\]Equivalently,\\[\n\\mu_1 - \\mu_2 = 0 \\\\\n\\mu_2 - \\mu_3 = 0 \\\\\n... \\\\\n\\mu_5  - \\mu_6 = 0\n\\]can test orthogonal polynomials 4 equally spaced time points. test example null hypothesis quadratic cubic effects jointly equal 0, define \\(\\mathbf{C}\\)\\[\n\\mathbf{C} = \n\\left(\n\\begin{array}\n{cccc}\n1 & -1 & -1 & 1 \\\\\n-1 & 3 & -3 & 1\n\\end{array}\n\\right)\n\\]","code":""},{"path":"multivariate-methods.html","id":"two-sample-tests","chapter":"17 Multivariate Methods","heading":"17.0.3.2 Two-Sample Tests","text":"Consider analogous two sample multivariate tests.Example: data two independent random samples, one sample two populations\\[\n\\mathbf{y}_{1i} \\sim N_p (\\mathbf{\\mu_1, \\Sigma}) \\\\\n\\mathbf{y}_{2j} \\sim N_p (\\mathbf{\\mu_2, \\Sigma})\n\\]assumenormalitynormalityequal variance-covariance matricesequal variance-covariance matricesindependent random samplesindependent random samplesWe can summarize data using sufficient statistics \\(\\mathbf{\\bar{y}}_1, \\mathbf{S}_1, \\mathbf{\\bar{y}}_2, \\mathbf{S}_2\\) respective sample sizes, \\(n_1,n_2\\)Since assume \\(\\mathbf{\\Sigma}_1 = \\mathbf{\\Sigma}_2 = \\mathbf{\\Sigma}\\), compute pooled estimate variance-covariance matrix \\(n_1 + n_2 - 2\\) df\\[\n\\mathbf{S} = \\frac{(n_1 - 1)\\mathbf{S}_1 + (n_2-1) \\mathbf{S}_2}{(n_1 -1) + (n_2 - 1)}\n\\]\\[\nH_0: \\mathbf{\\mu}_1 = \\mathbf{\\mu}_2 \\\\\nH_a: \\mathbf{\\mu}_1 \\neq \\mathbf{\\mu}_2\n\\]least one element mean vectors differentWe use\\(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2\\) estimate \\(\\mu_1 - \\mu_2\\)\\(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2\\) estimate \\(\\mu_1 - \\mu_2\\)\\(\\mathbf{S}\\) estimate \\(\\mathbf{\\Sigma}\\)\nNote: assume two populations independent, covariance\n\\(cov(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) = var(\\mathbf{\\bar{y}}_1) + var(\\mathbf{\\bar{y}}_2) = \\frac{\\mathbf{\\Sigma_1}}{n_1} + \\frac{\\mathbf{\\Sigma_2}}{n_2} = \\mathbf{\\Sigma}(\\frac{1}{n_1} + \\frac{1}{n_2})\\)\\(\\mathbf{S}\\) estimate \\(\\mathbf{\\Sigma}\\)Note: assume two populations independent, covariance\\(cov(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) = var(\\mathbf{\\bar{y}}_1) + var(\\mathbf{\\bar{y}}_2) = \\frac{\\mathbf{\\Sigma_1}}{n_1} + \\frac{\\mathbf{\\Sigma_2}}{n_2} = \\mathbf{\\Sigma}(\\frac{1}{n_1} + \\frac{1}{n_2})\\)Reject \\(H_0\\) \\[\n\\begin{aligned}\nT^2 &= (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)'\\{ \\mathbf{S} (\\frac{1}{n_1} + \\frac{1}{n_2})\\}^{-1} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)\\\\\n&= \\frac{n_1 n_2}{n_1 +n_2} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)'\\{ \\mathbf{S} \\}^{-1} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)\\\\\n& \\ge \\frac{(n_1 + n_2 -2)p}{n_1 + n_2 - p - 1} f_{(1- \\alpha,n_1 + n_2 - p -1)}\n\\end{aligned}\n\\]equivalently, \\[\nF = \\frac{n_1 + n_2 - p -1}{(n_1 + n_2 -2)p} T^2 \\ge f_{(1- \\alpha, p , n_1 + n_2 -p -1)}\n\\]\\(100(1-\\alpha) \\%\\) confidence region \\(\\mu_1 - \\mu_2\\) consists vector \\(\\delta\\) satisfy\\[\n\\frac{n_1 n_2}{n_1 + n_2} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2 - \\mathbf{\\delta})' \\mathbf{S}^{-1}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2 - \\mathbf{\\delta}) \\le \\frac{(n_1 + n_2 - 2)p}{n_1 + n_2 -p - 1}f_{(1-\\alpha, p , n_1 + n_2 - p -1)}\n\\]simultaneous confidence intervals linear combinations \\(\\mu_1 - \\mu_2\\) form\\[\n\\mathbf{'}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\pm \\sqrt{\\frac{(n_1 + n_2 -2)p}{n_1 + n_2 - p -1}}f_{(1-\\alpha, p, n_1 + n_2 -p -1)} \\times \\sqrt{\\mathbf{'Sa}(\\frac{1}{n_1} + \\frac{1}{n_2})}\n\\]Bonferroni intervals, k combinations\\[\n(\\bar{y}_{1i} - \\bar{y}_{2i}) \\pm t_{(1-\\alpha/2k, n_1 + n_2 - 2)}\\sqrt{(\\frac{1}{n_1}  + \\frac{1}{n_2})s_{ii}}\n\\]","code":""},{"path":"multivariate-methods.html","id":"model-assumptions","chapter":"17 Multivariate Methods","heading":"17.0.3.3 Model Assumptions","text":"model assumption metUnequal Covariance Matrices\n\\(n_1 = n_2\\) (large samples) little effect Type error rate power fo two sample test\n\\(n_1 > n_2\\) eigenvalues \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}^{-1}_2\\) less 1, Type error level inflated\n\\(n_1 > n_2\\) eigenvalues \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}_2^{-1}\\) greater 1, Type error rate small, leading reduction power\nUnequal Covariance MatricesIf \\(n_1 = n_2\\) (large samples) little effect Type error rate power fo two sample testIf \\(n_1 = n_2\\) (large samples) little effect Type error rate power fo two sample testIf \\(n_1 > n_2\\) eigenvalues \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}^{-1}_2\\) less 1, Type error level inflatedIf \\(n_1 > n_2\\) eigenvalues \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}^{-1}_2\\) less 1, Type error level inflatedIf \\(n_1 > n_2\\) eigenvalues \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}_2^{-1}\\) greater 1, Type error rate small, leading reduction powerIf \\(n_1 > n_2\\) eigenvalues \\(\\mathbf{\\Sigma}_1 \\mathbf{\\Sigma}_2^{-1}\\) greater 1, Type error rate small, leading reduction powerSample Normal\nType error level two sample \\(T^2\\) test isn’t much affect moderate departures normality two populations sampled similar distributions\nOne sample \\(T^2\\) test much sensitive lack normality, especially distribution skewed.\nIntuitively, can think one sample distribution sensitive, distribution difference two similar distributions sensitive.\nSolutions:\nTransform make data normal\nLarge large samples, use \\(\\chi^2\\) (Wald) test, populations don’t need normal, equal sample sizes, equal variance-covariance matrices\n\\(H_0: \\mu_1 - \\mu_2 =0\\) use \\((\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)'( \\frac{1}{n_1} \\mathbf{S}_1 + \\frac{1}{n_2}\\mathbf{S}_2)^{-1}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\dot{\\sim} \\chi^2_{(p)}\\)\n\n\nSample NormalType error level two sample \\(T^2\\) test isn’t much affect moderate departures normality two populations sampled similar distributionsType error level two sample \\(T^2\\) test isn’t much affect moderate departures normality two populations sampled similar distributionsOne sample \\(T^2\\) test much sensitive lack normality, especially distribution skewed.One sample \\(T^2\\) test much sensitive lack normality, especially distribution skewed.Intuitively, can think one sample distribution sensitive, distribution difference two similar distributions sensitive.Intuitively, can think one sample distribution sensitive, distribution difference two similar distributions sensitive.Solutions:\nTransform make data normal\nLarge large samples, use \\(\\chi^2\\) (Wald) test, populations don’t need normal, equal sample sizes, equal variance-covariance matrices\n\\(H_0: \\mu_1 - \\mu_2 =0\\) use \\((\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)'( \\frac{1}{n_1} \\mathbf{S}_1 + \\frac{1}{n_2}\\mathbf{S}_2)^{-1}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\dot{\\sim} \\chi^2_{(p)}\\)\n\nSolutions:Transform make data normalTransform make data normalLarge large samples, use \\(\\chi^2\\) (Wald) test, populations don’t need normal, equal sample sizes, equal variance-covariance matrices\n\\(H_0: \\mu_1 - \\mu_2 =0\\) use \\((\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)'( \\frac{1}{n_1} \\mathbf{S}_1 + \\frac{1}{n_2}\\mathbf{S}_2)^{-1}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\dot{\\sim} \\chi^2_{(p)}\\)\nLarge large samples, use \\(\\chi^2\\) (Wald) test, populations don’t need normal, equal sample sizes, equal variance-covariance matrices\\(H_0: \\mu_1 - \\mu_2 =0\\) use \\((\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)'( \\frac{1}{n_1} \\mathbf{S}_1 + \\frac{1}{n_2}\\mathbf{S}_2)^{-1}(\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2) \\dot{\\sim} \\chi^2_{(p)}\\)","code":""},{"path":"multivariate-methods.html","id":"equal-covariance-matrices-tests","chapter":"17 Multivariate Methods","heading":"17.0.3.3.1 Equal Covariance Matrices Tests","text":"independent random samples k populations p-dimensional vectors. compute sample covariance matrix , \\(\\mathbf{S}_i\\), \\(= 1,...,k\\)\\[\nH_0: \\mathbf{\\Sigma}_1 = \\mathbf{\\Sigma}_2 = \\ldots = \\mathbf{\\Sigma}_k = \\mathbf{\\Sigma} \\\\\nH_a: \\text{least 2 different}\n\\]Assume \\(H_0\\) true, use pooled estimate common covariance matrix, \\(\\mathbf{\\Sigma}\\)\\[\n\\mathbf{S} = \\frac{\\sum_{=1}^k (n_i -1)\\mathbf{S}_i}{\\sum_{=1}^k (n_i - 1)}\n\\]\\(\\sum_{=1}^k (n_i -1)\\)","code":""},{},{"path":"multivariate-methods.html","id":"two-sample-repeated-measurements","chapter":"17 Multivariate Methods","heading":"17.0.3.4 Two-Sample Repeated Measurements","text":"Define \\(\\mathbf{y}_{hi} = (y_{hi1}, ..., y_{hit})'\\) observations -th subject h-th group times 1 TDefine \\(\\mathbf{y}_{hi} = (y_{hi1}, ..., y_{hit})'\\) observations -th subject h-th group times 1 TAssume \\(\\mathbf{y}_{11}, ..., \\mathbf{y}_{1n_1}\\) iid \\(N_t(\\mathbf{\\mu}_1, \\mathbf{\\Sigma})\\) \\(\\mathbf{y}_{21},...,\\mathbf{y}_{2n_2}\\) iid \\(N_t(\\mathbf{\\mu}_2, \\mathbf{\\Sigma})\\)Assume \\(\\mathbf{y}_{11}, ..., \\mathbf{y}_{1n_1}\\) iid \\(N_t(\\mathbf{\\mu}_1, \\mathbf{\\Sigma})\\) \\(\\mathbf{y}_{21},...,\\mathbf{y}_{2n_2}\\) iid \\(N_t(\\mathbf{\\mu}_2, \\mathbf{\\Sigma})\\)\\(H_0: \\mathbf{C}(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2) = \\mathbf{0}_c\\) \\(\\mathbf{C}\\) \\(c \\times t\\) matrix rank \\(c\\) \\(c \\le t\\)\\(H_0: \\mathbf{C}(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2) = \\mathbf{0}_c\\) \\(\\mathbf{C}\\) \\(c \\times t\\) matrix rank \\(c\\) \\(c \\le t\\)test statistic formThe test statistic form\\[\nT^2 = \\frac{n_1 n_2}{n_1 + n_2} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)' \\mathbf{C}'(\\mathbf{CSC}')^{-1}\\mathbf{C} (\\mathbf{\\bar{y}}_1 - \\mathbf{\\bar{y}}_2)\n\\]\\(\\mathbf{S}\\) pooled covariance estimate. ,\\[\nF = \\frac{n_1 + n_2 - c -1}{(n_1 + n_2-2)c} T^2 \\sim f_{(c, n_1 + n_2 - c-1)}\n\\]\\(H_0\\) trueIf null hypothesis \\(H_0: \\mu_1 = \\mu_2\\) rejected. weaker hypothesis profiles two groups parallel.\\[\n\\mu_{11} - \\mu_{21} = \\mu_{12} - \\mu_{22} \\\\\n\\vdots \\\\\n\\mu_{1t-1} - \\mu_{2t-1} = \\mu_{1t} - \\mu_{2t}\n\\]null hypothesis matrix term \\(H_0: \\mathbf{C}(\\mu_1 - \\mu_2) = \\mathbf{0}_c\\) , \\(c = t - 1\\) \\[\n\\mathbf{C} = \n\\left(\n\\begin{array}\n{ccccc}\n1 & -1 & 0 & \\ldots & 0 \\\\\n0 & 1 & -1 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\ldots & -1 \n\\end{array}\n\\right)_{(t-1) \\times t}\n\\]","code":"\n# One-sample Hotelling's T^2 test\n#  Create data frame\nplants <- data.frame(\n    y1 = c(2.11, 2.36, 2.13, 2.78, 2.17),\n    y2 = c(10.1, 35.0, 2.0, 6.0, 2.0),\n    y3 = c(3.4, 4.1, 1.9, 3.8, 1.7)\n)\n\n# Center the data with the hypothesized means and make a matrix\nplants_ctr <- plants %>%\n    transmute(y1_ctr = y1 - 2.85,\n              y2_ctr = y2 - 15.0,\n              y3_ctr = y3 - 6.0) %>%\n    as.matrix()\n\n# Use anova.mlm to calculate Wilks' lambda\nonesamp_fit <- anova(lm(plants_ctr ~ 1), test = \"Wilks\")\nonesamp_fit # can't reject the null of hypothesized vector of means## Analysis of Variance Table\n## \n##             Df    Wilks approx F num Df den Df  Pr(>F)  \n## (Intercept)  1 0.054219   11.629      3      2 0.08022 .\n## Residuals    4                                          \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# Paired-Sample Hotelling's T^2 test\nlibrary(ICSNP)\n\n#  Create data frame\nwaste <- data.frame(\n    case = 1:11,\n    com_y1 = c(6, 6, 18, 8, 11, 34, 28, 71, 43, 33, 20),\n    com_y2 = c(27, 23, 64, 44, 30, 75, 26, 124, 54, 30, 14),\n    state_y1 = c(25, 28, 36, 35, 15, 44, 42, 54, 34, 29, 39),\n    state_y2 = c(15, 13, 22, 29, 31, 64, 30, 64, 56, 20, 21)\n)\n\n# Calculate the difference between commercial and state labs\nwaste_diff <- waste %>%\n    transmute(y1_diff = com_y1 - state_y1,\n              y2_diff = com_y2 - state_y2)\n# Run the test\npaired_fit <- HotellingsT2(waste_diff)\npaired_fit # value T.2 in the output corresponds to the approximate F-value in the output from anova.mlm## \n##  Hotelling's one sample T2-test\n## \n## data:  waste_diff\n## T.2 = 6.1377, df1 = 2, df2 = 9, p-value = 0.02083\n## alternative hypothesis: true location is not equal to c(0,0)\n# reject the null that the two labs' measurements are equal\n# Independent-Sample Hotelling's T^2 test with Bartlett's test\n\n# Read in data\nsteel <- read.table(\"images/steel.dat\")\nnames(steel) <- c(\"Temp\", \"Yield\", \"Strength\")\nstr(steel)## 'data.frame':    12 obs. of  3 variables:\n##  $ Temp    : int  1 1 1 1 1 2 2 2 2 2 ...\n##  $ Yield   : int  33 36 35 38 40 35 36 38 39 41 ...\n##  $ Strength: int  60 61 64 63 65 57 59 59 61 63 ...\n# Plot the data\nggplot(steel, aes(x = Yield, y = Strength)) +\n    geom_text(aes(label = Temp), size = 5) +\n    geom_segment(aes(\n        x = 33,\n        y = 57.5,\n        xend = 42,\n        yend = 65\n    ), col = \"red\")\n# Bartlett's test for equality of covariance matrices\n# same thing as Box's M test in the multivariate setting\nbart_test <- boxM(steel[, -1], steel$Temp)\nbart_test # fail to reject the null of equal covariances ## \n##  Box's M-test for Homogeneity of Covariance Matrices\n## \n## data:  steel[, -1]\n## Chi-Sq (approx.) = 0.38077, df = 3, p-value = 0.9442\n# anova.mlm\ntwosamp_fit <-\n    anova(lm(cbind(Yield, Strength) ~ factor(Temp), data = steel), test = \"Wilks\")\ntwosamp_fit## Analysis of Variance Table\n## \n##              Df    Wilks approx F num Df den Df    Pr(>F)    \n## (Intercept)   1 0.001177   3818.1      2      9 6.589e-14 ***\n## factor(Temp)  1 0.294883     10.8      2      9  0.004106 ** \n## Residuals    10                                              \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# ICSNP package\ntwosamp_fit2 <-\n    HotellingsT2(cbind(steel$Yield, steel$Strength) ~ factor(steel$Temp))\ntwosamp_fit2## \n##  Hotelling's two sample T2-test\n## \n## data:  cbind(steel$Yield, steel$Strength) by factor(steel$Temp)\n## T.2 = 10.76, df1 = 2, df2 = 9, p-value = 0.004106\n## alternative hypothesis: true location difference is not equal to c(0,0)\n# reject null. Hence, there is a difference in the means of the bivariate normal distributions "},{"path":"multivariate-methods.html","id":"manova","chapter":"17 Multivariate Methods","heading":"17.1 MANOVA","text":"Multivariate Analysis VarianceOne-way MANOVACompare treatment means h different populationsPopulation 1: \\(\\mathbf{y}_{11}, \\mathbf{y}_{12}, \\dots, \\mathbf{y}_{1n_1} \\sim idd N_p (\\mathbf{\\mu}_1, \\mathbf{\\Sigma})\\)\\(\\vdots\\)Population h: \\(\\mathbf{y}_{h1}, \\mathbf{y}_{h2}, \\dots, \\mathbf{y}_{hn_h} \\sim idd N_p (\\mathbf{\\mu}_h, \\mathbf{\\Sigma})\\)AssumptionsIndependent random samples \\(h\\) different populationsCommon covariance matricesEach population multivariate normalCalculate summary statistics \\(\\mathbf{\\bar{y}}_i, \\mathbf{S}\\) pooled estimate covariance matrix \\(\\mathbf{S}\\)Similar univariate one-way ANVOA, can use effects model formulation \\(\\mathbf{\\mu}_i = \\mathbf{\\mu} + \\mathbf{\\tau}_i\\), \\(\\mathbf{\\mu}_i\\) population mean population \\(\\mathbf{\\mu}_i\\) population mean population \\(\\mathbf{\\mu}\\) overall mean effect\\(\\mathbf{\\mu}\\) overall mean effect\\(\\mathbf{\\tau}_i\\) treatment effect -th treatment.\\(\\mathbf{\\tau}_i\\) treatment effect -th treatment.one-way model: \\(\\mathbf{y}_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\) \\(= 1,..,h; j = 1,..., n_i\\) \\(\\epsilon_{ij} \\sim N_p(\\mathbf{0, \\Sigma})\\)However, model -parameterized (.e., infinite number ways define \\(\\mathbf{\\mu}\\) \\(\\mathbf{\\tau}_i\\)’s add \\(\\mu_i\\). Thus can constrain \\[\n\\sum_{=1}^h n_i \\tau_i = 0 \n\\]\\[\n\\mathbf{\\tau}_h = 0\n\\]observational equivalent effects model \\[\n\\begin{aligned}\n\\mathbf{y}_{ij} &= \\mathbf{\\bar{y}} + (\\mathbf{\\bar{y}}_i - \\mathbf{\\bar{y}}) + (\\mathbf{y}_{ij} - \\mathbf{\\bar{y}}_i) \\\\\n&= \\text{overall sample mean} + \\text{treatement effect} + \\text{residual} \\text{ (univariate ANOVA)}\n\\end{aligned} \n\\]manipulation\\[\n\\sum_{= 1}^h \\sum_{j = 1}^{n_i} (\\mathbf{\\bar{y}}_{ij} - \\mathbf{\\bar{y}})(\\mathbf{\\bar{y}}_{ij} - \\mathbf{\\bar{y}})' = \\sum_{= 1}^h n_i (\\mathbf{\\bar{y}}_i - \\mathbf{\\bar{y}})(\\mathbf{\\bar{y}}_i - \\mathbf{\\bar{y}})' + \\sum_{=1}^h \\sum_{j = 1}^{n_i} (\\mathbf{\\bar{y}}_{ij} - \\mathbf{\\bar{y}})(\\mathbf{\\bar{y}}_{ij} - \\mathbf{\\bar{y}}_i)'\n\\]LHS = Total corrected sums squares cross products (SSCP) matrixRHS =1st term = Treatment (subjects) sum squares cross product matrix (denoted H;B)1st term = Treatment (subjects) sum squares cross product matrix (denoted H;B)2nd term = residual (within subject) SSCP matrix denoted (E;W)2nd term = residual (within subject) SSCP matrix denoted (E;W)Note:\\[\n\\mathbf{E} = (n_1 - 1)\\mathbf{S}_1  + ... + (n_h -1) \\mathbf{S}_h = (n-h) \\mathbf{S}\n\\]MANOVA tableMONOVA table\\[\nH_0: \\tau_1 = \\tau_2 = \\dots = \\tau_h = \\mathbf{0}\n\\]consider relative “sizes” \\(\\mathbf{E}\\) \\(\\mathbf{H+E}\\)Wilk’s LambdaDefine Wilk’s Lambda\\[\n\\Lambda^* = \\frac{|\\mathbf{E}|}{|\\mathbf{H+E}|}\n\\]Properties:Wilk’s Lambda equivalent F-statistic univariate caseWilk’s Lambda equivalent F-statistic univariate caseThe exact distribution \\(\\Lambda^*\\) can determined especial cases.exact distribution \\(\\Lambda^*\\) can determined especial cases.large sample sizes, reject \\(H_0\\) ifFor large sample sizes, reject \\(H_0\\) \\[\n-(\\sum_{=1}^h n_i - 1 - \\frac{p+h}{2}) \\log(\\Lambda^*) > \\chi^2_{(1-\\alpha, p(h-1))}\n\\]","code":""},{"path":"multivariate-methods.html","id":"testing-general-hypotheses","chapter":"17 Multivariate Methods","heading":"17.1.1 Testing General Hypotheses","text":"\\(h\\) different treatments\\(h\\) different treatmentswith -th treatmentwith -th treatmentapplied \\(n_i\\) subjects thatapplied \\(n_i\\) subjects thatare observed \\(p\\) repeated measures.observed \\(p\\) repeated measures.Consider \\(p\\) dimensional obs random sample \\(h\\) different treatment populations.\\[\n\\mathbf{y}_{ij} = \\mathbf{\\mu} + \\mathbf{\\tau}_i + \\mathbf{\\epsilon}_{ij}\n\\]\\(= 1,..,h\\) \\(j = 1,..,n_i\\)Equivalently,\\[\n\\mathbf{Y} = \\mathbf{XB} + \\mathbf{\\epsilon}\n\\]\\(n = \\sum_{= 1}^h n_i\\) restriction \\(\\mathbf{\\tau}_h = 0\\)\\[\n\\mathbf{Y}_{(n \\times p)} = \n\\left[\n\\begin{array}\n{c}\n\\mathbf{y}_{11}' \\\\\n\\vdots \\\\\n\\mathbf{y}_{1n_1}' \\\\\n\\vdots \\\\\n\\mathbf{y}_{hn_h}'\n\\end{array}\n\\right],\n\\mathbf{B}_{(h \\times p)} = \n\\left[\n\\begin{array}\n{c}\n\\mathbf{\\mu}' \\\\\n\\mathbf{\\tau}_1' \\\\\n\\vdots \\\\\n\\mathbf{\\tau}_{h-1}'\n\\end{array}\n\\right],\n\\mathbf{\\epsilon}_{(n \\times p)} = \n\\left[\n\\begin{array}\n{c}\n\\epsilon_{11}' \\\\\n\\vdots \\\\\n\\epsilon_{1n_1}' \\\\\n\\vdots \\\\\n\\epsilon_{hn_h}'\n\\end{array}\n\\right]\n\\]\\[\n\\mathbf{X}_{(n \\times h)} = \n\\left[\n\\begin{array}\n{ccccc}\n1 & 1 & 0 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n1 & 1 & 0 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ldots & \\vdots \\\\\n1 & 0 & 0 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots \\\\\n1 & 0 & 0 & \\ldots & 0 \n\\end{array}\n\\right]\n\\]Estimation\\[\n\\mathbf{\\hat{B}} = (\\mathbf{X'X})^{-1} \\mathbf{X'Y}\n\\]Rows \\(\\mathbf{Y}\\) independent (.e., \\(var(\\mathbf{Y}) = \\mathbf{}_n \\otimes \\mathbf{\\Sigma}\\) , \\(np \\times np\\) matrix, \\(\\otimes\\) Kronecker product).\\[\nH_0: \\mathbf{LBM} = 0 \\\\\nH_a: \\mathbf{LBM} \\neq 0\n\\]\\(\\mathbf{L}\\) \\(g \\times h\\) matrix full row rank (\\(g \\le h\\)) = comparisons across groups\\(\\mathbf{L}\\) \\(g \\times h\\) matrix full row rank (\\(g \\le h\\)) = comparisons across groups\\(\\mathbf{M}\\) \\(p \\times u\\) matrix full column rank (\\(u \\le p\\)) = comparisons across traits\\(\\mathbf{M}\\) \\(p \\times u\\) matrix full column rank (\\(u \\le p\\)) = comparisons across traitsThe general treatment corrected sums squares cross product \\[\n\\mathbf{H} = \\mathbf{M'Y'X(X'X)^{-1}L'[L(X'X)^{-1}L']^{-1}L(X'X)^{-1}X'YM}\n\\]null hypothesis \\(H_0: \\mathbf{LBM} = \\mathbf{D}\\)\\[\n\\mathbf{H} = (\\mathbf{\\hat{LBM}} - \\mathbf{D})'[\\mathbf{X(X'X)^{-1}L}]^{-1}(\\mathbf{\\hat{LBM}} - \\mathbf{D})\n\\]general matrix residual sums squares cross product\\[\n\\mathbf{E} = \\mathbf{M'Y'[-X(X'X)^{-1}X']YM} = \\mathbf{M'[Y'Y - \\hat{B}'(X'X)^{-1}\\hat{B}]M}\n\\]can compute following statistic eigenvalues \\(\\mathbf{}^{-1}\\)Wilk’s Criterion: \\(\\Lambda^* = \\frac{|\\mathbf{E}|}{|\\mathbf{H} + \\mathbf{E}|}\\) . df depend rank \\(\\mathbf{L}, \\mathbf{M}, \\mathbf{X}\\)Wilk’s Criterion: \\(\\Lambda^* = \\frac{|\\mathbf{E}|}{|\\mathbf{H} + \\mathbf{E}|}\\) . df depend rank \\(\\mathbf{L}, \\mathbf{M}, \\mathbf{X}\\)Lawley-Hotelling Trace: \\(U = tr(\\mathbf{}^{-1})\\)Lawley-Hotelling Trace: \\(U = tr(\\mathbf{}^{-1})\\)Pillai Trace: \\(V = tr(\\mathbf{H}(\\mathbf{H}+ \\mathbf{E}^{-1})\\)Pillai Trace: \\(V = tr(\\mathbf{H}(\\mathbf{H}+ \\mathbf{E}^{-1})\\)Roy’s Maximum Root: largest eigenvalue \\(\\mathbf{}^{-1}\\)Roy’s Maximum Root: largest eigenvalue \\(\\mathbf{}^{-1}\\)\\(H_0\\) true n large, \\(-(n-1- \\frac{p+h}{2})\\ln \\Lambda^* \\sim \\chi^2_{p(h-1)}\\). special values p h can give exact F-dist \\(H_0\\)reject null hypothesis difference means treatmentsthere significant difference means control bww9 drugthere significant difference means ax23 drug treatment rest treatments","code":"\n# One-way MANOVA\n\nlibrary(car)\nlibrary(emmeans)## Warning: package 'emmeans' was built under R version 4.0.5\nlibrary(profileR)## Warning: package 'profileR' was built under R version 4.0.5## Warning: package 'lavaan' was built under R version 4.0.5\nlibrary(tidyverse)\n\n## Read in the data\ngpagmat <- read.table(\"images/gpagmat.dat\")\n\n## Change the variable names\nnames(gpagmat) <- c(\"y1\", \"y2\", \"admit\")\n\n## Check the structure\nstr(gpagmat)## 'data.frame':    85 obs. of  3 variables:\n##  $ y1   : num  2.96 3.14 3.22 3.29 3.69 3.46 3.03 3.19 3.63 3.59 ...\n##  $ y2   : int  596 473 482 527 505 693 626 663 447 588 ...\n##  $ admit: int  1 1 1 1 1 1 1 1 1 1 ...\n## Plot the data\ngg <- ggplot(gpagmat, aes(x = y1, y = y2)) +\n    geom_text(aes(label = admit, col = as.character(admit))) +\n    scale_color_discrete(name = \"Admission\",\n                         labels = c(\"Admit\", \"Do not admit\", \"Borderline\")) +\n    scale_x_continuous(name = \"GPA\") +\n    scale_y_continuous(name = \"GMAT\")\n\n## Fit one-way MANOVA\noneway_fit <- manova(cbind(y1, y2) ~ admit, data = gpagmat)\nsummary(oneway_fit, test = \"Wilks\")##           Df  Wilks approx F num Df den Df    Pr(>F)    \n## admit      1 0.6126   25.927      2     82 1.881e-09 ***\n## Residuals 83                                            \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# reject the null of equal multivariate mean vectors between the three admmission groups\n# Repeated Measures MANOVA\n\n\n## Create data frame\nstress <- data.frame(\n    subject = 1:8,\n    begin = c(3, 2, 5, 6, 1, 5, 1, 5),\n    middle = c(3, 4, 3, 7, 4, 7, 1, 2),\n    final = c(6, 7, 4, 7, 6, 7, 3, 5)\n)\n\n# If independent = time with 3 levels -> univariate ANOVA (require sphericity assumption (i.e., the variances for all differences are equal))\n# If each level of indepednet time as a separate variable -> MANOVA (does not require sphericity assumption)\n\n\n\n## MANOVA\nstress_mod <- lm(cbind(begin, middle, final) ~ 1, data = stress)\nidata <-\n    data.frame(time = factor(\n        c(\"begin\", \"middle\", \"final\"),\n        levels = c(\"begin\", \"middle\", \"final\")\n    ))\nrepeat_fit <-\n    Anova(\n        stress_mod,\n        idata = idata,\n        idesign = ~ time,\n        icontrasts = \"contr.poly\"\n    )\nsummary(repeat_fit) # can't reject the null hypothesis of sphericity, hence univariate ANOVA is also appropriate.## \n## Type III Repeated Measures MANOVA Tests:\n## \n## ------------------------------------------\n##  \n## Term: (Intercept) \n## \n##  Response transformation matrix:\n##        (Intercept)\n## begin            1\n## middle           1\n## final            1\n## \n## Sum of squares and products for the hypothesis:\n##             (Intercept)\n## (Intercept)        1352\n## \n## Multivariate Tests: (Intercept)\n##                  Df test stat approx F num Df den Df     Pr(>F)    \n## Pillai            1  0.896552 60.66667      1      7 0.00010808 ***\n## Wilks             1  0.103448 60.66667      1      7 0.00010808 ***\n## Hotelling-Lawley  1  8.666667 60.66667      1      7 0.00010808 ***\n## Roy               1  8.666667 60.66667      1      7 0.00010808 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## ------------------------------------------\n##  \n## Term: time \n## \n##  Response transformation matrix:\n##               time.L     time.Q\n## begin  -7.071068e-01  0.4082483\n## middle -7.850462e-17 -0.8164966\n## final   7.071068e-01  0.4082483\n## \n## Sum of squares and products for the hypothesis:\n##           time.L   time.Q\n## time.L 18.062500 6.747781\n## time.Q  6.747781 2.520833\n## \n## Multivariate Tests: time\n##                  Df test stat approx F num Df den Df   Pr(>F)  \n## Pillai            1 0.7080717 7.276498      2      6 0.024879 *\n## Wilks             1 0.2919283 7.276498      2      6 0.024879 *\n## Hotelling-Lawley  1 2.4254992 7.276498      2      6 0.024879 *\n## Roy               1 2.4254992 7.276498      2      6 0.024879 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Univariate Type III Repeated-Measures ANOVA Assuming Sphericity\n## \n##             Sum Sq num Df Error SS den Df F value    Pr(>F)    \n## (Intercept) 450.67      1    52.00      7 60.6667 0.0001081 ***\n## time         20.58      2    24.75     14  5.8215 0.0144578 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## \n## Mauchly Tests for Sphericity\n## \n##      Test statistic p-value\n## time         0.7085 0.35565\n## \n## \n## Greenhouse-Geisser and Huynh-Feldt Corrections\n##  for Departure from Sphericity\n## \n##       GG eps Pr(>F[GG])  \n## time 0.77429    0.02439 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##         HF eps Pr(>F[HF])\n## time 0.9528433 0.01611634\n# we also see linear significant time effect, but no quadratic time effect\n\n\n## Polynomial contrasts\n# What is the reference for the marginal means?\nref_grid(stress_mod, mult.name = \"time\")## 'emmGrid' object with variables:\n##     1 = 1\n##     time = multivariate response levels: begin, middle, final\n# marginal means for the levels of time\ncontr_means <- emmeans(stress_mod, ~ time, mult.name = \"time\")\ncontrast(contr_means, method = \"poly\")##  contrast  estimate    SE df t.ratio p.value\n##  linear        2.12 0.766  7   2.773  0.0276\n##  quadratic     1.38 0.944  7   1.457  0.1885\n# MANOVA\n\n## Read in Data\nheart <- read.table(\"images/heart.dat\")\nnames(heart) <- c(\"drug\", \"y1\", \"y2\", \"y3\", \"y4\")\n## Create a subject ID nested within drug\nheart <- heart %>%\n    group_by(drug) %>%\n    mutate(subject = row_number()) %>%\n    ungroup()\nstr(heart)## tibble [24 x 6] (S3: tbl_df/tbl/data.frame)\n##  $ drug   : chr [1:24] \"ax23\" \"ax23\" \"ax23\" \"ax23\" ...\n##  $ y1     : int [1:24] 72 78 71 72 66 74 62 69 85 82 ...\n##  $ y2     : int [1:24] 86 83 82 83 79 83 73 75 86 86 ...\n##  $ y3     : int [1:24] 81 88 81 83 77 84 78 76 83 80 ...\n##  $ y4     : int [1:24] 77 82 75 69 66 77 70 70 80 84 ...\n##  $ subject: int [1:24] 1 2 3 4 5 6 7 8 1 2 ...\n## Create means summary for profile plot, pivot longer for plotting with ggplot\nheart_means <- heart %>%\n    group_by(drug) %>%\n    summarize_at(vars(starts_with(\"y\")), mean) %>%\n    ungroup() %>%\n    pivot_longer(-drug, names_to = \"time\", values_to = \"mean\") %>%\n    mutate(time = as.numeric(as.factor(time)))\ngg_profile <- ggplot(heart_means, aes(x = time, y = mean)) +\n    geom_line(aes(col = drug)) +\n    geom_point(aes(col = drug)) +\n    ggtitle(\"Profile Plot\") +\n    scale_y_continuous(name = \"Response\") +\n    scale_x_discrete(name = \"Time\")\ngg_profile\n## Fit model\nheart_mod <- lm(cbind(y1, y2, y3, y4) ~ drug, data = heart)\nman_fit <- car::Anova(heart_mod)\nsummary(man_fit)## \n## Type II MANOVA Tests:\n## \n## Sum of squares and products for error:\n##        y1      y2      y3     y4\n## y1 641.00 601.750 535.250 426.00\n## y2 601.75 823.875 615.500 534.25\n## y3 535.25 615.500 655.875 555.25\n## y4 426.00 534.250 555.250 674.50\n## \n## ------------------------------------------\n##  \n## Term: drug \n## \n## Sum of squares and products for the hypothesis:\n##        y1       y2       y3    y4\n## y1 567.00 335.2500  42.7500 387.0\n## y2 335.25 569.0833 404.5417 367.5\n## y3  42.75 404.5417 391.0833 171.0\n## y4 387.00 367.5000 171.0000 316.0\n## \n## Multivariate Tests: drug\n##                  Df test stat  approx F num Df den Df     Pr(>F)    \n## Pillai            2  1.283456  8.508082      8     38 1.5010e-06 ***\n## Wilks             2  0.079007 11.509581      8     36 6.3081e-08 ***\n## Hotelling-Lawley  2  7.069384 15.022441      8     34 3.9048e-09 ***\n## Roy               2  6.346509 30.145916      4     19 5.4493e-08 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# reject the null hypothesis of no difference in means between treatments\n## Contrasts\nheart$drug <- factor(heart$drug)\nL <- matrix(c(0, 2,\n              1, -1,-1, -1), nrow = 3, byrow = T)\ncolnames(L) <- c(\"bww9:ctrl\", \"ax23:rest\")\nrownames(L) <- unique(heart$drug)\ncontrasts(heart$drug) <- L\ncontrasts(heart$drug)##      bww9:ctrl ax23:rest\n## ax23         0         2\n## bww9         1        -1\n## ctrl        -1        -1\n# do not set contrast L if you do further analysis (e.g., Anova, lm)\n# do M matrix instead\n\nM <- matrix(c(1, -1, 0, 0,\n              0, 1, -1, 0,\n              0, 0, 1, -1), nrow = 4)\n## update model to test contrasts\nheart_mod2 <- update(heart_mod)\ncoef(heart_mod2)##                  y1         y2        y3    y4\n## (Intercept)   75.00 78.9583333 77.041667 74.75\n## drugbww9:ctrl  4.50  5.8125000  3.562500  4.25\n## drugax23:rest -2.25  0.7708333  1.979167 -0.75\n# Hypothesis test for bww9 vs control after transformation M\n# same as linearHypothesis(heart_mod, hypothesis.matrix = c(0,1,-1), P = M)\nbww9vctrl <-\n    car::linearHypothesis(heart_mod2,\n                     hypothesis.matrix = c(0, 1, 0),\n                     P = M)\nbww9vctrl## \n##  Response transformation matrix:\n##    [,1] [,2] [,3]\n## y1    1    0    0\n## y2   -1    1    0\n## y3    0   -1    1\n## y4    0    0   -1\n## \n## Sum of squares and products for the hypothesis:\n##          [,1]   [,2]     [,3]\n## [1,]  27.5625 -47.25  14.4375\n## [2,] -47.2500  81.00 -24.7500\n## [3,]  14.4375 -24.75   7.5625\n## \n## Sum of squares and products for error:\n##          [,1]     [,2]    [,3]\n## [1,]  261.375 -141.875  28.000\n## [2,] -141.875  248.750 -19.375\n## [3,]   28.000  -19.375 219.875\n## \n## Multivariate Tests: \n##                  Df test stat approx F num Df den Df Pr(>F)\n## Pillai            1 0.2564306 2.184141      3     19 0.1233\n## Wilks             1 0.7435694 2.184141      3     19 0.1233\n## Hotelling-Lawley  1 0.3448644 2.184141      3     19 0.1233\n## Roy               1 0.3448644 2.184141      3     19 0.1233\nbww9vctrl <-\n    car::linearHypothesis(heart_mod,\n                     hypothesis.matrix = c(0, 1, -1),\n                     P = M)\nbww9vctrl## \n##  Response transformation matrix:\n##    [,1] [,2] [,3]\n## y1    1    0    0\n## y2   -1    1    0\n## y3    0   -1    1\n## y4    0    0   -1\n## \n## Sum of squares and products for the hypothesis:\n##          [,1]   [,2]     [,3]\n## [1,]  27.5625 -47.25  14.4375\n## [2,] -47.2500  81.00 -24.7500\n## [3,]  14.4375 -24.75   7.5625\n## \n## Sum of squares and products for error:\n##          [,1]     [,2]    [,3]\n## [1,]  261.375 -141.875  28.000\n## [2,] -141.875  248.750 -19.375\n## [3,]   28.000  -19.375 219.875\n## \n## Multivariate Tests: \n##                  Df test stat approx F num Df den Df Pr(>F)\n## Pillai            1 0.2564306 2.184141      3     19 0.1233\n## Wilks             1 0.7435694 2.184141      3     19 0.1233\n## Hotelling-Lawley  1 0.3448644 2.184141      3     19 0.1233\n## Roy               1 0.3448644 2.184141      3     19 0.1233\n# Hypothesis test for ax23 vs rest after transformation M\naxx23vrest <-\n    car::linearHypothesis(heart_mod2,\n                     hypothesis.matrix = c(0, 0, 1),\n                     P = M)\naxx23vrest## \n##  Response transformation matrix:\n##    [,1] [,2] [,3]\n## y1    1    0    0\n## y2   -1    1    0\n## y3    0   -1    1\n## y4    0    0   -1\n## \n## Sum of squares and products for the hypothesis:\n##           [,1]       [,2]      [,3]\n## [1,]  438.0208  175.20833 -395.7292\n## [2,]  175.2083   70.08333 -158.2917\n## [3,] -395.7292 -158.29167  357.5208\n## \n## Sum of squares and products for error:\n##          [,1]     [,2]    [,3]\n## [1,]  261.375 -141.875  28.000\n## [2,] -141.875  248.750 -19.375\n## [3,]   28.000  -19.375 219.875\n## \n## Multivariate Tests: \n##                  Df test stat approx F num Df den Df     Pr(>F)    \n## Pillai            1  0.855364 37.45483      3     19 3.5484e-08 ***\n## Wilks             1  0.144636 37.45483      3     19 3.5484e-08 ***\n## Hotelling-Lawley  1  5.913921 37.45483      3     19 3.5484e-08 ***\n## Roy               1  5.913921 37.45483      3     19 3.5484e-08 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\naxx23vrest <-\n    car::linearHypothesis(heart_mod,\n                     hypothesis.matrix = c(2, -1, 1),\n                     P = M)\naxx23vrest## \n##  Response transformation matrix:\n##    [,1] [,2] [,3]\n## y1    1    0    0\n## y2   -1    1    0\n## y3    0   -1    1\n## y4    0    0   -1\n## \n## Sum of squares and products for the hypothesis:\n##           [,1]       [,2]      [,3]\n## [1,]  402.5208  127.41667 -390.9375\n## [2,]  127.4167   40.33333 -123.7500\n## [3,] -390.9375 -123.75000  379.6875\n## \n## Sum of squares and products for error:\n##          [,1]     [,2]    [,3]\n## [1,]  261.375 -141.875  28.000\n## [2,] -141.875  248.750 -19.375\n## [3,]   28.000  -19.375 219.875\n## \n## Multivariate Tests: \n##                  Df test stat approx F num Df den Df     Pr(>F)    \n## Pillai            1  0.842450 33.86563      3     19 7.9422e-08 ***\n## Wilks             1  0.157550 33.86563      3     19 7.9422e-08 ***\n## Hotelling-Lawley  1  5.347205 33.86563      3     19 7.9422e-08 ***\n## Roy               1  5.347205 33.86563      3     19 7.9422e-08 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"multivariate-methods.html","id":"profile-analysis","chapter":"17 Multivariate Methods","heading":"17.1.2 Profile Analysis","text":"Examine similarities treatment effects (subjects), useful longitudinal analysis. Null treatments average effect.\\[\nH_0: \\mu_1 = \\mu_2 = \\dots = \\mu_h\n\\]Equivalently,\\[\nH_0: \\tau_1 = \\tau_2 = \\dots = \\tau_h\n\\]exact nature similarities differences treatments can examined analysis.Sequential steps profile analysis:profiles parallel? (.e., interaction treatment time)profiles coincidental? (.e., profiles identical?)profiles horizontal? (.e., differences time points?)reject null hypothesis profiles parallel, can testAre differences among groups within subset total time points?differences among groups within subset total time points?differences among time points particular group (groups)?differences among time points particular group (groups)?differences within subset total time points particular group (groups)?differences within subset total time points particular group (groups)?Example4 times (p = 4)4 times (p = 4)3 treatments (h=3)3 treatments (h=3)","code":""},{"path":"multivariate-methods.html","id":"parallel-profile","chapter":"17 Multivariate Methods","heading":"17.1.2.1 Parallel Profile","text":"profiles population identical expect mean shift?\\[\nH_0: \\mu_{11} - \\mu_{21} - \\mu_{12} - \\mu_{22} = \\dots = \\mu_{1t} - \\mu_{2t} \\\\\n\\mu_{11} - \\mu_{31} - \\mu_{12} - \\mu_{32} = \\dots = \\mu_{1t} - \\mu_{3t} \\\\\n\\dots\n\\]\\(h-1\\) equationsEquivalently,\\[\nH_0: \\mathbf{LBM = 0}\n\\]\\[\n\\mathbf{LBM} =\n\\left[\n\\begin{array}\n{ccc}\n1 & -1 & 0 \\\\\n1 & 0 & -1\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{ccc}\n\\mu_{11} & \\dots & \\mu_{14} \\\\\n\\mu_{21} & \\dots & \\mu_{24} \\\\\n\\mu_{31} & \\dots & \\mu_{34} \n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{ccc}\n1 & 1 & 1 \\\\\n-1 & 0 & 0 \\\\\n0 & -1 & 0 \\\\\n0 & 0 & -1\n\\end{array}\n\\right]\n= \n\\mathbf{0}\n\\]cell means parameterization \\(\\mathbf{B}\\)multiplication first 2 matrices \\(\\mathbf{LB}\\) \\[\n\\left[\n\\begin{array}\n{cccc}\n\\mu_{11} - \\mu_{21} & \\mu_{12} - \\mu_{22} & \\mu_{13} - \\mu_{23} & \\mu_{14} - \\mu_{24}\\\\\n\\mu_{11} - \\mu_{31} & \\mu_{12} - \\mu_{32} & \\mu_{13} - \\mu_{33} & \\mu_{14} - \\mu_{34} \n\\end{array}\n\\right]\n\\]differences treatment means timeMultiplying \\(\\mathbf{M}\\), get comparison across time\\[\n\\left[\n\\begin{array}\n{ccc}\n(\\mu_{11} - \\mu_{21}) - (\\mu_{12} - \\mu_{22}) & (\\mu_{11} - \\mu_{21}) -(\\mu_{13} - \\mu_{23}) & (\\mu_{11} - \\mu_{21}) - (\\mu_{14} - \\mu_{24}) \\\\\n(\\mu_{11} - \\mu_{31}) - (\\mu_{12} - \\mu_{32}) & (\\mu_{11} - \\mu_{31}) - (\\mu_{13} - \\mu_{33}) & (\\mu_{11} - \\mu_{31}) -(\\mu_{14} - \\mu_{34}) \n\\end{array}\n\\right]\n\\]Alternatively, can also use effects parameterization\\[\n\\mathbf{LBM} =\n\\left[\n\\begin{array}\n{cccc}\n0 & 1 & -1 & 0 \\\\\n0 & 1 & 0 & -1 \n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{c}\n\\mu' \\\\\n\\tau'_1 \\\\\n\\tau_2' \\\\\n\\tau_3'\n\\end{array}\n\\right]\n\\left[\n\\begin{array}\n{ccc}\n1 & 1 & 1 \\\\\n-1 & 0 & 0 \\\\\n0 & -1 & 0 \\\\\n0 & 0 & -1\n\\end{array}\n\\right]\n= \\mathbf{0}\n\\]parameterizations, \\(rank(\\mathbf{L}) = h-1\\) \\(rank(\\mathbf{M}) = p-1\\)also choose \\(\\mathbf{L}\\) \\(\\mathbf{M}\\) forms\\[\n\\mathbf{L} = \\left[\n\\begin{array}\n{cccc}\n0 & 1 & 0 & -1 \\\\\n0 & 0 & 1 & -1 \n\\end{array}\n\\right]\n\\]\\[\n\\mathbf{M} = \\left[\n\\begin{array}\n{ccc}\n1 & 0 & 0 \\\\\n-1 & 1 & 0 \\\\\n0 & -1 & 1 \\\\\n0 & 0 & -1\n\\end{array}\n\\right]\n\\]still obtain result.","code":""},{"path":"multivariate-methods.html","id":"coincidental-profiles","chapter":"17 Multivariate Methods","heading":"17.1.2.2 Coincidental Profiles","text":"evidence profiles parallel (.e., fail reject parallel profile test), can ask whether identical?Given profiles parallel, sums components \\(\\mu_i\\) identical treatments, profiles identical.\\[\nH_0: \\mathbf{1'}_p \\mu_1 = \\mathbf{1'}_p \\mu_2 = \\dots = \\mathbf{1'}_p \\mu_h \n\\]Equivalently,\\[\nH_0: \\mathbf{LBM} = \\mathbf{0}\n\\]cell means parameterization\\[\n\\mathbf{L} = \n\\left[\n\\begin{array}\n{ccc}\n1 & 0 & -1 \\\\\n0 & 1 & -1\n\\end{array}\n\\right]\n\\]\\[\n\\mathbf{M} = \n\\left[\n\\begin{array}\n{cccc}\n1 & 1 & 1 & 1\n\\end{array}\n\\right]'\n\\]multiplication yields\\[\n\\left[\n\\begin{array}\n{c}\n(\\mu_{11} + \\mu_{12} + \\mu_{13} + \\mu_{14}) - (\\mu_{31} + \\mu_{32} + \\mu_{33} + \\mu_{34}) \\\\\n(\\mu_{21} + \\mu_{22} + \\mu_{23} + \\mu_{24}) - (\\mu_{31} + \\mu_{32} + \\mu_{33} + \\mu_{34})\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}\n{c}\n0 \\\\\n0 \n\\end{array}\n\\right]\n\\]Different choices \\(\\mathbf{L}\\) \\(\\mathbf{M}\\) can yield result","code":""},{"path":"multivariate-methods.html","id":"horizontal-profiles","chapter":"17 Multivariate Methods","heading":"17.1.2.3 Horizontal Profiles","text":"Given can’t reject null hypothesis \\(h\\) profiles , can ask whether elements common profile equal? (.e., horizontal)\\[\nH_0: \\mathbf{LBM} = \\mathbf{0}\n\\]\\[\n\\mathbf{L} = \n\\left[\n\\begin{array}\n{ccc}\n1 & 0 & 0 \n\\end{array}\n\\right]\n\\]\\[\n\\mathbf{M} = \\left[\n\\begin{array}\n{ccc}\n1 & 0 & 0 \\\\\n-1 & 1 & 0 \\\\\n0 & -1 & 1 \\\\\n0 & 0 & -1\n\\end{array}\n\\right]\n\\]hence,\\[\n\\left[\n\\begin{array}\n{ccc}\n(\\mu_{11} - \\mu_{12}) & (\\mu_{12} - \\mu_{13}) & (\\mu_{13} + \\mu_{14}) \n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}\n{ccc}\n0 & 0 & 0\n\\end{array}\n\\right]\n\\]Note:fail reject 3 hypotheses, fail reject null hypotheses difference treatments differences traits.","code":"\nprofile_fit <-\n    pbg(\n        data = as.matrix(heart[, 2:5]),\n        group = as.matrix(heart[, 1]),\n        original.names = TRUE,\n        profile.plot = FALSE\n    )\nsummary(profile_fit)## Call:\n## pbg(data = as.matrix(heart[, 2:5]), group = as.matrix(heart[, \n##     1]), original.names = TRUE, profile.plot = FALSE)\n## \n## Hypothesis Tests:\n## $`Ho: Profiles are parallel`\n##   Multivariate.Test Statistic  Approx.F num.df den.df      p.value\n## 1             Wilks 0.1102861 12.737599      6     38 7.891497e-08\n## 2            Pillai 1.0891707  7.972007      6     40 1.092397e-05\n## 3  Hotelling-Lawley 6.2587852 18.776356      6     36 9.258571e-10\n## 4               Roy 5.9550887 39.700592      3     20 1.302458e-08\n## \n## $`Ho: Profiles have equal levels`\n##             Df Sum Sq Mean Sq F value  Pr(>F)   \n## group        2  328.7  164.35   5.918 0.00915 **\n## Residuals   21  583.2   27.77                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## $`Ho: Profiles are flat`\n##          F df1 df2      p-value\n## 1 14.30928   3  19 4.096803e-05\n# reject null hypothesis of parallel profiles\n# reject the null hypothesis of coincidental profiles\n# reject the null hypothesis that the profiles are flat"},{"path":"multivariate-methods.html","id":"summary-4","chapter":"17 Multivariate Methods","heading":"17.1.3 Summary","text":"","code":""},{"path":"multivariate-methods.html","id":"principal-components","chapter":"17 Multivariate Methods","heading":"17.2 Principal Components","text":"Unsupervised learningfind important featuresreduce dimensions data set“decorrelate” multivariate vectors dependence.uses eigenvector/eigvenvalue decomposition covariance (correlation) matrices.According “spectral decomposition theorem,” \\(\\mathbf{\\Sigma}_{p \\times p}\\) s positive semi-definite, symmetric, real matrix, exists orthogonal matrix \\(\\mathbf{}\\) \\(\\mathbf{'\\Sigma } = \\Lambda\\) \\(\\Lambda\\) diagonal matrix containing eigenvalues \\(\\mathbf{\\Sigma}\\)\\[\n\\mathbf{\\Lambda} = \n\\left(\n\\begin{array}\n{cccc}\n\\lambda_1 & 0 & \\ldots & 0 \\\\\n0 & \\lambda_2 & \\ldots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\ldots & \\lambda_p\n\\end{array}\n\\right)\n\\]\\[\n\\mathbf{} =\n\\left(\n\\begin{array}\n{cccc}\n\\mathbf{}_1 & \\mathbf{}_2 & \\ldots & \\mathbf{}_p\n\\end{array}\n\\right)\n\\]-th column \\(\\mathbf{}\\) , \\(\\mathbf{}_i\\), -th \\(p \\times 1\\) eigenvector \\(\\mathbf{\\Sigma}\\) corresponds eigenvalue, \\(\\lambda_i\\) , \\(\\lambda_1 \\ge \\lambda_2 \\ge \\ldots \\ge \\lambda_p\\) . Alternatively, express matrix decomposition:\\[\n\\mathbf{\\Sigma} = \\mathbf{\\Lambda }'\n\\]\\[\n\\mathbf{\\Sigma} = \\mathbf{}\n\\left(\n\\begin{array}\n{cccc}\n\\lambda_1 & 0 & \\ldots & 0 \\\\\n0 & \\lambda_2 & \\ldots & 0 \\\\\n\\vdots & \\vdots& \\ddots & \\vdots \\\\\n0 & 0 & \\ldots & \\lambda_p\n\\end{array}\n\\right)\n\\mathbf{}'\n= \\sum_{=1}^p \\lambda_i \\mathbf{}_i \\mathbf{}_i'\n\\]outer product \\(\\mathbf{}_i \\mathbf{}_i'\\) \\(p \\times p\\) matrix rank 1.example,\\(\\mathbf{x} \\sim N_2(\\mathbf{\\mu}, \\mathbf{\\Sigma})\\)\\[\n\\mathbf{\\mu} = \n\\left(\n\\begin{array}\n{c}\n5 \\\\ \n12 \n\\end{array} \n\\right);\n\\mathbf{\\Sigma} = \n\\left(\n\\begin{array}\n{cc}\n4 & 1 \\\\\n1 & 2 \n\\end{array}\n\\right)\n\\],\\[\n\\mathbf{} = \n\\left(\n\\begin{array}\n{cc}\n0.9239 & -0.3827 \\\\\n0.3827 & 0.9239 \\\\\n\\end{array}\n\\right)\n\\]Columns \\(\\mathbf{}\\) eigenvectors decompositionUnder matrix multiplication (\\(\\mathbf{'\\Sigma }\\) \\(\\mathbf{'}\\) ), -diagonal elements equal 0Multiplying data matrix (.e., projecting data onto orthogonal axes); distriubiton resulting data (.e., “scores”) \\[\nN_2 (\\mathbf{'\\mu,'\\Sigma }) = N_2 (\\mathbf{'\\mu, \\Lambda})\n\\]Equivalently,\\[\n\\mathbf{y} = \\mathbf{'x} \\sim N\n\\left[\n\\left(\n\\begin{array}\n{c}\n9.2119 \\\\\n9.1733 \n\\end{array}\n\\right),\n\\left(\n\\begin{array}\n{cc}\n4.4144 & 0 \\\\\n0 & 1.5859 \n\\end{array}\n\\right)\n\\right]\n\\]dependence data structure, plotNotes:-th eigenvalue variance linear combination elements \\(\\mathbf{x}\\) ; \\(var(y_i) = var(\\mathbf{'_i x}) = \\lambda_i\\)-th eigenvalue variance linear combination elements \\(\\mathbf{x}\\) ; \\(var(y_i) = var(\\mathbf{'_i x}) = \\lambda_i\\)values transformed set axes (.e., \\(y_i\\)’s) called scores. orthogonal projections data onto \"new principal component axesThe values transformed set axes (.e., \\(y_i\\)’s) called scores. orthogonal projections data onto \"new principal component axesVariances \\(y_1\\) greater possible projectionVariances \\(y_1\\) greater possible projectionCovariance matrix decomposition projection onto orthogonal axes = PCA","code":"\nlibrary(MASS)## Warning: package 'MASS' was built under R version 4.0.5## \n## Attaching package: 'MASS'## The following object is masked from 'package:dplyr':\n## \n##     select\nmu = as.matrix(c(5,12))\nSigma = matrix(c(4,1,1,2),nrow = 2, byrow = T)\nsim <- mvrnorm(n = 1000, mu = mu, Sigma = Sigma)\nplot(sim[,1],sim[,2])\nA_matrix = matrix(c(0.9239,-0.3827,0.3827,0.9239),nrow = 2, byrow = T)\nt(A_matrix) %*% A_matrix##          [,1]     [,2]\n## [1,] 1.000051 0.000000\n## [2,] 0.000000 1.000051\nsim1 <- mvrnorm(n = 1000, mu = t(A_matrix) %*% mu, Sigma = t(A_matrix) %*% Sigma %*% A_matrix)\nplot(sim1[,1],sim1[,2])"},{"path":"multivariate-methods.html","id":"population-principal-components","chapter":"17 Multivariate Methods","heading":"17.2.1 Population Principal Components","text":"\\(p \\times 1\\) vectors \\(\\mathbf{x}_1, \\dots , \\mathbf{x}_n\\) iid \\(var(\\mathbf{x}_i) = \\mathbf{\\Sigma}\\)first PC linear combination \\(y_1 = \\mathbf{}_1' \\mathbf{x} = a_{11}x_1 + \\dots + a_{1p}x_p\\) \\(\\mathbf{}_1' \\mathbf{}_1 = 1\\) \\(var(y_1)\\) maximum linear combinations \\(\\mathbf{x}\\) unit lengthThe first PC linear combination \\(y_1 = \\mathbf{}_1' \\mathbf{x} = a_{11}x_1 + \\dots + a_{1p}x_p\\) \\(\\mathbf{}_1' \\mathbf{}_1 = 1\\) \\(var(y_1)\\) maximum linear combinations \\(\\mathbf{x}\\) unit lengthThe second PC linear combination \\(y_1 = \\mathbf{}_2' \\mathbf{x} = a_{21}x_1 + \\dots + a_{2p}x_p\\) \\(\\mathbf{}_2' \\mathbf{}_2 = 1\\) \\(var(y_1)\\) maximum linear combinations \\(\\mathbf{x}\\) unit length uncorrelated \\(y_1\\) (.e., \\(cov(\\mathbf{}_1' \\mathbf{x}, \\mathbf{}'_2 \\mathbf{x}) =0\\)second PC linear combination \\(y_1 = \\mathbf{}_2' \\mathbf{x} = a_{21}x_1 + \\dots + a_{2p}x_p\\) \\(\\mathbf{}_2' \\mathbf{}_2 = 1\\) \\(var(y_1)\\) maximum linear combinations \\(\\mathbf{x}\\) unit length uncorrelated \\(y_1\\) (.e., \\(cov(\\mathbf{}_1' \\mathbf{x}, \\mathbf{}'_2 \\mathbf{x}) =0\\)continues \\(y_i\\) \\(y_p\\)continues \\(y_i\\) \\(y_p\\)\\(\\mathbf{}_i\\)’s make matrix \\(\\mathbf{}\\) symmetric decomposition \\(\\mathbf{'\\Sigma } = \\mathbf{\\Lambda}\\) , \\(var(y_1) = \\lambda_1, \\dots , var(y_p) = \\lambda_p\\) total variance \\(\\mathbf{x}\\) \\[\n\\begin{aligned}\nvar(x_1) + \\dots + var(x_p) &= tr(\\Sigma) = \\lambda_1 + \\dots + \\lambda_p \\\\\n&= var(y_1) + \\dots + var(y_p) \n\\end{aligned}\n\\]Data ReductionTo reduce dimension data p (original) k dimensions without much “loss information,” can use properties population principal componentsSuppose \\(\\mathbf{\\Sigma} \\approx \\sum_{=1}^k \\lambda_i \\mathbf{}_i \\mathbf{}_i'\\) . Even thought true variance-covariance matrix rank \\(p\\) , can well approximate matrix rank k (k <p)Suppose \\(\\mathbf{\\Sigma} \\approx \\sum_{=1}^k \\lambda_i \\mathbf{}_i \\mathbf{}_i'\\) . Even thought true variance-covariance matrix rank \\(p\\) , can well approximate matrix rank k (k <p)New “traits” linear combinations measured traits. can attempt make meaningful interpretation fo combinations (orthogonality constraints).New “traits” linear combinations measured traits. can attempt make meaningful interpretation fo combinations (orthogonality constraints).proportion total variance accounted j-th principal component isThe proportion total variance accounted j-th principal component \\[\n\\frac{var(y_j)}{\\sum_{=1}^p var(y_i)} = \\frac{\\lambda_j}{\\sum_{=1}^p \\lambda_i}\n\\]proportion total variation accounted first k principal components \\(\\frac{\\sum_{=1}^k \\lambda_i}{\\sum_{=1}^p \\lambda_i}\\)proportion total variation accounted first k principal components \\(\\frac{\\sum_{=1}^k \\lambda_i}{\\sum_{=1}^p \\lambda_i}\\)example , \\(4.4144/(4+2) = .735\\) total variability can explained first principal componentAbove example , \\(4.4144/(4+2) = .735\\) total variability can explained first principal component","code":""},{"path":"multivariate-methods.html","id":"sample-principal-components","chapter":"17 Multivariate Methods","heading":"17.2.2 Sample Principal Components","text":"Since \\(\\mathbf{\\Sigma}\\) unknown, use\\[\n\\mathbf{S} = \\frac{1}{n-1}\\sum_{=1}^n (\\mathbf{x}_i - \\bar{\\mathbf{x}})(\\mathbf{x}_i - \\bar{\\mathbf{x}})'\n\\]Let \\(\\hat{\\lambda}_1 \\ge \\hat{\\lambda}_2 \\ge \\dots \\ge \\hat{\\lambda}_p \\ge 0\\) eigenvalues \\(\\mathbf{S}\\) \\(\\hat{\\mathbf{}}_1, \\hat{\\mathbf{}}_2, \\dots, \\hat{\\mathbf{}}_p\\) denote eigenvectors \\(\\mathbf{S}\\), -th sample principal component score (principal component score) \\[\n\\hat{y}_{ij} = \\sum_{k=1}^p \\hat{}_{ik}x_{kj} = \\hat{\\mathbf{}}_i'\\mathbf{x}_j\n\\]Properties Sample Principal ComponentsThe estimated variance \\(y_i = \\hat{\\mathbf{}}_i'\\mathbf{x}_j\\) \\(\\hat{\\lambda}_i\\)estimated variance \\(y_i = \\hat{\\mathbf{}}_i'\\mathbf{x}_j\\) \\(\\hat{\\lambda}_i\\)sample covariance \\(\\hat{y}_i\\) \\(\\hat{y}_{'}\\) 0 \\(\\neq '\\)sample covariance \\(\\hat{y}_i\\) \\(\\hat{y}_{'}\\) 0 \\(\\neq '\\)proportion total sample variance accounted -th sample principal component \\(\\frac{\\hat{\\lambda}_i}{\\sum_{k=1}^p \\hat{\\lambda}_k}\\)proportion total sample variance accounted -th sample principal component \\(\\frac{\\hat{\\lambda}_i}{\\sum_{k=1}^p \\hat{\\lambda}_k}\\)estimated correlation -th principal component score l-th attribute \\(\\mathbf{x}\\) isThe estimated correlation -th principal component score l-th attribute \\(\\mathbf{x}\\) \\[\nr_{x_l , \\hat{y}_i} = \\frac{\\hat{}_{il}\\sqrt{\\lambda_i}}{\\sqrt{s_{ll}}}\n\\]correlation coefficient typically used interpret components (.e., correlation high suggests l-th original trait important -th principle component). According (Johnson Wichern 1988, 433–34), \\(r_{x_l, \\hat{y}_i}\\) measures univariate contribution individual X component Y without taking account presence X’s. Hence, prefer \\(\\hat{}_{il}\\) coefficient interpret principal component.correlation coefficient typically used interpret components (.e., correlation high suggests l-th original trait important -th principle component). According (Johnson Wichern 1988, 433–34), \\(r_{x_l, \\hat{y}_i}\\) measures univariate contribution individual X component Y without taking account presence X’s. Hence, prefer \\(\\hat{}_{il}\\) coefficient interpret principal component.\\(r_{x_l, \\hat{y}_i} ; \\hat{}_{il}\\) referred “loadings”\\(r_{x_l, \\hat{y}_i} ; \\hat{}_{il}\\) referred “loadings”use k principal components, must calculate scores data vector sample\\[\n\\mathbf{y}_j = \n\\left(\n\\begin{array}\n{c}\ny_{1j} \\\\\ny_{2j} \\\\\n\\vdots \\\\\ny_{kj} \n\\end{array}\n\\right) = \n\\left(\n\\begin{array}\n{c}\n\\hat{\\mathbf{}}_1' \\mathbf{x}_j \\\\\n\\hat{\\mathbf{}}_2' \\mathbf{x}_j \\\\\n\\vdots \\\\\n\\hat{\\mathbf{}}_k' \\mathbf{x}_j\n\\end{array}\n\\right) = \n\\left(\n\\begin{array}\n{c}\n\\hat{\\mathbf{}}_1' \\\\\n\\hat{\\mathbf{}}_2' \\\\\n\\vdots \\\\\n\\hat{\\mathbf{}}_k'\n\\end{array}\n\\right) \\mathbf{x}_j\n\\]Issues:Large sample theory exists eigenvalues eigenvectors sample covariance matrices inference necessary. inference PCA, use exploratory descriptive analysis.Large sample theory exists eigenvalues eigenvectors sample covariance matrices inference necessary. inference PCA, use exploratory descriptive analysis.PC invariant changes scale (Exception: trait resecaled multiplying constant, feet inches).\nPCA based correlation matrix \\(\\mathbf{R}\\) different based covariance matrix \\(\\mathbf{\\Sigma}\\)\nPCA correlation matrix just rescaling trait unit variance\nTransform \\(\\mathbf{x}\\) \\(\\mathbf{z}\\) \\(z_{ij} = (x_{ij} - \\bar{x}_i)/\\sqrt{s_{ii}}\\) denominator affects PCA\ntransformation, \\(cov(\\mathbf{z}) = \\mathbf{R}\\)\nPCA \\(\\mathbf{R}\\) calculated way \\(\\mathbf{S}\\) (\\(\\hat{\\lambda}{}_1 + \\dots + \\hat{\\lambda}{}_p = p\\) )\nuse \\(\\mathbf{R}, \\mathbf{S}\\) depends purpose PCA.\nscale observations different, covariance matrix preferable. dramatically different, analysis can still dominated large variance traits.\n\nmany PCs use can guided \nScree Graphs: plot eigenvalues indices. Look “elbow” steep decline graph suddenly flattens ; big gaps.\nminimum Percent total variation (e.g., choose enough components 50% 90%). can used interpretations.\nKaiser’s rule: use PC eigenvalues larger 1 (applied PCA correlation matrix) - ad hoc\nCompare eigenvalue scree plot data scree plot data randomized.\n\nPC invariant changes scale (Exception: trait resecaled multiplying constant, feet inches).PCA based correlation matrix \\(\\mathbf{R}\\) different based covariance matrix \\(\\mathbf{\\Sigma}\\)PCA based correlation matrix \\(\\mathbf{R}\\) different based covariance matrix \\(\\mathbf{\\Sigma}\\)PCA correlation matrix just rescaling trait unit variancePCA correlation matrix just rescaling trait unit varianceTransform \\(\\mathbf{x}\\) \\(\\mathbf{z}\\) \\(z_{ij} = (x_{ij} - \\bar{x}_i)/\\sqrt{s_{ii}}\\) denominator affects PCATransform \\(\\mathbf{x}\\) \\(\\mathbf{z}\\) \\(z_{ij} = (x_{ij} - \\bar{x}_i)/\\sqrt{s_{ii}}\\) denominator affects PCAAfter transformation, \\(cov(\\mathbf{z}) = \\mathbf{R}\\)transformation, \\(cov(\\mathbf{z}) = \\mathbf{R}\\)PCA \\(\\mathbf{R}\\) calculated way \\(\\mathbf{S}\\) (\\(\\hat{\\lambda}{}_1 + \\dots + \\hat{\\lambda}{}_p = p\\) )PCA \\(\\mathbf{R}\\) calculated way \\(\\mathbf{S}\\) (\\(\\hat{\\lambda}{}_1 + \\dots + \\hat{\\lambda}{}_p = p\\) )use \\(\\mathbf{R}, \\mathbf{S}\\) depends purpose PCA.\nscale observations different, covariance matrix preferable. dramatically different, analysis can still dominated large variance traits.\nuse \\(\\mathbf{R}, \\mathbf{S}\\) depends purpose PCA.scale observations different, covariance matrix preferable. dramatically different, analysis can still dominated large variance traits.many PCs use can guided \nScree Graphs: plot eigenvalues indices. Look “elbow” steep decline graph suddenly flattens ; big gaps.\nminimum Percent total variation (e.g., choose enough components 50% 90%). can used interpretations.\nKaiser’s rule: use PC eigenvalues larger 1 (applied PCA correlation matrix) - ad hoc\nCompare eigenvalue scree plot data scree plot data randomized.\nmany PCs use can guided byScree Graphs: plot eigenvalues indices. Look “elbow” steep decline graph suddenly flattens ; big gaps.Scree Graphs: plot eigenvalues indices. Look “elbow” steep decline graph suddenly flattens ; big gaps.minimum Percent total variation (e.g., choose enough components 50% 90%). can used interpretations.minimum Percent total variation (e.g., choose enough components 50% 90%). can used interpretations.Kaiser’s rule: use PC eigenvalues larger 1 (applied PCA correlation matrix) - ad hocKaiser’s rule: use PC eigenvalues larger 1 (applied PCA correlation matrix) - ad hocCompare eigenvalue scree plot data scree plot data randomized.Compare eigenvalue scree plot data scree plot data randomized.","code":""},{"path":"multivariate-methods.html","id":"application-11","chapter":"17 Multivariate Methods","heading":"17.2.3 Application","text":"PCA covariance matrix usually preferred due fact PCA invariant changes scale. Hence, PCA correlation matrix preferredThis also addresses problem multicollinearityThe eigvenvectors may differ multiplication -1 different implementation, interpretation.Covid ExampleTo reduce collinearity problem dataset, can use principal components regressors.MSE PC-based model larger regular regression, models large degree collinearity can still perform well.pcr function pls can used fitting PC regression (select optimal number components model).","code":"\nlibrary(tidyverse)\n## Read in and check data\nstock <- read.table(\"images/stock.dat\")\nnames(stock) <- c(\"allied\", \"dupont\", \"carbide\", \"exxon\", \"texaco\")\nstr(stock)## 'data.frame':    100 obs. of  5 variables:\n##  $ allied : num  0 0.027 0.1228 0.057 0.0637 ...\n##  $ dupont : num  0 -0.04485 0.06077 0.02995 -0.00379 ...\n##  $ carbide: num  0 -0.00303 0.08815 0.06681 -0.03979 ...\n##  $ exxon  : num  0.0395 -0.0145 0.0862 0.0135 -0.0186 ...\n##  $ texaco : num  0 0.0435 0.0781 0.0195 -0.0242 ...\n## Covariance matrix of data\ncov(stock)##               allied       dupont      carbide        exxon       texaco\n## allied  0.0016299269 0.0008166676 0.0008100713 0.0004422405 0.0005139715\n## dupont  0.0008166676 0.0012293759 0.0008276330 0.0003868550 0.0003109431\n## carbide 0.0008100713 0.0008276330 0.0015560763 0.0004872816 0.0004624767\n## exxon   0.0004422405 0.0003868550 0.0004872816 0.0008023323 0.0004084734\n## texaco  0.0005139715 0.0003109431 0.0004624767 0.0004084734 0.0007587370\n## Correlation matrix of data\ncor(stock)##            allied    dupont   carbide     exxon    texaco\n## allied  1.0000000 0.5769244 0.5086555 0.3867206 0.4621781\n## dupont  0.5769244 1.0000000 0.5983841 0.3895191 0.3219534\n## carbide 0.5086555 0.5983841 1.0000000 0.4361014 0.4256266\n## exxon   0.3867206 0.3895191 0.4361014 1.0000000 0.5235293\n## texaco  0.4621781 0.3219534 0.4256266 0.5235293 1.0000000\n# cov(scale(stock)) # give the same result\n\n## PCA with covariance\ncov_pca <- prcomp(stock) # uses singular value decomposition for calculation and an N -1 divisor\n# alternatively, princomp can do PCA via spectral decomposition, but it has worse numerical accuracy\n\n# eigen values\ncov_results <- data.frame(eigen_values = cov_pca$sdev ^ 2)\ncov_results %>%\n    mutate(proportion = eigen_values / sum(eigen_values),\n           cumulative = cumsum(proportion)) # first 2 PCs account for 73% variance in the data##   eigen_values proportion cumulative\n## 1 0.0035953867 0.60159252  0.6015925\n## 2 0.0007921798 0.13255027  0.7341428\n## 3 0.0007364426 0.12322412  0.8573669\n## 4 0.0005086686 0.08511218  0.9424791\n## 5 0.0003437707 0.05752091  1.0000000\n# eigen vectors\ncov_pca$rotation # prcomp calls rotation##               PC1         PC2        PC3         PC4         PC5\n## allied  0.5605914  0.73884565 -0.1260222  0.28373183 -0.20846832\n## dupont  0.4698673 -0.09286987 -0.4675066 -0.68793190  0.28069055\n## carbide 0.5473322 -0.65401929 -0.1140581  0.50045312 -0.09603973\n## exxon   0.2908932 -0.11267353  0.6099196 -0.43808002 -0.58203935\n## texaco  0.2842017  0.07103332  0.6168831  0.06227778  0.72784638\n# princomp calls loadings.\n\n# first PC = overall average\n# second PC compares Allied to Carbide\n\n## PCA with correlation\n#same as scale(stock) %>% prcomp\ncor_pca <- prcomp(stock, scale = T)\n\n\n\n# eigen values\ncor_results <- data.frame(eigen_values = cor_pca$sdev ^ 2)\ncor_results %>%\n    mutate(proportion = eigen_values / sum(eigen_values),\n           cumulative = cumsum(proportion))##   eigen_values proportion cumulative\n## 1    2.8564869 0.57129738  0.5712974\n## 2    0.8091185 0.16182370  0.7331211\n## 3    0.5400440 0.10800880  0.8411299\n## 4    0.4513468 0.09026936  0.9313992\n## 5    0.3430038 0.06860076  1.0000000\n# first egiven values corresponds to less variance than PCA based on the covariance matrix\n\n# eigen vectors\ncor_pca$rotation##               PC1        PC2        PC3        PC4        PC5\n## allied  0.4635405 -0.2408499  0.6133570 -0.3813727  0.4532876\n## dupont  0.4570764 -0.5090997 -0.1778996 -0.2113068 -0.6749814\n## carbide 0.4699804 -0.2605774 -0.3370355  0.6640985  0.3957247\n## exxon   0.4216770  0.5252647 -0.5390181 -0.4728036  0.1794482\n## texaco  0.4213291  0.5822416  0.4336029  0.3812273 -0.3874672\n# interpretation of PC2 is different from above: it is a comparison of Allied, Dupont and Carbid to Exxon and Texaco \nload('images/MOcovid.RData')\ncovidpca <- prcomp(ndat[,-1],scale = T,center = T)\n\ncovidpca$rotation[,1:2]##                                                          PC1         PC2\n## X..Population.in.Rural.Areas                      0.32865838  0.05090955\n## Area..sq..miles.                                  0.12014444 -0.28579183\n## Population.density..sq..miles.                   -0.29670124  0.28312922\n## Literacy.rate                                    -0.12517700 -0.08999542\n## Families                                         -0.25856941  0.16485752\n## Area.of.farm.land..sq..miles.                     0.02101106 -0.31070363\n## Number.of.farms                                  -0.03814582 -0.44809679\n## Average.value.of.all.property.per.farm..dollars. -0.05410709  0.14404306\n## Estimation.of.rurality..                         -0.19040210  0.12089501\n## Male..                                            0.02182394 -0.09568768\n## Number.of.Physcians.per.100.000                  -0.31451606  0.13598026\n## average.age                                       0.29414708  0.35593459\n## X0.4.age.proportion                              -0.11431336 -0.23574057\n## X20.44.age.proportion                            -0.32802128 -0.22718550\n## X65.and.over.age.proportion                       0.30585033  0.32201626\n## prop..White..nonHisp                              0.35627561 -0.14142646\n## prop..Hispanic                                   -0.16655381 -0.15105342\n## prop..Black                                      -0.33333359  0.24405802\n# Variability of each principal component: pr.var\npr.var <- covidpca$sdev ^ 2\n# Variance explained by each principal component: pve\npve <- pr.var / sum(pr.var)\nplot(\n    pve,\n    xlab = \"Principal Component\",\n    ylab = \"Proportion of Variance Explained\",\n    ylim = c(0, 0.5),\n    type = \"b\"\n)\nplot(\n    cumsum(pve),\n    xlab = \"Principal Component\",\n    ylab = \"Cumulative Proportion of Variance Explained\",\n    ylim = c(0, 1),\n    type = \"b\"\n)\n# the first six principe account for around 80% of the variance. \n\n\n#using base lm function for PC regression\npcadat <- data.frame(covidpca$x[, 1:6])\npcadat$y <- ndat$Y\npcr.man <- lm(log(y) ~ ., pcadat)\nmean(pcr.man$residuals ^ 2)## [1] 0.03453371\n#comparison to lm w/o prin comps\nlm.fit <- lm(log(Y) ~ ., data = ndat)\nmean(lm.fit$residuals ^ 2)## [1] 0.02335128"},{"path":"multivariate-methods.html","id":"factor-analysis","chapter":"17 Multivariate Methods","heading":"17.3 Factor Analysis","text":"PurposeUsing linear combinations underlying unobservable (latent) traits, try describe covariance relationship among large number measured traitsUsing linear combinations underlying unobservable (latent) traits, try describe covariance relationship among large number measured traitsSimilar PCA, factor analysis model basedSimilar PCA, factor analysis model basedMore details can found PSU stat UMN statLet \\(\\mathbf{y}\\) set \\(p\\) measured variables\\(E(\\mathbf{y}) = \\mathbf{\\mu}\\)\\(var(\\mathbf{y}) = \\mathbf{\\Sigma}\\)\\[\n\\begin{aligned}\n\\mathbf{y} - \\mathbf{\\mu} &= \\mathbf{Lf} + \\epsilon \\\\\n&= \n\\left(\n\\begin{array}\n{c}\nl_{11}f_1 + l_{12}f_2 + \\dots + l_{tm}f_m \\\\\n\\vdots \\\\\nl_{p1}f_1 + l_{p2}f_2 + \\dots + l_{pm} f_m\n\\end{array}\n\\right)\n+ \n\\left(\n\\begin{array}\n{c}\n\\epsilon_1 \\\\\n\\vdots \\\\\n\\epsilon_p\n\\end{array}\n\\right)\n\\end{aligned}\n\\]\\(\\mathbf{y} - \\mathbf{\\mu}\\) = p centered measurements\\(\\mathbf{y} - \\mathbf{\\mu}\\) = p centered measurements\\(\\mathbf{L}\\) = \\(p \\times m\\) matrix factor loadings\\(\\mathbf{L}\\) = \\(p \\times m\\) matrix factor loadings\\(\\mathbf{f}\\) = unobserved common factors population\\(\\mathbf{f}\\) = unobserved common factors population\\(\\mathbf{\\epsilon}\\) = random errors (.e., variation accounted common factors).\\(\\mathbf{\\epsilon}\\) = random errors (.e., variation accounted common factors).want \\(m\\) (number factors) much smaller \\(p\\) (number measured attributes)Restrictions model\\(E(\\epsilon) = \\mathbf{0}\\)\\(E(\\epsilon) = \\mathbf{0}\\)\\(var(\\epsilon) = \\Psi_{p \\times p} = diag( \\psi_1, \\dots, \\psi_p)\\)\\(var(\\epsilon) = \\Psi_{p \\times p} = diag( \\psi_1, \\dots, \\psi_p)\\)\\(\\mathbf{\\epsilon}, \\mathbf{f}\\) independent\\(\\mathbf{\\epsilon}, \\mathbf{f}\\) independentAdditional assumption \\(E(\\mathbf{f}) = \\mathbf{0}, var(\\mathbf{f}) = \\mathbf{}_{m \\times m}\\) (known orthogonal factor model) , imposes following covariance structure \\(\\mathbf{y}\\)Additional assumption \\(E(\\mathbf{f}) = \\mathbf{0}, var(\\mathbf{f}) = \\mathbf{}_{m \\times m}\\) (known orthogonal factor model) , imposes following covariance structure \\(\\mathbf{y}\\)\\[\n\\begin{aligned}\nvar(\\mathbf{y}) = \\mathbf{\\Sigma} &=  var(\\mathbf{Lf} + \\mathbf{\\epsilon}) \\\\\n&= var(\\mathbf{Lf}) + var(\\epsilon) \\\\\n&= \\mathbf{L} var(\\mathbf{f}) \\mathbf{L}' + \\mathbf{\\Psi} \\\\\n&= \\mathbf{LIL}' + \\mathbf{\\Psi} \\\\\n&= \\mathbf{LL}' + \\mathbf{\\Psi}\n\\end{aligned}\n\\]Since \\(\\mathbf{\\Psi}\\) diagonal, -diagonal elements \\(\\mathbf{LL}'\\) \\(\\sigma_{ij}\\), co variances \\(\\mathbf{\\Sigma}\\), means \\(cov(y_i, y_j) = \\sum_{k=1}^m l_{ik}l_{jk}\\) covariance \\(\\mathbf{y}\\) completely determined m factors ( \\(m <<p\\))\\(var(y_i) = \\sum_{k=1}^m l_{ik}^2 + \\psi_i\\) \\(\\psi_i\\) specific variance summation term -th communality (.e., portion variance -th variable contributed \\(m\\) common factors (\\(h_i^2 = \\sum_{k=1}^m l_{ik}^2\\))factor model uniquely determined orthogonal transformation factors.Let \\(\\mathbf{T}_{m \\times m}\\) orthogonal matrix \\(\\mathbf{TT}' = \\mathbf{T'T} = \\mathbf{}\\) \\[\n\\begin{aligned}\n\\mathbf{y} - \\mathbf{\\mu} &= \\mathbf{Lf} + \\epsilon \\\\\n&= \\mathbf{LTT'f} + \\epsilon \\\\\n&= \\mathbf{L}^*(\\mathbf{T'f}) + \\epsilon & \\text{} \\mathbf{L}^* = \\mathbf{LT}\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\n\\mathbf{\\Sigma} &= \\mathbf{LL}' + \\mathbf{\\Psi} \\\\\n&= \\mathbf{LTT'L} + \\mathbf{\\Psi} \\\\\n&= (\\mathbf{L}^*)(\\mathbf{L}^*)' + \\mathbf{\\Psi}\n\\end{aligned}\n\\]Hence, orthogonal transformation factors equally good description correlations among observed traits.Let \\(\\mathbf{y} = \\mathbf{Cx}\\) , \\(\\mathbf{C}\\) diagonal matrix, \\(\\mathbf{L}_y = \\mathbf{CL}_x\\) \\(\\mathbf{\\Psi}_y = \\mathbf{C\\Psi}_x\\mathbf{C}\\)Hence, can see factor analysis also invariant changes scale","code":""},{"path":"multivariate-methods.html","id":"methods-of-estimation","chapter":"17 Multivariate Methods","heading":"17.3.1 Methods of Estimation","text":"estimate \\(\\mathbf{L}\\)Principal Component MethodPrincipal Factor Method17.3.1.3","code":""},{"path":"multivariate-methods.html","id":"principal-component-method","chapter":"17 Multivariate Methods","heading":"17.3.1.1 Principal Component Method","text":"Spectral decomposition\\[\n\\begin{aligned}\n\\mathbf{\\Sigma} &= \\lambda_1 \\mathbf{}_1 \\mathbf{}_1' + \\dots + \\lambda_p \\mathbf{}_p \\mathbf{}_p' \\\\\n&= \\mathbf{\\Lambda }' \\\\\n&= \\sum_{k=1}^m \\lambda+k \\mathbf{}_k \\mathbf{}_k' + \\sum_{k= m+1}^p \\lambda_k \\mathbf{}_k \\mathbf{}_k' \\\\\n&= \\sum_{k=1}^m l_k l_k' + \\sum_{k=m+1}^p \\lambda_k \\mathbf{}_k \\mathbf{}_k'\n\\end{aligned}\n\\]\\(l_k = \\mathbf{}_k \\sqrt{\\lambda_k}\\) second term diagonal general.Assume\\[\n\\psi_i = \\sigma_{ii} - \\sum_{k=1}^m l_{ik}^2 = \\sigma_{ii} -  \\sum_{k=1}^m \\lambda_i a_{ik}^2\n\\]\\[\n\\mathbf{\\Sigma} \\approx \\mathbf{LL}' + \\mathbf{\\Psi}\n\\]estimate \\(\\mathbf{L}\\) \\(\\Psi\\) , use expected eigenvalues eigenvectors \\(\\mathbf{S}\\) \\(\\mathbf{R}\\)estimated factor loadings don’t change number actors increasesThe estimated factor loadings don’t change number actors increasesThe diagonal elements \\(\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}}\\) equal diagonal elements \\(\\mathbf{S}\\) \\(\\mathbf{R}\\), covariances may exactly reproducedThe diagonal elements \\(\\hat{\\mathbf{L}}\\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}}\\) equal diagonal elements \\(\\mathbf{S}\\) \\(\\mathbf{R}\\), covariances may exactly reproducedWe select \\(m\\) -diagonal elements close values \\(\\mathbf{S}\\) (make -diagonal elements \\(\\mathbf{S} - \\hat{\\mathbf{L}} \\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}}\\) small)select \\(m\\) -diagonal elements close values \\(\\mathbf{S}\\) (make -diagonal elements \\(\\mathbf{S} - \\hat{\\mathbf{L}} \\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}}\\) small)","code":""},{"path":"multivariate-methods.html","id":"principal-factor-method","chapter":"17 Multivariate Methods","heading":"17.3.1.2 Principal Factor Method","text":"Consider modeling correlation matrix, \\(\\mathbf{R} = \\mathbf{L} \\mathbf{L}' + \\mathbf{\\Psi}\\) . \\[\n\\mathbf{L} \\mathbf{L}' = \\mathbf{R} - \\mathbf{\\Psi} =\n\\left(\n\\begin{array}\n{cccc}\nh_1^2 & r_{12} & \\dots & r_{1p} \\\\\nr_{21} & h_2^2 & \\dots & r_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr_{p1} & r_{p2} & \\dots & h_p^2\n\\end{array}\n\\right)\n\\]\\(h_i^2 = 1- \\psi_i\\) (communality)Suppose initial estimates available communalities, \\((h_1^*)^2,(h_2^*)^2, \\dots , (h_p^*)^2\\), can regress trait others, use \\(r^2\\) \\(h^2\\)estimate \\(\\mathbf{R} - \\mathbf{\\Psi}\\) step k \\[\n(\\mathbf{R} - \\mathbf{\\Psi})_k = \n\\left(\n\\begin{array}\n{cccc}\n(h_1^*)^2 & r_{12} & \\dots & r_{1p} \\\\\nr_{21} & (h_2^*)^2 & \\dots & r_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nr_{p1} & r_{p2} & \\dots & (h_p^*)^2\n\\end{array}\n\\right) = \n\\mathbf{L}_k^*(\\mathbf{L}_k^*)' \n\\]\\[\n\\mathbf{L}_k^* = (\\sqrt{\\hat{\\lambda}_1^*\\hat{\\mathbf{}}_1^* , \\dots \\hat{\\lambda}_m^*\\hat{\\mathbf{}}_m^*})\n\\]\\[\n\\hat{\\psi}_{,k}^* = 1 - \\sum_{j=1}^m \\hat{\\lambda}_i^* (\\hat{}_{ij}^*)^2\n\\]used spectral decomposition estimated matrix \\((\\mathbf{R}- \\mathbf{\\Psi})\\) calculate \\(\\hat{\\lambda}_i^* s\\) \\(\\mathbf{\\hat{}}_i^* s\\)updating values \\((\\hat{h}_i^*)^2 = 1 - \\hat{\\psi}_{,k}^*\\) use form new \\(\\mathbf{L}_{k+1}^*\\) via another spectral decomposition. Repeat processNotes:matrix \\((\\mathbf{R} - \\mathbf{\\Psi})_k\\) necessarily positive definiteThe matrix \\((\\mathbf{R} - \\mathbf{\\Psi})_k\\) necessarily positive definiteThe principal component method similar principal factor one considers initial communalities \\(h^2 = 1\\)principal component method similar principal factor one considers initial communalities \\(h^2 = 1\\)\\(m\\) large, communalities may become larger 1, causing iterations terminate. combat, can\nfix communality greater 1 1 continues.\ncontinue iterations regardless size communalities. However, results can outside fo parameter space.\n\\(m\\) large, communalities may become larger 1, causing iterations terminate. combat, canfix communality greater 1 1 continues.fix communality greater 1 1 continues.continue iterations regardless size communalities. However, results can outside fo parameter space.continue iterations regardless size communalities. However, results can outside fo parameter space.","code":""},{"path":"multivariate-methods.html","id":"maximum-likelihood-method-factor-analysis","chapter":"17 Multivariate Methods","heading":"17.3.1.3 Maximum Likelihood Method","text":"Since need likelihood function, make additional (critical) assumption \\(\\mathbf{y}_j \\sim N(\\mathbf{\\mu},\\mathbf{\\Sigma})\\) \\(j = 1,..,n\\)\\(\\mathbf{y}_j \\sim N(\\mathbf{\\mu},\\mathbf{\\Sigma})\\) \\(j = 1,..,n\\)\\(\\mathbf{f} \\sim N(\\mathbf{0}, \\mathbf{})\\)\\(\\mathbf{f} \\sim N(\\mathbf{0}, \\mathbf{})\\)\\(\\epsilon_j \\sim N(\\mathbf{0}, \\mathbf{\\Psi})\\)\\(\\epsilon_j \\sim N(\\mathbf{0}, \\mathbf{\\Psi})\\)restriction\\(\\mathbf{L}' \\mathbf{\\Psi}^{-1}\\mathbf{L} = \\mathbf{\\Delta}\\) \\(\\mathbf{\\Delta}\\) diagonal matrix. (since factor loading matrix unique, need restriction).Notes:Finding MLE can computationally expensiveFinding MLE can computationally expensivewe typically use methods exploratory data analysiswe typically use methods exploratory data analysisLIkelihood ratio tests used testing hypotheses framework (.e., Confirmatory Factor Analysis)LIkelihood ratio tests used testing hypotheses framework (.e., Confirmatory Factor Analysis)","code":""},{"path":"multivariate-methods.html","id":"factor-rotation","chapter":"17 Multivariate Methods","heading":"17.3.2 Factor Rotation","text":"\\(\\mathbf{T}_{m \\times m}\\) orthogonal matrix property \\[\n\\hat{\\mathbf{L}} \\hat{\\mathbf{L}}' + \\hat{\\mathbf{\\Psi}} = \\hat{\\mathbf{L}}^*(\\hat{\\mathbf{L}}^*)' + \\hat{\\mathbf{\\Psi}}\n\\]\\(\\mathbf{L}^* = \\mathbf{LT}\\)means estimated specific variances communalities altered orthogonal transformation.Since infinite number choices \\(\\mathbf{T}\\), selection criterion necessaryFor example, can find orthogonal transformation maximizes objective function\\[\n\\sum_{j = 1}^m [\\frac{1}{p}\\sum_{=1}^p (\\frac{l_{ij}^{*2}}{h_i})^2 - \\{\\frac{\\gamma}{p} \\sum_{=1}^p (\\frac{l_{ij}^{*2}}{h_i})^2 \\}^2]\n\\]\\(\\frac{l_{ij}^{*2}}{h_i}\\) “scaled loadings,” gives variables small communalities influence.Different choices \\(\\gamma\\) objective function correspond different orthogonal rotation found literature;Varimax \\(\\gamma = 1\\) (rotate factors \\(p\\) variables high loading one factor, always possible).Varimax \\(\\gamma = 1\\) (rotate factors \\(p\\) variables high loading one factor, always possible).Quartimax \\(\\gamma = 0\\)Quartimax \\(\\gamma = 0\\)Equimax \\(\\gamma = m/2\\)Equimax \\(\\gamma = m/2\\)Parsimax \\(\\gamma = \\frac{p(m-1)}{p+m-2}\\)Parsimax \\(\\gamma = \\frac{p(m-1)}{p+m-2}\\)Promax: non-orthognal olique transformationsPromax: non-orthognal olique transformationsHarris-Kaiser (HK): non-orthognal olique transformationsHarris-Kaiser (HK): non-orthognal olique transformations","code":""},{"path":"multivariate-methods.html","id":"estimation-of-factor-scores","chapter":"17 Multivariate Methods","heading":"17.3.3 Estimation of Factor Scores","text":"Recall\\[\n(\\mathbf{y}_j - \\mathbf{\\mu}) = \\mathbf{L}_{p \\times m}\\mathbf{f}_j + \\epsilon_j\n\\]factor model correct \\[\nvar(\\epsilon_j) = \\mathbf{\\Psi} = diag (\\psi_1, \\dots , \\psi_p)\n\\]Thus consider using weighted least squares estimate \\(\\mathbf{f}_j\\) , vector factor scores j-th sampled unit \\[\n\\begin{aligned}\n\\hat{\\mathbf{f}} &= (\\mathbf{L}'\\mathbf{\\Psi}^{-1} \\mathbf{L})^{-1} \\mathbf{L}' \\mathbf{\\Psi}^{-1}(\\mathbf{y}_j - \\mathbf{\\mu}) \\\\\n& \\approx (\\mathbf{L}'\\mathbf{\\Psi}^{-1} \\mathbf{L})^{-1} \\mathbf{L}' \\mathbf{\\Psi}^{-1}(\\mathbf{y}_j - \\mathbf{\\bar{y}})\n\\end{aligned}\n\\]","code":""},{"path":"multivariate-methods.html","id":"the-regression-method","chapter":"17 Multivariate Methods","heading":"17.3.3.1 The Regression Method","text":"Alternatively, can use regression method estimate factor scoresConsider joint distribution \\((\\mathbf{y}_j - \\mathbf{\\mu})\\) \\(\\mathbf{f}_j\\) assuming multivariate normality, maximum likelihood approach. ,\\[\n\\left(\n\\begin{array}\n{c}\n\\mathbf{y}_j - \\mathbf{\\mu} \\\\\n\\mathbf{f}_j\n\\end{array}\n\\right) \\sim\nN_{p + m}\n\\left(\n\\left[\n\\begin{array}\n{cc}\n\\mathbf{LL}' + \\mathbf{\\Psi} & \\mathbf{L} \\\\\n\\mathbf{L}' & \\mathbf{}_{m\\times m}\n\\end{array}\n\\right]\n\\right)\n\\]\\(m\\) factor model correctHence,\\[\nE(\\mathbf{f}_j | \\mathbf{y}_j - \\mathbf{\\mu}) = \\mathbf{L}' (\\mathbf{LL}' + \\mathbf{\\Psi})^{-1}(\\mathbf{y}_j - \\mathbf{\\mu})\n\\]notice \\(\\mathbf{L}' (\\mathbf{LL}' + \\mathbf{\\Psi})^{-1}\\) \\(m \\times p\\) matrix regression coefficientsThen, use estimated conditional mean vector estimate factor scores\\[\n\\mathbf{\\hat{f}}_j = \\mathbf{\\hat{L}}'(\\mathbf{\\hat{L}}\\mathbf{\\hat{L}}' + \\mathbf{\\hat{\\Psi}})^{-1}(\\mathbf{y}_j - \\mathbf{\\bar{y}})\n\\]Alternatively, reduce effect possible incorrect determination fo number factors \\(m\\) using \\(\\mathbf{S}\\) substitute \\(\\mathbf{\\hat{L}}\\mathbf{\\hat{L}}' + \\mathbf{\\hat{\\Psi}}\\) \\[\n\\mathbf{\\hat{f}}_j = \\mathbf{\\hat{L}}'\\mathbf{S}^{-1}(\\mathbf{y}_j - \\mathbf{\\bar{y}})\n\\]\\(j = 1,\\dots,n\\)","code":""},{"path":"multivariate-methods.html","id":"model-diagnostic","chapter":"17 Multivariate Methods","heading":"17.3.4 Model Diagnostic","text":"PlotsPlotsCheck outliers (recall \\(\\mathbf{f}_j \\sim iid N(\\mathbf{0}, \\mathbf{}_{m \\times m})\\))Check outliers (recall \\(\\mathbf{f}_j \\sim iid N(\\mathbf{0}, \\mathbf{}_{m \\times m})\\))Check multivariate normality assumptionCheck multivariate normality assumptionUse univariate tests normality check factor scoresUse univariate tests normality check factor scoresConfirmatory Factor Analysis: formal testing hypotheses loadings, use MLE full/reduced model testing paradigm measures model fitConfirmatory Factor Analysis: formal testing hypotheses loadings, use MLE full/reduced model testing paradigm measures model fit","code":""},{"path":"multivariate-methods.html","id":"application-12","chapter":"17 Multivariate Methods","heading":"17.3.5 Application","text":"psych package,h2 = communalitiesh2 = communalitiesu2 = uniquenessu2 = uniquenesscom = complexitycom = complexityThe output info null hypothesis common factors statement “degrees freedom null model ..”output info null hypothesis number factors sufficient statement “total number observations …”One factor enough, two sufficient, enough data 3 factors (df -2 NA p-value). Hence, use 2-factor model.","code":"\nlibrary(psych)## Warning: package 'psych' was built under R version 4.0.5## \n## Attaching package: 'psych'## The following object is masked from 'package:lavaan':\n## \n##     cor2cov## The following objects are masked from 'package:ggplot2':\n## \n##     %+%, alpha## The following object is masked from 'package:car':\n## \n##     logit\nlibrary(tidyverse)\n## Load the data from the psych package\ndata(Harman.5)\nHarman.5##         population schooling employment professional housevalue\n## Tract1        5700      12.8       2500          270      25000\n## Tract2        1000      10.9        600           10      10000\n## Tract3        3400       8.8       1000           10       9000\n## Tract4        3800      13.6       1700          140      25000\n## Tract5        4000      12.8       1600          140      25000\n## Tract6        8200       8.3       2600           60      12000\n## Tract7        1200      11.4        400           10      16000\n## Tract8        9100      11.5       3300           60      14000\n## Tract9        9900      12.5       3400          180      18000\n## Tract10       9600      13.7       3600          390      25000\n## Tract11       9600       9.6       3300           80      12000\n## Tract12       9400      11.4       4000          100      13000\n# Correlation matrix\ncor_mat <- cor(Harman.5)\ncor_mat##              population  schooling employment professional housevalue\n## population   1.00000000 0.00975059  0.9724483    0.4388708 0.02241157\n## schooling    0.00975059 1.00000000  0.1542838    0.6914082 0.86307009\n## employment   0.97244826 0.15428378  1.0000000    0.5147184 0.12192599\n## professional 0.43887083 0.69140824  0.5147184    1.0000000 0.77765425\n## housevalue   0.02241157 0.86307009  0.1219260    0.7776543 1.00000000\n## Principal Component Method with Correlation\ncor_pca <- prcomp(Harman.5, scale = T)\n# eigen values\ncor_results <- data.frame(eigen_values = cor_pca$sdev ^ 2)\n\ncor_results <- cor_results %>%\n    mutate(\n        proportion = eigen_values / sum(eigen_values),\n        cumulative = cumsum(proportion),\n        number = row_number()\n    )\ncor_results##   eigen_values  proportion cumulative number\n## 1   2.87331359 0.574662719  0.5746627      1\n## 2   1.79666009 0.359332019  0.9339947      2\n## 3   0.21483689 0.042967377  0.9769621      3\n## 4   0.09993405 0.019986811  0.9969489      4\n## 5   0.01525537 0.003051075  1.0000000      5\n# Scree plot of Eigenvalues\nscree_gg <- ggplot(cor_results, aes(x = number, y = eigen_values)) +\n    geom_line(alpha = 0.5) +\n    geom_text(aes(label = number)) +\n    scale_x_continuous(name = \"Number\") +\n    scale_y_continuous(name = \"Eigenvalue\") +\n    theme_bw()\nscree_gg\nscreeplot(cor_pca, type = 'lines')\n## Keep 2 factors based on scree plot and eigenvalues\nfactor_pca <- principal(Harman.5, nfactors = 2, rotate = \"none\")\nfactor_pca## Principal Components Analysis\n## Call: principal(r = Harman.5, nfactors = 2, rotate = \"none\")\n## Standardized loadings (pattern matrix) based upon correlation matrix\n##               PC1   PC2   h2    u2 com\n## population   0.58  0.81 0.99 0.012 1.8\n## schooling    0.77 -0.54 0.89 0.115 1.8\n## employment   0.67  0.73 0.98 0.021 2.0\n## professional 0.93 -0.10 0.88 0.120 1.0\n## housevalue   0.79 -0.56 0.94 0.062 1.8\n## \n##                        PC1  PC2\n## SS loadings           2.87 1.80\n## Proportion Var        0.57 0.36\n## Cumulative Var        0.57 0.93\n## Proportion Explained  0.62 0.38\n## Cumulative Proportion 0.62 1.00\n## \n## Mean item complexity =  1.7\n## Test of the hypothesis that 2 components are sufficient.\n## \n## The root mean square of the residuals (RMSR) is  0.03 \n##  with the empirical chi square  0.29  with prob <  0.59 \n## \n## Fit based upon off diagonal values = 1\n# factor 1 = overall socioeconomic health\n# factor 2 = contrast of the population and employment against school and house value\n\n\n## Ssquared multiple correlation (SMC) prior, no rotation\nfactor_pca_smc <- fa(\n    Harman.5,\n    nfactors = 2,\n    fm = \"pa\",\n    rotate = \"none\",\n    SMC = TRUE\n)## Warning in fa.stats(r = r, f = f, phi = phi, n.obs = n.obs, np.obs = np.obs, :\n## The estimated weights for the factor scores are probably incorrect. Try a\n## different factor score estimation method.## Warning in fac(r = r, nfactors = nfactors, n.obs = n.obs, rotate = rotate, : An\n## ultra-Heywood case was detected. Examine the results carefully\nfactor_pca_smc## Factor Analysis using method =  pa\n## Call: fa(r = Harman.5, nfactors = 2, rotate = \"none\", SMC = TRUE, fm = \"pa\")\n## Standardized loadings (pattern matrix) based upon correlation matrix\n##               PA1   PA2   h2      u2 com\n## population   0.62  0.78 1.00 -0.0027 1.9\n## schooling    0.70 -0.53 0.77  0.2277 1.9\n## employment   0.70  0.68 0.96  0.0413 2.0\n## professional 0.88 -0.15 0.80  0.2017 1.1\n## housevalue   0.78 -0.60 0.96  0.0361 1.9\n## \n##                        PA1  PA2\n## SS loadings           2.76 1.74\n## Proportion Var        0.55 0.35\n## Cumulative Var        0.55 0.90\n## Proportion Explained  0.61 0.39\n## Cumulative Proportion 0.61 1.00\n## \n## Mean item complexity =  1.7\n## Test of the hypothesis that 2 factors are sufficient.\n## \n## The degrees of freedom for the null model are  10  and the objective function was  6.38 with Chi Square of  54.25\n## The degrees of freedom for the model are 1  and the objective function was  0.34 \n## \n## The root mean square of the residuals (RMSR) is  0.01 \n## The df corrected root mean square of the residuals is  0.03 \n## \n## The harmonic number of observations is  12 with the empirical chi square  0.02  with prob <  0.88 \n## The total number of observations was  12  with Likelihood Chi Square =  2.44  with prob <  0.12 \n## \n## Tucker Lewis Index of factoring reliability =  0.596\n## RMSEA index =  0.336  and the 90 % confidence intervals are  0 0.967\n## BIC =  -0.04\n## Fit based upon off diagonal values = 1\n## SMC prior, Promax rotation\nfactor_pca_smc_pro <- fa(\n    Harman.5,\n    nfactors = 2,\n    fm = \"pa\",\n    rotate = \"Promax\",\n    SMC = TRUE\n)## Warning in fa.stats(r = r, f = f, phi = phi, n.obs = n.obs, np.obs = np.obs, :\n## The estimated weights for the factor scores are probably incorrect. Try a\n## different factor score estimation method.\n\n## Warning in fa.stats(r = r, f = f, phi = phi, n.obs = n.obs, np.obs = np.obs, :\n## An ultra-Heywood case was detected. Examine the results carefully\nfactor_pca_smc_pro## Factor Analysis using method =  pa\n## Call: fa(r = Harman.5, nfactors = 2, rotate = \"Promax\", SMC = TRUE, \n##     fm = \"pa\")\n## Standardized loadings (pattern matrix) based upon correlation matrix\n##                PA1   PA2   h2      u2 com\n## population   -0.11  1.02 1.00 -0.0027 1.0\n## schooling     0.90 -0.11 0.77  0.2277 1.0\n## employment    0.02  0.97 0.96  0.0413 1.0\n## professional  0.75  0.33 0.80  0.2017 1.4\n## housevalue    1.01 -0.14 0.96  0.0361 1.0\n## \n##                        PA1  PA2\n## SS loadings           2.38 2.11\n## Proportion Var        0.48 0.42\n## Cumulative Var        0.48 0.90\n## Proportion Explained  0.53 0.47\n## Cumulative Proportion 0.53 1.00\n## \n##  With factor correlations of \n##      PA1  PA2\n## PA1 1.00 0.25\n## PA2 0.25 1.00\n## \n## Mean item complexity =  1.1\n## Test of the hypothesis that 2 factors are sufficient.\n## \n## The degrees of freedom for the null model are  10  and the objective function was  6.38 with Chi Square of  54.25\n## The degrees of freedom for the model are 1  and the objective function was  0.34 \n## \n## The root mean square of the residuals (RMSR) is  0.01 \n## The df corrected root mean square of the residuals is  0.03 \n## \n## The harmonic number of observations is  12 with the empirical chi square  0.02  with prob <  0.88 \n## The total number of observations was  12  with Likelihood Chi Square =  2.44  with prob <  0.12 \n## \n## Tucker Lewis Index of factoring reliability =  0.596\n## RMSEA index =  0.336  and the 90 % confidence intervals are  0 0.967\n## BIC =  -0.04\n## Fit based upon off diagonal values = 1\n## SMC prior, varimax rotation\nfactor_pca_smc_var <- fa(\n    Harman.5,\n    nfactors = 2,\n    fm = \"pa\",\n    rotate = \"varimax\",\n    SMC = TRUE\n)## Warning in fa.stats(r = r, f = f, phi = phi, n.obs = n.obs, np.obs = np.obs, :\n## The estimated weights for the factor scores are probably incorrect. Try a\n## different factor score estimation method.\n\n## Warning in fa.stats(r = r, f = f, phi = phi, n.obs = n.obs, np.obs = np.obs, :\n## An ultra-Heywood case was detected. Examine the results carefully\n## Make a data frame of the loadings for ggplot2\nfactors_df <-\n    bind_rows(\n        data.frame(\n            y = rownames(factor_pca_smc$loadings),\n            unclass(factor_pca_smc$loadings)\n        ),\n        data.frame(\n            y = rownames(factor_pca_smc_pro$loadings),\n            unclass(factor_pca_smc_pro$loadings)\n        ),\n        data.frame(\n            y = rownames(factor_pca_smc_var$loadings),\n            unclass(factor_pca_smc_var$loadings)\n        ),\n        .id = \"Rotation\"\n    )\nflag_gg <- ggplot(factors_df) +\n    geom_vline(aes(xintercept = 0)) +\n    geom_hline(aes(yintercept = 0)) +\n    geom_point(aes(\n        x = PA2,\n        y = PA1,\n        col = y,\n        shape = y\n    ), size = 2) +\n    scale_x_continuous(name = \"Factor 2\", limits = c(-1.1, 1.1)) +\n    scale_y_continuous(name = \"Factor1\", limits = c(-1.1, 1.1)) + facet_wrap(\"Rotation\",\n                                                                             labeller = labeller(Rotation = c(\n                                                                                 \"1\" = \"Original\", \"2\" = \"Promax\", \"3\" = \"Varimax\"\n                                                                             ))) +\n    coord_fixed(ratio = 1) # make aspect ratio of each facet 1\nflag_gg\n# promax and varimax did a good job to assign trait to a particular factor\n\nfactor_mle_1 <- fa(\n    Harman.5,\n    nfactors = 1,\n    fm = \"mle\",\n    rotate = \"none\",\n    SMC = TRUE\n)\nfactor_mle_1## Factor Analysis using method =  ml\n## Call: fa(r = Harman.5, nfactors = 1, rotate = \"none\", SMC = TRUE, fm = \"mle\")\n## Standardized loadings (pattern matrix) based upon correlation matrix\n##               ML1    h2     u2 com\n## population   0.97 0.950 0.0503   1\n## schooling    0.14 0.021 0.9791   1\n## employment   1.00 0.995 0.0049   1\n## professional 0.51 0.261 0.7388   1\n## housevalue   0.12 0.014 0.9864   1\n## \n##                 ML1\n## SS loadings    2.24\n## Proportion Var 0.45\n## \n## Mean item complexity =  1\n## Test of the hypothesis that 1 factor is sufficient.\n## \n## The degrees of freedom for the null model are  10  and the objective function was  6.38 with Chi Square of  54.25\n## The degrees of freedom for the model are 5  and the objective function was  3.14 \n## \n## The root mean square of the residuals (RMSR) is  0.41 \n## The df corrected root mean square of the residuals is  0.57 \n## \n## The harmonic number of observations is  12 with the empirical chi square  39.41  with prob <  2e-07 \n## The total number of observations was  12  with Likelihood Chi Square =  24.56  with prob <  0.00017 \n## \n## Tucker Lewis Index of factoring reliability =  0.022\n## RMSEA index =  0.564  and the 90 % confidence intervals are  0.374 0.841\n## BIC =  12.14\n## Fit based upon off diagonal values = 0.5\n## Measures of factor score adequacy             \n##                                                    ML1\n## Correlation of (regression) scores with factors   1.00\n## Multiple R square of scores with factors          1.00\n## Minimum correlation of possible factor scores     0.99\nfactor_mle_2 <- fa(\n    Harman.5,\n    nfactors = 2,\n    fm = \"mle\",\n    rotate = \"none\",\n    SMC = TRUE\n)\nfactor_mle_2## Factor Analysis using method =  ml\n## Call: fa(r = Harman.5, nfactors = 2, rotate = \"none\", SMC = TRUE, fm = \"mle\")\n## Standardized loadings (pattern matrix) based upon correlation matrix\n##                ML2  ML1   h2    u2 com\n## population   -0.03 1.00 1.00 0.005 1.0\n## schooling     0.90 0.04 0.81 0.193 1.0\n## employment    0.09 0.98 0.96 0.036 1.0\n## professional  0.78 0.46 0.81 0.185 1.6\n## housevalue    0.96 0.05 0.93 0.074 1.0\n## \n##                        ML2  ML1\n## SS loadings           2.34 2.16\n## Proportion Var        0.47 0.43\n## Cumulative Var        0.47 0.90\n## Proportion Explained  0.52 0.48\n## Cumulative Proportion 0.52 1.00\n## \n## Mean item complexity =  1.1\n## Test of the hypothesis that 2 factors are sufficient.\n## \n## The degrees of freedom for the null model are  10  and the objective function was  6.38 with Chi Square of  54.25\n## The degrees of freedom for the model are 1  and the objective function was  0.31 \n## \n## The root mean square of the residuals (RMSR) is  0.01 \n## The df corrected root mean square of the residuals is  0.05 \n## \n## The harmonic number of observations is  12 with the empirical chi square  0.05  with prob <  0.82 \n## The total number of observations was  12  with Likelihood Chi Square =  2.22  with prob <  0.14 \n## \n## Tucker Lewis Index of factoring reliability =  0.658\n## RMSEA index =  0.307  and the 90 % confidence intervals are  0 0.945\n## BIC =  -0.26\n## Fit based upon off diagonal values = 1\n## Measures of factor score adequacy             \n##                                                    ML2  ML1\n## Correlation of (regression) scores with factors   0.98 1.00\n## Multiple R square of scores with factors          0.95 1.00\n## Minimum correlation of possible factor scores     0.91 0.99\nfactor_mle_3 <- fa(\n    Harman.5,\n    nfactors = 3,\n    fm = \"mle\",\n    rotate = \"none\",\n    SMC = TRUE\n)\nfactor_mle_3## Factor Analysis using method =  ml\n## Call: fa(r = Harman.5, nfactors = 3, rotate = \"none\", SMC = TRUE, fm = \"mle\")\n## Standardized loadings (pattern matrix) based upon correlation matrix\n##                ML2  ML1   ML3   h2     u2 com\n## population   -0.12 0.98 -0.11 0.98 0.0162 1.1\n## schooling     0.89 0.15  0.29 0.90 0.0991 1.3\n## employment    0.00 1.00  0.04 0.99 0.0052 1.0\n## professional  0.72 0.52 -0.10 0.80 0.1971 1.9\n## housevalue    0.97 0.13 -0.09 0.97 0.0285 1.1\n## \n##                        ML2  ML1  ML3\n## SS loadings           2.28 2.26 0.11\n## Proportion Var        0.46 0.45 0.02\n## Cumulative Var        0.46 0.91 0.93\n## Proportion Explained  0.49 0.49 0.02\n## Cumulative Proportion 0.49 0.98 1.00\n## \n## Mean item complexity =  1.2\n## Test of the hypothesis that 3 factors are sufficient.\n## \n## The degrees of freedom for the null model are  10  and the objective function was  6.38 with Chi Square of  54.25\n## The degrees of freedom for the model are -2  and the objective function was  0 \n## \n## The root mean square of the residuals (RMSR) is  0 \n## The df corrected root mean square of the residuals is  NA \n## \n## The harmonic number of observations is  12 with the empirical chi square  0  with prob <  NA \n## The total number of observations was  12  with Likelihood Chi Square =  0  with prob <  NA \n## \n## Tucker Lewis Index of factoring reliability =  1.318\n## Fit based upon off diagonal values = 1\n## Measures of factor score adequacy             \n##                                                    ML2  ML1  ML3\n## Correlation of (regression) scores with factors   0.99 1.00 0.82\n## Multiple R square of scores with factors          0.98 1.00 0.68\n## Minimum correlation of possible factor scores     0.96 0.99 0.36"},{"path":"multivariate-methods.html","id":"discriminant-analysis","chapter":"17 Multivariate Methods","heading":"17.4 Discriminant Analysis","text":"Suppose two different populations observations come . Discriminant analysis seeks determine possible population observation comes making mistakes possibleThis alternative logistic approaches following advantages:\nclear separation classes, parameter estimates logic regression model can surprisingly unstable, discriminant approaches suffer\nX normal classes sample size small, discriminant approaches can accurate\nalternative logistic approaches following advantages:clear separation classes, parameter estimates logic regression model can surprisingly unstable, discriminant approaches sufferwhen clear separation classes, parameter estimates logic regression model can surprisingly unstable, discriminant approaches sufferIf X normal classes sample size small, discriminant approaches can accurateIf X normal classes sample size small, discriminant approaches can accurateNotationSImilar MANOVA, let \\(\\mathbf{y}_{j1},\\mathbf{y}_{j2},\\dots, \\mathbf{y}_{in_j} \\sim iid f_j (\\mathbf{y})\\) \\(j = 1,\\dots, h\\)Let \\(f_j(\\mathbf{y})\\) density function population j . Note vector \\(\\mathbf{y}\\) contain measurements \\(p\\) traitsAssume observation one \\(h\\) possible populations.want form discriminant rule allocate observation \\(\\mathbf{y}\\) population j \\(\\mathbf{y}\\) fact population","code":""},{"path":"multivariate-methods.html","id":"known-populations","chapter":"17 Multivariate Methods","heading":"17.4.1 Known Populations","text":"maximum likelihood discriminant rule assigning observation \\(\\mathbf{y}\\) one \\(h\\) populations allocates \\(\\mathbf{y}\\) population gives largest likelihood \\(\\mathbf{y}\\)Consider likelihood single observation \\(\\mathbf{y}\\), form \\(f_j (\\mathbf{y})\\) j true population.Since \\(j\\) unknown, make likelihood large possible, choose value j causes \\(f_j (\\mathbf{y})\\) large possibleConsider simple univariate example. Suppose data one two binomial populations.first population \\(n= 10\\) trials success probability \\(p = .5\\)first population \\(n= 10\\) trials success probability \\(p = .5\\)second population \\(n= 10\\) trials success probability \\(p = .7\\)second population \\(n= 10\\) trials success probability \\(p = .7\\)population assign observation \\(y = 7\\)population assign observation \\(y = 7\\)Note:\n\\(f(y = 7|n = 10, p = .5) = .117\\)\n\\(f(y = 7|n = 10, p = .7) = .267\\) \\(f(.)\\) binomial likelihood.\nHence, choose second population\nNote:\\(f(y = 7|n = 10, p = .5) = .117\\)\\(f(y = 7|n = 10, p = .5) = .117\\)\\(f(y = 7|n = 10, p = .7) = .267\\) \\(f(.)\\) binomial likelihood.\\(f(y = 7|n = 10, p = .7) = .267\\) \\(f(.)\\) binomial likelihood.Hence, choose second populationHence, choose second populationAnother exampleWe 2 populations, whereFirst population: \\(N(\\mu_1, \\sigma^2_1)\\)First population: \\(N(\\mu_1, \\sigma^2_1)\\)Second population: \\(N(\\mu_2, \\sigma^2_2)\\)Second population: \\(N(\\mu_2, \\sigma^2_2)\\)likelihood single observation \\[\nf_j (y) = (2\\pi \\sigma^2_j)^{-1/2} \\exp\\{ -\\frac{1}{2}(\\frac{y - \\mu_j}{\\sigma_j})^2\\}\n\\]Consider likelihood ratio rule\\[\n\\begin{aligned}\n\\Lambda &= \\frac{\\text{likelihood y pop 1}}{\\text{likelihood y pop 2}} \\\\\n&= \\frac{f_1(y)}{f_2(y)} \\\\\n&= \\frac{\\sigma_2}{\\sigma_1} \\exp\\{-\\frac{1}{2}[(\\frac{y - \\mu_1}{\\sigma_1})^2- (\\frac{y - \\mu_2}{\\sigma_2})^2] \\}\n\\end{aligned}\n\\]Hence, classify intopop 1 \\(\\Lambda >1\\)pop 1 \\(\\Lambda >1\\)pop 2 \\(\\Lambda <1\\)pop 2 \\(\\Lambda <1\\)ties, flip coinfor ties, flip coinAnother way think:classify population 1 “standardized distance” y \\(\\mu_1\\) less “standardized distance” y \\(\\mu_2\\) referred quadratic discriminant rule.(Significant simplification occurs th special case \\(\\sigma_1 = \\sigma_2 = \\sigma^2\\))Thus, classify population 1 \\[\n(y - \\mu_2)^2 > (y - \\mu_1)^2\n\\]\\[\n|y- \\mu_2| > |y - \\mu_1|\n\\]\\[\n-2 \\log (\\Lambda) = -2y  \\frac{(\\mu_1 - \\mu_2)}{\\sigma^2} + \\frac{(\\mu_1^2 - \\mu_2^2)}{\\sigma^2} = \\beta y + \\alpha\n\\]Thus, classify population 1 less 0.Discriminant classification rule linear y case.","code":""},{"path":"multivariate-methods.html","id":"multivariate-expansion","chapter":"17 Multivariate Methods","heading":"17.4.1.1 Multivariate Expansion","text":"Suppose 2 populations\\(N_p(\\mathbf{\\mu}_1, \\mathbf{\\Sigma}_1)\\)\\(N_p(\\mathbf{\\mu}_1, \\mathbf{\\Sigma}_1)\\)\\(N_p(\\mathbf{\\mu}_2, \\mathbf{\\Sigma}_2)\\)\\(N_p(\\mathbf{\\mu}_2, \\mathbf{\\Sigma}_2)\\)\\[\n\\begin{aligned}\n-2 \\log(\\frac{f_1 (\\mathbf{x})}{f_2 (\\mathbf{x})}) &= \\log|\\mathbf{\\Sigma}_1| + (\\mathbf{x} - \\mathbf{\\mu}_1)' \\mathbf{\\Sigma}^{-1}_1 (\\mathbf{x} - \\mathbf{\\mu}_1) \\\\\n&- [\\log|\\mathbf{\\Sigma}_2|+ (\\mathbf{x} - \\mathbf{\\mu}_2)' \\mathbf{\\Sigma}^{-1}_2 (\\mathbf{x} - \\mathbf{\\mu}_2) ]\n\\end{aligned}\n\\], classify population 1 less 0, otherwise, population 2. like univariate case non-equal variances, quadratic discriminant rule.covariance matrices equal: \\(\\mathbf{\\Sigma}_1 = \\mathbf{\\Sigma}_2 = \\mathbf{\\Sigma}_1\\) classify population 1 \\[\n(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)' \\mathbf{\\Sigma}^{-1}\\mathbf{x} - \\frac{1}{2} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)' \\mathbf{\\Sigma}^{-1} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2) \\ge 0\n\\]linear discriminant rule also referred Fisher’s linear discriminant functionBy assuming covariance matrices equal, assume shape orientation fo two populations must (can strong restriction)words, variable, can different mean variance.Note: LDA Bayes decision boundary linear. Hence, quadratic decision boundary might lead better classification. Moreover, assumption variance/covariance matrix across classes Gaussian densities imposes linear rule, allow predictors class follow MVN distribution class-specific mean vectors variance/covariance matrices, Quadratic Discriminant Analysis. hten, parameters estimate (gives flexibility LDA) cost variance (bias -variance tradeoff).\\(\\mathbf{\\mu}_1, \\mathbf{\\mu}_2, \\mathbf{\\Sigma}\\) known, probability misclassification can determined:\\[\n\\begin{aligned}\nP(2|1) &= P(\\text{calssify pop 2| x pop 1}) \\\\\n&= P((\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)' \\mathbf{\\Sigma}^{-1} \\mathbf{x} \\le \\frac{1}{2} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)' \\mathbf{\\Sigma}^{-1} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)|\\mathbf{x} \\sim N(\\mu_1, \\mathbf{\\Sigma}) \\\\\n&= \\Phi(-\\frac{1}{2} \\delta)\n\\end{aligned}\n\\]\\(\\delta^2 = (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)' \\mathbf{\\Sigma}^{-1} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)\\)\\(\\delta^2 = (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)' \\mathbf{\\Sigma}^{-1} (\\mathbf{\\mu}_1 - \\mathbf{\\mu}_2)\\)\\(\\Phi\\) standard normal cdf\\(\\Phi\\) standard normal cdfSuppose \\(h\\) possible populations, distributed \\(N_p (\\mathbf{\\mu}_p, \\mathbf{\\Sigma})\\). , maximum likelihood (linear) discriminant rule allocates \\(\\mathbf{y}\\) population j j minimizes squared Mahalanobis distance\\[\n(\\mathbf{y} - \\mathbf{\\mu}_j)' \\mathbf{\\Sigma}^{-1} (\\mathbf{y} - \\mathbf{\\mu}_j)\n\\]","code":""},{"path":"multivariate-methods.html","id":"bayes-discriminant-rules","chapter":"17 Multivariate Methods","heading":"17.4.1.2 Bayes Discriminant Rules","text":"know population j prior probabilities \\(\\pi_j\\) (assume \\(\\pi_j >0\\)) can form Bayes discriminant rule.rule allocates observation \\(\\mathbf{y}\\) population \\(\\pi_j f_j (\\mathbf{y})\\) maximized.Note:Maximum likelihood discriminant rule special case Bayes discriminant rule, sets \\(\\pi_j = 1/h\\)Optimal Properties Bayes Discriminant Ruleslet \\(p_{ii}\\) probability correctly assigning observation population ilet \\(p_{ii}\\) probability correctly assigning observation population ithen one rule (probabilities \\(p_{ii}\\) ) good another rule (probabilities \\(p_{ii}'\\) ) \\(p_{ii} \\ge p_{ii}'\\) \\(= 1,\\dots, h\\)one rule (probabilities \\(p_{ii}\\) ) good another rule (probabilities \\(p_{ii}'\\) ) \\(p_{ii} \\ge p_{ii}'\\) \\(= 1,\\dots, h\\)first rule better alternative \\(p_{ii} > p_{ii}'\\) least one .first rule better alternative \\(p_{ii} > p_{ii}'\\) least one .rule better alternative called admissibleA rule better alternative called admissibleBayes Discriminant Rules admissibleBayes Discriminant Rules admissibleIf utilized prior probabilities, can form posterior probability correct allocation, \\(\\sum_{=1}^h \\pi_i p_{ii}\\)utilized prior probabilities, can form posterior probability correct allocation, \\(\\sum_{=1}^h \\pi_i p_{ii}\\)Bayes Discriminant Rules largest possible posterior probability correct allocation respect priorBayes Discriminant Rules largest possible posterior probability correct allocation respect priorThese properties show Bayes Discriminant rule best approach.properties show Bayes Discriminant rule best approach.Unequal CostWe want consider cost misallocation Define \\(c_{ij}\\) cost associated allocation member population j population .want consider cost misallocation Define \\(c_{ij}\\) cost associated allocation member population j population .Assume \n\\(c_{ij} >0\\) \\(\\neq j\\)\n\\(c_{ij} = 0\\) \\(= j\\)\nAssume \\(c_{ij} >0\\) \\(\\neq j\\)\\(c_{ij} >0\\) \\(\\neq j\\)\\(c_{ij} = 0\\) \\(= j\\)\\(c_{ij} = 0\\) \\(= j\\)determine expected amount loss observation allocated population \\(\\sum_j c_{ij} p_{ij}\\) \\(p_{ij}s\\) probabilities allocating observation population j population iWe determine expected amount loss observation allocated population \\(\\sum_j c_{ij} p_{ij}\\) \\(p_{ij}s\\) probabilities allocating observation population j population iWe want minimize amount loss expected rule. Using Bayes Discrimination, allocate \\(\\mathbf{y}\\) population j minimizes \\(\\sum_{k \\neq j} c_{ij} \\pi_k f_k(\\mathbf{y})\\)want minimize amount loss expected rule. Using Bayes Discrimination, allocate \\(\\mathbf{y}\\) population j minimizes \\(\\sum_{k \\neq j} c_{ij} \\pi_k f_k(\\mathbf{y})\\)assign equal probabilities group get maximum likelihood type rule. , allocate \\(\\mathbf{y}\\) population j minimizes \\(\\sum_{k \\neq j}c_{jk} f_k(\\mathbf{y})\\)assign equal probabilities group get maximum likelihood type rule. , allocate \\(\\mathbf{y}\\) population j minimizes \\(\\sum_{k \\neq j}c_{jk} f_k(\\mathbf{y})\\)Example:Two binomial populations, size 10, probabilities \\(p_1 = .5\\) \\(p_2 = .7\\)probability first population .9However, suppose cost inappropriately allocating first population 1 cost incorrectly allocating second population 5.case, pick population 1 population 2In general, consider two regions, \\(R_1\\) \\(R_2\\) associated population 1 2:\\[\nR_1: \\frac{f_1 (\\mathbf{x})}{f_2 (\\mathbf{x})} \\ge \\frac{c_{12} \\pi_2}{c_{21} \\pi_1}\n\\]\\[\nR_2: \\frac{f_1 (\\mathbf{x})}{f_2 (\\mathbf{x})} < \\frac{c_{12} \\pi_2}{c_{21} \\pi_1}\n\\]\\(c_{12}\\) cost assigning member population 2 population 1.","code":""},{"path":"multivariate-methods.html","id":"discrimination-under-estimation","chapter":"17 Multivariate Methods","heading":"17.4.1.3 Discrimination Under Estimation","text":"Suppose know form distributions populations interests, still estimate parameters.Example:know distributions multivariate normal, estimate means variancesThe maximum likelihood discriminant rule allocates observation \\(\\mathbf{y}\\) population j j maximizes function\\[\nf_j (\\mathbf{y} |\\hat{\\theta})\n\\]\\(\\hat{\\theta}\\) maximum likelihood estimates unknown parametersFor instance, 2 multivariate normal populations distinct means, common variance covariance matrixMLEs \\(\\mathbf{\\mu}_1\\) \\(\\mathbf{\\mu}_2\\) \\(\\mathbf{\\bar{y}}_1\\) \\(\\mathbf{\\bar{y}}_2\\)common \\(\\mathbf{\\Sigma}\\) \\(\\mathbf{S}\\).Thus, estimated discriminant rule formed substituting sample values population values","code":""},{"path":"multivariate-methods.html","id":"native-bayes","chapter":"17 Multivariate Methods","heading":"17.4.1.4 Native Bayes","text":"challenge classification using Bayes’ don’t know (true) densities, \\(f_k, k = 1, \\dots, K\\), LDA QDA make strong multivariate normality assumptions deal .challenge classification using Bayes’ don’t know (true) densities, \\(f_k, k = 1, \\dots, K\\), LDA QDA make strong multivariate normality assumptions deal .Naive Bayes makes one assumption: within kth class, p predictors independent (.e,, \\(k = 1,\\dots, K\\)Naive Bayes makes one assumption: within kth class, p predictors independent (.e,, \\(k = 1,\\dots, K\\)\\[\nf_k(x) = f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\dots \\times f_{kp}(x_p)\n\\]\\(f_{kj}\\) density function j-th predictor among observation k-th class.assumption allows use joint distribution without need account dependence observations. However, (native) assumption can unrealistic, still works well cases number sample (n) large relative number features (p).assumption, \\[\nP(Y=k|X=x) = \\frac{\\pi_k \\times f_{k1}(x_1) \\times \\dots \\times f_{kp}(x_p)}{\\sum_{l=1}^K \\pi_l \\times f_{l1}(x_1)\\times \\dots f_{lp}(x_p)}\n\\]need estimate one-dimensional density function \\(f_{kj}\\) either approaches:\\(X_j\\) quantitative, assume univariate normal distribution (independence): \\(X_j | Y = k \\sim N(\\mu_{jk}, \\sigma^2_{jk})\\) restrictive QDA assumes predictors independent (e.g., diagonal covariance matrix)\\(X_j\\) quantitative, assume univariate normal distribution (independence): \\(X_j | Y = k \\sim N(\\mu_{jk}, \\sigma^2_{jk})\\) restrictive QDA assumes predictors independent (e.g., diagonal covariance matrix)\\(X_j\\) quantitative, use kernel density estimator Kernel Methods ; smoothed histogramWhen \\(X_j\\) quantitative, use kernel density estimator Kernel Methods ; smoothed histogramWhen \\(X_j\\) qualitative, count promotion training observations j-th predictor corresponding class.\\(X_j\\) qualitative, count promotion training observations j-th predictor corresponding class.","code":""},{"path":"multivariate-methods.html","id":"comparison-of-classification-methods","chapter":"17 Multivariate Methods","heading":"17.4.1.5 Comparison of Classification Methods","text":"Assuming K classes K baseline (James , Witten, Hastie, Tibshirani book)Comparing log odds relative K class","code":""},{"path":"multivariate-methods.html","id":"logistic-regression-1","chapter":"17 Multivariate Methods","heading":"17.4.1.5.1 Logistic Regression","text":"\\[\n\\log(\\frac{P(Y=k|X = x)}{P(Y = K| X = x)}) = \\beta_{k0} + \\sum_{j=1}^p \\beta_{kj}x_j\n\\]","code":""},{"path":"multivariate-methods.html","id":"lda","chapter":"17 Multivariate Methods","heading":"17.4.1.5.2 LDA","text":"\\[\n\\log(\\frac{P(Y = k | X = x)}{P(Y = K | X = x)} = a_k + \\sum_{j=1}^p b_{kj} x_j\n\\]\\(a_k\\) \\(b_{kj}\\) functions \\(\\pi_k, \\pi_K, \\mu_k , \\mu_K, \\mathbf{\\Sigma}\\)Similar logistic regression, LDA assumes log odds linear \\(x\\)Even though look like form, parameters logistic regression estimated MLE, LDA linear parameters specified prior normal distributionsWe expect LDA outperform logistic regression normality assumption (approximately) holds, logistic regression perform better ","code":""},{"path":"multivariate-methods.html","id":"qda","chapter":"17 Multivariate Methods","heading":"17.4.1.5.3 QDA","text":"\\[\n\\log(\\frac{P(Y=k|X=x}{P(Y=K | X = x}) = a_k + \\sum_{j=1}^{p}b_{kj}x_{j} + \\sum_{j=1}^p \\sum_{l=1}^p c_{kjl}x_j x_l \n\\]\\(a_k, b_{kj}, c_{kjl}\\) functions \\(\\pi_k , \\pi_K, \\mu_k, \\mu_K ,\\mathbf{\\Sigma}_k, \\mathbf{\\Sigma}_K\\)","code":""},{"path":"multivariate-methods.html","id":"naive-bayes","chapter":"17 Multivariate Methods","heading":"17.4.1.5.4 Naive Bayes","text":"\\[\n\\log (\\frac{P(Y = k | X = x)}{P(Y = K | X = x}) = a_k + \\sum_{j=1}^p g_{kj} (x_j)\n\\]\\(a_k = \\log (\\pi_k / \\pi_K)\\) \\(g_{kj}(x_j) = \\log(\\frac{f_{kj}(x_j)}{f_{Kj}(x_j)})\\) form generalized additive model","code":""},{"path":"multivariate-methods.html","id":"summary-5","chapter":"17 Multivariate Methods","heading":"17.4.1.5.5 Summary","text":"LDA special case QDALDA special case QDALDA robust comes high dimensionsLDA robust comes high dimensionsAny classifier linear decision boundary special case naive Bayes \\(g_{kj}(x_j) = b_{kj} x_j\\), means LDA special case naive Bayes. LDA assumes features normally distributed common within-class covariance matrix, naive Bayes assumes independence features.classifier linear decision boundary special case naive Bayes \\(g_{kj}(x_j) = b_{kj} x_j\\), means LDA special case naive Bayes. LDA assumes features normally distributed common within-class covariance matrix, naive Bayes assumes independence features.Naive bayes also special case LDA \\(\\mathbf{\\Sigma}\\) restricted diagonal matrix diagonals, \\(\\sigma^2\\) (another notation \\(diag (\\mathbf{\\Sigma})\\) ) assuming \\(f_{kj}(x_j) = N(\\mu_{kj}, \\sigma^2_j)\\)Naive bayes also special case LDA \\(\\mathbf{\\Sigma}\\) restricted diagonal matrix diagonals, \\(\\sigma^2\\) (another notation \\(diag (\\mathbf{\\Sigma})\\) ) assuming \\(f_{kj}(x_j) = N(\\mu_{kj}, \\sigma^2_j)\\)QDA naive Bayes special case . principal,e naive Bayes can produce flexible fit choice \\(g_{kj}(x_j)\\) , ’s restricted purely additive fit, QDA includes multiplicative terms form \\(c_{kjl}x_j x_l\\)QDA naive Bayes special case . principal,e naive Bayes can produce flexible fit choice \\(g_{kj}(x_j)\\) , ’s restricted purely additive fit, QDA includes multiplicative terms form \\(c_{kjl}x_j x_l\\)None methods uniformly dominates others: choice method depends true distribution predictors K classes, n p (.e., related bias-variance tradeoff).None methods uniformly dominates others: choice method depends true distribution predictors K classes, n p (.e., related bias-variance tradeoff).Compare non-parametric method (KNN)KNN outperform LDA logistic regression decision boundary highly nonlinear, can’t say predictors important, requires many observationsKNN outperform LDA logistic regression decision boundary highly nonlinear, can’t say predictors important, requires many observationsKNN also limited high-dimensions due curse dimensionalityKNN also limited high-dimensions due curse dimensionalitySince QDA special type nonlinear decision boundary (quadratic), can considered compromise linear methdos KNN classification. QDA can fewer training observations KNN flexible.Since QDA special type nonlinear decision boundary (quadratic), can considered compromise linear methdos KNN classification. QDA can fewer training observations KNN flexible.simulation:like linear regression, can also introduce flexibility including transformed features \\(\\sqrt{X}, X^2, X^3\\)","code":""},{"path":"multivariate-methods.html","id":"probabilities-of-misclassification","chapter":"17 Multivariate Methods","heading":"17.4.2 Probabilities of Misclassification","text":"distribution exactly known, can determine misclassification probabilities exactly. however, need estimate population parameters, estimate probability misclassificationNaive method\nPlugging parameters estimates form misclassification probabilities results derive estimates misclassification probability.\ntend optimistic number samples one populations small.\nNaive methodPlugging parameters estimates form misclassification probabilities results derive estimates misclassification probability.Plugging parameters estimates form misclassification probabilities results derive estimates misclassification probability.tend optimistic number samples one populations small.tend optimistic number samples one populations small.Resubstitution method\nUse proportion samples population allocated another population estimate misclassification probability\nalso optimistic number samples small\nResubstitution methodUse proportion samples population allocated another population estimate misclassification probabilityUse proportion samples population allocated another population estimate misclassification probabilityBut also optimistic number samples smallBut also optimistic number samples smallJack-knife estimates:\ntwo methods use observation estimate parameters also misclassification probabilities based upon discriminant rule\nAlternatively, determine discriminant rule based upon data except k-th observation j-th population\n, determine k-th observation misclassified rule\nperform process \\(n_j\\) observation population j . estimate fo misclassficaiton probability fraction \\(n_j\\) observations misclassified\nrepeat process \\(\\neq j\\) populations\nmethod reliable others, also computationally intensive\nJack-knife estimates:two methods use observation estimate parameters also misclassification probabilities based upon discriminant ruleThe two methods use observation estimate parameters also misclassification probabilities based upon discriminant ruleAlternatively, determine discriminant rule based upon data except k-th observation j-th populationAlternatively, determine discriminant rule based upon data except k-th observation j-th populationthen, determine k-th observation misclassified rulethen, determine k-th observation misclassified ruleperform process \\(n_j\\) observation population j . estimate fo misclassficaiton probability fraction \\(n_j\\) observations misclassifiedperform process \\(n_j\\) observation population j . estimate fo misclassficaiton probability fraction \\(n_j\\) observations misclassifiedrepeat process \\(\\neq j\\) populationsrepeat process \\(\\neq j\\) populationsThis method reliable others, also computationally intensiveThis method reliable others, also computationally intensiveCross-ValidationCross-ValidationSummaryConsider group-specific densities \\(f_j (\\mathbf{x})\\) multivariate vector \\(\\mathbf{x}\\).Assume equal misclassification costs, Bayes classification probability \\(\\mathbf{x}\\) belonging j-th population \\[\np(j |\\mathbf{x}) = \\frac{\\pi_j f_j (\\mathbf{x})}{\\sum_{k=1}^h \\pi_k f_k (\\mathbf{x})}\n\\]\\(j = 1,\\dots, h\\)\\(h\\) possible groups.classify group probability membership largestAlternatively, can write terms generalized squared distance formation\\[\nD_j^2 (\\mathbf{x}) = d_j^2 (\\mathbf{x})+ g_1(j) + g_2 (j)\n\\]\\(d_j^2(\\mathbf{x}) = (\\mathbf{x} - \\mathbf{\\mu}_j)' \\mathbf{V}_j^{-1} (\\mathbf{x} - \\mathbf{\\mu}_j)\\) squared Mahalanobis distance \\(\\mathbf{x}\\) centroid group j, \n\\(\\mathbf{V}_j = \\mathbf{S}_j\\) within group covariance matrices equal\n\\(\\mathbf{V}_j = \\mathbf{S}_p\\) pooled covariance estimate appropriate\n\\(d_j^2(\\mathbf{x}) = (\\mathbf{x} - \\mathbf{\\mu}_j)' \\mathbf{V}_j^{-1} (\\mathbf{x} - \\mathbf{\\mu}_j)\\) squared Mahalanobis distance \\(\\mathbf{x}\\) centroid group j, \\(\\mathbf{V}_j = \\mathbf{S}_j\\) within group covariance matrices equal\\(\\mathbf{V}_j = \\mathbf{S}_j\\) within group covariance matrices equal\\(\\mathbf{V}_j = \\mathbf{S}_p\\) pooled covariance estimate appropriate\\(\\mathbf{V}_j = \\mathbf{S}_p\\) pooled covariance estimate appropriateand\\[\ng_1(j) =\n\\begin{cases}\n\\ln |\\mathbf{S}_j| & \\text{within group covariances equal} \\\\\n0 & \\text{pooled covariance}\n\\end{cases}\n\\]\\[\ng_2(j) = \n\\begin{cases}\n-2 \\ln \\pi_j & \\text{prior probabilities equal} \\\\\n0 & \\text{prior probabilities equal}\n\\end{cases}\n\\], posterior probability belonging group j \\[\np(j| \\mathbf{x})  = \\frac{\\exp(-.5 D_j^2(\\mathbf{x}))}{\\sum_{k=1}^h \\exp(-.5 D^2_k (\\mathbf{x}))}\n\\]\\(j = 1,\\dots , h\\)\\(\\mathbf{x}\\) classified group j \\(p(j | \\mathbf{x})\\) largest \\(j = 1,\\dots,h\\) (, \\(D_j^2(\\mathbf{x})\\) smallest).","code":""},{"path":"multivariate-methods.html","id":"assessing-classification-performance","chapter":"17 Multivariate Methods","heading":"17.4.2.1 Assessing Classification Performance","text":"binary classification, confusion matrixand table 4.6 (James et al. 2013)ROC curve (receiver Operating Characteristics) graphical comparison sensitivity (true positive) specificity ( = 1 - false positive)y-axis = true positive ratex-axis = false positive rateas change threshold rate classifying observation 0 1AUC (area ROC) ideally equal 1, bad classifier AUC = 0.5 (pure chance)","code":""},{"path":"multivariate-methods.html","id":"unknown-populations-nonparametric-discrimination","chapter":"17 Multivariate Methods","heading":"17.4.3 Unknown Populations/ Nonparametric Discrimination","text":"multivariate data Gaussian, known distributional form , can use following methods","code":""},{"path":"multivariate-methods.html","id":"kernel-methods","chapter":"17 Multivariate Methods","heading":"17.4.3.1 Kernel Methods","text":"approximate \\(f_j (\\mathbf{x})\\) kernel density estimate\\[\n\\hat{f}_j(\\mathbf{x}) = \\frac{1}{n_j} \\sum_{= 1}^{n_j} K_j (\\mathbf{x} - \\mathbf{x}_i)\n\\]\\(K_j (.)\\) kernel function satisfying \\(\\int K_j(\\mathbf{z})d\\mathbf{z} =1\\)\\(K_j (.)\\) kernel function satisfying \\(\\int K_j(\\mathbf{z})d\\mathbf{z} =1\\)\\(\\mathbf{x}_i\\) , \\(= 1,\\dots , n_j\\) random sample j-th population.\\(\\mathbf{x}_i\\) , \\(= 1,\\dots , n_j\\) random sample j-th population.Thus, finding \\(\\hat{f}_j (\\mathbf{x})\\) \\(h\\) populations, posterior probability group membership \\[\np(j |\\mathbf{x}) = \\frac{\\pi_j \\hat{f}_j (\\mathbf{x})}{\\sum_{k-1}^h \\pi_k \\hat{f}_k (\\mathbf{x})}\n\\]\\(j = 1,\\dots, h\\)different choices kernel funciton:UniformUniformNormalNormalEpanechnikovEpanechnikovBiweightBiweightTriweightTriweightWe kernels, pick “radius” (variance, width, window width, bandwidth) kernel, smoothing parameter (larger radius, smooth kernel estimate density).select smoothness parameter, can use following methodIf believe populations close multivariate normal, \\[\nR = (\\frac{4/(2p+1)}{n_j})^{1/(p+1}\n\\]since know sure, might choose several different values select one vies best sample cross-validation discrimination.Moreover, also decide whether use different kernel smoothness different populations, similar individual andpooled covariances classical methodology.","code":""},{"path":"multivariate-methods.html","id":"nearest-neighbor-methods","chapter":"17 Multivariate Methods","heading":"17.4.3.2 Nearest Neighbor Methods","text":"nearest neighbor (also known k-nearest neighbor) method performs classification new observation vector based group membership nearest neighbors. practice, find\\[\nd_{ij}^2 (\\mathbf{x}, \\mathbf{x}_i) = (\\mathbf{x}, \\mathbf{x}_i) V_j^{-1}(\\mathbf{x}, \\mathbf{x}_i)\n\\]distance vector \\(\\mathbf{x}\\) -th observation group jWe consider different choices \\(\\mathbf{V}_j\\)example,\\[\n\\mathbf{V}_j = \\mathbf{S}_p \\\\\n\\mathbf{V}_j = \\mathbf{S}_j \\\\\n\\mathbf{V}_j = \\mathbf{} \\\\\n\\mathbf{V}_j = diag (\\mathbf{S}_p)\n\\]find \\(k\\) observations closest \\(\\mathbf{x}\\) (users pick k). classify common population, weighted prior.","code":""},{"path":"multivariate-methods.html","id":"modern-discriminant-methods","chapter":"17 Multivariate Methods","heading":"17.4.3.3 Modern Discriminant Methods","text":"Note:Logistic regression (without random effects) flexible model-based procedure classification two populations.extension logistic regression multi-group setting polychotomous logistic regression (, mulinomial regression).machine learning pattern recognition growing strong focus nonlienar discriminant analysis methods :radial basis function networksradial basis function networkssupport vector machinessupport vector machinesmultiplayer perceptrons (neural networks)multiplayer perceptrons (neural networks)general framework\\[\ng_j (\\mathbf{x}) = \\sum_{l = 1}^m w_{jl}\\phi_l (\\mathbf{x}; \\mathbf{\\theta}_l) + w_{j0}\n\\]\\(j = 1,\\dots, h\\)\\(j = 1,\\dots, h\\)\\(m\\) nonlinear basis functions \\(\\phi_l\\), \\(n_m\\) parameters given \\(\\theta_l = \\{ \\theta_{lk}: k = 1, \\dots , n_m \\}\\)\\(m\\) nonlinear basis functions \\(\\phi_l\\), \\(n_m\\) parameters given \\(\\theta_l = \\{ \\theta_{lk}: k = 1, \\dots , n_m \\}\\)assign \\(\\mathbf{x}\\) j-th population \\(g_j(\\mathbf{x})\\) maximum \\(j = 1,\\dots, h\\)Development usually focuses choice estimation basis functions, \\(\\phi_l\\) estimation weights \\(w_{jl}\\)[details](Statistical Pattern Recognition | Wiley Online Books)","code":""},{"path":"multivariate-methods.html","id":"application-13","chapter":"17 Multivariate Methods","heading":"17.4.4 Application","text":"","code":"\nlibrary(class)## Warning: package 'class' was built under R version 4.0.5## \n## Attaching package: 'class'## The following object is masked from 'package:reshape':\n## \n##     condense\nlibrary(klaR)## Warning: package 'klaR' was built under R version 4.0.5\nlibrary(MASS)\nlibrary(tidyverse)\n\n## Read in the data\ncrops <- read.table(\"images/crops.txt\")\nnames(crops) <- c(\"crop\", \"y1\", \"y2\", \"y3\", \"y4\")\nstr(crops)## 'data.frame':    36 obs. of  5 variables:\n##  $ crop: chr  \"Corn\" \"Corn\" \"Corn\" \"Corn\" ...\n##  $ y1  : int  16 15 16 18 15 15 12 20 24 21 ...\n##  $ y2  : int  27 23 27 20 15 32 15 23 24 25 ...\n##  $ y3  : int  31 30 27 25 31 32 16 23 25 23 ...\n##  $ y4  : int  33 30 26 23 32 15 73 25 32 24 ...\n## Read in test data\ncrops_test <- read.table(\"images/crops_test.txt\")\nnames(crops_test) <- c(\"crop\", \"y1\", \"y2\", \"y3\", \"y4\")\nstr(crops_test)## 'data.frame':    5 obs. of  5 variables:\n##  $ crop: chr  \"Corn\" \"Soybeans\" \"Cotton\" \"Sugarbeets\" ...\n##  $ y1  : int  16 21 29 54 32\n##  $ y2  : int  27 25 24 23 32\n##  $ y3  : int  31 23 26 21 62\n##  $ y4  : int  33 24 28 54 16"},{"path":"multivariate-methods.html","id":"lda-1","chapter":"17 Multivariate Methods","heading":"17.4.4.1 LDA","text":"Default prior proportional sample size lda qda fit constant intercept termLDA didn’t well within sample --sample data.","code":"\n## Linear discriminant analysis\nlda_mod <- lda(crop ~ y1 + y2 + y3 + y4,\n               data = crops)\nlda_mod## Call:\n## lda(crop ~ y1 + y2 + y3 + y4, data = crops)\n## \n## Prior probabilities of groups:\n##     Clover       Corn     Cotton   Soybeans Sugarbeets \n##  0.3055556  0.1944444  0.1666667  0.1666667  0.1666667 \n## \n## Group means:\n##                  y1       y2       y3       y4\n## Clover     46.36364 32.63636 34.18182 36.63636\n## Corn       15.28571 22.71429 27.42857 33.14286\n## Cotton     34.50000 32.66667 35.00000 39.16667\n## Soybeans   21.00000 27.00000 23.50000 29.66667\n## Sugarbeets 31.00000 32.16667 20.00000 40.50000\n## \n## Coefficients of linear discriminants:\n##              LD1          LD2         LD3          LD4\n## y1 -6.147360e-02  0.009215431 -0.02987075 -0.014680566\n## y2 -2.548964e-02  0.042838972  0.04631489  0.054842132\n## y3  1.642126e-02 -0.079471595  0.01971222  0.008938745\n## y4  5.143616e-05 -0.013917423  0.05381787 -0.025717667\n## \n## Proportion of trace:\n##    LD1    LD2    LD3    LD4 \n## 0.7364 0.1985 0.0576 0.0075\n## Look at accuracy on the training data\nlda_fitted <- predict(lda_mod,newdata = crops)\n# Contingency table\nlda_table <- table(truth = crops$crop, fitted = lda_fitted$class)\nlda_table##             fitted\n## truth        Clover Corn Cotton Soybeans Sugarbeets\n##   Clover          6    0      3        0          2\n##   Corn            0    6      0        1          0\n##   Cotton          3    0      1        2          0\n##   Soybeans        0    1      1        3          1\n##   Sugarbeets      1    1      0        2          2\n# accuracy of 0.5 is just random (not good)\n\n## Posterior probabilities of membership\ncrops_post <- cbind.data.frame(crops,\n                               crop_pred = lda_fitted$class,\n                               lda_fitted$posterior)\ncrops_post <- crops_post %>%\n    mutate(missed = crop != crop_pred)\nhead(crops_post)##   crop y1 y2 y3 y4 crop_pred     Clover      Corn    Cotton  Soybeans\n## 1 Corn 16 27 31 33      Corn 0.08935164 0.4054296 0.1763189 0.2391845\n## 2 Corn 15 23 30 30      Corn 0.07690181 0.4558027 0.1420920 0.2530101\n## 3 Corn 16 27 27 26      Corn 0.09817815 0.3422454 0.1365315 0.3073105\n## 4 Corn 18 20 25 23      Corn 0.10521511 0.3633673 0.1078076 0.3281477\n## 5 Corn 15 15 31 32      Corn 0.05879921 0.5753907 0.1173332 0.2086696\n## 6 Corn 15 32 32 15  Soybeans 0.09723648 0.3278382 0.1318370 0.3419924\n##   Sugarbeets missed\n## 1 0.08971545  FALSE\n## 2 0.07219340  FALSE\n## 3 0.11573442  FALSE\n## 4 0.09546233  FALSE\n## 5 0.03980738  FALSE\n## 6 0.10109590   TRUE\n# posterior shows that posterior of corn membershp is much higher than the prior\n\n## LOOCV\n# leave-one-out cross validation for linear discriminant analysis\n# cannot run the prdecit function using the object with CV = TRUE because it returns the wihtin sample predictions\nlda_cv <- lda(crop ~ y1 + y2 + y3 + y4,\n              data = crops, CV = TRUE)\n# Contingency table\nlda_table_cv <- table(truth = crops$crop, fitted = lda_cv$class)\nlda_table_cv##             fitted\n## truth        Clover Corn Cotton Soybeans Sugarbeets\n##   Clover          4    3      1        0          3\n##   Corn            0    4      1        2          0\n##   Cotton          3    0      0        2          1\n##   Soybeans        0    1      1        3          1\n##   Sugarbeets      2    1      0        2          1\n## Predict the test data\nlda_pred <- predict(lda_mod, newdata = crops_test)\n\n## Make a contingency table with truth and most likely class\ntable(truth=crops_test$crop, predict=lda_pred$class)##             predict\n## truth        Clover Corn Cotton Soybeans Sugarbeets\n##   Clover          0    0      1        0          0\n##   Corn            0    1      0        0          0\n##   Cotton          0    0      0        1          0\n##   Soybeans        0    0      0        1          0\n##   Sugarbeets      1    0      0        0          0"},{"path":"multivariate-methods.html","id":"qda-1","chapter":"17 Multivariate Methods","heading":"17.4.4.2 QDA","text":"","code":"\n## Quadratic discriminant analysis\nqda_mod <- qda(crop ~ y1 + y2 + y3 + y4,\n               data = crops)\n\n## Look at accuracy on the training data\nqda_fitted <- predict(qda_mod, newdata = crops)\n# Contingency table\nqda_table <- table(truth = crops$crop, fitted = qda_fitted$class)\nqda_table##             fitted\n## truth        Clover Corn Cotton Soybeans Sugarbeets\n##   Clover          9    0      0        0          2\n##   Corn            0    7      0        0          0\n##   Cotton          0    0      6        0          0\n##   Soybeans        0    0      0        6          0\n##   Sugarbeets      0    0      1        1          4\n## LOOCV\nqda_cv <- qda(crop ~ y1 + y2 + y3 + y4,\n              data = crops, CV = TRUE)\n# Contingency table\nqda_table_cv <- table(truth = crops$crop, fitted = qda_cv$class)\nqda_table_cv##             fitted\n## truth        Clover Corn Cotton Soybeans Sugarbeets\n##   Clover          9    0      0        0          2\n##   Corn            3    2      0        0          2\n##   Cotton          3    0      2        0          1\n##   Soybeans        3    0      0        2          1\n##   Sugarbeets      3    0      1        1          1\n## Predict the test data\nqda_pred <- predict(qda_mod, newdata = crops_test)\n## Make a contingency table with truth and most likely class\ntable(truth = crops_test$crop, predict = qda_pred$class)##             predict\n## truth        Clover Corn Cotton Soybeans Sugarbeets\n##   Clover          1    0      0        0          0\n##   Corn            0    1      0        0          0\n##   Cotton          0    0      1        0          0\n##   Soybeans        0    0      0        1          0\n##   Sugarbeets      0    0      0        0          1"},{"path":"multivariate-methods.html","id":"knn-1","chapter":"17 Multivariate Methods","heading":"17.4.4.3 KNN","text":"knn uses design matrices features.","code":"\n## Design matrices\nX_train <- crops %>%\n    dplyr::select(-crop)\nX_test <- crops_test %>%\n    dplyr::select(-crop)\nY_train <- crops$crop\nY_test <- crops_test$crop\n\n## Nearest neighbors with 2 neighbors\nknn_2 <- knn(X_train, X_train, Y_train, k = 2)\ntable(truth = Y_train, fitted = knn_2)##             fitted\n## truth        Clover Corn Cotton Soybeans Sugarbeets\n##   Clover          6    0      3        1          1\n##   Corn            0    7      0        0          0\n##   Cotton          1    0      4        0          1\n##   Soybeans        0    0      0        4          2\n##   Sugarbeets      0    0      1        1          4\n## Accuracy\nmean(Y_train==knn_2)## [1] 0.6944444\n## Performance on test data\nknn_2_test <- knn(X_train, X_test, Y_train, k = 2)\ntable(truth = Y_test, predict = knn_2_test)##             predict\n## truth        Clover Corn Cotton Soybeans Sugarbeets\n##   Clover          1    0      0        0          0\n##   Corn            0    1      0        0          0\n##   Cotton          0    0      0        0          1\n##   Soybeans        0    0      0        1          0\n##   Sugarbeets      0    0      0        0          1\n## Accuracy\nmean(Y_test==knn_2_test)## [1] 0.8\n## Nearest neighbors with 3 neighbors\nknn_3 <- knn(X_train, X_train, Y_train, k = 3)\ntable(truth = Y_train, fitted = knn_3)##             fitted\n## truth        Clover Corn Cotton Soybeans Sugarbeets\n##   Clover          7    0      3        1          0\n##   Corn            0    4      0        3          0\n##   Cotton          1    0      4        0          1\n##   Soybeans        0    1      0        3          2\n##   Sugarbeets      0    0      0        1          5\n## Accuracy\nmean(Y_train==knn_3)## [1] 0.6388889\n## Performance on test data\nknn_3_test <- knn(X_train, X_test, Y_train, k = 3)\ntable(truth = Y_test, predict = knn_3_test)##             predict\n## truth        Clover Corn Cotton Soybeans Sugarbeets\n##   Clover          1    0      0        0          0\n##   Corn            0    1      0        0          0\n##   Cotton          0    0      1        0          0\n##   Soybeans        0    0      0        1          0\n##   Sugarbeets      0    0      0        0          1\n## Accuracy\nmean(Y_test==knn_3_test)## [1] 1"},{"path":"multivariate-methods.html","id":"stepwise","chapter":"17 Multivariate Methods","heading":"17.4.4.4 Stepwise","text":"Stepwise discriminant analysis using stepclass function klaR package.Iris Data","code":"\nstep <- stepclass(\n    crop ~ y1 + y2 + y3 + y4,\n    data = crops,\n    method = \"qda\",\n    improvement = 0.15\n)##  `stepwise classification', using 10-fold cross-validated correctness rate of method qda'.## 36 observations of 4 variables in 5 classes; direction: both## stop criterion: improvement less than 15%.## correctness rate: 0.46667;  in: \"y1\";  variables (1): y1 \n## \n##  hr.elapsed min.elapsed sec.elapsed \n##        0.00        0.00        0.14\nstep$process##    step var varname result.pm\n## 0 start   0      -- 0.0000000\n## 1    in   1      y1 0.4666667\nstep$performance.measure## [1] \"correctness rate\"\nlibrary(dplyr)\ndata('iris')\nset.seed(1)\nsamp <-\n    sample.int(nrow(iris), size = floor(0.70 * nrow(iris)), replace = F)\n\ntrain.iris <- iris[samp,] %>% mutate_if(is.numeric,scale)\ntest.iris <- iris[-samp,] %>% mutate_if(is.numeric,scale)\n\nlibrary(ggplot2)\niris.model <- lda(Species ~ ., data = train.iris)\n#pred\npred.lda <- predict(iris.model, test.iris)\ntable(truth = test.iris$Species, prediction = pred.lda$class)##             prediction\n## truth        setosa versicolor virginica\n##   setosa         15          0         0\n##   versicolor      0         17         0\n##   virginica       0          0        13\nplot(iris.model)\niris.model.qda <- qda(Species~.,data=train.iris)\n#pred\npred.qda <- predict(iris.model.qda,test.iris)\ntable(truth=test.iris$Species,prediction=pred.qda$class)##             prediction\n## truth        setosa versicolor virginica\n##   setosa         15          0         0\n##   versicolor      0         16         1\n##   virginica       0          0        13"},{"path":"multivariate-methods.html","id":"pca-with-discriminant-analysis","chapter":"17 Multivariate Methods","heading":"17.4.4.5 PCA with Discriminant Analysis","text":"can use PCA dimension reduction discriminant analysis","code":"\nzeros <- as.matrix(read.table(\"images/mnist0_train_b.txt\"))\nnines <- as.matrix(read.table(\"images/mnist9_train_b.txt\"))\ntrain <- rbind(zeros[1:1000, ], nines[1:1000, ])\ntrain <- train / 255 #divide by 255 per notes (so ranges from 0 to 1)\ntrain <- t(train) #each column is an observation\nimage(matrix(train[, 1], nrow = 28), main = 'Example image, unrotated')\ntest <- rbind(zeros[2501:3000, ], nines[2501:3000, ])\ntest <- test / 255\ntest <- t(test)\ny.train <- c(rep(0, 1000), rep(9, 1000))\ny.test <- c(rep(0, 500), rep(9, 500))\n\n\nlibrary(MASS)\npc <- prcomp(t(train))\ntrain.large <- data.frame(cbind(y.train, pc$x[, 1:10]))\nlarge <- lda(y.train ~ ., data = train.large)\n#the test data set needs to be constucted w/ the same 10 princomps\ntest.large <- data.frame(cbind(y.test, predict(pc, t(test))[, 1:10]))\npred.lda <- predict(large, test.large)\ntable(truth = test.large$y.test, prediction = pred.lda$class)##      prediction\n## truth   0   9\n##     0 491   9\n##     9   5 495\nlarge.qda <- qda(y.train~.,data=train.large)\n#prediction\npred.qda <- predict(large.qda,test.large)\ntable(truth=test.large$y.test,prediction=pred.qda$class)##      prediction\n## truth   0   9\n##     0 493   7\n##     9   3 497"},{"path":"causal-inference.html","id":"causal-inference","chapter":"18 Causal Inference","heading":"18 Causal Inference","text":"mumbo jumbo learned far, want now talk concept causality. usually say correlation causation. , causation?\nOne favorite books explained concept beautifully (Mackenzie Pearl 2018). just going quickly summarize gist understanding. hope can give initial grasp concept later can continue read develop deeper understanding.’s important deep understanding regarding method research. However, one needs aware limitation. mentioned various sections throughout book, see need ask experts number baseline visit literature gain insight past research., dive conceptual side statistical analysis whole, regardless particular approach.probably heard scientists say correlation doesn’t mean causation. ridiculous spurious correlations give firm grip previous phrase means. pioneer tried use regression infer causation social science Yule (1899) (fatal attempt found relief policy increases poverty). make causal inference statistics, equation (function form) must stable intervention (.e., variables manipulated). Statistics used causality-free enterprise past.development path analysis Sewall Wright 1920s discipline started pay attention causation. , remained dormant Causal Revolution (quoted Judea Pearl’s words). revolution introduced calculus causation includes (1) causal diagrams), (2) symbolic languageThe world using \\(P(Y|X)\\) (statistics use derive ), want compare difference \\(P(Y|(X))\\): treatment group\\(P(Y|(X))\\): treatment group\\(P(Y|(-X))\\): control group\\(P(Y|(-X))\\): control groupHence, can see clear difference \\(P(Y|X) \\neq P(Y|(X))\\)conclusion want make data counterfactuals: happened X?teach robot make inference, need inference enginep. 12 (Mackenzie Pearl 2018)Levels cognitive ability causal learner:SeeingDoingImaginingLadder causation (associated levels cognitive ability well):Association: conditional probability, correlation, regressionInterventionCounterfactualsAssociation\\(P(y|x)\\)?seeing X change belief Y?Intervention\\(P(y|(x),z)\\)DoingInterveningWhat ?X?Counterfactuals\\(P(y_x|x',y')\\)?\nX caused Y?acted differentlyTable (Pearl 2019, 2)define causation probability aloneIf say X causes Y X raises probability Y.\" surface, might sound intuitively right. translate probability notation: \\(P(Y|X) >P(Y)\\) , can’t wrong. Just seeing X (1st level), doesn’t mean probability Y increases.either (1) X causes Y, (2) Z affects X Y. Hence, people might use control variables, translate: \\(P(Y|X, Z=z) > P(Y|Z=z)\\), can confident probabilistic observation. However, question can choose \\(Z\\)invention -operator, now can represent X causes Y \\[\nP(Y|(X)) > P(Y)\n\\]help causal diagram, now can answer questions 2nd level (Intervention)Note: people econometrics might still use “Granger causality” “vector autoregression” use probability language represent causality (’s ).7 tools Structural Causal Model framework (Pearl 2019):Encoding Causal Assumptions - transparency testability (graphical representation)Encoding Causal Assumptions - transparency testability (graphical representation)-calculus control confounding: “back-door”-calculus control confounding: “back-door”algorithmization CounterfactualsThe algorithmization CounterfactualsMediation Analysis Assessment Direct Indirect EffectsMediation Analysis Assessment Direct Indirect EffectsAdaptability, External validity Sample Selection Bias: still researched “domain adaptation,” “transfer learning”Adaptability, External validity Sample Selection Bias: still researched “domain adaptation,” “transfer learning”Recovering missing dataRecovering missing dataCausal Discovery:\nd-separation\nFunctional decomposition (Hoyer et al. 2008; Shimizu, Hoyer, Hyvärinen 2009; Chen Chan 2012)\nSpontaneous local changes (Pearl 2013)\nCausal Discovery:d-separationd-separationFunctional decomposition (Hoyer et al. 2008; Shimizu, Hoyer, Hyvärinen 2009; Chen Chan 2012)Functional decomposition (Hoyer et al. 2008; Shimizu, Hoyer, Hyvärinen 2009; Chen Chan 2012)Spontaneous local changes (Pearl 2013)Spontaneous local changes (Pearl 2013)Simpson’s Paradox:statistical association seen entire population reversed sub-population.Structural Causal Model accompanies graphical causal model create efficient language represent causalityStructural Causal Model solution curse dimensionality (.e., large numbers variable \\(p\\), small dataset \\(n\\)) thanks product decomposition. allows us solve problems without knowing function, parameters, distributions error terms.Suppose causal chain \\(X \\Y \\Z\\):\\[\nP(X=x,Y=y, Z=z) = P(X=x)P(Y=y|X=x)P(Z=z|Y=y)\n\\]Tools hierarchical orderExperimental Design: Randomized Control Trials (Gold standard): Tier 1Experimental Design: Randomized Control Trials (Gold standard): Tier 1Quasi-experimental\nRegression Discontinuity Tier 1A\nDifference--Differences Tier 2\nSynthetic Control Tier 2A\nFixed Effects Estimator 12.4.2.2: Tier 3\nEndogenous Treatment: mostly Instrumental Variable: Tier 3A\nMatching Methods Tier 4\nInterrupted Time Series Tier 4A\nEndogenous Sample Selection 21.4: mostly Heckman’s correction\nQuasi-experimentalRegression Discontinuity Tier 1ARegression Discontinuity Tier 1ADifference--Differences Tier 2Difference--Differences Tier 2Synthetic Control Tier 2ASynthetic Control Tier 2AFixed Effects Estimator 12.4.2.2: Tier 3Fixed Effects Estimator 12.4.2.2: Tier 3Endogenous Treatment: mostly Instrumental Variable: Tier 3AEndogenous Treatment: mostly Instrumental Variable: Tier 3AMatching Methods Tier 4Matching Methods Tier 4Interrupted Time Series Tier 4AInterrupted Time Series Tier 4AEndogenous Sample Selection 21.4: mostly Heckman’s correctionEndogenous Sample Selection 21.4: mostly Heckman’s correctionInternal vs. External ValidityInternal Validity: Economists applied scientists mostly care aboutInternal Validity: Economists applied scientists mostly care aboutExternal Validity: Localness might affect external validityExternal Validity: Localness might affect external validityFor many economic policies, difference treatment intention treat.example, might effective vaccine (.e., intention treat), mean everybody take (.e., treatment).four types subjects deal :Non-switchers: don’t care non-switchers even introduce don’t introduce intervention, won’t affect .\nAlways takers\nNever takers\nNon-switchers: don’t care non-switchers even introduce don’t introduce intervention, won’t affect .Always takersAlways takersNever takersNever takersSwitchers\nCompliers: defined respect intervention.\ncare compliers introduce intervention, something. don’t interventions, won’t .\nTools used identify causal impact intervention compliers\ncompliers dataset, intention treatment = treatment effect.\n\nDefiers: go opposite direction treatment.\ntypically aren’t interested defiers opposite want . typically small group; hence, just assume don’t exist.\n\nSwitchersCompliers: defined respect intervention.\ncare compliers introduce intervention, something. don’t interventions, won’t .\nTools used identify causal impact intervention compliers\ncompliers dataset, intention treatment = treatment effect.\nCompliers: defined respect intervention.care compliers introduce intervention, something. don’t interventions, won’t .care compliers introduce intervention, something. don’t interventions, won’t .Tools used identify causal impact intervention compliersTools used identify causal impact intervention compliersIf compliers dataset, intention treatment = treatment effect.compliers dataset, intention treatment = treatment effect.Defiers: go opposite direction treatment.\ntypically aren’t interested defiers opposite want . typically small group; hence, just assume don’t exist.\nDefiers: go opposite direction treatment.typically aren’t interested defiers opposite want . typically small group; hence, just assume don’t exist.Directional Bias due selection treatment comes 2 general opposite sourcesMitigation-based: select treatment combat problemPreference-based: select treatment units like kind treatment.section based Bernard Koch’s presentaiton SICSS - Los Angeles 2021Identification Selection observable/ back-door criterionConditions:Strong conditional ignorability\n\\(Y(0),Y(1) \\perp T|X\\)\nhidden confounders\nStrong conditional ignorability\\(Y(0),Y(1) \\perp T|X\\)\\(Y(0),Y(1) \\perp T|X\\)hidden confoundersNo hidden confoundersOverlap\n\\(\\forall x \\X, t \\\\{0, 1\\}: p (T = t | X = x> 0\\)\ntreatments non-zero probability observed\nOverlap\\(\\forall x \\X, t \\\\{0, 1\\}: p (T = t | X = x> 0\\)\\(\\forall x \\X, t \\\\{0, 1\\}: p (T = t | X = x> 0\\)treatments non-zero probability observedAll treatments non-zero probability observedSUTVA/ Consistency\nTreatment outcomes different subjects independent\nSUTVA/ ConsistencyTreatment outcomes different subjects independentExampleWe havebinary treatment \\(T \\\\{ 0,1\\}\\)binary treatment \\(T \\\\{ 0,1\\}\\)\\(Y(1), Y(0)\\) potential outcomes\\(Y(1), Y(0)\\) potential outcomesThe average treatment effect isThe average treatment effect \\[\nATE = E(Y(1) - Y(0)) = E(\\tau(x))\n\\]conditional average treatment effect \\[\nCATE = \\tau(x) = E(Y(1) - Y(0)|X = x)\n\\]see https://github.com/maxwshen/iap-cidl/blob/master/iap-cidl-lecture1_fredrik_potential_outcomes.pdf","code":""},{"path":"experimental-design.html","id":"experimental-design","chapter":"19 Experimental Design","heading":"19 Experimental Design","text":"Randomized Control Trials (RCT) Experiments always likely continue future holy grail causal inference.RCT means two group treatment (experimental) gorp control group. Hence, introduce treatment (exogenous variable) treatment group, expected difference outcomes two group due treatment.Subjects population randomly assigned either treatment control group. random assignment give us confidence changes outcome variable due treatment, source (variable).can easier hard science RCT can introduce treatment, control environments. ’s hard social scientists subjects usually human, treatment can hard introduce, environments uncontrollable. Hence, social scientists develop different tools (Quasi-experimental) recover causal inference recreate treatment control group environment.RCT, can easily establish internal validityEven though random assignment thing ceteris paribus (.e., holding everything else constant), effect (.e., random manipulation, things equal can observed, average, across treatment control groups).Selection ProblemAssume havebinary treatment \\(D_i =(0,1)\\)binary treatment \\(D_i =(0,1)\\)outcome interest \\(Y_i\\) individual \\(\\)\n\\(Y_{0i}\\) treated\n\\(Y_{1i}\\) treated\noutcome interest \\(Y_i\\) individual \\(\\)\\(Y_{0i}\\) treated\\(Y_{0i}\\) treated\\(Y_{1i}\\) treated\\(Y_{1i}\\) treated\\[\n\\text{Potential Outcome} =\n\\begin{cases}\nY_{1i} \\text{ } D_i = 1 \\\\\nY_{0i} \\text{ } D_i = 0\n\\end{cases}\n\\], observe outcome variable \\[\nY_i = Y_{0i} + (Y_{1i} - Y_{0i})D_i\n\\]’s likely \\(Y_{1i}\\) \\(Y_{0i}\\) distributions (.e., different treatment effect different people). Since can’t see outcomes individual (unless time machine), can make inference regarding average outcome treated .\\[\n\\begin{aligned}\nE[Y_i | D_i = 1] - E[Y_i | D_i = 0] &= (E[Y_{1i} | D_i = 1] - E[Y_{0i}|D_i = 1] ) + (E[Y_{0i} |D_i = 1] - E[Y_{0i} |D_i = 0]) \\\\\n &= (E[Y_{1i}-Y_{0i}|D_1 = 1] ) + (E[Y_{0i} |D_i = 1] - E[Y_{0i} |D_i = 0]) \\\\\n\\text{Observed difference treatment} &= \\text{Average treatment effect treated} + \\text{Selection bias}\n\\end{aligned}\n\\]average treatment effect average person treated person (another parallel universe) treatedThe average treatment effect average person treated person (another parallel universe) treatedThe selection bias difference treated weren’t treatedThe selection bias difference treated weren’t treatedWith random assignment treatment (\\(D_i\\)) Experimental Design, can \\(D_i\\) independent potential outcomes\\[\n\\begin{aligned}\nE[Y_i | D_i = 1] - E[Y_i|D_i = 0] &= E[Y_{1i}|D_i = 1]-E[Y_{0i}|D_i = 0)]\\\\\n&= E[Y_{1i}|D_i = 1]-E[Y_{0i}|D_i = 0)] && D_i \\perp Y_i \\\\\n&= E[Y_{1i} - Y_{0i}|D_i = 1] \\\\\n&= E[Y_{1i} - Y_{0i}]\n\\end{aligned}\n\\]Another representation regressionSuppose know effect \\[\nY_{1i} - Y_{0i} = \\rho\n\\]observed outcome variable (individual) can rewritten \\[\n\\begin{aligned}\nY_i &= E(Y_{0i}) + (Y_{1i}-Y_{0i})D_i + [Y_{0i} - E(Y_{0i})]\\\\\n&= \\alpha + \\rho D_i + \\eta_i\n\\end{aligned}\n\\]\\(\\eta_i\\) = random variation \\(Y_{0i}\\)Hence, conditional expectation individual outcome treatment status \\[\n\\begin{aligned}\nE[Y_i |D_i = 1] &= \\alpha + \\rho &+ E[\\eta_i |D_i = 1] \\\\\nE[Y_i |D_i = 0] &= \\alpha &+ E[\\eta_i |D_i = 0]\n\\end{aligned}\n\\]Thus,\\[\nE[Y_i |D_i = 1] - E[Y_i |D_i = 0] = \\rho + E[\\eta_i |D_i = 1] -E[\\eta_i |D_i = 0]\n\\]\\(E[\\eta_i |D_i = 1] -E[\\eta_i |D_i = 0]\\) selection bias - correlation regression error term (\\(\\eta_i\\)), regressor (\\(D_i\\))regression, \\[\nE[\\eta_i |D_i = 1] -E[\\eta_i |D_i = 0] = E[Y_{0i} |D_i = 1] -E[Y_{0i}|D_i = 0]\n\\]difference outcomes weren’t treated get treated weren’t treated stay untreatedSay control variables (\\(X_i\\)), uncorrelated treatment (\\(D_i\\)), can include model, won’t (principle) affect estimate treatment effect (\\(\\rho\\)) added benefit reducing residual variance, subsequently reduces standard error estimates.\\[\nY_i = \\alpha + \\rho D_i + X_i'\\gamma + \\eta_i\n\\]","code":""},{"path":"experimental-design.html","id":"semi-random-experiment","chapter":"19 Experimental Design","heading":"19.1 Semi-random Experiment","text":"Chicago Open Enrollment Program (Cullen, Jacob, Levitt 2005)Students can apply “choice” schoolsStudents can apply “choice” schoolsMany schools oversubscribed (Demand > Supply)Many schools oversubscribed (Demand > Supply)Resolve scarcity via random lotteriesResolve scarcity via random lotteriesNon-random enrollment, random lottery mean aboveNon-random enrollment, random lottery mean aboveLet\\[\n\\delta_j = E(Y_i | Enroll_{ij} = 1; Apply_{ij} = 1) - E(Y_i | Enroll_{ij} = 0; Apply_{ij} = 1)\n\\]\\[\n\\theta_j = E(Y_i | Win_{ij} = 1; Apply_{ij} = 1) - E(Y_i | Win_{ij} = 0; Apply_{ij} = 1)\n\\]Hence, can clearly see \\(\\delta_j \\neq \\theta_j\\) can enroll, ensure win. Thus, intention treat different treatment effect.Non-random enrollment, random lottery means can estimate \\(\\theta_j\\)recover true treatment effect, can use\\[\n\\delta_j = \\frac{E(Y_i|W_{ij} = 1; A_{ij} = 1) - E(Y_i | W_{ij}=0; A_{ij} = 1)}{P(Enroll_{ij} = 1| W_{ij}= 1; A_{ij}=1) - P(Enroll_{ij} = 1| W_{ij}=0; A_{ij}=1)}\n\\]\\(\\delta_j\\) = treatment effect\\(\\delta_j\\) = treatment effect\\(W\\) = Whether students win lottery\\(W\\) = Whether students win lottery\\(\\) = Whether student apply lottery\\(\\) = Whether student apply lotteryi = applicationi = applicationj = schoolj = schoolSay have10 win10 loseIntent treatment = Average effect give option choose\\[\n\\begin{aligned}\nE(Y_i | W_{ij}=1; A_{ij} = 1) &= \\frac{1*(1.2)+ 2*(1) + 7 * (-0.1)}{10}\\\\\n&= 0.25\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nE(Y_i | W_{ij}=0; A_{ij} = 1) &= \\frac{1*(1.2)+ 2*(0) + 7 * (-0.1)}{10}\\\\\n&= 0.05\n\\end{aligned}\n\\]Hence,\\[\n\\begin{aligned}\n\\text{Intent treatment} &= 0.25 - 0.05 = 0.2 \\\\\n\\text{Treatment effect} &= 1\n\\end{aligned}\n\\]\\[\n\\begin{aligned}\nP(Enroll_{ij} = 1 | W_{ij} = 1; A_{ij}=1 ) &= \\frac{1+2}{10} = 0.3 \\\\\nP(Enroll_{ij} = 1 | W_{ij} = 0; A_{ij}=1 ) &= \\frac{1}{10} = 0.1\n\\end{aligned}\n\\]\\[\n\\text{Treatment effect} = \\frac{0.2}{0.3-0.1} = 1\n\\]knowing recover treatment effect, turn attention main model\\[\nY_{ia} = \\delta W_{ia} + \\lambda L_{ia} + e_{ia}\n\\]\\(W\\) = whether student wins lottery\\(W\\) = whether student wins lottery\\(L\\) = whether student enrolls lottery\\(L\\) = whether student enrolls lottery\\(\\delta\\) = intent treat\\(\\delta\\) = intent treatHence,Conditional lottery, \\(\\delta\\) validConditional lottery, \\(\\delta\\) validBut without lottery, \\(\\delta\\) randomBut without lottery, \\(\\delta\\) randomWinning losing identified within lotteryWinning losing identified within lotteryEach lottery multiple entries. Thus, can within estimatorEach lottery multiple entries. Thus, can within estimatorWe can also include control variables (\\(X_i \\theta\\))\\[\nY_{ia} = \\delta_1 W_{ia} + \\lambda_1 L_{ia} + X_i \\theta + u_{ia}\n\\]\\[\n\\begin{aligned}\nE(\\delta) &= E(\\delta_1) \\\\\nE(\\lambda) &\\neq E(\\lambda_1) && \\text{choosing lottery random}\n\\end{aligned}\n\\]Including \\((X_i \\theta)\\) just shifts around control variables (.e., reweighting lottery), affect treatment effect \\(E(\\delta)\\)","code":""},{"path":"quasi-experimental.html","id":"quasi-experimental","chapter":"20 Quasi-experimental","heading":"20 Quasi-experimental","text":"cases, means pre- post-intervention data.","code":""},{"path":"quasi-experimental.html","id":"regression-discontinuity","chapter":"20 Quasi-experimental","heading":"20.1 Regression Discontinuity","text":"regression discontinuity occurs discrete change (jump) treatment likelihood distribution continuous (roughly continuous) variable (.e., running/forcing/assignment variable).\nRunning variable can also time, argument time continuous hard argue usually see increment time (e.g., quarterly annual data). Unless minute hour data, might able argue .\nregression discontinuity occurs discrete change (jump) treatment likelihood distribution continuous (roughly continuous) variable (.e., running/forcing/assignment variable).Running variable can also time, argument time continuous hard argue usually see increment time (e.g., quarterly annual data). Unless minute hour data, might able argue .RD localized experiment cutoff point\nHence, always qualify (perfunctory) statement research articles “research might generalize beyond bandwidth.”\nRD localized experiment cutoff pointHence, always qualify (perfunctory) statement research articles “research might generalize beyond bandwidth.”reality, RD experimental (random assignment) estimates similar ((Chaplin et al. 2018; Bertanha Imbens 2014); Mathematica ). still, ’s hard prove empirically every context (might future study finds huge difference local estimate - causal - overall estimate - random assignment.reality, RD experimental (random assignment) estimates similar ((Chaplin et al. 2018; Bertanha Imbens 2014); Mathematica ). still, ’s hard prove empirically every context (might future study finds huge difference local estimate - causal - overall estimate - random assignment.Threats: valid near threshold: inference threshold valid average. Interestingly, random experiment showed validity already.Threats: valid near threshold: inference threshold valid average. Interestingly, random experiment showed validity already.Tradeoff efficiency biasTradeoff efficiency biasRegression discontinuity framework Instrumental VariableRegression discontinuity framework Instrumental VariableThe hard part find setting can apply, find one, ’s easy applyThe hard part find setting can apply, find one, ’s easy applyWe can also multiple cutoff lines. However, cutoff line, can one breakup pointWe can also multiple cutoff lines. However, cutoff line, can one breakup pointRD can multiple coinciding effects (.e., joint distribution bundled treatment), RD effect case joint effect.RD can multiple coinciding effects (.e., joint distribution bundled treatment), RD effect case joint effect.running variable becomes discrete framework Interrupted Time Series, granular levels can use RD. infinite data (substantially large) two frameworks identical. RD always better [Interrupted Times Series]running variable becomes discrete framework Interrupted Time Series, granular levels can use RD. infinite data (substantially large) two frameworks identical. RD always better [Interrupted Times Series]two types Regression Discontinuity:Sharp RD: Change treatment probability cutoff point 1Fuzzy RD: Change treatment probability less 1Consider\\[\nD_i = 1_{X_i > c}\n\\]\\[\nD_i = \n\\begin{cases}\nD_i = 1 \\text{ } X_i > C \\\\\nD_i = 0 \\text{ } X_i < C\n\\end{cases}\n\\]\\(D_i\\) = treatment effect\\(D_i\\) = treatment effect\\(X_i\\) = score variable (continuous)\\(X_i\\) = score variable (continuous)\\(c\\) = cutoff point\\(c\\) = cutoff pointIdentifying assumption RD:\\[\n\\begin{aligned}\n\\alpha_{SRDD} &= E[Y_{1i} - Y_{0i} | X_i = c] \\\\\n&= E[Y_{1i}|X_i = c] - E[Y_{0i}|X_i = c]\\\\\n&= \\lim_{x \\c^+} E[Y_{1i}|X_i = c] - \\lim_{x \\c^=} E[Y_{0i}|X_i = c]\n\\end{aligned}\n\\]RDD estimates local average treatment effect (LATE), cutoff point individual population levels.Since researchers typically care internal validity, external validity, localness affects external validity.Assumptions:Independent assignmentIndependent assignmentContinuity conditional regression functions\n\\(E[Y(0)|X=x]\\) \\(E[Y(1)|X=x]\\) continuous x.\nContinuity conditional regression functions\\(E[Y(0)|X=x]\\) \\(E[Y(1)|X=x]\\) continuous x.RD valid cutpoint exogenous (.e., endogenous selection) running variable manipulableRD valid cutpoint exogenous (.e., endogenous selection) running variable manipulableGeneral Model\\[\nY_i = \\beta_0 + f(x_i) \\beta_1 + [(x_i \\ge c)]\\beta_2 + \\epsilon_i\n\\]\\(f(x_i)\\) functional form \\(x_i\\)Simple caseWhen \\(f(x_i) = x_i\\) (linear function)\\[\nY_i = \\beta_0 + x_i \\beta_1 + [(x_i \\ge c)]\\beta_2 + \\epsilon_i\n\\]RD gives \\(\\beta_2\\) (causal effect) \\(X\\) \\(Y\\) cutoff pointIn practice, everyone \\[\nY_i = \\alpha_0 + f(x) \\alpha _1 + [(x_i \\ge c)]\\alpha_2 + [f(x_i)\\times [(x_i \\ge c)]\\alpha_3 + u_i\n\\]estimate different slope different sides lineand estimate \\(\\alpha_3\\) different 0 return simple caseNotes:Sparse data can make \\(\\alpha_3\\) large differential effectSparse data can make \\(\\alpha_3\\) large differential effectPeople skeptical complex \\(f(x_i)\\), usual simple function forms (e.g., linear, squared term, etc.) goodPeople skeptical complex \\(f(x_i)\\), usual simple function forms (e.g., linear, squared term, etc.) goodBandwidth \\(c\\) (window)Closer \\(c\\) can give lower bias, also efficiencyCloser \\(c\\) can give lower bias, also efficiencyWider \\(c\\) can increase bias, higher efficiency.Wider \\(c\\) can increase bias, higher efficiency.Optimal bandwidth controversial, usually appendix research article anyway.Optimal bandwidth controversial, usually appendix research article anyway.can either\ndrop observations outside bandwidth \nweight depends far close \\(c\\)\ncan eitherdrop observations outside bandwidth ordrop observations outside bandwidth orweight depends far close \\(c\\)weight depends far close \\(c\\)","code":""},{"path":"quasi-experimental.html","id":"bunching-test","chapter":"20 Quasi-experimental","heading":"20.1.1 Bunching Test","text":"Bunching happens people self-select specific value range variable (e.g., key policy thresholds).Bunching happens people self-select specific value range variable (e.g., key policy thresholds).Review paper (Kleven 2016)Review paper (Kleven 2016)Histogram bunching similar density curve (want narrower bins, wider bins bias elasticity estimates)Histogram bunching similar density curve (want narrower bins, wider bins bias elasticity estimates)can also use bunching method study individuals’ firm’s responsiveness changes policy.can also use bunching method study individuals’ firm’s responsiveness changes policy.RD, assume don’t manipulation running variable. However, bunching behavior manipulation firms individuals. Thus, violating assumption.\nBunching can fix problem estimating densities individuals without manipulation (.e., manipulation-free counterfactual).\nfraction persons manipulated calculated comparing observed distribution manipulation-free counterfactual distributions.\nRD, need step observed manipulation-free counterfactual distributions assumed . RD assume manipulation (.e., assume manipulation-free counterfactual distribution)\nRD, assume don’t manipulation running variable. However, bunching behavior manipulation firms individuals. Thus, violating assumption.Bunching can fix problem estimating densities individuals without manipulation (.e., manipulation-free counterfactual).Bunching can fix problem estimating densities individuals without manipulation (.e., manipulation-free counterfactual).fraction persons manipulated calculated comparing observed distribution manipulation-free counterfactual distributions.fraction persons manipulated calculated comparing observed distribution manipulation-free counterfactual distributions.RD, need step observed manipulation-free counterfactual distributions assumed . RD assume manipulation (.e., assume manipulation-free counterfactual distribution)RD, need step observed manipulation-free counterfactual distributions assumed . RD assume manipulation (.e., assume manipulation-free counterfactual distribution)Assumptions:\nManipulation one-sided: People move one way (.e., either threshold threshold vice versa, away threshold), similar monotonicity assumption instrumental variable 21.3.1\nManipulation bounded (also known regularity assumption): can use people far away threshold derive counterfactual distribution (Blomquist et al. 2017)\nAssumptions:Manipulation one-sided: People move one way (.e., either threshold threshold vice versa, away threshold), similar monotonicity assumption instrumental variable 21.3.1Manipulation one-sided: People move one way (.e., either threshold threshold vice versa, away threshold), similar monotonicity assumption instrumental variable 21.3.1Manipulation bounded (also known regularity assumption): can use people far away threshold derive counterfactual distribution (Blomquist et al. 2017)Manipulation bounded (also known regularity assumption): can use people far away threshold derive counterfactual distribution (Blomquist et al. 2017)Steps:Identify window running variable contains bunching behavior. can step empirically based data (Bosch, Dekker, Strohmaier 2020). Additionally robustness test needed (.e., varying manipulation window).Estimate manipulation-free counterfactualCalculating standard errors inference can follow (R. Chetty et al. 2011) bootstrap resampling residuals estimation counts individuals within bins (large data can render step unnecessary).pass bunching test, can move Placebo Test","code":""},{"path":"quasi-experimental.html","id":"placebo-test","chapter":"20 Quasi-experimental","heading":"20.1.2 Placebo Test","text":"cutoff point, can run placebo test see whether X’s different).placebo test expect coefficients different 0.Balance observable characteristics sides\\[\nZ_i = \\alpha_0 + \\alpha_1 f(x_i) + [(x_i \\ge c)] \\alpha_2 + [f(x_i) \\times (x_i \\ge c)]\\alpha_3 + u_i\n\\]\\(x_i\\) running variable\\(x_i\\) running variable\\(Z_i\\) characteristics people (e.g., age, etc)\\(Z_i\\) characteristics people (e.g., age, etc)Theoretically, \\(Z_i\\) affected treatment. Hence, \\(E(\\alpha_2) = 0\\)Moreover, multiple \\(Z_i\\), typically simulate joint distribution (avoid significant coefficient based chance).way don’t need generate joint distribution \\(Z_i\\)’s independent (unlikely reality).RD, shouldn’t Matching Methods. just like random assignment, need make balanced dataset cutoff. balancing, RD assumptions probably wrong first place.","code":""},{"path":"quasi-experimental.html","id":"examples","chapter":"20 Quasi-experimental","heading":"20.1.3 Examples","text":"","code":""},{"path":"quasi-experimental.html","id":"example-1-1","chapter":"20 Quasi-experimental","heading":"20.1.3.1 Example 1","text":"Example Leihua Ye\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 W_i + u_i\n\\]\\[\nX_i = \n\\begin{cases}\n1, W_i \\ge c \\\\\n0, W_i < c\n\\end{cases}\n\\]","code":"\n#cutoff point = 3.5\nGPA <- runif(1000, 0, 4)\nfuture_success <- 10 + 2 * GPA + 10 * (GPA >= 3.5) + rnorm(1000)\n#install and load the package ‘rddtools’\n#install.packages(“rddtools”)\nlibrary(rddtools)## Warning: package 'rddtools' was built under R version 4.0.5## Warning: package 'car' was built under R version 4.0.5## Warning: package 'zoo' was built under R version 4.0.5## Warning: package 'survival' was built under R version 4.0.5## Warning: package 'np' was built under R version 4.0.5\ndata <- rdd_data(future_success, GPA, cutpoint = 3.5)\n# plot the dataset\nplot(\n    data,\n    col =  \"red\",\n    cex = 0.1,\n    xlab =  \"GPA\",\n    ylab =  \"future_success\"\n)\n# estimate the sharp RDD model\nrdd_mod <- rdd_reg_lm(rdd_object = data, slope =  \"same\")\nsummary(rdd_mod)## \n## Call:\n## lm(formula = y ~ ., data = dat_step1, weights = weights)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.8510 -0.6757  0.0094  0.6880  3.2894 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 16.95480    0.06708  252.76   <2e-16 ***\n## D           10.00710    0.11916   83.98   <2e-16 ***\n## x            1.97835    0.03348   59.09   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.023 on 997 degrees of freedom\n## Multiple R-squared:  0.9587, Adjusted R-squared:  0.9586 \n## F-statistic: 1.157e+04 on 2 and 997 DF,  p-value: < 2.2e-16\n# plot the RDD model along with binned observations\nplot(\n    rdd_mod,\n    cex = 0.1,\n    col =  \"red\",\n    xlab =  \"GPA\",\n    ylab =  \"future_success\"\n)"},{"path":"quasi-experimental.html","id":"example-2","chapter":"20 Quasi-experimental","heading":"20.1.3.2 Example 2","text":"Bowblis Smith (2019)Occupational licensing can either increase decrease market efficiency:information means efficiencyMore information means efficiencyIncreased entry barriers (.e., friction) increase efficiencyIncreased entry barriers (.e., friction) increase efficiencyComponents RDRunning variableCutoff: 120 beds aboveTreatment: treatment cutoff point.OLS\\[\nY_i = \\alpha_0 + X_i \\alpha_1 + LW_i \\alpha_2 + \\epsilon_i\n\\]\\(LW_i\\) Licensed/certified workers (fraction format center).\\(LW_i\\) Licensed/certified workers (fraction format center).\\(Y_i\\) = Quality service\\(Y_i\\) = Quality serviceBias \\(\\alpha_2\\)Mitigation-based: terrible quality can lead hiring, negatively bias \\(\\alpha_2\\)Mitigation-based: terrible quality can lead hiring, negatively bias \\(\\alpha_2\\)Preference-based: places higher quality staff want keep high quality staffs.Preference-based: places higher quality staff want keep high quality staffs.RD\\[\nY_{ist} = \\beta_0 + [(Bed \\ge121)_{ist}]\\beta_1 + f(Size_{ist}) \\beta_2 + [f(Size_{ist}) \\times (Bed \\ge 121)_{ist}] \\beta_3 \\\\\n+ X_{} \\delta + \\gamma_s + \\theta_t + \\epsilon_{ist}\n\\]\\(s\\) = state\\(s\\) = state\\(t\\) = year\\(t\\) = year\\(\\) = hospital\\(\\) = hospitalThis RD fuzzyIf right near threshold (bandwidth), states different sorting (.e., non-random), need fixed-effect state \\(s\\). RD assumption wrong anyway, won’t first placeTechnically, also run fixed-effect regression, ’s lower causal inference hierarchy. Hence, don’t .Moreover, RD framework, don’t include \\(t\\) treatment (FE include )include \\(\\pi_i\\) hospital, don’t variation causal estimates (hardly hospital changes bed size panel)\\(\\beta_1\\) intent treat (treatment effect coincide intent treat)take fuzzy cases , introduce selection bias.Note drop cases based behavioral choice (exclude non-compliers), can drop particular behaviors ((e.g., people like round numbers).Thus, use Instrument variable 21.3.1Stage 1:\\[\nQSW_{ist} = \\alpha_0 + [(Bed \\ge121)_{ist}]\\alpha_1 + f(Size_{ist}) \\alpha_2 + [f(Size_{ist}) \\times (Bed \\ge 121)_{ist}] \\alpha_3 \\\\\n+ X_{} \\delta + \\gamma_s + \\theta_t + \\epsilon_{ist}\n\\](Note: different fixed effects error term - \\(\\delta, \\gamma_s, \\theta_t, \\epsilon_{ist}\\) first equation, ran Greek letters)Stage 2:\\[\nY_{ist} = \\gamma_0 + \\gamma_1 \\hat{QWS}_{ist} + f(Size_{ist}) \\delta_2 + [f(Size_{ist}) \\times (Bed \\ge 121)] \\delta_3 \\\\\n + X_{} \\lambda + \\eta_s + \\tau_t + u_{ist}\n\\]bigger jump (discontinuity), similar 2 coefficients (\\(\\gamma_1 \\approx \\beta_1\\)) \\(\\gamma_1\\) average treatment effect (exposing policy)\\(\\beta_1\\) always closer 0 \\(\\gamma_1\\)Figure 1 shows bunching every 5 units cutoff, 120 still .manipulable bunching, decrease 130Since limited number mass points (round numbers), clustered standard errors mass point","code":""},{"path":"quasi-experimental.html","id":"difference-in-differences","chapter":"20 Quasi-experimental","heading":"20.2 Difference-In-Differences","text":"","code":""},{"path":"quasi-experimental.html","id":"simple-dif-n-dif","chapter":"20 Quasi-experimental","heading":"20.2.1 Simple Dif-n-dif","text":"tool developed intuitively study “natural experiment,” uses much broader.tool developed intuitively study “natural experiment,” uses much broader.Fixed Effects Estimator foundation DIDFixed Effects Estimator foundation DIDConsider\\(D_i = 1\\) treatment group\\(D_i = 1\\) treatment group\\(D_i = 0\\) control group\\(D_i = 0\\) control group\\(T= 1\\) treatment\\(T= 1\\) treatment\\(T =0\\) treatment\\(T =0\\) treatmentmissing \\(E[Y_{0i}(1)|D=1]\\)Average Treatment Effect Treated\\[\nE[Y_1(1) - Y_0(1)|D=1] \\\\\n= \\{E[Y(1)|D=1] - E[Y(1)|D=0] \\} - \\{E[Y(0)|D=1] - E[Y(0)|D=0] \\}\n\\]Assumption:Parallel Trends: Difference treatment control groups remain constant treatment.used cases whereyou observe eventyou observe eventyou treatment control groupsyou treatment control groupsnot cases wheretreatment randomtreatment randomconfounders.confounders.Example Princetoncreate dummy variable indicate time treatment startedcreate dummy variable identify treatment groupcreate interaction time treatedestimate estimatorThe coefficient differences--differences estimator. Treat negative effect","code":"\nlibrary(foreign)\nmydata = read.dta(\"http://dss.princeton.edu/training/Panel101.dta\")\nmydata$time = ifelse(mydata$year >= 1994, 1, 0)\nmydata$treated = ifelse(mydata$country == \"E\" |\n                            mydata$country == \"F\" | mydata$country == \"G\" ,\n                        1,\n                        0)\nmydata$did = mydata$time * mydata$treated\ndidreg = lm(y ~ treated + time + did, data = mydata)\nsummary(didreg)## \n## Call:\n## lm(formula = y ~ treated + time + did, data = mydata)\n## \n## Residuals:\n##        Min         1Q     Median         3Q        Max \n## -9.768e+09 -1.623e+09  1.167e+08  1.393e+09  6.807e+09 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)  \n## (Intercept)  3.581e+08  7.382e+08   0.485   0.6292  \n## treated      1.776e+09  1.128e+09   1.575   0.1200  \n## time         2.289e+09  9.530e+08   2.402   0.0191 *\n## did         -2.520e+09  1.456e+09  -1.731   0.0882 .\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.953e+09 on 66 degrees of freedom\n## Multiple R-squared:  0.08273,    Adjusted R-squared:  0.04104 \n## F-statistic: 1.984 on 3 and 66 DF,  p-value: 0.1249"},{"path":"quasi-experimental.html","id":"example-by-card1993","chapter":"20 Quasi-experimental","heading":"20.2.1.1 Example by Card and Krueger (1993)","text":"found increase minimum wage increases employmentExperimental Setting:New Jersey (treatment) increased minimum wageNew Jersey (treatment) increased minimum wagePenn (control) increase minimum wagePenn (control) increase minimum wagewhereA - B = treatment effect + effect time (additive)- B = treatment effect + effect time (additive)C - D = effect timeC - D = effect time(- B) - (C - D) = dif-n-dif(- B) - (C - D) = dif-n-difThe identifying assumptions:Can’t switchersCan’t switchersPA control group\ngood counter factual\nNJ look like hadn’t treatment\nPA control groupis good counter factualis good counter factualis NJ look like hadn’t treatmentis NJ look like hadn’t treatment\\[\nY_{jt} = \\beta_0 + NJ_j \\beta_1 + POST_t \\beta_2 + (NJ_j \\times POST_t)\\beta_3+ X_{jt}\\beta_4 + \\epsilon_{jt}\n\\]\\(j\\) = restaurant\\(j\\) = restaurant\\(NJ\\) = dummy 1 = NJ, 0 = PA\\(NJ\\) = dummy 1 = NJ, 0 = PA\\(POST\\) = dummy 1 = post, 0 = pre\\(POST\\) = dummy 1 = post, 0 = preWe don’t need \\(\\beta_4\\) model unbiased \\(\\beta_3\\), including give coefficients efficiencyIf use \\(\\Delta Y_{jt}\\) dependent variable, don’t need \\(POST_t \\beta_2\\) anymoreAlternative model specification authors use NJ high wage restaurant control group (still choose close border)reason can’t control everything (PA + NJ high wage) ’s hard interpret causal treatmentDif-n-dif utilizes similarity pretrend dependent variables. However, neither necessary sufficient identifying assumption.’s sufficient can multiple treatments (technically, include control, treatment can’t interact)’s sufficient can multiple treatments (technically, include control, treatment can’t interact)’s necessary trends can e parallel treatmentIt’s necessary trends can e parallel treatmentHowever, can’t never certain; just try find evidence consistent theory dif-n-dif can work.Notice don’t need treatment levels dependent variable (e.g., wage average NJ PA), dif-n-dif needs pre-trend (.e., slope) two groups.","code":""},{"path":"quasi-experimental.html","id":"example-by-butcher2014","chapter":"20 Quasi-experimental","heading":"20.2.1.2 Example by Butcher, McEwan, and Weerapana (2014)","text":"Theory:Highest achieving students usually hard science. ?\nHard give students students benefit doubt hard science\nunpleasant easy get job. Degrees lower market value typically want make feel pleasant\nHighest achieving students usually hard science. ?Hard give students students benefit doubt hard scienceHard give students students benefit doubt hard scienceHow unpleasant easy get job. Degrees lower market value typically want make feel pleasantHow unpleasant easy get job. Degrees lower market value typically want make feel pleasantUnder OLS\\[\nE_{ij} = \\beta_0 + X_i \\beta_1 + G_j \\beta_2 + \\epsilon_{ij}\n\\]\\(X_i\\) = student attributes\\(X_i\\) = student attributes\\(\\beta_2\\) = causal estimate (grade change)\\(\\beta_2\\) = causal estimate (grade change)\\(E_{ij}\\) = choose enroll major \\(j\\)\\(E_{ij}\\) = choose enroll major \\(j\\)\\(G_j\\) = grade given major \\(j\\)\\(G_j\\) = grade given major \\(j\\)Examine \\(\\hat{\\beta}_2\\)Negative bias: Endogenous response department lower enrollment rate give better gradeNegative bias: Endogenous response department lower enrollment rate give better gradePositive bias: hard science already best students (.e., ability), don’t grades can even lowerPositive bias: hard science already best students (.e., ability), don’t grades can even lowerUnder dif-n-dif\\[\nY_{idt} = \\beta_0 + POST_t \\beta_1 + Treat_d \\beta_2 + (POST_t \\times Treat_d)\\beta_3 + X_{idt} + \\epsilon_{idt}\n\\]\\(Y_{idt}\\) = grade averageA general specification dif-n-dif \\[\nY_{idt} = \\alpha_0 + (POST_t \\times Treat_d) \\alpha_1 + \\theta_d + \\delta_t + X_{idt} + u_{idt}\n\\]\\((\\theta_d + \\delta_t)\\) richer , df \\(Treat_d \\beta_2 + Post_t \\beta_1\\) (fixed effects subsume Post treat)\\((\\theta_d + \\delta_t)\\) richer , df \\(Treat_d \\beta_2 + Post_t \\beta_1\\) (fixed effects subsume Post treat)\\(\\alpha_1\\) equivalent \\(\\beta_3\\) (model assumptions correct)\\(\\alpha_1\\) equivalent \\(\\beta_3\\) (model assumptions correct)causal inference, \\(R^2\\) important.","code":""},{"path":"quasi-experimental.html","id":"staggered-dif-n-dif","chapter":"20 Quasi-experimental","heading":"20.2.2 Staggered Dif-n-dif","text":"","code":""},{"path":"quasi-experimental.html","id":"example-by-doleac2020","chapter":"20 Quasi-experimental","heading":"20.2.2.1 Example by Doleac and Hansen (2020)","text":"purpose banning checking box ex-criminal banned thought gives access felonsThe purpose banning checking box ex-criminal banned thought gives access felonsEven ban box, employers wouldn’t just change behaviors. unintended consequence employers statistically discriminate based raceEven ban box, employers wouldn’t just change behaviors. unintended consequence employers statistically discriminate based race3 types ban boxPublic employer onlyPrivate employer government contractAll employersMain identification strategyIf county Metropolitan Statistical Area (MSA) adopts ban box, means whole MSA treated. state adopts “ban ban,” every county treatedUnder Simple Dif-n-dif\\[\nY_{} = \\beta_0 + \\beta_1 Post_t + \\beta_2 treat_i + \\beta_2 (Post_t \\times Treat_i) + \\epsilon_{}\n\\]common post time, use Staggered Dif-n-dif\\[\nE_{imrt} = \\alpha + \\beta_1 BTB_{imt} W_{imt} + \\beta_2 BTB_{mt} + \\beta_3 BTB_{mt} H_{imt}+ \\delta_m + D_{imt} \\beta_5 + \\lambda_{rt} + \\delta_m\\times f(t) \\beta_7 + e_{imrt}\n\\]\\(\\) = person; \\(m\\) = MSA; \\(r\\) = region (US regions e.g., midwest) ; \\(r\\) = region; \\(t\\) = year\\(\\) = person; \\(m\\) = MSA; \\(r\\) = region (US regions e.g., midwest) ; \\(r\\) = region; \\(t\\) = year\\(W\\) = White; \\(B\\) = Black; \\(H\\) = Hispanic\\(W\\) = White; \\(B\\) = Black; \\(H\\) = Hispanic\\(\\beta_1 BTB_{imt} W_{imt} + \\beta_2 BTB_{mt} + \\beta_3 BTB_{mt} H_{imt}\\) 3 dif-n-dif variables (\\(BTB\\) = “ban box”)\\(\\beta_1 BTB_{imt} W_{imt} + \\beta_2 BTB_{mt} + \\beta_3 BTB_{mt} H_{imt}\\) 3 dif-n-dif variables (\\(BTB\\) = “ban box”)\\(\\delta_m\\) = dummy MSI\\(\\delta_m\\) = dummy MSI\\(D_{imt}\\) = control people\\(D_{imt}\\) = control people\\(\\lambda_{rt}\\) = region time fixed effect\\(\\lambda_{rt}\\) = region time fixed effect\\(\\delta_m \\times f(t)\\) = linear time trend within MSA (need good pre-trend)\\(\\delta_m \\times f(t)\\) = linear time trend within MSA (need good pre-trend)put \\(\\lambda_r - \\lambda_t\\) (separately) broad fixed effect, \\(\\lambda_{rt}\\) give us deeper narrower fixed effect.running model, drop races. \\(\\beta_1, \\beta_2, \\beta_3\\) collinear interaction terms \\(BTB_{mt}\\)just want estimate model black men, modify \\[\nE_{imrt} = \\alpha + BTB_{mt} \\beta_1 + \\delta_m + D_{imt} \\beta_5 + \\lambda_{rt} + (\\delta_m \\times f(t)) \\beta_7 + e_{imrt}\n\\]\\[\nE_{imrt} = \\alpha + BTB_{m (t - 3t)} \\theta_1 + BTB_{m(t-2)} \\theta_2 + BTB_{mt} \\theta_4 \\\\\n+ BTB_{m(t+1)}\\theta_5 + BTB_{m(t+2)}\\theta_6 + BTB_{m(t+3t)}\\theta_7 \\\\\n+ [\\delta_m + D_{imt}\\beta_5 + \\lambda_r + (\\delta_m \\times (f(t))\\beta_7 + e_{imrt}]\n\\]leave \\(BTB_{m(t-1)}\\theta_3\\) category perfect collinearitySo year BTB (\\(\\theta_1, \\theta_2, \\theta_3\\)) similar (.e., pre-trend). Remember, run places BTB.\\(\\theta_2\\) statistically different \\(\\theta_3\\) (baseline), problem, also make sense pre-trend announcement.Example Philipp Leppert replicating Card Krueger (1994)Example Anthony Schmidt","code":""},{"path":"quasi-experimental.html","id":"synthetic-control","chapter":"20 Quasi-experimental","heading":"20.3 Synthetic Control","text":"Synthetic control method (SCM) generalization dif--dif modelAdvantages dif--dif:Maximization observable similarity control treatment (maybe also unobservables)Can also used cases untreated case similar matching dimensions treated casesObjective selection controls.data driven procedure construct comparable control groups (.e., black box).causal inference control treatment group using Matching Methods, typically similar covariates control treated groups. However, don’t methods like Propensity Scores can perform rather poorly (.e., large bias).SCM recommended whenSocial events evaluate large-scale program policyOnly one treated case several control candidates.Advantages:selection criteria, researchers can understand relative importance candidatePost-intervention outcomes used synthetic. Hence, can’t retro-fit.Observable similarity control treatment cases maximizedSynth provides algorithm finds weighted combination comparison units weights chosen best resembles values predictors outcome variable affected units intervention.","code":""},{"path":"quasi-experimental.html","id":"example-1-2","chapter":"20 Quasi-experimental","heading":"20.3.1 Example 1","text":"Danilo Freiresimulate data 10 states 30 years. State receives treatment T = 20 year 15.Gaps plot:Alternatively, gsynth provides options estimate iterative fixed effects, handle multiple treated units tat time., use two=way fixed effects bootstrapped standard errors","code":"\n# install.packages(\"Synth\")\n# install.packages(\"gsynth\")\nlibrary(\"Synth\")## Warning: package 'Synth' was built under R version 4.0.5\nlibrary(\"gsynth\")## Warning: package 'gsynth' was built under R version 4.0.5\nset.seed(1)\nyear <- rep(1:30, 10) \nstate <- rep(LETTERS[1:10], each = 30)\nX1 <- round(rnorm(300, mean = 2, sd = 1), 2)\nX2 <- round(rbinom(300, 1, 0.5) + rnorm(300), 2)\nY <- round(1 + 2*X1 + rnorm(300), 2)\ndf <- as.data.frame(cbind(Y, X1, X2, state, year))\ndf$Y <- as.numeric(as.character(df$Y))\ndf$X1 <- as.numeric(as.character(df$X1))\ndf$X2 <- as.numeric(as.character(df$X2))\ndf$year <- as.numeric(as.character(df$year))\ndf$state.num <- rep(1:10, each = 30)\ndf$state <- as.character(df$state)\ndf$`T` <- ifelse(df$state == \"A\" & df$year >= 15, 1, 0)\ndf$Y <- ifelse(df$state == \"A\" & df$year >= 15, df$Y + 20, df$Y)\nstr(df)## 'data.frame':    300 obs. of  7 variables:\n##  $ Y        : num  2.29 4.51 2.07 8.87 4.37 1.32 8 7.49 6.98 3.72 ...\n##  $ X1       : num  1.37 2.18 1.16 3.6 2.33 1.18 2.49 2.74 2.58 1.69 ...\n##  $ X2       : num  1.96 0.4 -0.75 -0.56 -0.45 1.06 0.51 -2.1 0 0.54 ...\n##  $ state    : chr  \"A\" \"A\" \"A\" \"A\" ...\n##  $ year     : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ state.num: int  1 1 1 1 1 1 1 1 1 1 ...\n##  $ T        : num  0 0 0 0 0 0 0 0 0 0 ...\ndataprep.out <-\n    dataprep(\n        df,\n        predictors = c(\"X1\", \"X2\"),\n        dependent     = \"Y\",\n        unit.variable = \"state.num\",\n        time.variable = \"year\",\n        unit.names.variable = \"state\",\n        treatment.identifier  = 1,\n        controls.identifier   = c(2:10),\n        time.predictors.prior = c(1:14),\n        time.optimize.ssr     = c(1:14),\n        time.plot             = c(1:30)\n    )\n\n\nsynth.out <- synth(dataprep.out)## \n## X1, X0, Z1, Z0 all come directly from dataprep object.\n## \n## \n## **************** \n##  searching for synthetic control unit  \n##  \n## \n## **************** \n## **************** \n## **************** \n## \n## MSPE (LOSS V): 9.831789 \n## \n## solution.v:\n##  0.3888387 0.6111613 \n## \n## solution.w:\n##  0.1115941 0.1832781 0.1027237 0.312091 0.06096758 0.03509706 0.05893735 0.05746256 0.07784853\nprint(synth.tables   <- synth.tab(\n        dataprep.res = dataprep.out,\n        synth.res    = synth.out)\n      )## $tab.pred\n##    Treated Synthetic Sample Mean\n## X1   2.028     2.028       2.017\n## X2   0.513     0.513       0.394\n## \n## $tab.v\n##    v.weights\n## X1 0.389    \n## X2 0.611    \n## \n## $tab.w\n##    w.weights unit.names unit.numbers\n## 2      0.112          B            2\n## 3      0.183          C            3\n## 4      0.103          D            4\n## 5      0.312          E            5\n## 6      0.061          F            6\n## 7      0.035          G            7\n## 8      0.059          H            8\n## 9      0.057          I            9\n## 10     0.078          J           10\n## \n## $tab.loss\n##            Loss W   Loss V\n## [1,] 9.761708e-12 9.831789\npath.plot(synth.res    = synth.out,\n          dataprep.res = dataprep.out,\n          Ylab         = c(\"Y\"),\n          Xlab         = c(\"Year\"),\n          Legend       = c(\"State A\",\"Synthetic State A\"),\n          Legend.position = c(\"topleft\")\n)\n\nabline(v   = 15,\n       lty = 2)\ngaps.plot(synth.res    = synth.out,\n          dataprep.res = dataprep.out,\n          Ylab         = c(\"Gap\"),\n          Xlab         = c(\"Year\"),\n          Ylim         = c(-30, 30),\n          Main         = \"\"\n)\n\nabline(v   = 15,\n       lty = 2)\ngsynth.out <- gsynth(\n  Y ~ `T` + X1 + X2,\n  data = df,\n  index = c(\"state\", \"year\"),\n  force = \"two-way\",\n  CV = TRUE,\n  r = c(0, 5),\n  se = TRUE,\n  inference = \"parametric\",\n  nboots = 1000,\n  parallel = F # TRUE\n)## Cross-validating ... \n##  r = 0; sigma2 = 1.13533; IC = 0.95632; PC = 0.96713; MSPE = 1.65502\n##  r = 1; sigma2 = 0.96885; IC = 1.54420; PC = 4.30644; MSPE = 1.33375\n##  r = 2; sigma2 = 0.81855; IC = 2.08062; PC = 6.58556; MSPE = 1.27341*\n##  r = 3; sigma2 = 0.71670; IC = 2.61125; PC = 8.35187; MSPE = 1.79319\n##  r = 4; sigma2 = 0.62823; IC = 3.10156; PC = 9.59221; MSPE = 2.02301\n##  r = 5; sigma2 = 0.55497; IC = 3.55814; PC = 10.48406; MSPE = 2.79596\n## \n##  r* = 2\n## \n## \nSimulating errors .............\nBootstrapping ...\n## ..........\nplot(gsynth.out)\nplot(gsynth.out, type = \"counterfactual\")\nplot(gsynth.out, type = \"counterfactual\", raw = \"all\") # shows estimations for the control cases"},{"path":"quasi-experimental.html","id":"example-2-1","chapter":"20 Quasi-experimental","heading":"20.3.2 Example 2","text":"Leihua Yetransform data used synth()whereX1 = control case treatmentX1 = control case treatmentX0 = control cases treatmentX0 = control cases treatmentZ1: treatment case treatmentZ1: treatment case treatmentZ0: treatment case treatmentZ0: treatment case treatmentCalculate difference real basque region synthetic controlRelative importance unitDoubly Robust Difference--DifferencesExample DRDID packageEstimate Average Treatment Effect Treated using Improved Locally Efficient Doubly Robust estimator","code":"\nlibrary(Synth)\ndata(\"basque\")\ndim(basque) #774*17## [1] 774  17\nhead(basque)##   regionno     regionname year   gdpcap sec.agriculture sec.energy sec.industry\n## 1        1 Spain (Espana) 1955 2.354542              NA         NA           NA\n## 2        1 Spain (Espana) 1956 2.480149              NA         NA           NA\n## 3        1 Spain (Espana) 1957 2.603613              NA         NA           NA\n## 4        1 Spain (Espana) 1958 2.637104              NA         NA           NA\n## 5        1 Spain (Espana) 1959 2.669880              NA         NA           NA\n## 6        1 Spain (Espana) 1960 2.869966              NA         NA           NA\n##   sec.construction sec.services.venta sec.services.nonventa school.illit\n## 1               NA                 NA                    NA           NA\n## 2               NA                 NA                    NA           NA\n## 3               NA                 NA                    NA           NA\n## 4               NA                 NA                    NA           NA\n## 5               NA                 NA                    NA           NA\n## 6               NA                 NA                    NA           NA\n##   school.prim school.med school.high school.post.high popdens invest\n## 1          NA         NA          NA               NA      NA     NA\n## 2          NA         NA          NA               NA      NA     NA\n## 3          NA         NA          NA               NA      NA     NA\n## 4          NA         NA          NA               NA      NA     NA\n## 5          NA         NA          NA               NA      NA     NA\n## 6          NA         NA          NA               NA      NA     NA\ndataprep.out <- dataprep(\n    foo = basque,\n    predictors = c(\n        \"school.illit\",\n        \"school.prim\",\n        \"school.med\",\n        \"school.high\",\n        \"school.post.high\",\n        \"invest\"\n    ),\n    predictors.op =  \"mean\",\n    # the operator\n    time.predictors.prior = 1964:1969,\n    #the entire time frame from the #beginning to the end\n    special.predictors = list(\n        list(\"gdpcap\", 1960:1969,  \"mean\"),\n        list(\"sec.agriculture\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.energy\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.industry\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.construction\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.services.venta\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.services.nonventa\", seq(1961, 1969, 2), \"mean\"),\n        list(\"popdens\", 1969,  \"mean\")\n    ),\n    dependent =  \"gdpcap\",\n    # dv\n    unit.variable =  \"regionno\",\n    #identifying unit numbers\n    unit.names.variable =  \"regionname\",\n    #identifying unit names\n    time.variable =  \"year\",\n    #time-periods\n    treatment.identifier = 17,\n    #the treated case\n    controls.identifier = c(2:16, 18),\n    #the control cases; all others #except number 17\n    time.optimize.ssr = 1960:1969,\n    #the time-period over which to optimize\n    time.plot = 1955:1997\n)#the entire time period before/after the treatment\nsynth.out = synth(data.prep.obj = dataprep.out, method = \"BFGS\")## \n## X1, X0, Z1, Z0 all come directly from dataprep object.\n## \n## \n## **************** \n##  searching for synthetic control unit  \n##  \n## \n## **************** \n## **************** \n## **************** \n## \n## MSPE (LOSS V): 0.008864606 \n## \n## solution.v:\n##  0.02773094 1.194e-07 1.60609e-05 0.0007163836 1.486e-07 0.002423908 0.0587055 0.2651997 0.02851006 0.291276 0.007994382 0.004053188 0.009398579 0.303975 \n## \n## solution.w:\n##  2.53e-08 4.63e-08 6.44e-08 2.81e-08 3.37e-08 4.844e-07 4.2e-08 4.69e-08 0.8508145 9.75e-08 3.2e-08 5.54e-08 0.1491843 4.86e-08 9.89e-08 1.162e-07\ngaps = dataprep.out$Y1plot - (dataprep.out$Y0plot \n                                     %*% synth.out$solution.w)\ngaps[1:3,1]##       1955       1956       1957 \n## 0.15023473 0.09168035 0.03716475\nsynth.tables = synth.tab(dataprep.res = dataprep.out,\n                         synth.res = synth.out)\nnames(synth.tables)## [1] \"tab.pred\" \"tab.v\"    \"tab.w\"    \"tab.loss\"\nsynth.tables$tab.pred[1:13,]##                                          Treated Synthetic Sample Mean\n## school.illit                              39.888   256.337     170.786\n## school.prim                             1031.742  2730.104    1127.186\n## school.med                                90.359   223.340      76.260\n## school.high                               25.728    63.437      24.235\n## school.post.high                          13.480    36.153      13.478\n## invest                                    24.647    21.583      21.424\n## special.gdpcap.1960.1969                   5.285     5.271       3.581\n## special.sec.agriculture.1961.1969          6.844     6.179      21.353\n## special.sec.energy.1961.1969               4.106     2.760       5.310\n## special.sec.industry.1961.1969            45.082    37.636      22.425\n## special.sec.construction.1961.1969         6.150     6.952       7.276\n## special.sec.services.venta.1961.1969      33.754    41.104      36.528\n## special.sec.services.nonventa.1961.1969    4.072     5.371       7.111\nsynth.tables$tab.w[8:14, ]##    w.weights            unit.names unit.numbers\n## 9      0.000    Castilla-La Mancha            9\n## 10     0.851              Cataluna           10\n## 11     0.000  Comunidad Valenciana           11\n## 12     0.000           Extremadura           12\n## 13     0.000               Galicia           13\n## 14     0.149 Madrid (Comunidad De)           14\n## 15     0.000    Murcia (Region de)           15\n# plot the changes before and after the treatment \npath.plot(\n    synth.res = synth.out,\n    dataprep.res = dataprep.out,\n    Ylab = \"real per-capita gdp (1986 USD, thousand)\",\n    Xlab = \"year\",\n    Ylim = c(0, 12),\n    Legend = c(\"Basque country\",\n               \"synthetic Basque country\"),\n    Legend.position = \"bottomright\"\n)\ngaps.plot(\n    synth.res = synth.out,\n    dataprep.res = dataprep.out,\n    Ylab =  \"gap in real per - capita GDP (1986 USD, thousand)\",\n    Xlab =  \"year\",\n    Ylim = c(-1.5, 1.5),\n    Main = NA\n)\nlibrary(DRDID)## Warning: package 'DRDID' was built under R version 4.0.5\ndata(nsw_long)\n# Form the Lalonde sample with CPS comparison group\neval_lalonde_cps <- subset(nsw_long, nsw_long$treated == 0 | nsw_long$sample == 2)\nout <-\n    drdid(\n        yname = \"re\",\n        tname = \"year\",\n        idname = \"id\",\n        dname = \"experimental\",\n        xformla = ~ age + educ + black + married + nodegree + hisp + re74,\n        data = eval_lalonde_cps,\n        panel = TRUE\n    )\nsummary(out)##  Call:\n## drdid(yname = \"re\", tname = \"year\", idname = \"id\", dname = \"experimental\", \n##     xformla = ~age + educ + black + married + nodegree + hisp + \n##         re74, data = eval_lalonde_cps, panel = TRUE)\n## ------------------------------------------------------------------\n##  Further improved locally efficient DR DID estimator for the ATT:\n##  \n##    ATT     Std. Error  t value    Pr(>|t|)  [95% Conf. Interval] \n## -901.2703   393.6247   -2.2897     0.022    -1672.7747  -129.766 \n## ------------------------------------------------------------------\n##  Estimator based on panel data.\n##  Outcome regression est. method: weighted least squares.\n##  Propensity score est. method: inverse prob. tilting.\n##  Analytical standard error.\n## ------------------------------------------------------------------\n##  See Sant'Anna and Zhao (2020) for details."},{"path":"quasi-experimental.html","id":"example-3","chapter":"20 Quasi-experimental","heading":"20.3.3 Example 3","text":"Synth package’s authorssynth() requires\\(X_1\\) vector treatment predictors\\(X_1\\) vector treatment predictors\\(X_0\\) matrix variables control group\\(X_0\\) matrix variables control group\\(Z_1\\) vector outcome variable treatment group\\(Z_1\\) vector outcome variable treatment group\\(Z_0\\) matrix outcome variable control group\\(Z_0\\) matrix outcome variable control groupuse dataprep() prepare data format can used throughout Synth packagefind optimal weights identifies synthetic control treatment groupYou also run placebo tests","code":"\nlibrary(Synth)\ndata(\"basque\")\ndataprep.out <- dataprep(\n    foo = basque,\n    predictors = c(\n        \"school.illit\",\n        \"school.prim\",\n        \"school.med\",\n        \"school.high\",\n        \"school.post.high\",\n        \"invest\"\n    ),\n    predictors.op = \"mean\",\n    time.predictors.prior = 1964:1969,\n    special.predictors = list(\n        list(\"gdpcap\", 1960:1969 , \"mean\"),\n        list(\"sec.agriculture\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.energy\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.industry\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.construction\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.services.venta\", seq(1961, 1969, 2), \"mean\"),\n        list(\"sec.services.nonventa\", seq(1961, 1969, 2), \"mean\"),\n        list(\"popdens\", 1969, \"mean\")\n    ),\n    dependent = \"gdpcap\",\n    unit.variable = \"regionno\",\n    unit.names.variable = \"regionname\",\n    time.variable = \"year\",\n    treatment.identifier = 17,\n    controls.identifier = c(2:16, 18),\n    time.optimize.ssr = 1960:1969,\n    time.plot = 1955:1997\n)\nsynth.out <- synth(data.prep.obj = dataprep.out, method = \"BFGS\")## \n## X1, X0, Z1, Z0 all come directly from dataprep object.\n## \n## \n## **************** \n##  searching for synthetic control unit  \n##  \n## \n## **************** \n## **************** \n## **************** \n## \n## MSPE (LOSS V): 0.008864606 \n## \n## solution.v:\n##  0.02773094 1.194e-07 1.60609e-05 0.0007163836 1.486e-07 0.002423908 0.0587055 0.2651997 0.02851006 0.291276 0.007994382 0.004053188 0.009398579 0.303975 \n## \n## solution.w:\n##  2.53e-08 4.63e-08 6.44e-08 2.81e-08 3.37e-08 4.844e-07 4.2e-08 4.69e-08 0.8508145 9.75e-08 3.2e-08 5.54e-08 0.1491843 4.86e-08 9.89e-08 1.162e-07\ngaps <- dataprep.out$Y1plot - (dataprep.out$Y0plot %*% synth.out$solution.w)\ngaps[1:3, 1]##       1955       1956       1957 \n## 0.15023473 0.09168035 0.03716475\nsynth.tables <-\n    synth.tab(dataprep.res = dataprep.out, synth.res = synth.out)\nnames(synth.tables) # you can pick tables to see ## [1] \"tab.pred\" \"tab.v\"    \"tab.w\"    \"tab.loss\"\npath.plot(\n    synth.res = synth.out,\n    dataprep.res = dataprep.out,\n    Ylab = \"real per-capita GDP (1986 USD, thousand)\",\n    Xlab = \"year\",\n    Ylim = c(0, 12),\n    Legend = c(\"Basque country\",\n               \"synthetic Basque country\"),\n    Legend.position = \"bottomright\"\n)\ngaps.plot(\n    synth.res = synth.out,\n    dataprep.res = dataprep.out,\n    Ylab = \"gap in real per-capita GDP (1986 USD, thousand)\",\n    Xlab = \"year\",\n    Ylim = c(-1.5, 1.5),\n    Main = NA\n)"},{"path":"quasi-experimental.html","id":"example-4","chapter":"20 Quasi-experimental","heading":"20.3.4 Example 4","text":"Michael Robbins Steven Davenport authors MicroSynth following improvements:Standardization use.survey = TRUE permutation ( perm = 250 jack = TRUE ) placebo testsStandardization use.survey = TRUE permutation ( perm = 250 jack = TRUE ) placebo testsOmnibus statistic (set omnibus.var ) multiple outcome variablesOmnibus statistic (set omnibus.var ) multiple outcome variablesincorporate multiple follow-periods end.postincorporate multiple follow-periods end.postNotes:predictors outcome used match units intervention\nOutcome variable time-variant\nPredictors time-invariant\npredictors outcome used match units interventionOutcome variable time-variantOutcome variable time-variantPredictors time-invariantPredictors time-invariant","code":"\nlibrary(microsynth)## Warning: package 'microsynth' was built under R version 4.0.5\ndata(\"seattledmi\")\n\ncov.var <- c(\"TotalPop\", \"BLACK\", \"HISPANIC\", \"Males_1521\", \"HOUSEHOLDS\", \n             \"FAMILYHOUS\", \"FEMALE_HOU\", \"RENTER_HOU\", \"VACANT_HOU\")\nmatch.out <- c(\"i_felony\", \"i_misdemea\", \"i_drugs\", \"any_crime\")\nsea1 <- microsynth(\n    seattledmi,\n    idvar = \"ID\",\n    timevar = \"time\",\n    intvar = \"Intervention\",\n    start.pre = 1,\n    end.pre = 12,\n    end.post = 16,\n    match.out = match.out, # outcome variable will be matched on exactly\n    match.covar = cov.var, # specify covariates will be matched on exactly\n    result.var = match.out, # used to report results\n    omnibus.var = match.out, # feature in the omnibus p-value\n    test = \"lower\",\n    n.cores = min(parallel::detectCores(), 2)\n)## Calculating weights...## Created main weights for synthetic control: Time = 1.03## Matching summary for main weights:##               Targets Weighted.Control All.scaled\n## Intercept          39          39.0002    39.0000\n## TotalPop         2994        2994.0519  2384.7477\n## BLACK             173         173.0010   190.5224\n## HISPANIC          149         149.0026   159.2682\n## Males_1521         49          49.0000    97.3746\n## HOUSEHOLDS       1968        1968.0340  1113.5588\n## FAMILYHOUS        519         519.0108   475.1876\n## FEMALE_HOU        101         101.0010    81.1549\n## RENTER_HOU       1868        1868.0203   581.9340\n## VACANT_HOU        160         160.0115    98.4222\n## i_felony.12        14          14.0000     4.9023\n## i_felony.11        11          11.0002     4.6313\n## i_felony.10         9           9.0000     3.0741\n## i_felony.9          5           5.0000     3.2642\n## i_felony.8         20          20.0000     4.4331\n## i_felony.7          8           8.0000     3.7617\n## i_felony.6         13          13.0000     3.0012\n## i_felony.5         20          20.0007     3.1549\n## i_felony.4         10          10.0000     4.0246\n## i_felony.3          7           7.0000     3.3693\n## i_felony.2         13          13.0002     3.2803\n## i_felony.1         12          12.0000     3.4381\n## i_misdemea.12      15          15.0002     4.2470\n## i_misdemea.11      12          12.0000     4.6070\n## i_misdemea.10      12          12.0000     4.0772\n## i_misdemea.9       14          14.0000     3.7414\n## i_misdemea.8       12          12.0000     3.9680\n## i_misdemea.7       20          20.0000     4.2551\n## i_misdemea.6       16          16.0005     3.5594\n## i_misdemea.5       24          24.0000     3.5635\n## i_misdemea.4       21          21.0002     4.3360\n## i_misdemea.3       21          21.0000     4.3846\n## i_misdemea.2       14          14.0000     3.5352\n## i_misdemea.1       16          16.0000     4.1540\n## i_drugs.12         13          13.0000     1.6543\n## i_drugs.11          8           8.0000     1.5128\n## i_drugs.10          3           3.0000     1.3227\n## i_drugs.9           4           4.0000     0.9788\n## i_drugs.8           4           4.0000     1.1123\n## i_drugs.7          10          10.0000     1.0516\n## i_drugs.6           4           4.0000     1.2377\n## i_drugs.5           2           2.0000     1.2296\n## i_drugs.4           1           1.0000     1.1245\n## i_drugs.3           5           5.0000     1.3550\n## i_drugs.2          12          12.0000     1.1366\n## i_drugs.1           8           8.0002     1.3591\n## any_crime.12      272         272.0012    65.3398\n## any_crime.11      227         227.0017    64.2396\n## any_crime.10      183         183.0010    55.6929\n## any_crime.9       176         176.0005    53.2377\n## any_crime.8       228         228.0005    55.8143\n## any_crime.7       246         246.0024    55.8062\n## any_crime.6       200         200.0010    52.8292\n## any_crime.5       270         270.0014    50.6531\n## any_crime.4       250         250.0010    57.2946\n## any_crime.3       236         236.0010    58.8681\n## any_crime.2       250         250.0012    51.5429\n## any_crime.1       242         242.0010    55.1145\n## \n## Calculation of weights complete: Total time = 1.68\n## \n## Calculating basic statistics for end.post = 16...\n## Completed calculation of basic statistics for end.post = 16.  Time = 3.22\n## \n## Calculating survey statistics for end.post = 16...\n## Completed survey statistics for main weights: Time = 5.51\n## Completed calculation of survey statistics for end.post = 16.  Time = 5.51\n## \n## microsynth complete: Overall time = 13.59\nsea1##  microsynth object\n## \n## Scope:\n##  Units:          Total: 9642 Treated: 39 Untreated: 9603\n##  Study Period(s):    Pre-period: 1 - 12  Post-period: 13 - 16\n##  Constraints:        Exact Match: 58     Minimized Distance: 0\n## Time-variant outcomes:\n##  Exact Match: i_felony, i_misdemea, i_drugs, any_crime (4)\n##  Minimized Distance: (0)\n## Time-invariant covariates:\n##  Exact Match: TotalPop, BLACK, HISPANIC, Males_1521, HOUSEHOLDS, FAMILYHOUS, FEMALE_HOU, RENTER_HOU, VACANT_HOU (9)\n##  Minimized Distance: (0)\n## \n## Results:\n## end.post = 16\n##            Trt    Con Pct.Chng Linear.pVal Linear.Lower Linear.Upper\n## i_felony    46  68.22   -32.6%      0.0109       -50.3%        -8.4%\n## i_misdemea  45  71.80   -37.3%      0.0019       -52.8%       -16.7%\n## i_drugs     20  23.76   -15.8%      0.2559       -46.4%        32.1%\n## any_crime  788 986.44   -20.1%      0.0146       -32.9%        -4.9%\n## Omnibus     --     --       --      0.0006           --           --\nsummary(sea1)## Weight Balance Table: \n## \n##               Targets Weighted.Control   All.scaled\n## Intercept          39        39.000239   39.0000000\n## TotalPop         2994      2994.051921 2384.7476665\n## BLACK             173       173.000957  190.5224020\n## HISPANIC          149       149.002632  159.2682016\n## Males_1521         49        49.000000   97.3746111\n## HOUSEHOLDS       1968      1968.033976 1113.5588052\n## FAMILYHOUS        519       519.010767  475.1876167\n## FEMALE_HOU        101       101.000957   81.1549471\n## RENTER_HOU       1868      1868.020338  581.9340386\n## VACANT_HOU        160       160.011485   98.4222153\n## i_felony.12        14        14.000000    4.9023024\n## i_felony.11        11        11.000239    4.6313006\n## i_felony.10         9         9.000000    3.0740510\n## i_felony.9          5         5.000000    3.2641568\n## i_felony.8         20        20.000000    4.4331052\n## i_felony.7          8         8.000000    3.7616677\n## i_felony.6         13        13.000000    3.0012446\n## i_felony.5         20        20.000718    3.1549471\n## i_felony.4         10        10.000000    4.0245800\n## i_felony.3          7         7.000000    3.3693217\n## i_felony.2         13        13.000239    3.2803360\n## i_felony.1         12        12.000000    3.4380834\n## i_misdemea.12      15        15.000239    4.2470442\n## i_misdemea.11      12        12.000000    4.6070317\n## i_misdemea.10      12        12.000000    4.0771624\n## i_misdemea.9       14        14.000000    3.7414437\n## i_misdemea.8       12        12.000000    3.9679527\n## i_misdemea.7       20        20.000000    4.2551338\n## i_misdemea.6       16        16.000479    3.5594275\n## i_misdemea.5       24        24.000000    3.5634723\n## i_misdemea.4       21        21.000239    4.3360299\n## i_misdemea.3       21        21.000000    4.3845675\n## i_misdemea.2       14        14.000000    3.5351587\n## i_misdemea.1       16        16.000000    4.1540137\n## i_drugs.12         13        13.000000    1.6543248\n## i_drugs.11          8         8.000000    1.5127567\n## i_drugs.10          3         3.000000    1.3226509\n## i_drugs.9           4         4.000000    0.9788426\n## i_drugs.8           4         4.000000    1.1123211\n## i_drugs.7          10        10.000000    1.0516490\n## i_drugs.6           4         4.000000    1.2377100\n## i_drugs.5           2         2.000000    1.2296204\n## i_drugs.4           1         1.000000    1.1244555\n## i_drugs.3           5         5.000000    1.3550093\n## i_drugs.2          12        12.000000    1.1365899\n## i_drugs.1           8         8.000239    1.3590541\n## any_crime.12      272       272.001196   65.3397635\n## any_crime.11      227       227.001675   64.2395769\n## any_crime.10      183       183.000957   55.6929060\n## any_crime.9       176       176.000479   53.2377100\n## any_crime.8       228       228.000479   55.8142502\n## any_crime.7       246       246.002393   55.8061605\n## any_crime.6       200       200.000957   52.8291848\n## any_crime.5       270       270.001436   50.6530803\n## any_crime.4       250       250.000957   57.2946484\n## any_crime.3       236       236.000957   58.8680772\n## any_crime.2       250       250.001196   51.5429371\n## any_crime.1       242       242.000957   55.1144991\n## \n## Results: \n## \n## end.post = 16\n##            Trt    Con Pct.Chng Linear.pVal Linear.Lower Linear.Upper\n## i_felony    46  68.22   -32.6%      0.0109       -50.3%        -8.4%\n## i_misdemea  45  71.80   -37.3%      0.0019       -52.8%       -16.7%\n## i_drugs     20  23.76   -15.8%      0.2559       -46.4%        32.1%\n## any_crime  788 986.44   -20.1%      0.0146       -32.9%        -4.9%\n## Omnibus     --     --       --      0.0006           --           --\nplot_microsynth(sea1)\nsea2 <- microsynth(seattledmi, \n                   idvar=\"ID\", timevar=\"time\", intvar=\"Intervention\", \n                   start.pre=1, end.pre=12, end.post=c(14, 16),\n                   match.out=match.out, match.covar=cov.var, \n                   result.var=match.out, omnibus.var=match.out, \n                   test=\"lower\", \n                   perm=250, jack=TRUE,\n                   n.cores = min(parallel::detectCores(), 2))"},{"path":"quasi-experimental.html","id":"selection-on-observables","chapter":"20 Quasi-experimental","heading":"20.4 Selection on observables","text":"ExampleAaronson, Barrow, Sander (2007)teachers qualifications (causally) affect student test scores?Step 1:\\[\nY_{ijt} = \\delta_0 + Y_{ij(t-1)} \\delta_1 + X_{} \\delta_2 + Z_{jt} \\delta_3 + \\epsilon_{ijt}\n\\]can always another variableAny observable sorting imperfectStep 2:\\[\nY_{ijst} = \\alpha_0 + Y_{ij(t-1)}\\alpha_1 + X_{} \\alpha_2 + Z_{jt} \\alpha_3 + \\gamma_s + u_{isjt}\n\\]\\(\\delta_3 >0\\)\\(\\delta_3 >0\\)\\(\\delta_3 > \\alpha_3\\)\\(\\delta_3 > \\alpha_3\\)\\(\\gamma_s\\) = school fixed effect\\(\\gamma_s\\) = school fixed effectSorting less within school. Hence, can introduce school fixed effectStep 3:Find schools look like putting students class randomly (good random) + run step 2\\[\nY_{isjt} = Y_{isj(t-1)} \\lambda + X_{} \\alpha_1 +Z_{jt} \\alpha_{21}+ (Z_{jt} \\times D_i)\\alpha_{22}+ \\gamma_5 + u_{isjt}\n\\]\\(D_{}\\) element \\(X_{}\\)\\(D_{}\\) element \\(X_{}\\)\\(Z_{}\\) = teacher experience\\(Z_{}\\) = teacher experience\\[\nD_{}=\n\\begin{cases}\n1 & \\text{ high poverty} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\\(H_0:\\) \\(\\alpha_{22} = 0\\) test effect heterogeneity whether effect teacher experience (\\(Z_{jt}\\)) differentFor low poverty \\(\\alpha_{21}\\)low poverty \\(\\alpha_{21}\\)high poverty effect \\(\\alpha_{21} + \\alpha_{22}\\)high poverty effect \\(\\alpha_{21} + \\alpha_{22}\\)","code":""},{"path":"quasi-experimental.html","id":"matching-methods","chapter":"20 Quasi-experimental","heading":"20.5 Matching Methods","text":"MotivationEffect college quality earningsThey ultimately estimate treatment effect treated attending top (high ACT) versus bottom (low ACT) quartile collegeMatching Selection observables works good observables.Relative OLSMatching makes common support explicit (changes default “ignore” “enforce”)Relaxes linear function form. Thus, less parametric.also helps high ratio controls treatments.detail summary (Stuart 2010)Matching defined “method aims equate (”balance“) distribution covariates treated control groups.” (Stuart 2010, 1)Equivalently, matching selection observables identifications strategy.think OLS estimate biased, matching estimate (almost surely) .Unconditionally, consider\\[\nE(Y_i^T | T) - E(Y_i^C |C) + E(Y_i^C | T) - E(Y_i^C | T) \\\\\n= E(Y_i^T - Y_i^C | T) + [E(Y_i^C | T) - E(Y_i^C |C)] \\\\\n= E(Y_i^T - Y_i^C | T) + \\text{selection bias}\n\\]\\(E(Y_i^T - Y_i^C | T)\\) causal inference want know.Randomization eliminates selection bias.don’t randomization, \\(E(Y_i^C | T) \\neq E(Y_i^C |C)\\)Matching tries selection observables \\(E(Y_i^C | X, T) = E(Y_i^C|X, C)\\)Propensity Scores basically \\(E(Y_i^C| P(X) , T) = E(Y_i^C | P(X), C)\\)Matching standard errors exceed OLS standard errorsThe treatment larger predictive power control use treatment pick control (control pick treatment).average treatment effect (ATE) \\[\n\\frac{1}{N_T} \\sum_{=1}^{N_T} (Y_i^T - \\frac{1}{N_{C_T}} \\sum_{=1}^{N_{C_T}} Y_i^C)\n\\]Since closed-form solution standard error average treatment effect, use bootstrapping get standard error.Professor Gary King advocates instead using word “matching,” use “pruning” (.e., deleting observations). preprocessing step prunes nonmatches make control variables less important analysis.Without MatchingImbalance data leads model dependence lead lot researcher discretion leads biasWith MatchingWe balance data essentially erase human discretionTable @ref(tab:Gary King - International Methods Colloquium talk 2015)Fully blocked superior onimbalanceimbalancemodel dependencemodel dependencepowerpowerefficiencyefficiencybiasbiasresearch costsresearch costsrobustnessrobustnessMatching used whenOutcomes available select subjects follow-upOutcomes available select subjects follow-upOutcomes available improve precision estimate (.e., reduce bias)Outcomes available improve precision estimate (.e., reduce bias)Hence, can observe one outcome unit (either treated control), can think problem missing data well. Thus, section closely related Imputation (Missing Data)observational studies, randomize treatment effect. Subjects select treatments, introduce selection bias (.e., systematic differences group differences confound effects response variable differences).Matching used toreduce model dependencereduce model dependencediagnose balance datasetdiagnose balance datasetAssumptions matching:treatment assignment independent potential outcomes given covariates\n\\(T \\perp (Y(0),Y(1))|X\\)\nknown ignorability, ignorable, hidden bias, unconfounded.\ntypically satisfy assumption unobserved covariates correlated observed covariates.\nunobserved covariates unrelated observed covariates, can use sensitivity analysis check result, use “design sensitivity” (Heller, Rosenbaum, Small 2009)\n\ntreatment assignment independent potential outcomes given covariates\\(T \\perp (Y(0),Y(1))|X\\)\\(T \\perp (Y(0),Y(1))|X\\)known ignorability, ignorable, hidden bias, unconfounded.known ignorability, ignorable, hidden bias, unconfounded.typically satisfy assumption unobserved covariates correlated observed covariates.\nunobserved covariates unrelated observed covariates, can use sensitivity analysis check result, use “design sensitivity” (Heller, Rosenbaum, Small 2009)\ntypically satisfy assumption unobserved covariates correlated observed covariates.unobserved covariates unrelated observed covariates, can use sensitivity analysis check result, use “design sensitivity” (Heller, Rosenbaum, Small 2009)positive probability receiving treatment X\n\\(0 < P(T=1|X)<1 \\forall X\\)\npositive probability receiving treatment X\\(0 < P(T=1|X)<1 \\forall X\\)Stable Unit Treatment value Assumption (SUTVA)\nOutcomes affected treatment B.\nhard cases “spillover” effects (interactions control treatment). combat, need reduce interactions.\n\nStable Unit Treatment value Assumption (SUTVA)Outcomes affected treatment B.\nhard cases “spillover” effects (interactions control treatment). combat, need reduce interactions.\nOutcomes affected treatment B.hard cases “spillover” effects (interactions control treatment). combat, need reduce interactions.Generalization\\(P_t\\): treated population -> \\(N_t\\): random sample treated\\(P_t\\): treated population -> \\(N_t\\): random sample treated\\(P_c\\): control population -> \\(N_c\\): random sample control\\(P_c\\): control population -> \\(N_c\\): random sample control\\(\\mu_i\\) = means ; \\(\\Sigma_i\\) = variance covariance matrix \\(p\\) covariates group (\\(= t,c\\))\\(\\mu_i\\) = means ; \\(\\Sigma_i\\) = variance covariance matrix \\(p\\) covariates group (\\(= t,c\\))\\(X_j\\) = \\(p\\) covariates individual \\(j\\)\\(X_j\\) = \\(p\\) covariates individual \\(j\\)\\(T_j\\) = treatment assignment\\(T_j\\) = treatment assignment\\(Y_j\\) = observed outcome\\(Y_j\\) = observed outcomeAssume: \\(N_t < N_c\\)Assume: \\(N_t < N_c\\)Treatment effect \\(\\tau(x) = R_1(x) - R_0(x)\\) \n\\(R_1(x) = E(Y(1)|X)\\)\n\\(R_0(x) = E(Y(0)|X)\\)\nTreatment effect \\(\\tau(x) = R_1(x) - R_0(x)\\) \\(R_1(x) = E(Y(1)|X)\\)\\(R_1(x) = E(Y(1)|X)\\)\\(R_0(x) = E(Y(0)|X)\\)\\(R_0(x) = E(Y(0)|X)\\)Assume: parallel trends hence \\(\\tau(x) = \\tau \\forall x\\)\nparallel trends assumed, average effect can estimated.\nAssume: parallel trends hence \\(\\tau(x) = \\tau \\forall x\\)parallel trends assumed, average effect can estimated.Common estimands:\nAverage effect treatment treated (ATT): effects treatment group\nAverage treatment effect (ATE): effect treatment control\nCommon estimands:Average effect treatment treated (ATT): effects treatment groupAverage effect treatment treated (ATT): effects treatment groupAverage treatment effect (ATE): effect treatment controlAverage treatment effect (ATE): effect treatment controlSteps:Define “closeness”: decide distance measure used\nvariables include:\nIgnorability (unobserved differences treatment control)\nSince cost including unrelated variables small, include many possible (unless sample size/power doesn’t allow increased variance)\ninclude variables affected treatment.\nNote: matching variable (.e., heavy drug users) highly correlated outcome variable (.e., heavy drinkers) , better exclude matching set.\n\n\ndistance measures: \nDefine “closeness”: decide distance measure usedWhich variables include:\nIgnorability (unobserved differences treatment control)\nSince cost including unrelated variables small, include many possible (unless sample size/power doesn’t allow increased variance)\ninclude variables affected treatment.\nNote: matching variable (.e., heavy drug users) highly correlated outcome variable (.e., heavy drinkers) , better exclude matching set.\n\nvariables include:Ignorability (unobserved differences treatment control)\nSince cost including unrelated variables small, include many possible (unless sample size/power doesn’t allow increased variance)\ninclude variables affected treatment.\nNote: matching variable (.e., heavy drug users) highly correlated outcome variable (.e., heavy drinkers) , better exclude matching set.\nIgnorability (unobserved differences treatment control)Since cost including unrelated variables small, include many possible (unless sample size/power doesn’t allow increased variance)Since cost including unrelated variables small, include many possible (unless sample size/power doesn’t allow increased variance)include variables affected treatment.include variables affected treatment.Note: matching variable (.e., heavy drug users) highly correlated outcome variable (.e., heavy drinkers) , better exclude matching set.Note: matching variable (.e., heavy drug users) highly correlated outcome variable (.e., heavy drinkers) , better exclude matching set.distance measures: belowWhich distance measures: belowMatching methods\nNearest neighbor matching\nSimple (greedy) matching: performs poorly competition controls.\nOptimal matching: considers global distance measure\nRatio matching: combat increase bias reduced variation k:1 matching, one can use approximations Rubin Thomas (1996).\nwithout replacement: replacement typically better, one needs account dependent matched sample later analysis (can use frequency weights combat).\n\nSubclassification, Full Matching Weighting\nNeareast neighbor matching assign 0 (control) 1 (treated), methods use weights 0 1.\nSubclassification: distribution multiple subclass (e.g., 5-10)\nFull matching: optimal ly minimize average distances treated unit control unit within matched set.\nWeighting adjustments: weighting technique uses propensity scores estimate ATE. weights extreme, variance can large due underlying probabilities, due estimation procure. combat , use (1) weight trimming, (2) doubly -robust methods propensity scores used weighing matching.\nInverse probability treatment weighting (IPTW) \\(w_i = \\frac{T_i}{\\hat{e}_i} + \\frac{1 - T_i}{1 - \\hat{e}_i}\\)\nOdds \\(w_i = T_i + (1-T_i) \\frac{\\hat{e}_i}{1-\\hat{e}_i}\\)\nKernel weighting (e.g., economics) averages multiple units control group.\n\n\nAssessing Common Support\ncommon support means overlapping propensity score distributions treatment control groups. Propensity score used discard control units common support. Alternatively, convex hull covariates multi-dimensional space.\n\nMatching methodsNearest neighbor matching\nSimple (greedy) matching: performs poorly competition controls.\nOptimal matching: considers global distance measure\nRatio matching: combat increase bias reduced variation k:1 matching, one can use approximations Rubin Thomas (1996).\nwithout replacement: replacement typically better, one needs account dependent matched sample later analysis (can use frequency weights combat).\nNearest neighbor matchingSimple (greedy) matching: performs poorly competition controls.Simple (greedy) matching: performs poorly competition controls.Optimal matching: considers global distance measureOptimal matching: considers global distance measureRatio matching: combat increase bias reduced variation k:1 matching, one can use approximations Rubin Thomas (1996).Ratio matching: combat increase bias reduced variation k:1 matching, one can use approximations Rubin Thomas (1996).without replacement: replacement typically better, one needs account dependent matched sample later analysis (can use frequency weights combat).without replacement: replacement typically better, one needs account dependent matched sample later analysis (can use frequency weights combat).Subclassification, Full Matching Weighting\nNeareast neighbor matching assign 0 (control) 1 (treated), methods use weights 0 1.\nSubclassification: distribution multiple subclass (e.g., 5-10)\nFull matching: optimal ly minimize average distances treated unit control unit within matched set.\nWeighting adjustments: weighting technique uses propensity scores estimate ATE. weights extreme, variance can large due underlying probabilities, due estimation procure. combat , use (1) weight trimming, (2) doubly -robust methods propensity scores used weighing matching.\nInverse probability treatment weighting (IPTW) \\(w_i = \\frac{T_i}{\\hat{e}_i} + \\frac{1 - T_i}{1 - \\hat{e}_i}\\)\nOdds \\(w_i = T_i + (1-T_i) \\frac{\\hat{e}_i}{1-\\hat{e}_i}\\)\nKernel weighting (e.g., economics) averages multiple units control group.\n\nSubclassification, Full Matching WeightingNeareast neighbor matching assign 0 (control) 1 (treated), methods use weights 0 1.Subclassification: distribution multiple subclass (e.g., 5-10)Subclassification: distribution multiple subclass (e.g., 5-10)Full matching: optimal ly minimize average distances treated unit control unit within matched set.Full matching: optimal ly minimize average distances treated unit control unit within matched set.Weighting adjustments: weighting technique uses propensity scores estimate ATE. weights extreme, variance can large due underlying probabilities, due estimation procure. combat , use (1) weight trimming, (2) doubly -robust methods propensity scores used weighing matching.\nInverse probability treatment weighting (IPTW) \\(w_i = \\frac{T_i}{\\hat{e}_i} + \\frac{1 - T_i}{1 - \\hat{e}_i}\\)\nOdds \\(w_i = T_i + (1-T_i) \\frac{\\hat{e}_i}{1-\\hat{e}_i}\\)\nKernel weighting (e.g., economics) averages multiple units control group.\nWeighting adjustments: weighting technique uses propensity scores estimate ATE. weights extreme, variance can large due underlying probabilities, due estimation procure. combat , use (1) weight trimming, (2) doubly -robust methods propensity scores used weighing matching.Inverse probability treatment weighting (IPTW) \\(w_i = \\frac{T_i}{\\hat{e}_i} + \\frac{1 - T_i}{1 - \\hat{e}_i}\\)Inverse probability treatment weighting (IPTW) \\(w_i = \\frac{T_i}{\\hat{e}_i} + \\frac{1 - T_i}{1 - \\hat{e}_i}\\)Odds \\(w_i = T_i + (1-T_i) \\frac{\\hat{e}_i}{1-\\hat{e}_i}\\)Odds \\(w_i = T_i + (1-T_i) \\frac{\\hat{e}_i}{1-\\hat{e}_i}\\)Kernel weighting (e.g., economics) averages multiple units control group.Kernel weighting (e.g., economics) averages multiple units control group.Assessing Common Support\ncommon support means overlapping propensity score distributions treatment control groups. Propensity score used discard control units common support. Alternatively, convex hull covariates multi-dimensional space.\nAssessing Common Supportcommon support means overlapping propensity score distributions treatment control groups. Propensity score used discard control units common support. Alternatively, convex hull covariates multi-dimensional space.Assessing quality matched samples (Diagnose)\nBalance = similarity empirical distribution full set covariates matched treated control groups. Equivalently, treatment unrelated covariates\n\\(\\tilde{p}(X|T=1) = \\tilde{p}(X|T=0)\\) \\(\\tilde{p}\\) empirical distribution.\n\nNumerical Diagnostics\nstandardized difference means covariate (common), also known “standardized bias,” “standardized difference means.”\nstandardized difference means propensity score (< 0.25) (Rubin 2001)\nratio variances propensity score treated control groups (0.5 2). (Rubin 2001)\ncovariate, ratio fo variance residuals orthogonal propensity score treated control groups.\nNote: can’t use hypothesis tests p-values (1) -sample property (population), (2) conflation changes balance changes statistical power.\n\nGraphical Diagnostics\nQQ plots\nEmpirical Distribution Plot\n\nAssessing quality matched samples (Diagnose)Balance = similarity empirical distribution full set covariates matched treated control groups. Equivalently, treatment unrelated covariates\n\\(\\tilde{p}(X|T=1) = \\tilde{p}(X|T=0)\\) \\(\\tilde{p}\\) empirical distribution.\nBalance = similarity empirical distribution full set covariates matched treated control groups. Equivalently, treatment unrelated covariates\\(\\tilde{p}(X|T=1) = \\tilde{p}(X|T=0)\\) \\(\\tilde{p}\\) empirical distribution.Numerical Diagnostics\nstandardized difference means covariate (common), also known “standardized bias,” “standardized difference means.”\nstandardized difference means propensity score (< 0.25) (Rubin 2001)\nratio variances propensity score treated control groups (0.5 2). (Rubin 2001)\ncovariate, ratio fo variance residuals orthogonal propensity score treated control groups.\nNote: can’t use hypothesis tests p-values (1) -sample property (population), (2) conflation changes balance changes statistical power.\nNumerical Diagnosticsstandardized difference means covariate (common), also known “standardized bias,” “standardized difference means.”standardized difference means covariate (common), also known “standardized bias,” “standardized difference means.”standardized difference means propensity score (< 0.25) (Rubin 2001)standardized difference means propensity score (< 0.25) (Rubin 2001)ratio variances propensity score treated control groups (0.5 2). (Rubin 2001)ratio variances propensity score treated control groups (0.5 2). (Rubin 2001)covariate, ratio fo variance residuals orthogonal propensity score treated control groups.\nNote: can’t use hypothesis tests p-values (1) -sample property (population), (2) conflation changes balance changes statistical power.covariate, ratio fo variance residuals orthogonal propensity score treated control groups.Note: can’t use hypothesis tests p-values (1) -sample property (population), (2) conflation changes balance changes statistical power.Graphical Diagnostics\nQQ plots\nEmpirical Distribution Plot\nGraphical DiagnosticsQQ plotsQQ plotsEmpirical Distribution PlotEmpirical Distribution PlotEstimate treatment effect\nk:1\nNeed account weights use matching replacement.\n\nSubclassification Full Matching\nweighting subclass estimates number treated units subclass ATT\nWEighting overall number individual subclass ATE.\n\nVariance estimation: incorporate uncertainties matching procedure (step 3) estimation procedure (step 4)\nEstimate treatment effectAfter k:1\nNeed account weights use matching replacement.\nk:1Need account weights use matching replacement.Subclassification Full Matching\nweighting subclass estimates number treated units subclass ATT\nWEighting overall number individual subclass ATE.\nSubclassification Full Matchingweighting subclass estimates number treated units subclass ATTweighting subclass estimates number treated units subclass ATTWEighting overall number individual subclass ATE.WEighting overall number individual subclass ATE.Variance estimation: incorporate uncertainties matching procedure (step 3) estimation procedure (step 4)Variance estimation: incorporate uncertainties matching procedure (step 3) estimation procedure (step 4)Notes:missing data, use generalized boosted models, multiple imputation (Qu Lipkovich 2009)missing data, use generalized boosted models, multiple imputation (Qu Lipkovich 2009)Violation ignorable treatment assignment (.e., unobservables affect treatment outcome). control \nmeasure pre-treatment measure outcome variable\nfind difference outcomes multiple control groups. significant difference, evidence violation.\nfind range correlations unobservables treatment assignment outcome nullify significant effect.\nViolation ignorable treatment assignment (.e., unobservables affect treatment outcome). control bymeasure pre-treatment measure outcome variablemeasure pre-treatment measure outcome variablefind difference outcomes multiple control groups. significant difference, evidence violation.find difference outcomes multiple control groups. significant difference, evidence violation.find range correlations unobservables treatment assignment outcome nullify significant effect.find range correlations unobservables treatment assignment outcome nullify significant effect.Choosing methods\nsmallest standardized difference mean across largest number covariates\nminimize standardized difference means particularly prognostic covariates\nfest number large standardized difference means (> 0.25)\n(Diamond Sekhon 2013) automates process\nChoosing methodssmallest standardized difference mean across largest number covariatessmallest standardized difference mean across largest number covariatesminimize standardized difference means particularly prognostic covariatesminimize standardized difference means particularly prognostic covariatesfest number large standardized difference means (> 0.25)fest number large standardized difference means (> 0.25)(Diamond Sekhon 2013) automates process(Diamond Sekhon 2013) automates processIn practice\nATE, ask enough overlap treated control groups’ propensity score estimate ATE, use ATT instead\nATT, ask controls across full range treated group\npracticeIf ATE, ask enough overlap treated control groups’ propensity score estimate ATE, use ATT insteadIf ATE, ask enough overlap treated control groups’ propensity score estimate ATE, use ATT insteadIf ATT, ask controls across full range treated groupIf ATT, ask controls across full range treated groupChoose matching method\nATE, use IPTW full matching\nATT, controls treated (least 3 times), k:1 nearest neighbor without replacement\nATT, controls , use subclassification, full matching, weighting odds\nChoose matching methodIf ATE, use IPTW full matchingIf ATE, use IPTW full matchingIf ATT, controls treated (least 3 times), k:1 nearest neighbor without replacementIf ATT, controls treated (least 3 times), k:1 nearest neighbor without replacementIf ATT, controls , use subclassification, full matching, weighting oddsIf ATT, controls , use subclassification, full matching, weighting oddsDiagnostic\nbalance, use regression matched samples\nimbalance covariates, treat Mahalanobis\nimbalance many covariates, try k:1 matching replacement\nDiagnosticIf balance, use regression matched samplesIf balance, use regression matched samplesIf imbalance covariates, treat MahalanobisIf imbalance covariates, treat MahalanobisIf imbalance many covariates, try k:1 matching replacementIf imbalance many covariates, try k:1 matching replacementWays define distance \\(D_{ij}\\)Exact\\[\nD_{ij} = \n\\begin{cases}\n0, \\text{ } X_i = X_j, \\\\\n\\infty, \\text{ } X_i \\neq X_j\n\\end{cases}\n\\]advanced Coarsened Exact MatchingMahalanobis\\[\nD_{ij} = (X_i - X_j)'\\Sigma^{-1} (X_i - X_j)\n\\]\\(\\Sigma\\) = variance covariance matrix X thecontrol group ATT interestedcontrol group ATT interestedpolled treatment control groups ATE interestedpolled treatment control groups ATE interestedPropensity score:\\[\nD_{ij} = |e_i - e_j|\n\\]\\(e_k\\) = propensity score individual kAn advanced Prognosis score (Hansen 2008), know (.e., specify) relationship covariates outcome.Linear propensity score\\[\nD_{ij} = |logit(e_i) - logit(e_j)|\n\\]exact Mahalanobis good high dimensional non normally distributed X’s cases.can combine Mahalanobis matching propensity score calipers (Rubin Thomas 2000)advanced methods longitudinal settingsmarginal structural models (Robins, Hernán, Brumback 2000)marginal structural models (Robins, Hernán, Brumback 2000)balanced risk set matching (Li, Propert, Rosenbaum 2001)balanced risk set matching (Li, Propert, Rosenbaum 2001)\nmatching methods based (ex-post)propensity scorepropensity scoredistance metricdistance metriccovariatescovariatesPackagescem Coarsened exact matchingcem Coarsened exact matchingMatching Multivariate propensity score matching balance optimizationMatching Multivariate propensity score matching balance optimizationMatchIt Nonparametric preprocessing parametric causal inference. nearest neighbor, Mahalanobis, caliper, exact, full, optimal, subclassificationMatchIt Nonparametric preprocessing parametric causal inference. nearest neighbor, Mahalanobis, caliper, exact, full, optimal, subclassificationMatchingFrontier optimize balance sample size (G. King, Lucas, Nielsen 2016)MatchingFrontier optimize balance sample size (G. King, Lucas, Nielsen 2016)optmatchoptimal matching variable ratio, optimal full matchingoptmatchoptimal matching variable ratio, optimal full matchingPSAgraphics Propensity score graphicsPSAgraphics Propensity score graphicsrbounds sensitivity analysis matched data, examine ignorable treatment assignment assumptionrbounds sensitivity analysis matched data, examine ignorable treatment assignment assumptiontwang weighting analysis non-equivalent groupstwang weighting analysis non-equivalent groupsCBPS covariate balancing propensity score. Can also used longitudinal setting marginal structural models.CBPS covariate balancing propensity score. Can also used longitudinal setting marginal structural models.PanelMatch based Imai, Kim, Wang (2018)PanelMatch based Imai, Kim, Wang (2018)Easier asses whether ’s workingEasier explainallows nice visualization evaluationHowever, problem omitted variables (.e., affect outcome whether observation treated) - unobserved confounders still present matching methods.Difference matching regression following Jorn-Ste§en Pischke’s lectureSuppose want estimate effect treatment treated\\[\n\\begin{aligned}\n\\delta_{TOT} &= E[ Y_{1i} - Y_{0i} | D_i = 1 ] \\\\\n&= E\\{E[Y_{1i} | X_i, D_i = 1] - E[Y_{0i}|X_i, D_i = 1]|D_i = 1\\} && \\text{law itereated expectations}\n\\end{aligned}\n\\]conditional independence\\[\nE[Y_{0i} |X_i , D_i = 0 ] = E[Y_{0i} | X_i, D_i = 1]\n\\]\\[\n\\begin{aligned}\n\\delta_{TOT} &= E \\{ E[ Y_{1i} | X_i, D_i = 1] - E[ Y_{0i}|X_i, D_i = 0 ]|D_i = 1\\} \\\\\n&= E\\{E[y_i | X_i, D_i = 1] - E[y_i |X_i, D_i = 0 ] | D_i = 1\\} \\\\\n&= E[\\delta_X |D_i = 1]\n\\end{aligned}\n\\]\\(\\delta_X\\) X-specific difference means covariate value \\(X_i\\)\\(X_i\\) discrete, matching estimand \\[\n\\delta_M = \\sum_x \\delta_x P(X_i = x |D_i = 1)\n\\]\\(P(X_i = x |D_i = 1)\\) probability mass function \\(X_i\\) given \\(D_i = 1\\)According Bayes rule,\\[\nP(X_i = x | D_i = 1) = \\frac{P(D_i = 1 | X_i = x) \\times P(X_i = x)}{P(D_i = 1)}\n\\]hence,\\[\n\\begin{aligned}\n\\delta_M &= \\frac{\\sum_x \\delta_x P (D_i = 1 | X_i = x) P (X_i = x)}{\\sum_x P(D_i = 1 |X_i = x)P(X_i = x)} \\\\\n&= \\sum_x \\delta_x \\frac{ P (D_i = 1 | X_i = x) P (X_i = x)}{\\sum_x P(D_i = 1 |X_i = x)P(X_i = x)}\n\\end{aligned}\n\\]hand, suppose regression\\[\ny_i = \\sum_x d_{ix} \\beta_x + \\delta_R D_i + \\epsilon_i\n\\]\\(d_{ix}\\) = dummy indicates \\(X_i = x\\)\\(d_{ix}\\) = dummy indicates \\(X_i = x\\)\\(\\beta_x\\) = regression-effect \\(X_i = x\\)\\(\\beta_x\\) = regression-effect \\(X_i = x\\)\\(\\delta_R\\) = regression estimand \\(\\delta_R\\) = regression estimand \\[\n\\begin{aligned}\n\\delta_R &= \\frac{\\sum_x \\delta_x [P(D_i = 1 | X_i = x) (1 - P(D_i = 1 | X_i = x))]P(X_i = x)}{\\sum_x [P(D_i = 1| X_i = x)(1 - P(D_i = 1 | X_i = x))]P(X_i = x)} \\\\\n&= \\sum_x \\delta_x \\frac{[P(D_i = 1 | X_i = x) (1 - P(D_i = 1 | X_i = x))]P(X_i = x)}{\\sum_x [P(D_i = 1| X_i = x)(1 - P(D_i = 1 | X_i = x))]P(X_i = x)}\n\\end{aligned}\n\\]difference regression matching estimand weights use combine covariate specific treatment effect \\(\\delta_x\\)\\(P(D_i = 1|X_i = x)\\)fraction treated observations covariate cell (.e., mean \\(D_i\\))\\(P(D_i = 1 |X_i = x)(1 - P(D_i = 1| X_i ))\\)variance \\(D_i\\) covariate cellThe goal matching produce covariate balance (.e., distributions covariates treatment control groups approximately similar successful randomized experiment).","code":""},{"path":"quasi-experimental.html","id":"matchit","chapter":"20 Quasi-experimental","heading":"20.5.1 MatchIt","text":"Procedure typically involves (proposed Noah Freifer using MatchIt)planningmatchingchecking (balance)estimating treatment effectexamine treat re78Planningselect type effect estimated (e.g., mediation effect, conditional effect, marginal effect)select type effect estimated (e.g., mediation effect, conditional effect, marginal effect)select target populationselect target populationselect variables match/balance (Austin 2011) (VanderWeele 2019)select variables match/balance (Austin 2011) (VanderWeele 2019)Check Initial ImbalanceMatchingCheck balanceSometimes make trade-balance sample size.Try Full Match (.e., every treated matches one control, every control one treated).Checking balance againExact MatchingSubclassficationOptimal MatchingGenetic MatchingEstimating Treatment Effecttreat coefficient = estimated ATTWhen reporting, remember mentionthe matching specification (method, additional options)distance measure (e.g., propensity score)methods, rationale final chosen method.balance statistics matched dataset.number matched, unmatched, discardedestimation method treatment effect.","code":"\nlibrary(MatchIt)## Warning: package 'MatchIt' was built under R version 4.0.5\ndata(\"lalonde\")\n# No matching; constructing a pre-match matchit object\nm.out0 <- matchit(\n    treat ~ age + educ + race + married +\n        nodegree + re74 + re75,\n    data = lalonde,\n    method = NULL, # assess balance before matching\n    distance = \"glm\" # logistic regression\n)\n\n# Checking balance prior to matching\nsummary(m.out0)## \n## Call:\n## matchit(formula = treat ~ age + educ + race + married + nodegree + \n##     re74 + re75, data = lalonde, method = NULL, distance = \"glm\")\n## \n## Summary of Balance for All Data:\n##            Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n## distance          0.5774        0.1822          1.7941     0.9211    0.3774\n## age              25.8162       28.0303         -0.3094     0.4400    0.0813\n## educ             10.3459       10.2354          0.0550     0.4959    0.0347\n## raceblack         0.8432        0.2028          1.7615          .    0.6404\n## racehispan        0.0595        0.1422         -0.3498          .    0.0827\n## racewhite         0.0973        0.6550         -1.8819          .    0.5577\n## married           0.1892        0.5128         -0.8263          .    0.3236\n## nodegree          0.7081        0.5967          0.2450          .    0.1114\n## re74           2095.5737     5619.2365         -0.7211     0.5181    0.2248\n## re75           1532.0553     2466.4844         -0.2903     0.9563    0.1342\n##            eCDF Max\n## distance     0.6444\n## age          0.1577\n## educ         0.1114\n## raceblack    0.6404\n## racehispan   0.0827\n## racewhite    0.5577\n## married      0.3236\n## nodegree     0.1114\n## re74         0.4470\n## re75         0.2876\n## \n## \n## Sample Sizes:\n##           Control Treated\n## All           429     185\n## Matched       429     185\n## Unmatched       0       0\n## Discarded       0       0\n# 1:1 NN PS matching w/o replacement\nm.out1 <- matchit(treat ~ age + educ + race + married + \n                   nodegree + re74 + re75, data = lalonde,\n                 method = \"nearest\", distance = \"glm\")\nm.out1## A matchit object\n##  - method: Variable ratio 1:1 nearest neighbor matching without replacement\n##  - distance: Propensity score\n##              - estimated with logistic regression\n##  - number of obs.: 614 (original), 370 (matched)\n##  - target estimand: ATT\n##  - covariates: age, educ, race, married, nodegree, re74, re75\n# Checking balance after NN matching\nsummary(m.out1, un = FALSE)## \n## Call:\n## matchit(formula = treat ~ age + educ + race + married + nodegree + \n##     re74 + re75, data = lalonde, method = \"nearest\", distance = \"glm\")\n## \n## Summary of Balance for Matched Data:\n##            Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n## distance          0.5774        0.3629          0.9739     0.7566    0.1321\n## age              25.8162       25.3027          0.0718     0.4568    0.0847\n## educ             10.3459       10.6054         -0.1290     0.5721    0.0239\n## raceblack         0.8432        0.4703          1.0259          .    0.3730\n## racehispan        0.0595        0.2162         -0.6629          .    0.1568\n## racewhite         0.0973        0.3135         -0.7296          .    0.2162\n## married           0.1892        0.2108         -0.0552          .    0.0216\n## nodegree          0.7081        0.6378          0.1546          .    0.0703\n## re74           2095.5737     2342.1076         -0.0505     1.3289    0.0469\n## re75           1532.0553     1614.7451         -0.0257     1.4956    0.0452\n##            eCDF Max Std. Pair Dist.\n## distance     0.4216          0.9740\n## age          0.2541          1.3938\n## educ         0.0757          1.2474\n## raceblack    0.3730          1.0259\n## racehispan   0.1568          1.0743\n## racewhite    0.2162          0.8390\n## married      0.0216          0.8281\n## nodegree     0.0703          1.0106\n## re74         0.2757          0.7965\n## re75         0.2054          0.7381\n## \n## Sample Sizes:\n##           Control Treated\n## All           429     185\n## Matched       185     185\n## Unmatched     244       0\n## Discarded       0       0\n# examine visually \nplot(m.out1, type = \"jitter\", interactive = FALSE)\nplot(m.out1, type = \"qq\", interactive = FALSE,\n     which.xs = c(\"age\", \"married\", \"re75\"))\n# Full matching on a probit PS\nm.out2 <- matchit(treat ~ age + educ + race + married + \n                   nodegree + re74 + re75, data = lalonde,\n                 method = \"full\", distance = \"glm\", link = \"probit\")\nm.out2## A matchit object\n##  - method: Optimal full matching\n##  - distance: Propensity score\n##              - estimated with probit regression\n##  - number of obs.: 614 (original), 614 (matched)\n##  - target estimand: ATT\n##  - covariates: age, educ, race, married, nodegree, re74, re75\n# Checking balance after full matching\nsummary(m.out2, un = FALSE)## \n## Call:\n## matchit(formula = treat ~ age + educ + race + married + nodegree + \n##     re74 + re75, data = lalonde, method = \"full\", distance = \"glm\", \n##     link = \"probit\")\n## \n## Summary of Balance for Matched Data:\n##            Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n## distance          0.5773        0.5765          0.0040     0.9943    0.0042\n## age              25.8162       25.6722          0.0201     0.4614    0.0848\n## educ             10.3459       10.3693         -0.0116     0.6173    0.0194\n## raceblack         0.8432        0.8389          0.0119          .    0.0043\n## racehispan        0.0595        0.0500          0.0402          .    0.0095\n## racewhite         0.0973        0.1111         -0.0467          .    0.0138\n## married           0.1892        0.1580          0.0797          .    0.0312\n## nodegree          0.7081        0.6898          0.0404          .    0.0184\n## re74           2095.5737     2103.5534         -0.0016     1.3513    0.0328\n## re75           1532.0553     1552.4673         -0.0063     1.5678    0.0496\n##            eCDF Max Std. Pair Dist.\n## distance     0.0541          0.0198\n## age          0.2846          1.2741\n## educ         0.0597          1.2233\n## raceblack    0.0043          0.0162\n## racehispan   0.0095          0.4985\n## racewhite    0.0138          0.3911\n## married      0.0312          0.4866\n## nodegree     0.0184          0.9593\n## re74         0.2159          0.8533\n## re75         0.2013          0.8279\n## \n## Sample Sizes:\n##               Control Treated\n## All            429.       185\n## Matched (ESS)   53.51     185\n## Matched        429.       185\n## Unmatched        0.         0\n## Discarded        0.         0\nplot(summary(m.out2))\n# Full matching on a probit PS\nm.out3 <- matchit(treat ~ age + educ + race + married + \n                   nodegree + re74 + re75, data = lalonde,\n                 method = \"exact\")\nm.out3## A matchit object\n##  - method: Exact matching\n##  - number of obs.: 614 (original), 25 (matched)\n##  - target estimand: ATT\n##  - covariates: age, educ, race, married, nodegree, re74, re75\nm.out4 <- matchit(treat ~ age + educ + race + married +\n                   nodegree + re74 + re75, data = lalonde,\n                 method = \"subclass\")\nm.out4## A matchit object\n##  - method: Subclassification (6 subclasses)\n##  - distance: Propensity score\n##              - estimated with logistic regression\n##  - number of obs.: 614 (original), 614 (matched)\n##  - target estimand: ATT\n##  - covariates: age, educ, race, married, nodegree, re74, re75\n# Or you can use in conjunction with \"nearest\"\nm.out4 <- matchit(treat ~ age + educ + race + married + \n                   nodegree + re74 + re75, data = lalonde,\n                 method = \"nearest\", option = \"subclass\")\nm.out4## A matchit object\n##  - method: Variable ratio 1:1 nearest neighbor matching without replacement\n##  - distance: Propensity score\n##              - estimated with logistic regression\n##  - number of obs.: 614 (original), 370 (matched)\n##  - target estimand: ATT\n##  - covariates: age, educ, race, married, nodegree, re74, re75\nm.out5 <- matchit(treat ~ age + educ + race + married + \n                   nodegree + re74 + re75, data = lalonde,\n                 method = \"optimal\",ratio = 2)\nm.out5## A matchit object\n##  - method: Variable ratio 2:1 optimal pair matching\n##  - distance: Propensity score\n##              - estimated with logistic regression\n##  - number of obs.: 614 (original), 555 (matched)\n##  - target estimand: ATT\n##  - covariates: age, educ, race, married, nodegree, re74, re75\nm.out6 <- matchit(treat ~ age + educ + race + married + \n                   nodegree + re74 + re75, data = lalonde,\n                 method = \"genetic\")## Warning: (from Matching) The key tuning parameters for optimization were are\n## all left at their default values. The 'pop.size' option in particular should\n## probably be increased for optimal results. For details please see the help page\n## and http://sekhon.berkeley.edu/papers/MatchingJSS.pdf\nm.out6## A matchit object\n##  - method: 1:1 genetic matching without replacement\n##  - distance: Propensity score\n##              - estimated with logistic regression\n##  - number of obs.: 614 (original), 370 (matched)\n##  - target estimand: ATT\n##  - covariates: age, educ, race, married, nodegree, re74, re75\n# get matched data\nm.data1 <- match.data(m.out1)\n\nhead(m.data1)##      treat age educ   race married nodegree re74 re75       re78  distance\n## NSW1     1  37   11  black       1        1    0    0  9930.0460 0.6387699\n## NSW2     1  22    9 hispan       0        1    0    0  3595.8940 0.2246342\n## NSW3     1  30   12  black       0        0    0    0 24909.4500 0.6782439\n## NSW4     1  27   11  black       0        1    0    0  7506.1460 0.7763241\n## NSW5     1  33    8  black       0        1    0    0   289.7899 0.7016387\n## NSW6     1  22    9  black       0        1    0    0  4056.4940 0.6990699\n##      weights subclass\n## NSW1       1        1\n## NSW2       1       98\n## NSW3       1      109\n## NSW4       1      120\n## NSW5       1      131\n## NSW6       1      142\nlibrary(\"lmtest\") #coeftest\nlibrary(\"sandwich\") #vcovCL\n\n# imbalance matched dataset\nfit1 <- lm(re78 ~ treat + age + educ + race + married + nodegree + \n             re74 + re75, data = m.data1, weights = weights)\n\ncoeftest(fit1, vcov. = vcovCL, cluster = ~subclass)## \n## t test of coefficients:\n## \n##                Estimate  Std. Error t value Pr(>|t|)   \n## (Intercept) -2.5816e+03  3.3209e+03 -0.7774 0.437439   \n## treat        1.3449e+03  7.3084e+02  1.8403 0.066552 . \n## age          7.8035e+00  4.4148e+01  0.1768 0.859797   \n## educ         6.0220e+02  2.1007e+02  2.8667 0.004391 **\n## racehispan   1.5335e+03  1.0248e+03  1.4964 0.135417   \n## racewhite    4.6943e+02  8.9854e+02  0.5224 0.601687   \n## married     -1.5825e+02  9.3354e+02 -0.1695 0.865482   \n## nodegree     9.2328e+02  1.1496e+03  0.8032 0.422412   \n## re74         2.6362e-02  1.6646e-01  0.1584 0.874257   \n## re75         2.2068e-01  1.6771e-01  1.3158 0.189069   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# balance matched dataset \nm.data2 <- match.data(m.out2)\n\nfit2 <- lm(re78 ~ treat + age + educ + race + married + nodegree + \n             re74 + re75, data = m.data2, weights = weights)\n\ncoeftest(fit2, vcov. = vcovCL, cluster = ~subclass)## \n## t test of coefficients:\n## \n##                Estimate  Std. Error t value  Pr(>|t|)    \n## (Intercept)  2.8493e+03  3.1547e+03  0.9032 0.3667819    \n## treat        1.9797e+03  7.5611e+02  2.6183 0.0090589 ** \n## age         -4.5799e+01  3.7917e+01 -1.2079 0.2275592    \n## educ         2.3234e+02  2.0245e+02  1.1477 0.2515594    \n## racehispan   9.6380e+02  1.4435e+03  0.6677 0.5045794    \n## racewhite    1.7067e+03  8.2231e+02  2.0755 0.0383636 *  \n## married      9.0378e+02  1.1858e+03  0.7622 0.4462384    \n## nodegree    -1.2712e+03  1.2691e+03 -1.0017 0.3169017    \n## re74        -1.1459e-02  1.4547e-01 -0.0788 0.9372369    \n## re75         5.4080e-01  1.4212e-01  3.8053 0.0001561 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"quasi-experimental.html","id":"matchingfrontier","chapter":"20 Quasi-experimental","heading":"20.5.2 MatchingFrontier","text":"mentioned MatchIt, make trade-(also known bias-variance trade-) balance sample size. automated procedure optimize trade-implemented MatchingFrontier (G. King, Lucas, Nielsen 2016), solves joint optimization problem.follow MatchingFrontier guide","code":"\n# library(devtools)\n# install_github('ChristopherLucas/MatchingFrontier')\nlibrary(MatchingFrontier)\ndata(\"lalonde\")\n# choose var to match on\nmatch.on <- colnames(lalonde)[!(colnames(lalonde) %in% c('re78', 'treat'))]\nmatch.on## [1] \"age\"       \"education\" \"black\"     \"hispanic\"  \"married\"   \"nodegree\" \n## [7] \"re74\"      \"re75\"\n# Mahanlanobis frontier (default)\nmahal.frontier <-\n    makeFrontier(\n        dataset = lalonde,\n        treatment = \"treat\",\n        match.on = match.on\n    )## Calculating Mahalanobis distances...\n## Calculating theoretical frontier...\n## Calculating information for plotting the frontier...\nmahal.frontier## An imbalance frontier with 997 points.\n# L1 frontier\nL1.frontier <-\n    makeFrontier(\n        dataset = lalonde,\n        treatment = 'treat',\n        match.on = match.on,\n        QOI = 'SATT',\n        metric = 'L1',\n        ratio = 'fixed'\n    )## Calculating L1 binnings...\n## Calculating L1 frontier... This may take a few minutes...\nL1.frontier## An imbalance frontier with 976 points.\n# estimate effects along the frontier\n\n# Set base form\nmy.form <-\n    as.formula(re78 ~ treat + age + black + education + hispanic + married + nodegree + re74 + re75)\n\n# Estimate effects for the mahalanobis frontier\nmahal.estimates <-\n    estimateEffects(\n        mahal.frontier,\n        're78 ~ treat',\n        mod.dependence.formula = my.form,\n        continuous.vars = c('age', 'education', 're74', 're75'),\n        prop.estimated = .1,\n        means.as.cutpoints = TRUE\n    )## \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n# Estimate effects for the L1 frontier\nL1.estimates <-\n    estimateEffects(\n        L1.frontier,\n        're78 ~ treat',\n        mod.dependence.formula = my.form,\n        continuous.vars = c('age', 'education', 're74', 're75'),\n        prop.estimated = .1,\n        means.as.cutpoints = TRUE\n    )## \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n# Plot covariates means \n# plotPrunedMeans()\n\n\n# Plot estimates (deprecated)\n# plotEstimates(\n#     L1.estimates,\n#     ylim = c(-10000, 3000),\n#     cex.lab = 1.4,\n#     cex.axis = 1.4,\n#     panel.first = grid(NULL, NULL, lwd = 2,)\n# )\n\n# Plot estimates\nplotMeans(L1.frontier)\n# parallel plot\nparallelPlot(\n    L1.frontier,\n    N = 400,\n    variables = c('age', 're74', 're75', 'black'),\n    treated.col = 'blue',\n    control.col = 'gray'\n)\n# export matched dataset\nmatched.data <- generateDataset(L1.frontier, N = 400) # take 400 units"},{"path":"quasi-experimental.html","id":"propensity-scores","chapter":"20 Quasi-experimental","heading":"20.5.3 Propensity Scores","text":"Even though mention propensity scores matching method , longer recommended use method research publication (G. King Nielsen 2019) increasesimbalanceimbalanceinefficiencyinefficiencymodel dependence: small changes model specification lead big changes model resultsmodel dependence: small changes model specification lead big changes model resultsbiasbiasPSM tries accomplish complete randomization methods try achieve fully blocked. Hence, probably better use methods.Propensity “probability receiving treatment given observed covariates.” (Rosenbaum Rubin 1985)Equivalently, can understood probability treated.\\[\ne_i (X_i) = P(T_i = 1 | X_i)\n\\]Estimation usinglogistic regressionlogistic regressionNon parametric methods:\nboosted CART\ngeneralized boosted models (gbm)\nNon parametric methods:boosted CARTboosted CARTgeneralized boosted models (gbm)generalized boosted models (gbm)Steps Gary King’s slidesreduce k elements X scalarreduce k elements X scalar\\(\\pi_i \\equiv P(T_i = 1|X) = \\frac{1}{1+e^{X_i \\beta}}\\)\\(\\pi_i \\equiv P(T_i = 1|X) = \\frac{1}{1+e^{X_i \\beta}}\\)Distance (\\(X_c, X_t\\)) = \\(|\\pi_c - \\pi_t|\\)Distance (\\(X_c, X_t\\)) = \\(|\\pi_c - \\pi_t|\\)match treated unit nearest control unitmatch treated unit nearest control unitcontrol units: reused; pruned unusedcontrol units: reused; pruned unusedprune matches distances > caliperprune matches distances > caliperIn best case scenario, randomly prune, increases imbalanceOther methods dominate try match exactly hence\\(X_c = X_t \\\\pi_c = \\pi_t\\) (exact match leads equal propensity scores) \\(X_c = X_t \\\\pi_c = \\pi_t\\) (exact match leads equal propensity scores) \\(\\pi_c = \\pi_t \\nrightarrow X_c = X_t\\) (equal propensity scores necessarily lead exact match)\\(\\pi_c = \\pi_t \\nrightarrow X_c = X_t\\) (equal propensity scores necessarily lead exact match)include/control irrelevant covariates leads PSM random, hence imbalanceWhat left pruning important start throw .Diagnostics:balance covariatesbalance covariatesno need concern collinearityno need concern collinearitycan’t use c-stat stepwise model fit stat applycan’t use c-stat stepwise model fit stat apply","code":""},{"path":"quasi-experimental.html","id":"mahalanobis-distance","chapter":"20 Quasi-experimental","heading":"20.5.4 Mahalanobis Distance","text":"Approximates fully blocked experimentDistance \\((X_c,X_t)\\) = \\(\\sqrt{(X_c - X_t)'S^{-1}(X_c - X_t)}\\)\\(S^{-1}\\) standardize distanceIn application use Euclidean distance.Prune unused control units, prune matches distance > caliper","code":""},{"path":"quasi-experimental.html","id":"coarsened-exact-matching","chapter":"20 Quasi-experimental","heading":"20.5.5 Coarsened Exact Matching","text":"Steps Gray King’s slides International Methods Colloquium talk 2015Temporarily coarsen XTemporarily coarsen XApply exact matching coarsened X, C(X)\nsort observation strata, unique values C(X)\nprune stratum 0 treated 0 control units\nApply exact matching coarsened X, C(X)sort observation strata, unique values C(X)sort observation strata, unique values C(X)prune stratum 0 treated 0 control unitsprune stratum 0 treated 0 control unitsPass original (uncoarsened) units except prunedPass original (uncoarsened) units except prunedProperties:Monotonic imbalance bounding (MIB) matching method\nmaximum imbalance treated control chosen ex ante\nMonotonic imbalance bounding (MIB) matching methodmaximum imbalance treated control chosen ex antemeets congruence principlemeets congruence principlerobust measurement errorrobust measurement errorcan implemented multiple imputationcan implemented multiple imputationworks well multi-category treatmentsworks well multi-category treatmentsAssumptions:Ignorability (.e., omitted variable bias)detail (Iacus, King, Porro 2012)Example package’s authorsautomated coarseningcoarsening explicit user choiceCan also use progressive coarsening method control number matches.Can also use progressive coarsening method control number matches.cem can also handle missingness.cem can also handle missingness.","code":"\nlibrary(cem)## Warning: package 'cem' was built under R version 4.0.5## Warning: package 'lattice' was built under R version 4.0.5\ndata(LeLonde)\n\nLe <- data.frame(na.omit(LeLonde)) # remove missing data\n# treated and control groups\ntr <- which(Le$treated==1)\nct <- which(Le$treated==0)\nntr <- length(tr)\nnct <- length(ct)\n\n# unadjusted, biased difference in means\nmean(Le$re78[tr]) - mean(Le$re78[ct])## [1] 759.0479\n# pre-treatment covariates\nvars <-\n    c(\n        \"age\",\n        \"education\",\n        \"black\",\n        \"married\",\n        \"nodegree\",\n        \"re74\",\n        \"re75\",\n        \"hispanic\",\n        \"u74\",\n        \"u75\",\n        \"q1\"\n    )\n\n# overall imbalance statistics\nimbalance(group=Le$treated, data=Le[vars]) # L1 = 0.902## \n## Multivariate Imbalance Measure: L1=0.902\n## Percentage of local common support: LCS=5.8%\n## \n## Univariate Imbalance Measures:\n## \n##               statistic   type           L1 min 25%      50%       75%\n## age        -0.252373042 (diff) 5.102041e-03   0   0   0.0000   -1.0000\n## education   0.153634710 (diff) 8.463851e-02   1   0   1.0000    1.0000\n## black      -0.010322734 (diff) 1.032273e-02   0   0   0.0000    0.0000\n## married    -0.009551495 (diff) 9.551495e-03   0   0   0.0000    0.0000\n## nodegree   -0.081217371 (diff) 8.121737e-02   0  -1   0.0000    0.0000\n## re74      -18.160446880 (diff) 5.551115e-17   0   0 284.0715  806.3452\n## re75      101.501761679 (diff) 5.551115e-17   0   0 485.6310 1238.4114\n## hispanic   -0.010144756 (diff) 1.014476e-02   0   0   0.0000    0.0000\n## u74        -0.045582186 (diff) 4.558219e-02   0   0   0.0000    0.0000\n## u75        -0.065555292 (diff) 6.555529e-02   0   0   0.0000    0.0000\n## q1          7.494021189 (Chi2) 1.067078e-01  NA  NA       NA        NA\n##                  max\n## age          -6.0000\n## education     1.0000\n## black         0.0000\n## married       0.0000\n## nodegree      0.0000\n## re74      -2139.0195\n## re75        490.3945\n## hispanic      0.0000\n## u74           0.0000\n## u75           0.0000\n## q1                NA\n# drop other variables that are not pre-treatmentt matching variables\ntodrop <- c(\"treated\", \"re78\")\nimbalance(group=Le$treated, data=Le, drop=todrop)## \n## Multivariate Imbalance Measure: L1=0.902\n## Percentage of local common support: LCS=5.8%\n## \n## Univariate Imbalance Measures:\n## \n##               statistic   type           L1 min 25%      50%       75%\n## age        -0.252373042 (diff) 5.102041e-03   0   0   0.0000   -1.0000\n## education   0.153634710 (diff) 8.463851e-02   1   0   1.0000    1.0000\n## black      -0.010322734 (diff) 1.032273e-02   0   0   0.0000    0.0000\n## married    -0.009551495 (diff) 9.551495e-03   0   0   0.0000    0.0000\n## nodegree   -0.081217371 (diff) 8.121737e-02   0  -1   0.0000    0.0000\n## re74      -18.160446880 (diff) 5.551115e-17   0   0 284.0715  806.3452\n## re75      101.501761679 (diff) 5.551115e-17   0   0 485.6310 1238.4114\n## hispanic   -0.010144756 (diff) 1.014476e-02   0   0   0.0000    0.0000\n## u74        -0.045582186 (diff) 4.558219e-02   0   0   0.0000    0.0000\n## u75        -0.065555292 (diff) 6.555529e-02   0   0   0.0000    0.0000\n## q1          7.494021189 (Chi2) 1.067078e-01  NA  NA       NA        NA\n##                  max\n## age          -6.0000\n## education     1.0000\n## black         0.0000\n## married       0.0000\n## nodegree      0.0000\n## re74      -2139.0195\n## re75        490.3945\n## hispanic      0.0000\n## u74           0.0000\n## u75           0.0000\n## q1                NA\nmat <- cem(treatment = \"treated\", data = Le, drop = \"re78\",keep.all=TRUE)## \n## Using 'treated'='1' as baseline group\nmat##            G0  G1\n## All       392 258\n## Matched    95  84\n## Unmatched 297 174\n# mat$w\n# categorial variables\nlevels(Le$q1) # grouping option## [1] \"agree\"             \"disagree\"          \"neutral\"          \n## [4] \"no opinion\"        \"strongly agree\"    \"strongly disagree\"\nq1.grp <- list(c(\"strongly agree\", \"agree\"), c(\"neutral\", \"no opinion\"), c(\"strongly disagree\",\"disagree\")) # if you want ordered categories\n\n# continuous variables \ntable(Le$education)## \n##   3   4   5   6   7   8   9  10  11  12  13  14  15 \n##   1   5   4   6  12  55 106 146 173 113  19   9   1\neducut <- c(0, 6.5, 8.5, 12.5, 17)  # use cutpoints\n\nmat1 <- cem(treatment = \"treated\", data = Le, drop = \"re78\", cutpoints = list(education=educut), grouping=list(q1=q1.grp))## \n## Using 'treated'='1' as baseline group\nmat1##            G0  G1\n## All       392 258\n## Matched   158 115\n## Unmatched 234 143"},{"path":"quasi-experimental.html","id":"genetic-matching","chapter":"20 Quasi-experimental","heading":"20.5.6 Genetic Matching","text":"GM uses iterative checking process propensity scores, combines propensity scores Mahalanobis distance.GM arguably “superior” method nearest neighbor full matching imbalanced dataUse genetic search algorithm find weights covariate optimal balance.Use genetic search algorithm find weights covariate optimal balance.Implementation\nuse replacement\nbalance can based \npaired t-tests (dichotomous variables)\nKolmogorov-Smirnov (multinomial continuous)\n\nImplementationcould use replacementcould use replacementbalance can based \npaired t-tests (dichotomous variables)\nKolmogorov-Smirnov (multinomial continuous)\nbalance can based onpaired t-tests (dichotomous variables)paired t-tests (dichotomous variables)Kolmogorov-Smirnov (multinomial continuous)Kolmogorov-Smirnov (multinomial continuous)PackagesMatching","code":"\nlibrary(Matching)## Warning: package 'Matching' was built under R version 4.0.5## Loading required package: MASS## Warning: package 'MASS' was built under R version 4.0.5## ## \n## ##  Matching (Version 4.9-9, Build Date: 2021-03-15)\n## ##  See http://sekhon.berkeley.edu/matching for additional documentation.\n## ##  Please cite software as:\n## ##   Jasjeet S. Sekhon. 2011. ``Multivariate and Propensity Score Matching\n## ##   Software with Automated Balance Optimization: The Matching package for R.''\n## ##   Journal of Statistical Software, 42(7): 1-52. \n## ##\ndata(lalonde)\nattach(lalonde)\n\n#The covariates we want to match on\nX = cbind(age, educ, black, hisp, married, nodegr, u74, u75, re75, re74)\n\n#The covariates we want to obtain balance on\nBalanceMat <- cbind(age, educ, black, hisp, married, nodegr, u74, u75, re75, re74,\n                    I(re74*re75))\n\n#\n#Let's call GenMatch() to find the optimal weight to give each\n#covariate in 'X' so as we have achieved balance on the covariates in\n#'BalanceMat'. This is only an example so we want GenMatch to be quick\n#so the population size has been set to be only 16 via the 'pop.size'\n#option. This is *WAY* too small for actual problems.\n#For details see http://sekhon.berkeley.edu/papers/MatchingJSS.pdf.\n#\ngenout <- GenMatch(Tr=treat, X=X, BalanceMatrix=BalanceMat, estimand=\"ATE\", M=1,\n                   pop.size=16, max.generations=10, wait.generations=1)## \n## \n## Mon Nov 22 14:39:20 2021\n## Domains:\n##  0.000000e+00   <=  X1   <=    1.000000e+03 \n##  0.000000e+00   <=  X2   <=    1.000000e+03 \n##  0.000000e+00   <=  X3   <=    1.000000e+03 \n##  0.000000e+00   <=  X4   <=    1.000000e+03 \n##  0.000000e+00   <=  X5   <=    1.000000e+03 \n##  0.000000e+00   <=  X6   <=    1.000000e+03 \n##  0.000000e+00   <=  X7   <=    1.000000e+03 \n##  0.000000e+00   <=  X8   <=    1.000000e+03 \n##  0.000000e+00   <=  X9   <=    1.000000e+03 \n##  0.000000e+00   <=  X10  <=    1.000000e+03 \n## \n## Data Type: Floating Point\n## Operators (code number, name, population) \n##  (1) Cloning...........................  1\n##  (2) Uniform Mutation..................  2\n##  (3) Boundary Mutation.................  2\n##  (4) Non-Uniform Mutation..............  2\n##  (5) Polytope Crossover................  2\n##  (6) Simple Crossover..................  2\n##  (7) Whole Non-Uniform Mutation........  2\n##  (8) Heuristic Crossover...............  2\n##  (9) Local-Minimum Crossover...........  0\n## \n## SOFT Maximum Number of Generations: 10\n## Maximum Nonchanging Generations: 1\n## Population size       : 16\n## Convergence Tolerance: 1.000000e-03\n## \n## Not Using the BFGS Derivative Based Optimizer on the Best Individual Each Generation.\n## Not Checking Gradients before Stopping.\n## Using Out of Bounds Individuals.\n## \n## Maximization Problem.\n## GENERATION: 0 (initializing the population)\n## Lexical Fit..... 1.747466e-01  1.795176e-01  1.795176e-01  2.755332e-01  3.173114e-01  3.173114e-01  3.857502e-01  3.937274e-01  7.390086e-01  7.390086e-01  8.024999e-01  8.910491e-01  9.688444e-01  9.917159e-01  9.989079e-01  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n## #unique......... 16, #Total UniqueCount: 16\n## var 1:\n## best............ 3.758091e+02\n## mean............ 4.897395e+02\n## variance........ 9.320437e+04\n## var 2:\n## best............ 4.944768e+02\n## mean............ 5.218593e+02\n## variance........ 6.830606e+04\n## var 3:\n## best............ 1.880547e+02\n## mean............ 4.838327e+02\n## variance........ 1.030481e+05\n## var 4:\n## best............ 8.125678e+02\n## mean............ 4.445995e+02\n## variance........ 8.135038e+04\n## var 5:\n## best............ 9.058067e+02\n## mean............ 4.753729e+02\n## variance........ 1.184631e+05\n## var 6:\n## best............ 1.733063e+02\n## mean............ 4.782400e+02\n## variance........ 8.948808e+04\n## var 7:\n## best............ 5.766096e+02\n## mean............ 4.722599e+02\n## variance........ 7.199369e+04\n## var 8:\n## best............ 3.736603e+02\n## mean............ 4.108310e+02\n## variance........ 8.454007e+04\n## var 9:\n## best............ 5.987977e+02\n## mean............ 3.762504e+02\n## variance........ 7.462591e+04\n## var 10:\n## best............ 5.352480e+02\n## mean............ 4.491692e+02\n## variance........ 8.739694e+04\n## \n## GENERATION: 1\n## Lexical Fit..... 1.747466e-01  1.795176e-01  1.795176e-01  2.755332e-01  3.173114e-01  3.173114e-01  3.857502e-01  3.937274e-01  7.390086e-01  7.390086e-01  8.024999e-01  8.910491e-01  9.688444e-01  9.917159e-01  9.989079e-01  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n## #unique......... 12, #Total UniqueCount: 28\n## var 1:\n## best............ 3.758091e+02\n## mean............ 3.556052e+02\n## variance........ 6.953381e+03\n## var 2:\n## best............ 4.944768e+02\n## mean............ 5.087018e+02\n## variance........ 3.156408e+03\n## var 3:\n## best............ 1.880547e+02\n## mean............ 1.798786e+02\n## variance........ 4.539894e+03\n## var 4:\n## best............ 8.125678e+02\n## mean............ 6.669235e+02\n## variance........ 5.540181e+04\n## var 5:\n## best............ 9.058067e+02\n## mean............ 9.117080e+02\n## variance........ 3.397508e+03\n## var 6:\n## best............ 1.733063e+02\n## mean............ 2.041563e+02\n## variance........ 3.552940e+04\n## var 7:\n## best............ 5.766096e+02\n## mean............ 4.995566e+02\n## variance........ 2.585541e+04\n## var 8:\n## best............ 3.736603e+02\n## mean............ 5.068097e+02\n## variance........ 4.273372e+04\n## var 9:\n## best............ 5.987977e+02\n## mean............ 4.917595e+02\n## variance........ 3.084008e+04\n## var 10:\n## best............ 5.352480e+02\n## mean............ 5.445554e+02\n## variance........ 8.587358e+03\n## \n## GENERATION: 2\n## Lexical Fit..... 1.747466e-01  1.795176e-01  1.795176e-01  2.755332e-01  3.173114e-01  3.173114e-01  3.857502e-01  3.937274e-01  7.390086e-01  7.390086e-01  8.024999e-01  8.910491e-01  9.688444e-01  9.917159e-01  9.989079e-01  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n## #unique......... 8, #Total UniqueCount: 36\n## var 1:\n## best............ 3.758091e+02\n## mean............ 3.562379e+02\n## variance........ 2.385358e+03\n## var 2:\n## best............ 4.944768e+02\n## mean............ 4.956205e+02\n## variance........ 1.566415e+02\n## var 3:\n## best............ 1.880547e+02\n## mean............ 2.166244e+02\n## variance........ 6.801191e+03\n## var 4:\n## best............ 8.125678e+02\n## mean............ 8.059555e+02\n## variance........ 6.565662e+02\n## var 5:\n## best............ 9.058067e+02\n## mean............ 8.852994e+02\n## variance........ 3.056774e+03\n## var 6:\n## best............ 1.733063e+02\n## mean............ 2.210856e+02\n## variance........ 2.369334e+04\n## var 7:\n## best............ 5.766096e+02\n## mean............ 5.482967e+02\n## variance........ 4.613957e+03\n## var 8:\n## best............ 3.736603e+02\n## mean............ 3.943396e+02\n## variance........ 1.895797e+03\n## var 9:\n## best............ 5.987977e+02\n## mean............ 6.005851e+02\n## variance........ 3.308459e+02\n## var 10:\n## best............ 5.352480e+02\n## mean............ 5.444285e+02\n## variance........ 4.950778e+02\n## \n## 'wait.generations' limit reached.\n## No significant improvement in 1 generations.\n## \n## Solution Lexical Fitness Value:\n## 1.747466e-01  1.795176e-01  1.795176e-01  2.755332e-01  3.173114e-01  3.173114e-01  3.857502e-01  3.937274e-01  7.390086e-01  7.390086e-01  8.024999e-01  8.910491e-01  9.688444e-01  9.917159e-01  9.989079e-01  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  \n## \n## Parameters at the Solution:\n## \n##  X[ 1] : 3.758091e+02\n##  X[ 2] : 4.944768e+02\n##  X[ 3] : 1.880547e+02\n##  X[ 4] : 8.125678e+02\n##  X[ 5] : 9.058067e+02\n##  X[ 6] : 1.733063e+02\n##  X[ 7] : 5.766096e+02\n##  X[ 8] : 3.736603e+02\n##  X[ 9] : 5.987977e+02\n##  X[10] : 5.352480e+02\n## \n## Solution Found Generation 1\n## Number of Generations Run 2\n## \n## Mon Nov 22 14:39:21 2021\n## Total run time : 0 hours 0 minutes and 1 seconds\n#The outcome variable\nY=re78/1000\n\n#\n# Now that GenMatch() has found the optimal weights, let's estimate\n# our causal effect of interest using those weights\n#\nmout <- Match(Y=Y, Tr=treat, X=X, estimand=\"ATE\", Weight.matrix=genout)\nsummary(mout)## \n## Estimate...  1.9046 \n## AI SE......  0.82198 \n## T-stat.....  2.3171 \n## p.val......  0.020498 \n## \n## Original number of observations..............  445 \n## Original number of treated obs...............  185 \n## Matched number of observations...............  445 \n## Matched number of observations  (unweighted).  597\n#                        \n#Let's determine if balance has actually been obtained on the variables of interest\n#                        \nmb <- MatchBalance(treat~age +educ+black+ hisp+ married+ nodegr+ u74+ u75+\n                   re75+ re74+ I(re74*re75),\n                   match.out=mout, nboots=500)## \n## ***** (V1) age *****\n##                        Before Matching        After Matching\n## mean treatment........     25.816             25.112 \n## mean control..........     25.054             24.902 \n## std mean diff.........     10.655             3.1286 \n## \n## mean raw eQQ diff.....    0.94054            0.36181 \n## med  raw eQQ diff.....          1                  0 \n## max  raw eQQ diff.....          7                  8 \n## \n## mean eCDF diff........   0.025364          0.0099025 \n## med  eCDF diff........   0.022193          0.0075377 \n## max  eCDF diff........   0.065177           0.028476 \n## \n## var ratio (Tr/Co).....     1.0278            0.98286 \n## T-test p-value........    0.26594            0.27553 \n## KS Bootstrap p-value..      0.504              0.842 \n## KS Naive p-value......     0.7481            0.96884 \n## KS Statistic..........   0.065177           0.028476 \n## \n## \n## ***** (V2) educ *****\n##                        Before Matching        After Matching\n## mean treatment........     10.346             10.189 \n## mean control..........     10.088             10.222 \n## std mean diff.........     12.806            -1.9653 \n## \n## mean raw eQQ diff.....    0.40541           0.098827 \n## med  raw eQQ diff.....          0                  0 \n## max  raw eQQ diff.....          2                  2 \n## \n## mean eCDF diff........   0.028698          0.0070591 \n## med  eCDF diff........   0.012682          0.0033501 \n## max  eCDF diff........    0.12651           0.033501 \n## \n## var ratio (Tr/Co).....     1.5513             1.0458 \n## T-test p-value........    0.15017            0.38575 \n## KS Bootstrap p-value..       0.02              0.454 \n## KS Naive p-value......   0.062873            0.89105 \n## KS Statistic..........    0.12651           0.033501 \n## \n## \n## ***** (V3) black *****\n##                        Before Matching        After Matching\n## mean treatment........    0.84324             0.8382 \n## mean control..........    0.82692             0.8382 \n## std mean diff.........     4.4767                  0 \n## \n## mean raw eQQ diff.....   0.016216                  0 \n## med  raw eQQ diff.....          0                  0 \n## max  raw eQQ diff.....          1                  0 \n## \n## mean eCDF diff........  0.0081601                  0 \n## med  eCDF diff........  0.0081601                  0 \n## max  eCDF diff........    0.01632                  0 \n## \n## var ratio (Tr/Co).....    0.92503                  1 \n## T-test p-value........    0.64736                  1 \n## \n## \n## ***** (V4) hisp *****\n##                        Before Matching        After Matching\n## mean treatment........   0.059459            0.08764 \n## mean control..........    0.10769            0.08764 \n## std mean diff.........    -20.341                  0 \n## \n## mean raw eQQ diff.....   0.048649                  0 \n## med  raw eQQ diff.....          0                  0 \n## max  raw eQQ diff.....          1                  0 \n## \n## mean eCDF diff........   0.024116                  0 \n## med  eCDF diff........   0.024116                  0 \n## max  eCDF diff........   0.048233                  0 \n## \n## var ratio (Tr/Co).....    0.58288                  1 \n## T-test p-value........   0.064043                  1 \n## \n## \n## ***** (V5) married *****\n##                        Before Matching        After Matching\n## mean treatment........    0.18919            0.16854 \n## mean control..........    0.15385            0.16854 \n## std mean diff.........     8.9995                  0 \n## \n## mean raw eQQ diff.....   0.037838                  0 \n## med  raw eQQ diff.....          0                  0 \n## max  raw eQQ diff.....          1                  0 \n## \n## mean eCDF diff........   0.017672                  0 \n## med  eCDF diff........   0.017672                  0 \n## max  eCDF diff........   0.035343                  0 \n## \n## var ratio (Tr/Co).....     1.1802                  1 \n## T-test p-value........    0.33425                  1 \n## \n## \n## ***** (V6) nodegr *****\n##                        Before Matching        After Matching\n## mean treatment........    0.70811            0.78876 \n## mean control..........    0.83462            0.78652 \n## std mean diff.........    -27.751            0.54991 \n## \n## mean raw eQQ diff.....    0.12432           0.001675 \n## med  raw eQQ diff.....          0                  0 \n## max  raw eQQ diff.....          1                  1 \n## \n## mean eCDF diff........   0.063254         0.00083752 \n## med  eCDF diff........   0.063254         0.00083752 \n## max  eCDF diff........    0.12651           0.001675 \n## \n## var ratio (Tr/Co).....     1.4998             0.9923 \n## T-test p-value........  0.0020368            0.73901 \n## \n## \n## ***** (V7) u74 *****\n##                        Before Matching        After Matching\n## mean treatment........    0.70811            0.73258 \n## mean control..........       0.75            0.73034 \n## std mean diff.........    -9.1895            0.50714 \n## \n## mean raw eQQ diff.....   0.037838           0.001675 \n## med  raw eQQ diff.....          0                  0 \n## max  raw eQQ diff.....          1                  1 \n## \n## mean eCDF diff........   0.020946         0.00083752 \n## med  eCDF diff........   0.020946         0.00083752 \n## max  eCDF diff........   0.041892           0.001675 \n## \n## var ratio (Tr/Co).....     1.1041            0.99472 \n## T-test p-value........    0.33033            0.31731 \n## \n## \n## ***** (V8) u75 *****\n##                        Before Matching        After Matching\n## mean treatment........        0.6            0.64494 \n## mean control..........    0.68462            0.65169 \n## std mean diff.........    -17.225            -1.4072 \n## \n## mean raw eQQ diff.....   0.081081          0.0050251 \n## med  raw eQQ diff.....          0                  0 \n## max  raw eQQ diff.....          1                  1 \n## \n## mean eCDF diff........   0.042308          0.0025126 \n## med  eCDF diff........   0.042308          0.0025126 \n## max  eCDF diff........   0.084615          0.0050251 \n## \n## var ratio (Tr/Co).....     1.1133             1.0088 \n## T-test p-value........   0.068031            0.17952 \n## \n## \n## ***** (V9) re75 *****\n##                        Before Matching        After Matching\n## mean treatment........     1532.1             1300.3 \n## mean control..........     1266.9             1317.3 \n## std mean diff.........     8.2363            -0.5676 \n## \n## mean raw eQQ diff.....     367.61             104.16 \n## med  raw eQQ diff.....          0                  0 \n## max  raw eQQ diff.....     2110.2             2510.6 \n## \n## mean eCDF diff........   0.050834          0.0080924 \n## med  eCDF diff........   0.061954          0.0067002 \n## max  eCDF diff........    0.10748           0.021776 \n## \n## var ratio (Tr/Co).....     1.0763            0.97978 \n## T-test p-value........    0.38527             0.8025 \n## KS Bootstrap p-value..      0.044              0.824 \n## KS Naive p-value......    0.16449            0.99891 \n## KS Statistic..........    0.10748           0.021776 \n## \n## \n## ***** (V10) re74 *****\n##                        Before Matching        After Matching\n## mean treatment........     2095.6             2019.8 \n## mean control..........       2107             2106.4 \n## std mean diff.........   -0.23437             -1.768 \n## \n## mean raw eQQ diff.....     487.98             243.25 \n## med  raw eQQ diff.....          0                  0 \n## max  raw eQQ diff.....       8413             7870.3 \n## \n## mean eCDF diff........   0.019223          0.0083159 \n## med  eCDF diff........     0.0158          0.0067002 \n## max  eCDF diff........   0.047089           0.025126 \n## \n## var ratio (Tr/Co).....     0.7381            0.85755 \n## T-test p-value........    0.98186            0.39373 \n## KS Bootstrap p-value..      0.556              0.596 \n## KS Naive p-value......    0.97023            0.99172 \n## KS Statistic..........   0.047089           0.025126 \n## \n## \n## ***** (V11) I(re74 * re75) *****\n##                        Before Matching        After Matching\n## mean treatment........   13118591           12261673 \n## mean control..........   14530303           14240665 \n## std mean diff.........    -2.7799            -4.2761 \n## \n## mean raw eQQ diff.....    3278733            2602379 \n## med  raw eQQ diff.....          0                  0 \n## max  raw eQQ diff.....  188160151          223801517 \n## \n## mean eCDF diff........   0.022723          0.0043797 \n## med  eCDF diff........   0.014449          0.0033501 \n## max  eCDF diff........   0.061019           0.011725 \n## \n## var ratio (Tr/Co).....    0.69439            0.58474 \n## T-test p-value........    0.79058            0.17475 \n## KS Bootstrap p-value..      0.266              0.994 \n## KS Naive p-value......    0.81575                  1 \n## KS Statistic..........   0.061019           0.011725 \n## \n## \n## Before Matching Minimum p.value: 0.0020368 \n## Variable Name(s): nodegr  Number(s): 6 \n## \n## After Matching Minimum p.value: 0.17475 \n## Variable Name(s): I(re74 * re75)  Number(s): 11"},{"path":"quasi-experimental.html","id":"matching-for-time-series-cross-section-data","chapter":"20 Quasi-experimental","heading":"20.5.7 Matching for time series-cross-section data","text":"Examples: (SCHEVE STASAVAGE 2012) (Acemoglu et al. 2014)Materials Imai et al.’s slidesIdentification strategy:Within-unit -time variationWithin-unit -time variationwithin-time across-units variationwithin-time across-units variation","code":""},{"path":"quasi-experimental.html","id":"interrupted-time-series","chapter":"20 Quasi-experimental","heading":"20.6 Interrupted Time Series","text":"Control \nSeasonable trends\nConcurrent events\nControl forSeasonable trendsSeasonable trendsConcurrent eventsConcurrent eventsPros (Penfold Zhang 2013)\ncontrol long-term trends\nPros (Penfold Zhang 2013)control long-term trendsCons\nMin 8 data points 8 intervention\nMultiple events hard distinguish\nConsMin 8 data points 8 interventionMin 8 data points 8 interventionMultiple events hard distinguishMultiple events hard distinguishExample Leihua Ye","code":"\n# data preparation\nset.seed(1)\nCaseID = rep(1:100, 6)\n\n# intervention\nIntervention = c(rep(0, 300), rep(1, 300))\nOutcome_Variable = c(rnorm(300), abs(rnorm(300) * 4))\n\nmydata = cbind(CaseID, Intervention, Outcome_Variable)\n\nmydata = as.data.frame(mydata)\n\n#construct a simple OLS model\nmodel = lm(Outcome_Variable ~ Intervention, data = mydata)\nsummary(model)## \n## Call:\n## lm(formula = Outcome_Variable ~ Intervention, data = mydata)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.3050 -1.2315 -0.1734  0.8691 11.9185 \n## \n## Coefficients:\n##              Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   0.03358    0.11021   0.305    0.761    \n## Intervention  3.28903    0.15586  21.103   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 1.909 on 598 degrees of freedom\n## Multiple R-squared:  0.4268, Adjusted R-squared:  0.4259 \n## F-statistic: 445.3 on 1 and 598 DF,  p-value: < 2.2e-16"},{"path":"endogeneity.html","id":"endogeneity","chapter":"21 Endogeneity","heading":"21 Endogeneity","text":"RefresherA general model framework\\[\n\\mathbf{Y = X \\beta + \\epsilon}\n\\]\\(\\mathbf{Y} = n \\times 1\\)\\(\\mathbf{Y} = n \\times 1\\)\\(\\mathbf{X} = n \\times k\\)\\(\\mathbf{X} = n \\times k\\)\\(\\beta = k \\times 1\\)\\(\\beta = k \\times 1\\)\\(\\epsilon = n \\times 1\\)\\(\\epsilon = n \\times 1\\), OLS estimates coefficients \\[\n\\begin{aligned}\n\\hat{\\beta}_{OLS} &= (\\mathbf{X}'\\mathbf{X})^{-1}(\\mathbf{X}'\\mathbf{Y}) \\\\\n&= (\\mathbf{X}'\\mathbf{X})^{-1}(\\mathbf{X}'(\\mathbf{X \\beta + \\epsilon})) \\\\\n&= (\\mathbf{X}'\\mathbf{X})^{-1} (\\mathbf{X}'\\mathbf{X}) \\beta + (\\mathbf{X}'\\mathbf{X})^{-1} (\\mathbf{X}'\\mathbf{\\epsilon}) \\\\\n\\hat{\\beta}_{OLS} & \\\\beta + (\\mathbf{X}'\\mathbf{X})^{-1} (\\mathbf{X}'\\mathbf{\\epsilon})\n\\end{aligned}\n\\]unbiased estimates, get rid second part \\((\\mathbf{X}'\\mathbf{X})^{-1} (\\mathbf{X}'\\mathbf{\\epsilon})\\)2 conditions achieve unbiased estimates:\\(E(\\epsilon |X) = 0\\) (easy, putting intercept can solve issue)\\(Cov(\\mathbf{X}, \\epsilon) = 0\\) (hard part)care omitted variableUsually, problem stem Omitted Variables Bias, care omitted variable bias whenOmitted variables correlate variables care (\\(X\\)). OMV correlate \\(X\\), don’t care, random assignment makes correlation goes 0)Omitted variables correlates outcome/ dependent variableThere types endogeneity listed .Types endogeneityEndogenous TreatmentOmitted Variables Bias\nMotivation/choice\nAbility/talent\nSelf-selection\nOmitted Variables BiasMotivation/choiceAbility/talentSelf-selectionFeedback Effect (Simultaneity): also known bidirectionalityFeedback Effect (Simultaneity): also known bidirectionalityMeasurement ErrorMeasurement ErrorEndogenous Sample SelectionTo deal problem, toolbox (mentioned previous chapter 18 )Tools hierarchical orderExperimental Design: Randomized Control Trials (Gold standard): Tier 1Experimental Design: Randomized Control Trials (Gold standard): Tier 1Quasi-experimental\nRegression Discontinuity Tier 1A\nDifference--Differences Tier 2\nSynthetic Control Tier 2A\nFixed Effects Estimator 12.4.2.2: Tier 3\nEndogenous Treatment: mostly Instrumental Variable: Tier 3A\nMatching Methods Tier 4\nInterrupted Time Series Tier 4A\nEndogenous Sample Selection 21.4: mostly Heckman’s correction\nQuasi-experimentalRegression Discontinuity Tier 1ARegression Discontinuity Tier 1ADifference--Differences Tier 2Difference--Differences Tier 2Synthetic Control Tier 2ASynthetic Control Tier 2AFixed Effects Estimator 12.4.2.2: Tier 3Fixed Effects Estimator 12.4.2.2: Tier 3Endogenous Treatment: mostly Instrumental Variable: Tier 3AEndogenous Treatment: mostly Instrumental Variable: Tier 3AMatching Methods Tier 4Matching Methods Tier 4Interrupted Time Series Tier 4AInterrupted Time Series Tier 4AEndogenous Sample Selection 21.4: mostly Heckman’s correctionEndogenous Sample Selection 21.4: mostly Heckman’s correctionUsing control variables regression “selection observables” identification strategy.words, believe omitted variable, can measure , including regression model solves problem. uninterested variables called control variables model.However, rarely case (problem don’t measurements). Hence, need elaborate methods:Endogenous TreatmentEndogenous TreatmentEndogenous Sample SelectionEndogenous Sample SelectionBefore get methods deal bias arises omitted variables, consider cases measurements variable, measurement error (bias).","code":""},{"path":"endogeneity.html","id":"measurement-error","chapter":"21 Endogeneity","heading":"21.1 Measurement Error","text":"Data error can stem \nCoding errors\nReporting errors\nData error can stem fromCoding errorsCoding errorsReporting errorsReporting errors","code":""},{"path":"endogeneity.html","id":"classical-measurement-errors","chapter":"21 Endogeneity","heading":"21.1.1 Classical Measurement Errors","text":"","code":""},{"path":"endogeneity.html","id":"right-hand-side","chapter":"21 Endogeneity","heading":"21.1.1.1 Right-hand side","text":"Right-hand side measurement error: measurement covariates, endogeneity problem.Say know true model \\[\nY_i = \\beta_0 + \\beta_1 X_i + u_i\n\\]don’t observe \\(X_i\\), observe\\[\n\\tilde{X}_i = X_i + e_i\n\\]known classical measurement errors assume \\(e_i\\) uncorrelated \\(X_i\\) (.e., \\(E(X_i e_i) = 0\\)), estimate observed variables, (substitute \\(X_i\\) \\(\\tilde{X}_i - e_i\\) ):\\[\n\\begin{aligned}\nY_i &= \\beta_0 + \\beta_1 (\\tilde{X}_i - e_i)+ u_i \\\\\n&= \\beta_0 + \\beta_1 \\tilde{X}_i + u_i - \\beta_1 e_i \\\\\n&= \\beta_0 + \\beta_1 \\tilde{X}_i + v_i\n\\end{aligned}\n\\]words, measurement error \\(X_i\\) now part error term regression equation \\(v_i\\). Hence, endogeneity bias.Endogeneity arises \\[\n\\begin{aligned}\nE(\\tilde{X}_i v_i) &= E((X_i + e_i )(u_i - \\beta_1 e_i)) \\\\\n&= -\\beta_1 Var(e_i) \\neq 0\n\\end{aligned}\n\\]Since \\(\\tilde{X}_i\\) \\(e_i\\) positively correlated, leads toa negative bias \\(\\hat{\\beta}_1\\) true \\(\\beta_1\\) positivea negative bias \\(\\hat{\\beta}_1\\) true \\(\\beta_1\\) positivea positive bias \\(\\beta_1\\) negativea positive bias \\(\\beta_1\\) negativeIn words, measurement errors cause attenuation bias, inter turn pushes coefficient towards 0As \\(Var(e_i)\\) increases \\(\\frac{Var(e_i)}{Var(\\tilde{X})} \\1\\) \\(e_i\\) random (noise) \\(\\beta_1 \\0\\) (random variable \\(\\tilde{X}\\) relation \\(Y_i\\))Technical note:size bias OLS-estimator \\[\n\\hat{\\beta}_{OLS} = \\frac{ cov(\\tilde{X}, Y)}{var(\\tilde{X})} = \\frac{cov(X + e, \\beta X + u)}{var(X + e)}\n\\]\\[\nplim \\hat{\\beta}_{OLS} = \\beta \\frac{\\sigma^2_X}{\\sigma^2_X + \\sigma^2_e} = \\beta \\lambda\n\\]\\(\\lambda\\) reliability signal--total variance ratio attenuation factorReliability affect extent measurement error attenuates \\(\\hat{\\beta}\\). attenuation bias \\[\n\\hat{\\beta}_{OLS} - \\beta = -(1-\\lambda)\\beta\n\\]Thus, \\(\\hat{\\beta}_{OLS} < \\beta\\) (unless \\(\\lambda = 1\\), case don’t even measurement error).Note:Data transformation worsen (magnify) measurement error\\[\ny= \\beta x + \\gamma x^2 + \\epsilon\n\\], attenuation factor \\(\\hat{\\gamma}\\) square attenuation factor \\(\\hat{\\beta}\\) (.e., \\(\\lambda_{\\hat{\\gamma}} = \\lambda_{\\hat{\\beta}}^2\\))Adding covariates increases attenuation biasTo fix classical measurement error problem, canFind estimates either \\(\\sigma^2_X, \\sigma^2_\\epsilon\\) \\(\\lambda\\) validation studies, survey data.Endogenous Treatment Use instrument \\(Z\\) correlated \\(X\\) uncorrelated \\(\\epsilon\\)Abandon project","code":""},{"path":"endogeneity.html","id":"left-hand-side","chapter":"21 Endogeneity","heading":"21.1.1.2 Left-hand side","text":"measurement outcome variable, econometricians causal scientists care still unbiased estimate coefficients (zero conditional mean assumption violated, hence don’t endogeneity). However, statisticians might care might inflate uncertainty coefficient estimates (.e., higher standard errors).\\[\n\\tilde{Y} = Y + v\n\\]model estimate \\[\n\\tilde{Y} = \\beta X + u + v\n\\]Since \\(v\\) uncorrelated \\(X\\), \\(\\hat{\\beta}\\) consistently estimated OLSIf measurement error \\(Y_i\\), pass \\(\\beta_1\\) go \\(u_i\\)","code":""},{"path":"endogeneity.html","id":"non-classical-measurement-errors","chapter":"21 Endogeneity","heading":"21.1.2 Non-classical Measurement Errors","text":"Relaxing assumption \\(X\\) \\(\\epsilon\\) uncorrelatedRecall true model true estimate \\[\n\\hat{\\beta} = \\frac{cov(X + \\epsilon, \\beta X + u)}{var(X + \\epsilon)}\n\\]without assumption, \\[\n\\begin{aligned}\nplim \\hat{\\beta} &= \\frac{\\beta (\\sigma^2_X + \\sigma_{X \\epsilon})}{\\sigma^2_X + \\sigma^2_\\epsilon + 2 \\sigma_{X \\epsilon}} \\\\\n&= (1 - \\frac{\\sigma^2_{\\epsilon} + \\sigma_{X \\epsilon}}{\\sigma^2_X + \\sigma^2_\\epsilon + 2 \\sigma_{X \\epsilon}}) \\beta \\\\\n&= (1 - b_{\\epsilon \\tilde{X}}) \\beta\n\\end{aligned}\n\\]\\(b_{\\epsilon \\tilde{X}}\\) covariance \\(\\tilde{X}\\) \\(\\epsilon\\) (also regression coefficient regression \\(\\epsilon\\) \\(\\tilde{X}\\))Hence, Classical Measurement Errors just special case Non-classical Measurement Errors \\(b_{\\epsilon \\tilde{X}} = 1 - \\lambda\\)\\(\\sigma_{X \\epsilon} = 0\\) (Classical Measurement Errors), increasing covariance \\(b_{\\epsilon \\tilde{X}}\\) increases covariance increases attenuation factor half variance \\(\\tilde{X}\\) measurement error, decreases attenuation factor otherwise. also known **mean reverting measurement error (** Bound Krueger 1989)general framework right-hand side left-hand side measurement error (Bound et al. 1994):consider true model\\[\n\\mathbf{Y = X \\beta + \\epsilon}\n\\]\\[\n\\begin{aligned}\n\\hat{\\beta} &= \\mathbf{(\\tilde{X}' \\tilde{X})^{-1}\\tilde{X} \\tilde{Y}} \\\\\n&= \\mathbf{(\\tilde{X}' \\tilde{X})^{-1} \\tilde{X}' (\\tilde{X} \\beta - U \\beta + v + \\epsilon )} \\\\\n&= \\mathbf{\\beta + (\\tilde{X}' \\tilde{X})^{-1} \\tilde{X}' (-U \\beta + v + \\epsilon)} \\\\\nplim \\hat{\\beta} &= \\beta + plim (\\tilde{X}' \\tilde{X})^{-1} \\tilde{X}' ( -U\\beta + v) \\\\\n&= \\beta + plim (\\tilde{X}' \\tilde{X})^{-1} \\tilde{X}' W \n\\left[\n\\begin{array}\n{c}\n- \\beta \\\\\n1\n\\end{array}\n\\right]\n\\end{aligned}\n\\]Since collect measurement errors matrix \\(W = [U|v]\\), \\[\n( -U\\beta + v) = W \n\\left[\n\\begin{array}\n{c}\n- \\beta \\\\\n1\n\\end{array}\n\\right]\n\\]Hence, general, biases coefficients \\(\\beta\\) regression coefficients regressing measurement errors mis-measured \\(\\tilde{X}\\)Notes:Instrumental Variable can help fix problemInstrumental Variable can help fix problemThere can also measurement error dummy variables can still use Instrumental Variable fix .can also measurement error dummy variables can still use Instrumental Variable fix .","code":""},{"path":"endogeneity.html","id":"simultaneity","chapter":"21 Endogeneity","heading":"21.2 Simultaneity","text":"independent variables (\\(X\\)’s) jointly determined dependent variable \\(Y\\), typically equilibrium mechanism, violates second condition causality (.e., temporal order).independent variables (\\(X\\)’s) jointly determined dependent variable \\(Y\\), typically equilibrium mechanism, violates second condition causality (.e., temporal order).Examples: quantity price demand supply, investment productivity, sales advertisementExamples: quantity price demand supply, investment productivity, sales advertisementGeneral Simultaneous (Structural) Equations\\[\nY_i = \\beta_0 + \\beta_1 X_i + u_i \\\\\nX_i = \\alpha_0 + \\alpha_1 Y_i + v_i\n\\]Hence, solutions \\[\nY_i = \\frac{\\beta_0 + \\beta_1 \\alpha_0}{1 - \\alpha_1 \\beta_1} + \\frac{\\beta_1 v_i + u_i}{1 - \\alpha_1 \\beta_1} \\\\\nX_i = \\frac{\\alpha_0 + \\alpha_1 \\beta_0}{1 - \\alpha_1 \\beta_1} + \\frac{v_i + \\alpha_1 u_i}{1 - \\alpha_1 \\beta_1}\n\\]run one regression, biased estimators (simultaneity bias):\\[\n\\begin{aligned}\nCov(X_i, u_i) &= Cov(\\frac{v_i + \\alpha_1 u_i}{1 - \\alpha_1 \\beta_1}, u_i) \\\\\n&= \\frac{\\alpha_1}{1- \\alpha_1 \\beta_1} Var(u_i)\n\\end{aligned}\n\\]even general model\\[\n\\begin{cases}\nY_i = \\beta_0 + \\beta_1 X_i + \\beta_2 T_i + u_i \\\\\nX_i = \\alpha_0 + \\alpha_1 Y_i + \\alpha_2 Z_i + v_i\n\\end{cases}\n\\]\\(X_i, Y_i\\) endogenous variables determined within system\\(X_i, Y_i\\) endogenous variables determined within system\\(T_i, Z_i\\) exogenous variables\\(T_i, Z_i\\) exogenous variablesThen, reduced form model \\[\n\\begin{cases}\n\\begin{aligned}\nY_i &= \\frac{\\beta_0 + \\beta_1 \\alpha_0}{1 - \\alpha_1 \\beta_1} + \\frac{\\beta_1 \\alpha_2}{1 - \\alpha_1 \\beta_1} Z_i + \\frac{\\beta_2}{1 - \\alpha_1 \\beta_1} T_i + \\tilde{u}_i \\\\\n&= B_0 + B_1 Z_i + B_2 T_i + \\tilde{u}_i\n\\end{aligned}\n\\\\\n\\begin{aligned}\nX_i &= \\frac{\\alpha_0 + \\alpha_1 \\beta_0}{1 - \\alpha_1 \\beta_1} + \\frac{\\alpha_2}{1 - \\alpha_1 \\beta_1} Z_i + \\frac{\\alpha_1\\beta_2}{1 - \\alpha_1 \\beta_1} T_i + \\tilde{v}_i \\\\\n&= A_0 + A_1 Z_i + A_2 T_i + \\tilde{v}_i\n\\end{aligned}\n\\end{cases}\n\\], now can get consistent estimates reduced form parametersAnd get original parameter estimates\\[\n\\frac{B_1}{A_1} = \\beta_1 \\\\\nB_2 (1 - \\frac{B_1 A_2}{A_1B_2}) = \\beta_2 \\\\\n\\frac{A_2}{B_2} = \\alpha_1 \\\\\nA_1 (1 - \\frac{B_1 A_2}{A_1 B_2}) = \\alpha_2\n\\]Rules IdentificationOrder Condition (necessary sufficient)\\[\nK - k \\ge m - 1\n\\]\\(M\\) = number endogenous variables model\\(M\\) = number endogenous variables modelK = number exogenous variables int modelK = number exogenous variables int modelm = number endogenous variables givenm = number endogenous variables givenk = number exogenous variables given equationk = number exogenous variables given equationThis actually general framework instrumental variables","code":""},{"path":"endogeneity.html","id":"endogenous-treatment","chapter":"21 Endogeneity","heading":"21.3 Endogenous Treatment","text":"Using OLS estimates reference point","code":"\nlibrary(AER)## Loading required package: car## Warning: package 'car' was built under R version 4.0.5## Loading required package: carData## Loading required package: lmtest## Loading required package: zoo## Warning: package 'zoo' was built under R version 4.0.5## \n## Attaching package: 'zoo'## The following objects are masked from 'package:base':\n## \n##     as.Date, as.Date.numeric## Loading required package: sandwich## Loading required package: survival## Warning: package 'survival' was built under R version 4.0.5\nlibrary(REndo)## Warning: package 'REndo' was built under R version 4.0.5\nset.seed(421)\ndata(\"CASchools\")\nschool <- CASchools\nschool$stratio <- with(CASchools, students / teachers)\nm1.ols <-\n  lm(read ~ stratio + english + lunch + grades + income + calworks + county,\n     data = school)\nsummary(m1.ols)$coefficients[1:7, ]##                 Estimate Std. Error     t value      Pr(>|t|)\n## (Intercept) 683.45305948 9.56214469  71.4748711 3.011667e-218\n## stratio      -0.30035544 0.25797023  -1.1643027  2.450536e-01\n## english      -0.20550107 0.03765408  -5.4576041  8.871666e-08\n## lunch        -0.38684059 0.03700982 -10.4523759  1.427370e-22\n## gradesKK-08  -1.91291321 1.35865394  -1.4079474  1.599886e-01\n## income        0.71615378 0.09832843   7.2832829  1.986712e-12\n## calworks     -0.05273312 0.06154758  -0.8567863  3.921191e-01"},{"path":"endogeneity.html","id":"instrumental-variable","chapter":"21 Endogeneity","heading":"21.3.1 Instrumental Variable","text":"A3a requires \\(\\epsilon_i\\) uncorrelated \\(\\mathbf{x}_i\\)Assume A1 , A2, A5\\[\nplim(\\hat{\\beta}_{OLS}) = \\beta + [E(\\mathbf{x_i'x_i})]^{-1}E(\\mathbf{x_i'}\\epsilon_i)\n\\]A3a weakest assumption needed OLS consistent[A3] fails \\(x_{ik}\\) correlated \\(\\epsilon_i\\)Omitted Variables Bias: \\(\\epsilon_i\\) includes factors may influence dependent variable (linearly)Simultaneity Demand prices simultaneously determined.Endogenous Sample Selection iid sampleMeasurement ErrorNoteOmitted Variable: omitted variable variable, omitted model (\\(\\epsilon_i\\)) unobserved predictive power towards outcome.Omitted Variable Bias: bias (inconsistency looking large sample properties) OLS estimator omitted variable.cam positive negative selection bias (depends story )structural equation used emphasize interested understanding causal relationship\\[\ny_{i1} = \\beta_0 + \\mathbf{z}_i1 \\beta_1 + y_{i2}\\beta_2 +  \\epsilon_i\n\\]\\(y_{}\\) outcome variable (inherently correlated \\(\\epsilon_i\\))\\(y_{i2}\\) endogenous covariate (presumed correlated \\(\\epsilon_i\\))\\(\\beta_1\\) represents causal effect \\(y_{i2}\\) \\(y_{i1}\\)\\(\\mathbf{z}_{i1}\\) exogenous controls (uncorrelated \\(\\epsilon_i\\)) (\\(E(z_{1i}'\\epsilon_i) = 0\\))OLS inconsistent estimator causal effect \\(\\beta_2\\)endogeneity\\(E(y_{i2}'\\epsilon_i) = 0\\)exogenous variation \\(y_{i2}\\) identifies causal effectIf endogeneityAny wiggle \\(y_{i2}\\) shift simultaneously \\(\\epsilon_i\\)\\[\nplim(\\hat{\\beta}_{OLS}) = \\beta + [E(\\mathbf{x'_ix_i})]^{-1}E(\\mathbf{x'_i}\\epsilon_i)\n\\]\\(\\beta\\) causal effect\\([E(\\mathbf{x'_ix_i})]^{-1}E(\\mathbf{x'_i}\\epsilon_i)\\) endogenous effectHence \\(\\hat{\\beta}_{OLS}\\) can either positive negative true causal effect.Motivation Two Stage Least Squares (2SLS)\\[\ny_{i1}=\\beta_0 + \\mathbf{z}_{i1}\\beta_1 + y_{i2}\\beta_2 + \\epsilon_i\n\\]want understand movement \\(y_{i2}\\) effects movement \\(y_{i1}\\), whenever move \\(y_{i2}\\), \\(\\epsilon_i\\) also moves.Solution\nneed way move \\(y_{i2}\\) independently \\(\\epsilon_i\\), can analyze response \\(y_{i1}\\) causal effectFind instrumental variable(s) \\(z_{i2}\\)\nInstrument Relevance: \\(z_{i2}\\) moves \\(y_{i2}\\) also moves\nInstrument Exogeneity**: \\(z_{i2}\\) moves \\(\\epsilon_i\\) move.\nFind instrumental variable(s) \\(z_{i2}\\)Instrument Relevance: \\(z_{i2}\\) moves \\(y_{i2}\\) also movesInstrument Exogeneity**: \\(z_{i2}\\) moves \\(\\epsilon_i\\) move.\\(z_{i2}\\) exogenous variation identifies causal effect \\(\\beta_2\\)\\(z_{i2}\\) exogenous variation identifies causal effect \\(\\beta_2\\)Finding Instrumental variable:Random Assignment: + Effect class size educational outcomes: instrument initial randomRelation’s Choice + Effect Education Fertility: instrument parent’s educational levelEligibility + Trade-IRA 401K retirement savings: instrument 401k eligibilityExampleReturn Collegeeducation correlated ability - endogenouseducation correlated ability - endogenousNear 4year instrument\nInstrument Relevance: near moves education also moves\nInstrument Exogeneity: near moves \\(\\epsilon_i\\) move.\nNear 4year instrumentInstrument Relevance: near moves education also movesInstrument Exogeneity: near moves \\(\\epsilon_i\\) move.potential instruments; near 2-year college. Parent’s Education. Owning Library CardOther potential instruments; near 2-year college. Parent’s Education. Owning Library Card\\[\ny_{i1}=\\beta_0 + \\mathbf{z}_{i1}\\beta_1 + y_{i2}\\beta_2 + \\epsilon_i\n\\]First Stage (Reduced Form) Equation:\\[\ny_{i2} = \\pi_0 + \\mathbf{z_{i1}\\pi_1} + \\mathbf{z_{i2}\\pi_2} + v_i\n\\]\\(\\pi_0 + \\mathbf{z_{i1}\\pi_1} + \\mathbf{z_{i2}\\pi_2}\\) exogenous variation \\(v_i\\) endogenous variationThis called reduced form equationNot interested causal interpretation \\(\\pi_1\\) \\(\\pi_2\\)interested causal interpretation \\(\\pi_1\\) \\(\\pi_2\\)linear projection \\(z_{i1}\\) \\(z_{i2}\\) \\(y_{i2}\\) (simple correlations)linear projection \\(z_{i1}\\) \\(z_{i2}\\) \\(y_{i2}\\) (simple correlations)projections \\(\\pi_1\\) \\(\\pi_2\\) guarantee \\(E(z_{i1}'v_i)=0\\) \\(E(z_{i2}'v_i)=0\\)projections \\(\\pi_1\\) \\(\\pi_2\\) guarantee \\(E(z_{i1}'v_i)=0\\) \\(E(z_{i2}'v_i)=0\\)Instrumental variable \\(z_{i2}\\)Instrument Relevance: \\(\\pi_2 \\neq 0\\)Instrument Exogeneity: \\(E(\\mathbf{z_{i2}\\epsilon_i})=0\\)Moving exogenous part \\(y_i2\\) moving\\[\n\\tilde{y}_{i2} = \\pi_0 + \\mathbf{z_{i1}\\pi_1 + z_{i2}\\pi_2}\n\\]two Stage Least Squares (2SLS)\\[\ny_{i1} = \\beta_0 +\\mathbf{z_{i1}\\beta_1}+ y_{i2}\\beta_2 + \\epsilon_i\n\\]\\[\ny_{i2} = \\pi_0 + \\mathbf{z_{i2}\\pi_2} + \\mathbf{v_i}\n\\]Equivalently,\\(\\tilde{y}_{i2} =\\pi_0 + \\mathbf{z_{i2}\\pi_2}\\)\\(u_i = v_i \\beta_2+ \\epsilon_i\\)(21.1) holds A1, A5A2 holds instrument relevant \\(\\pi_2 \\neq 0\\) + \\(y_{i1} = \\beta_0 + \\mathbf{z_{i1}\\beta_1 + (\\pi_0 + z_{i1}\\pi_1 + z_{i2}\\pi_2)}\\beta_2 + u_i\\)A3a holds instrument exogenous \\(E(\\mathbf{z}_{i2}\\epsilon_i)=0\\)\\[\n\\begin{aligned}\nE(\\tilde{y}_{i2}'u_i) &= E((\\pi_0 + \\mathbf{z_{i1}\\pi_1+z_{i2}})(v_i\\beta_2 + \\epsilon_i)) \\\\\n&= E((\\pi_0 + \\mathbf{z_{i1}\\pi_1+z_{i2}})( \\epsilon_i)) \\\\\n&= E(\\epsilon_i)\\pi_0 + E(\\epsilon_iz_{i1})\\pi_1 + E(\\epsilon_iz_{i2}) \\\\\n&=0 \n\\end{aligned}\n\\]Hence, (21.1) consistentThe 2SLS Estimator\n1. Estimate first stage using OLS\\[\ny_{i2} = \\pi_0 + \\mathbf{z_{i2}\\pi_2} + \\mathbf{v_i}\n\\]obtained estimated value \\(\\hat{y}_{i2}\\)Estimate altered equation using OLS\\[\ny_{i1} = \\beta_0 +\\mathbf{z_{i1}\\beta_1}+ \\hat{y}_{i2}\\beta_2 + \\epsilon_i \\\\\n\\]Properties 2SLS EstimatorUnder A1, A2, A3a (\\(z_{i1}\\)), A5 instrument satisfies following two conditions, + Instrument Relevance: \\(\\pi_2 \\neq 0\\) + Instrument Exogeneity: \\(E(\\mathbf{z}_{i2}'\\epsilon_i) = 0\\) 2SLS estimator consistentCan handle one endogenous variable one instrumental variable\\[\ny_{i1} = \\beta_0 + z_{i1}\\beta_1 + y_{i2}\\beta_2 + y_{i3}\\beta_3 + \\epsilon_i \\\\\ny_{i2} = \\pi_0 + z_{i1}\\pi_1 + z_{i2}\\pi_2 + z_{i3}\\pi_3 + z_{i4}\\pi_4 + v_{i2} \\\\\ny_{i3} = \\gamma_0 + z_{i1}\\gamma_1 + z_{i2}\\gamma_2 + z_{i3}\\gamma_3 + z_{i4}\\gamma_4 + v_{i3}\n\\]Standard errors produced second step correct\nknow \\(\\tilde{y}\\) perfectly need estimate firs step, introducing additional variation\nproblem FGLS “first stage orthogonal second stage.” generally true multi-step procedure.\nA4 hold, need report robust standard errors.\nStandard errors produced second step correctBecause know \\(\\tilde{y}\\) perfectly need estimate firs step, introducing additional variationWe problem FGLS “first stage orthogonal second stage.” generally true multi-step procedure.A4 hold, need report robust standard errors.2SLS less efficient OLS always larger standard errors.\nFirst, \\(Var(u_i) = Var(v_i\\beta_2 + \\epsilon_i) > Var(\\epsilon_i)\\)\nSecond, \\(\\hat{y}_{i2}\\) generally highly collinear \\(\\mathbf{z}_{i1}\\)\n2SLS less efficient OLS always larger standard errors.First, \\(Var(u_i) = Var(v_i\\beta_2 + \\epsilon_i) > Var(\\epsilon_i)\\)Second, \\(\\hat{y}_{i2}\\) generally highly collinear \\(\\mathbf{z}_{i1}\\)number instruments need least many number endogenous variables.number instruments need least many number endogenous variables.Note2SLS can combined FGLS make estimator efficient: first-stage, second-stage, instead using OLS, can use FLGS weight matrix \\(\\hat{w}\\)Generalized Method Moments can efficient 2SLS.second-stage 2SLS, can also use MLE, making assumption distribution outcome variable, endogenous variable, relationship (joint distribution).","code":"    + **IV estimator**: one endogenous variable with a single instrument \n    + **2SLS estimator**: one endogenous variable with multiple instruments \n    + **GMM estimator**: multiple endogenous variables with multiple instruments\n    "},{"path":"endogeneity.html","id":"testing-assumption","chapter":"21 Endogeneity","heading":"21.3.1.1 Testing Assumption","text":"Test Endogeneity: \\(y_{i2}\\) truly endogenous (.e., can just use OLS instead 2SLS)?Test Endogeneity: \\(y_{i2}\\) truly endogenous (.e., can just use OLS instead 2SLS)?Testing Instrument’s assumptions\nExogeneity\nRelevancy (need avoid “weak instruments”)\nTesting Instrument’s assumptionsExogeneityExogeneityRelevancy (need avoid “weak instruments”)Relevancy (need avoid “weak instruments”)","code":""},{"path":"endogeneity.html","id":"test-of-endogeneity","chapter":"21 Endogeneity","heading":"21.3.1.1.1 Test of Endogeneity","text":"2SLS generally inefficient may prefer OLS much endogeneity2SLS generally inefficient may prefer OLS much endogeneityBiased inefficient vs efficient biasedBiased inefficient vs efficient biasedWant sense “endogenous” \\(y_{i2}\\) \n“” endgeneous - use 2SLS\n“” endogenous - perhaps prefer OLS\nWant sense “endogenous” \\(y_{i2}\\) isif “” endgeneous - use 2SLSif “” endogenous - perhaps prefer OLSInvalid Test Endogeneity * \\(y_{i2}\\) endogenous correlated \\(\\epsilon_i\\),\\[\n\\epsilon_i = \\gamma_0 + y_{i2}\\gamma_1 + error_i\n\\]\\(\\gamma_1 \\neq 0\\) implies endogeneity\\(\\epsilon_i\\) observed, using residuals\\[\ne_i = \\gamma_0 + y_{i2}\\gamma_1 + error_i\n\\]valid test endogeneity + OLS residual, e mechanically uncorrelated \\(y_{i2}\\) (FOC OLS) + every situation, \\(\\gamma_1\\) essentially 0 never able reject null endogeneityValid test endogeneityIf \\(y_{i2}\\) endogenous \\(\\epsilon_i\\) v uncorrelated\\[\ny_{i1} = \\beta_0 + \\mathbf{z}_{i1}\\beta_1 + y_{i2}\\beta_2 + \\epsilon_i \\\\\ny_{i2} = \\pi_0 + \\mathbf{z}_{i1}\\pi_1 + z_{i2}\\pi_2 + v_i\n\\]variable Addition test: include first stage residuals additional variable,\\[\ny_{i1} = \\beta_0 + \\mathbf{z}_{i1}\\beta_1 + y_{i2}\\beta_2 + \\hat{v}_i \\theta + error_i\n\\]usual t-test significance valid test evaluate following hypothesis. note test requires instrument valid instrument.\\[\n\\begin{aligned}\nH_0: \\theta = 0 && \\text{  (endogenous)} \\\\\nH_1: \\theta \\neq 0 && \\text{  (endogenous)}\n\\end{aligned}\n\\]","code":""},{"path":"endogeneity.html","id":"testing-instruments-assumptions","chapter":"21 Endogeneity","heading":"21.3.1.1.2 Testing Instrument’s assumptions","text":"instrumental variable must satisfyExogeneityRelevancy (need avoid “weak instruments”)","code":""},{},{},{"path":"endogeneity.html","id":"good-instruments","chapter":"21 Endogeneity","heading":"21.3.1.2 Good Instruments","text":"","code":""},{"path":"endogeneity.html","id":"lagged-dependent-variable","chapter":"21 Endogeneity","heading":"21.3.1.2.1 Lagged dependent variable","text":"time series data sets, can use lagged dependent variable instrument influenced current shocks.Citations lagged dependent variable econ (Raj Chetty, Friedman, Rockoff 2013),","code":""},{"path":"endogeneity.html","id":"internal-instrumental-variable","chapter":"21 Endogeneity","heading":"21.3.2 Internal instrumental variable","text":"(also instrument free methods). section based Raluca Gui’s guide(also instrument free methods). section based Raluca Gui’s guidealternative external instrumental variable approachesalternative external instrumental variable approachesAll approaches assume continuous dependent variableAll approaches assume continuous dependent variable","code":""},{"path":"endogeneity.html","id":"non-hierarchical-data-cross-classified","chapter":"21 Endogeneity","heading":"21.3.2.1 Non-hierarchical Data (Cross-classified)","text":"\\[\nY_t = \\beta_0 + \\beta_1 P_t + \\beta_2 X_t + \\epsilon_t\n\\]\\(t = 1, .., T\\) (indexes either time cross-sectional units)\\(Y_t\\) k x 1 response variable\\(X_t\\) k x n exogenous regressor\\(P_t\\) k x 1 continuous endogenous regressor\\(\\epsilon_t\\) structural error term \\(\\mu_\\epsilon =0\\) \\(E(\\epsilon^2) = \\sigma^2\\)\\(\\beta\\) model parametersThe endogeneity problem arises correlation \\(P_t\\) \\(\\epsilon_t\\):\\[\nP_t = \\gamma Z_t + v_t\n\\]\\(Z_t\\) l x 1 vector internal instrumental variables\\(ν_t\\) random error \\(\\mu_{v_t}, E(v^2) = \\sigma^2_v, E(\\epsilon v) = \\sigma_{\\epsilon v}\\)\\(Z_t\\) assumed stochastic distribution G\\(ν_t\\) assumed density h(·)","code":""},{"path":"endogeneity.html","id":"latent-instrumental-variable","chapter":"21 Endogeneity","heading":"21.3.2.1.1 Latent Instrumental Variable","text":"(Ebbes et al. 2005)assume \\(Z_t\\) (unobserved) uncorrelated \\(\\epsilon_t\\), similar Instrumental Variable. Hence, \\(Z_t\\) \\(ν_t\\) can’t identified without distributional assumptionsThe distributions \\(Z_t\\) \\(ν_t\\) need specified :endogeneity \\(P_t\\) correctedthe distribution \\(P_t\\) empirically close integral expresses amount overlap Z shifted ν (= convolution \\(Z_t\\) \\(ν_t\\)).density h(·) = Normal, G normal parameters identified (Ebbes et al. 2005) .Hence,LIV model distribution \\(Z_t\\) discretein Higher Moments Method Joint Estimation Using Copula methods, distribution \\(Z_t\\) taken skewed.\\(Z_t\\) assumed unobserved, discrete exogenous, withan unknown number groups m\\(\\gamma\\) vector group means.Identification parameters relies distributional assumptions \\(P_t\\): non-Gaussian distribution\\(Z_t\\) discrete \\(m \\ge 2\\)Note:\\(Z_t\\) continuous, model unidentifiedIf \\(P_t \\sim N\\), inefficient estimates.return coefficient different methods since one endogenous variable.","code":"\nm3.liv <- latentIV(read ~ stratio, data=school)## No start parameters were given. The linear model read ~ stratio is fitted to derive them.## The start parameters c((Intercept)=706.449, stratio=-2.621, pi1=19.64, pi2=21.532, theta5=0.5, theta6=1, theta7=0.5, theta8=1) are used for optimization.\nsummary(m3.liv)$coefficients[1:7,]##                   Estimate    Std. Error       z-score     Pr(>|z|)\n## (Intercept)   6.996014e+02  2.686186e+02  2.604441e+00 9.529597e-03\n## stratio      -2.272673e+00  1.367757e+01 -1.661605e-01 8.681108e-01\n## pi1          -4.896363e+01  5.526907e-08 -8.859139e+08 0.000000e+00\n## pi2           1.963920e+01  9.225351e-02  2.128830e+02 0.000000e+00\n## theta5       6.939432e-152 3.354672e-160  2.068587e+08 0.000000e+00\n## theta6        3.787512e+02  4.249457e+01  8.912932e+00 1.541524e-17\n## theta7       -1.227543e+00  4.885276e+01 -2.512741e-02 9.799653e-01"},{"path":"endogeneity.html","id":"joint-estimation-using-copula","chapter":"21 Endogeneity","heading":"21.3.2.1.2 Joint Estimation Using Copula","text":"assume \\(Z_t\\) (unobserved) uncorrelated \\(\\epsilon_t\\), similar Instrumental Variable. Hence, \\(Z_t\\) \\(ν_t\\) can’t identified without distributional assumptions(Park Gupta 2012) allows joint estimation continuous \\(P_t\\) \\(\\epsilon_t\\) using Gaussian copulas, copula function maps several conditional distribution functions (CDF) joint CDF).underlying idea using information contained observed data, one selects marginal distributions \\(P_t\\) \\(\\epsilon_t\\). , copula model constructs flexible multivariate joint distribution allows wide range correlations two marginals.method allows continuous discrete \\(P_t\\).special case one continuous \\(P_t\\), estimation based MLE\nOtherwise, based Gaussian copulas, augmented OLS estimation used.Assumptions:skewed \\(P_t\\)skewed \\(P_t\\)recovery correct parameter estimatesthe recovery correct parameter estimates\\(\\epsilon_t \\sim\\) normal marginal distribution. marginal distribution \\(P_t\\) obtained using Epanechnikov kernel density estimator\\[\n\\hat{h}_p = \\frac{1}{T . b} \\sum_{t=1}^TK(\\frac{p - P_t}{b})\n\\] \\(\\epsilon_t \\sim\\) normal marginal distribution. marginal distribution \\(P_t\\) obtained using Epanechnikov kernel density estimator\\[\n\\hat{h}_p = \\frac{1}{T . b} \\sum_{t=1}^TK(\\frac{p - P_t}{b})\n\\] \\(P_t\\) = endogenous variables\\(P_t\\) = endogenous variables\\(K(x) = 0.75(1-x^2)(||x||\\le 1)\\)\\(K(x) = 0.75(1-x^2)(||x||\\le 1)\\)\\(b=0.9T^{-1/5}\\times min(s, IQR/1.34)\\) suggested (Silverman 1969)\nIQR = interquartile range\ns = sample standard deviation\nT = n time periods observed data\n\\(b=0.9T^{-1/5}\\times min(s, IQR/1.34)\\) suggested (Silverman 1969)IQR = interquartile ranges = sample standard deviationT = n time periods observed dataIn augmented OLS MLE, inference procedure occurs two stages:(1): empirical distribution \\(P_t\\) computed\n(2) used constructing likelihood function)\nHence, standard errors correct.use sampling distributions (bootstrapping) get standard errors variance-covariance matrix. Since distribution bootstraped parameters highly skewed, report percentile confidence intervals preferable.run model one endogenous continuous regressor (stratio). Sometimes, code converge, case can use differentoptimization algorithmstarting valuesmaximum number iterations","code":"\nset.seed(110)\nm4.cc <-\n        copulaCorrection(\n                read ~ stratio + english + lunch + calworks +\n                        grades + income + county | continuous(stratio),\n                data = school,\n                optimx.args = list(method = c(\"Nelder-Mead\"), itnmax = 60000),\n                num.boots = 2,\n                verbose = FALSE\n        )## Warning: It is recommended to run 1000 or more bootstraps.\nsummary(m4.cc)$coefficients[1:7, ]##             Point Estimate   Boots SE Lower Boots CI (95%) Upper Boots CI (95%)\n## (Intercept)   683.06900891 2.80554212                   NA                   NA\n## stratio        -0.32434608 0.02075999                   NA                   NA\n## english        -0.21576110 0.01450666                   NA                   NA\n## lunch          -0.37087664 0.01902052                   NA                   NA\n## calworks       -0.05569058 0.02076781                   NA                   NA\n## gradesKK-08    -1.92286128 0.25684614                   NA                   NA\n## income          0.73595353 0.04725700                   NA                   NA"},{"path":"endogeneity.html","id":"higher-moments-method","chapter":"21 Endogeneity","heading":"21.3.2.1.3 Higher Moments Method","text":"suggested (Lewbel 1997) identify \\(\\epsilon_t\\) caused measurement error.Identification achieved using third moments data, restrictions distribution \\(\\epsilon_t\\)\nfollowing instruments can used 2SLS estimation obtain consistent estimates:\\[\n\\begin{aligned}\nq_{1t} &=  (G_t - \\bar{G}) \\\\\nq_{2t} &=  (G_t - \\bar{G})(P_t - \\bar{P}) \\\\\nq_{3t} &=   (G_t - \\bar{G})(Y_t - \\bar{Y})\\\\\nq_{4t} &=  (Y_t - \\bar{Y})(P_t - \\bar{P}) \\\\\nq_{5t} &=  (P_t - \\bar{P})^2 \\\\\nq_{6t} &=  (Y_t - \\bar{Y})^2 \\\\\n\\end{aligned}\n\\]\\(G_t = G(X_t)\\) given function G finite third cross momentsX = exogenous variable\\(q_{5t}, q_{6t}\\) can used measurement \\(\\epsilon_t\\) symmetrically distributed. rest instruments require distributional assumptions \\(\\epsilon_t\\).Since regressors \\(G(X) = X\\) included instruments, \\(G(X)\\) can’t linear function X \\(q_{1t}\\)Since method strong assumptions, Higher Moments Method used case overidentificationrecommend using approach create additional instruments use external ones better efficiency.","code":"\nset.seed(111)\nm5.hetEr <-\n        hetErrorsIV(\n                read ~ stratio + english + lunch + calworks + income +\n                        grades + county | stratio | IIV(income, english),\n                data = school\n        )## Residuals were derived by fitting stratio ~ english + lunch + calworks + income + grades + county.## Warning: A studentized Breusch-Pagan test (stratio ~ english) indicates at a 95%\n## confidence level that the assumption of heteroscedasticity for the variable is\n## not satisfied (p-value: 0.2428). The instrument built from it therefore is weak.## The following internal instruments were built: IIV(income), IIV(english).## Fitting an instrumental variable regression with model read ~ stratio + english + lunch + calworks + income + grades + |english + lunch + calworks + income + grades + county + IIV(income) + IIV(english)    county|english + lunch + calworks + income + grades + county + IIV(income) + IIV(english).\nsummary(m5.hetEr)$coefficients[1:7, ]##                 Estimate  Std. Error    t value     Pr(>|t|)\n## (Intercept) 662.78791557 27.90173069 23.7543657 2.380436e-76\n## stratio       0.71480686  1.31077325  0.5453322 5.858545e-01\n## english      -0.19522271  0.04057527 -4.8113717 2.188618e-06\n## lunch        -0.37834232  0.03927793 -9.6324402 9.760809e-20\n## calworks     -0.05665126  0.06302095 -0.8989273 3.692776e-01\n## income        0.82693755  0.17236557  4.7975797 2.335271e-06\n## gradesKK-08  -1.93795843  1.38723186 -1.3969968 1.632541e-01"},{"path":"endogeneity.html","id":"heteroskedastic-error-approach","chapter":"21 Endogeneity","heading":"21.3.2.1.4 Heteroskedastic Error Approach","text":"using means variables uncorrelated product heteroskedastic errors identify structural parameters.method can use either don’t external instruments want use additional instruments improve efficiency IV estimator (Lewbel 2012)instruments constructed simple functions dataModel’s assumptions:\\[\nE(X \\epsilon) = 0 \\\\\nE(X v ) = 0 \\\\\ncov(Z, \\epsilon v) = 0  \\\\\ncov(Z, v^2) \\neq 0 \\text{  (identification)}\n\\]Structural parameters identified 2SLS regression Y X P, using X [Z − E(Z)]ν instruments.\\[\n\\text{instrument's strength} \\propto cov((Z-\\bar{Z})v,v)\n\\]\\(cov((Z-\\bar{Z})v,v)\\) degree heteroskedasticity ν respect Z (Lewbel 2012), can empirically tested.zero close zero (.e.,instrument weak), might imprecise estimates, large standard errors.homoskedasticity, parameters model unidentified.heteroskedasticity related least elements X, parameters model identified.","code":""},{"path":"endogeneity.html","id":"hierarchical-data","chapter":"21 Endogeneity","heading":"21.3.2.2 Hierarchical Data","text":"Multiple independent assumptions involving various random components different levels mean moderate correlation predictors random component error term can result significant bias coefficients variance components. (Kim Frees 2007) proposed generalized method moments uses , within variations exogenous variables, assumes within variation variables endogenous.Assumptionsthe errors level \\(\\sim iid N\\)slope variables exogenousthe level-1 \\(\\epsilon \\perp X, P\\). case, additional, external instruments necessaryHierarchical Model\\[\nY_{cst} = Z_{cst}^1 \\beta_{cs}^1 + X_{cst}^1 \\beta_1 + \\epsilon_{cst}^1 \\\\\n\\beta^1_{cs} = Z_{cs}^2 \\beta_{c}^2 + X_{cst}^2 \\beta_2 + \\epsilon_{cst}^2 \\\\\n\\beta^2_{c} = X^3_c \\beta_3 + \\epsilon_c^3\n\\]Bias stem :errors higher two levels (\\(\\epsilon_c^3,\\epsilon_{cst}^2\\)) correlated regressorsonly third level errors (\\(\\epsilon_c^3\\)) correlated regressors(Kim Frees 2007) proposedWhen variables assumed exogenous, proposed estimator equals random effects estimatorWhen variables assumed endogenous, equals fixed effects estimatoralso use omitted variable test (based Hausman-test (Hausman 1978) panel data), allows comparison robust estimator estimator efficient null hypothesis omitted variables comparison two robust estimators different levels.Another example using simulated datalevel-1 regressors: \\(X_{11}, X_{12}, X_{13}, X_{14}, X_{15}\\), \\(X_{15}\\) correlated level-2 error (.e., endogenous).level-2 regressors: \\(X_{21}, X_{22}, X_{23}, X_{24}\\)level-3 regressors: \\(X_{31}, X_{32}, X_{33}\\)estimate three-level model X15 assumed endogenous. three-level hierarchy, multilevelIV() returns five estimators, robust omitted variables (FE_L2), efficient (REF) (.e. lowest mean squared error).random effects estimator (REF) efficient assuming omitted variablesThe fixed effects estimator (FE) unbiased asymptotically normal even presence omitted variables.efficiency, random effects estimator preferable think omitted. variablesThe robust estimator preferable think omitted variables.True \\(\\beta_{X_{15}} =-1\\). can see estimators bias \\(X_{15}\\) correlated level-two error, FE_L2 GMM_L2 robustTo select appropriate estimator, use omitted variable test.three-level setting, can different estimator comparisons:Fixed effects vs. random effects estimators: Test omitted level-two level-three omitted effects, simultaneously, one compares FE_L2 REF. know omitted variables exist.Fixed effects vs. GMM estimators: existence omitted effects established sure level, test level-2 omitted effects comparing FE_L2 vs GMM_L3. reject null, omitted variables level-2 accomplished testing FE_L2 vs. GMM_L2, since latter consistent omitted effects level-2.Fixed effects vs. fixed effects estimators: can test omitted level-2 effects, allowing omitted level-3 effects comparing FE_L2 vs. FE_L3 since FE_L2 robust level-2 level-3 omitted effects FE_L3 robust level-3 omitted variables.Summary, use omitted variable test comparing REF vs. FE_L2 first.null hypothesis rejected, omitted variables either level-2 level-3If null hypothesis rejected, omitted variables either level-2 level-3Next, test whether level-2 omitted effects, since testing omitted level three effects relies assumption level-two omitted effects. can use pair comparisons:\nFE_L2 vs. FE_L3\nFE_L2 vs. GMM_L2\nNext, test whether level-2 omitted effects, since testing omitted level three effects relies assumption level-two omitted effects. can use pair comparisons:FE_L2 vs. FE_L3FE_L2 vs. GMM_L2If omitted variables level-2 found, test omitted level-3 effects comparing either\nFE_L3 vs. GMM_L3\nGMM_L2 vs. GMM_L3\nomitted variables level-2 found, test omitted level-3 effects comparing eitherFE_L3 vs. GMM_L3GMM_L2 vs. GMM_L3Since null hypothesis rejected (p = 0.000139), bias random effects estimator.test level-2 omitted effects (regardless level-3 omitted effects), compare FE_L2 versus FE_L3The null hypothesis omitted level-2 effects rejected (\\(p = 3.92e − 05\\)). Hence, omitted effects level-two. use FE_L2 consistent underlying data generated (level-2 error correlated \\(X_15\\), leads biased FE_L3 coefficients.omitted variable test FE_L2 GMM_L2 reject null hypothesis omitted level-2 effects (p-value 0).assume endogenous variable exogenous, RE GMM estimators biased wrong set internal instrumental variables. increase confidence, compare omitted variable tests variable considered endogenous vs. exogenous get sense whether variable truly endogenous.","code":"\nset.seed(113)\nschool$gr08 <- school$grades == \"KK-06\"\nm7.multilevel <-\n        multilevelIV(read ~ stratio + english + lunch + income + gr08 +\n                             calworks + (1 | county) | endo(stratio),\n                     data = school)## Fitting linear mixed-effects model read ~ stratio + english + lunch + income + gr08 + calworks +     (1 | county).## Detected multilevel model with 2 levels.## For county (Level 2), 45 groups were found.\nsummary(m7.multilevel)$coefficients[1:7, ]##                Estimate Std. Error     z-score     Pr(>|z|)\n## (Intercept) 675.8228656 5.58008680 121.1133248 0.000000e+00\n## stratio      -0.4956054 0.23922638  -2.0717005 3.829339e-02\n## english      -0.2599777 0.03413530  -7.6160948 2.614656e-14\n## lunch        -0.3692954 0.03560210 -10.3728537 3.295342e-25\n## income        0.6723141 0.08862012   7.5864728 3.287314e-14\n## gr08TRUE      2.1590333 1.28167222   1.6845440 9.207658e-02\n## calworks     -0.0570633 0.05711701  -0.9990596 3.177658e-01\ndata(dataMultilevelIV)\nset.seed(114)\nformula1 <-\n        y ~ X11 + X12 + X13 + X14 + X15 + X21 + X22 + X23 + X24 +\n        X31 + X32 + X33 + (1 | CID) + (1 | SID) | endo(X15)\nm8.multilevel <-\n        multilevelIV(formula = formula1, data = dataMultilevelIV)## Fitting linear mixed-effects model y ~ X11 + X12 + X13 + X14 + X15 + X21 + X22 + X23 + X24 + X31 +     X32 + X33 + (1 | CID) + (1 | SID).## Detected multilevel model with 3 levels.## For CID (Level 2), 1368 groups were found.## For SID (Level 3), 40 groups were found.\ncoef(m8.multilevel)##                    REF      FE_L2      FE_L3     GMM_L2     GMM_L3\n## (Intercept) 64.3168856  0.0000000  0.0000000 64.3485944 64.3168868\n## X11          3.0213405  3.0459605  3.0214255  3.0146686  3.0213403\n## X12          8.9522160  8.9839088  8.9524723  8.9747533  8.9522169\n## X13         -2.0194178 -2.0145054 -2.0193321 -2.0021426 -2.0194171\n## X14          1.9651420  1.9791437  1.9648317  1.9658681  1.9651421\n## X15         -0.5647915 -0.9777361 -0.5647621 -0.9750309 -0.5648070\n## X21         -2.3316225  0.0000000 -2.2845297 -2.3052516 -2.3316215\n## X22         -3.9564944  0.0000000 -3.9553644 -4.0130975 -3.9564966\n## X23         -2.9779887  0.0000000 -2.9756848 -2.9488487 -2.9779876\n## X24          4.9078293  0.0000000  4.9084694  4.7933756  4.9078250\n## X31          2.1142348  0.0000000  0.0000000  2.1164477  2.1142349\n## X32          0.3934770  0.0000000  0.0000000  0.3799626  0.3934764\n## X33          0.1082086  0.0000000  0.0000000  0.1108386  0.1082087\nsummary(m8.multilevel, \"REF\")## \n## Call:\n## multilevelIV(formula = formula1, data = dataMultilevelIV)\n## \n## Number of levels: 3\n## Number of observations: 2824\n## Number of groups: L2(CID): 1368  L3(SID): 40\n## \n## Coefficients for model REF:\n##             Estimate Std. Error z-score Pr(>|z|)    \n## (Intercept) 64.31689    7.87332   8.169 3.11e-16 ***\n## X11          3.02134    0.02576 117.306  < 2e-16 ***\n## X12          8.95222    0.02572 348.131  < 2e-16 ***\n## X13         -2.01942    0.02409 -83.835  < 2e-16 ***\n## X14          1.96514    0.02521  77.937  < 2e-16 ***\n## X15         -0.56479    0.01950 -28.962  < 2e-16 ***\n## X21         -2.33162    0.16228 -14.368  < 2e-16 ***\n## X22         -3.95649    0.13119 -30.160  < 2e-16 ***\n## X23         -2.97799    0.06611 -45.044  < 2e-16 ***\n## X24          4.90783    0.19796  24.792  < 2e-16 ***\n## X31          2.11423    0.10433  20.264  < 2e-16 ***\n## X32          0.39348    0.30426   1.293   0.1959    \n## X33          0.10821    0.05236   2.067   0.0388 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Omitted variable tests for model REF:\n##               df     Chisq  p-value    \n## GMM_L2_vs_REF  7     18.74 0.009040 ** \n## GMM_L3_vs_REF 13 -12872.98 1.000000    \n## FE_L2_vs_REF  13     39.99 0.000139 ***\n## FE_L3_vs_REF  13     39.99 0.000138 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nsummary(m8.multilevel, \"REF\")## \n## Call:\n## multilevelIV(formula = formula1, data = dataMultilevelIV)\n## \n## Number of levels: 3\n## Number of observations: 2824\n## Number of groups: L2(CID): 1368  L3(SID): 40\n## \n## Coefficients for model REF:\n##             Estimate Std. Error z-score Pr(>|z|)    \n## (Intercept) 64.31689    7.87332   8.169 3.11e-16 ***\n## X11          3.02134    0.02576 117.306  < 2e-16 ***\n## X12          8.95222    0.02572 348.131  < 2e-16 ***\n## X13         -2.01942    0.02409 -83.835  < 2e-16 ***\n## X14          1.96514    0.02521  77.937  < 2e-16 ***\n## X15         -0.56479    0.01950 -28.962  < 2e-16 ***\n## X21         -2.33162    0.16228 -14.368  < 2e-16 ***\n## X22         -3.95649    0.13119 -30.160  < 2e-16 ***\n## X23         -2.97799    0.06611 -45.044  < 2e-16 ***\n## X24          4.90783    0.19796  24.792  < 2e-16 ***\n## X31          2.11423    0.10433  20.264  < 2e-16 ***\n## X32          0.39348    0.30426   1.293   0.1959    \n## X33          0.10821    0.05236   2.067   0.0388 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Omitted variable tests for model REF:\n##               df     Chisq  p-value    \n## GMM_L2_vs_REF  7     18.74 0.009040 ** \n## GMM_L3_vs_REF 13 -12872.98 1.000000    \n## FE_L2_vs_REF  13     39.99 0.000139 ***\n## FE_L3_vs_REF  13     39.99 0.000138 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# compare REF with all the other estimators. Testing REF (the most efficient estimator) against FE_L2 (the most robust estimator), equivalently we are testing simultaneously for level-2 and level-3 omitted effects. \nsummary(m8.multilevel,\"FE_L2\")## \n## Call:\n## multilevelIV(formula = formula1, data = dataMultilevelIV)\n## \n## Number of levels: 3\n## Number of observations: 2824\n## Number of groups: L2(CID): 1368  L3(SID): 40\n## \n## Coefficients for model FE_L2:\n##               Estimate Std. Error z-score Pr(>|z|)    \n## (Intercept)  0.000e+00  4.275e-19    0.00        1    \n## X11          3.046e+00  2.978e-02  102.30   <2e-16 ***\n## X12          8.984e+00  3.360e-02  267.41   <2e-16 ***\n## X13         -2.015e+00  3.107e-02  -64.83   <2e-16 ***\n## X14          1.979e+00  3.203e-02   61.80   <2e-16 ***\n## X15         -9.777e-01  3.364e-02  -29.06   <2e-16 ***\n## X21          0.000e+00  1.824e-18    0.00        1    \n## X22          0.000e+00  1.303e-18    0.00        1    \n## X23          0.000e+00  4.389e-18    0.00        1    \n## X24          0.000e+00  1.724e-18    0.00        1    \n## X31          0.000e+00  1.468e-17    0.00        1    \n## X32          0.000e+00  8.265e-18    0.00        1    \n## X33          0.000e+00  2.793e-17    0.00        1    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Omitted variable tests for model FE_L2:\n##                 df Chisq  p-value    \n## FE_L2_vs_REF    13 39.99 0.000139 ***\n## FE_L2_vs_FE_L3   9 36.02 3.92e-05 ***\n## FE_L2_vs_GMM_L2 12 39.99 7.21e-05 ***\n## FE_L2_vs_GMM_L3 13 39.99 0.000139 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"endogeneity.html","id":"proxy-variables","chapter":"21 Endogeneity","heading":"21.3.3 Proxy Variables","text":"Can place omitted variableCan place omitted variablewill able estimate effect omitted variablewill able estimate effect omitted variablewill able reduce endogeneity caused bye omitted variablewill able reduce endogeneity caused bye omitted variablebut can Measurement Error. Hence, extremely careful using proxies.can Measurement Error. Hence, extremely careful using proxies.Criteria proxy variable:proxy correlated omitted variable.omitted variable regression solve problem endogeneityThe variation omitted variable unexplained proxy uncorrelated independent variables, including proxy.IQ test can proxy ability regression wage explained education.third requirement\\[\nability = \\gamma_0 + \\gamma_1 IQ + \\epsilon\n\\]\\(\\epsilon\\) uncorrelated education IQ test.","code":""},{"path":"endogeneity.html","id":"endogenous-sample-selection","chapter":"21 Endogeneity","heading":"21.4 Endogenous Sample Selection","text":"sample selection self-selection problemthe omitted variable people selected sampleSome disciplines consider nonresponse bias selection bias sample selection.unobservable factors affect sample independent unobservable factors affect outcome, sample selection endogenous. Hence, sample selection ignorable estimator ignores sample selection still consistent.unobservable factors affect included sample correlated unobservable factors affect outcome, sample selection endogenous ignorable, estimators ignore endogenous sample selection consistent (don’t know part observable outcome related causal relationship part due different people selected treatment control groups).combat Sample selection, canRandomization: participants randomly selected treatment control.Instruments determine treatment status (.e., treatment vs. control) outcome (Y)Functional form selection outcome processes: originated (Heckman 1976), later generalize (Amemiya 1984)main model\\[\n\\mathbf{y^* = xb + \\epsilon}\n\\]However, pattern missingness (.e., censored) related unobserved (latent) process:\\[\n\\mathbf{z^* = w \\gamma + u}\n\\]\\[\nz_i = \n\\begin{cases}\n1& \\text{} z_i^*>0 \\\\\n0&\\text{} z_i^*\\le0\\\\\n\\end{cases}\n\\]Equivalently, \\(z_i = 1\\) (\\(y_i\\) observed) \\[\nu_i \\ge -w_i \\gamma\n\\]Hence, probability observed \\(y_i\\) \\[\n\\begin{aligned}\nP(u_i \\ge -w_i \\gamma) &= 1 - \\Phi(-w_i \\gamma) \\\\\n&= \\Phi(w_i \\gamma) & \\text{symmetry standard normal distribution}\n\\end{aligned}\n\\]assumethe error term selection \\(\\mathbf{u \\sim N(0,)}\\)\\(Var(u_i) = 1\\) identification purposesVisually, \\(P(u_i \\ge -w_i \\gamma)\\) shaded area.Hence observed model, seeand joint distribution selection model (\\(u_i\\)), observed equation (\\(\\epsilon_i\\)) \\[\n\\left[\n\\begin{array}\n{c}\nu \\\\\n\\epsilon \\\\\n\\end{array}\n\\right]\n\\sim^{iid}N\n\\left(\n\\left[\n\\begin{array}\n{c}\n0 \\\\\n0 \\\\\n\\end{array}\n\\right],\n\\left[\n\\begin{array}\n{cc}\n1 & \\rho \\\\\n\\rho & \\sigma^2_{\\epsilon} \\\\\n\\end{array}\n\\right]\n\\right)\n\\]relation observed selection models:\\[\n\\begin{aligned}\nE(y_i | y_i \\text{ observed}) &= E(y_i| z^*>0) \\\\\n&= E(y_i| -w_i \\gamma) \\\\\n&= \\mathbf{x}_i \\beta + E(\\epsilon_i | u_i > -w_i \\gamma) \\\\\n&= \\mathbf{x}_i \\beta + \\rho \\sigma_\\epsilon \\frac{\\phi(w_i \\gamma)}{\\Phi(w_i \\gamma)}\n\\end{aligned}\n\\]\\(\\frac{\\phi(w_i \\gamma)}{\\Phi(w_i \\gamma)}\\) Inverse Mills Ratio. \\(\\rho \\sigma_\\epsilon \\frac{\\phi(w_i \\gamma)}{\\Phi(w_i \\gamma)} \\ge 0\\)Great visualization special cases correlation patterns amongst data errors professor Rob HickNote:(Bareinboim, Tian, Pearl 2014) excellent summary cases can still causal inference case selection bias. ’ll try summarize idea :Let X action, Y outcome, S binary indicator entry data pool (S = 1 = sample, S = 0 =sample) Q conditional distribution \\(Q = P(y|x)\\).Usually want understand , S, \\(P(y, x|S = 1)\\). Hence, ’d like recover \\(P(y|x)\\) \\(P(y, x|S = 1)\\)X Y affect S, can’t unbiasedly estimate \\(P(y|x)\\)case Omitted variable bias (U) sample selection bias (S), unblocked extraneous “flow” information X Y, causes spurious correlation X Y. Traditionally, recover Q parametric assumption ofthe data generating process (e.g., Heckman 2-step)type data-generating model (e..g, treatment-dependent outcome-dependent)selection’s probability \\(P(S = 1|P a_s)\\) non-parametrically based causal graphical models, authors proposed robust way model misspecification regardless type data-generating model, require selection’s probability. Hence, can recover Qwithout external datawith external datacausal effects Selection-backdoor criterion","code":"\nx = seq(-3, 3, length = 200)\ny = dnorm(x, mean = 0, sd = 1)\nplot(x,\n     y,\n     type = \"l\",\n     main = bquote(\"Probabibility distribution of\" ~ u[i]))\nx = seq(0.3, 3, length = 100)\ny = dnorm(x, mean = 0, sd = 1)\npolygon(c(0.3, x, 3), c(0, y, 0), col = \"gray\")\ntext(1, 0.1, bquote(1 - Phi ~ (-w[i] ~ gamma)))\narrows(-0.5, 0.1, 0.3, 0, length = .15)\ntext(-0.5, 0.12, bquote(-w[i] ~ gamma))\nlegend(\n  \"topright\",\n  \"Gray = Prob of Observed\",\n  pch = 1,\n  title = \"legend\",\n  inset = .02\n)"},{"path":"endogeneity.html","id":"tobit-2","chapter":"21 Endogeneity","heading":"21.4.1 Tobit-2","text":"also known Heckman’s standard sample selection model\nAssumption: joint normality errorsData taken (Mroz 1987)’s paper.want estimate log(wage) married women, education, experience, experience squared, dummy variable living big city. can observe wage women working, means lot married women 1975 labor force unaccounted . Hence, OLS estimate wage equation bias due sample selection. Since data non-participants (.e., working pay), can correct selection process.Tobit-2 estimates consistent","code":""},{"path":"endogeneity.html","id":"example-1-3","chapter":"21 Endogeneity","heading":"21.4.1.1 Example 1","text":"2-stage Heckman’s model:probit equation estimates selection process (labor force?)results 1st stage used construct variable captures selection effect wage equation. correction variable called inverse Mills ratio.Use variables affect selection process selection equation. Technically, selection equation equation interest set regressors. recommended use variables (least one) selection equation affect selection process, wage process (.e., instruments). , variable kids fulfill role: women kids may likely stay home, working moms kids wages change.Alternatively,Rho estimate correlation errors selection wage equations. lower panel, estimated coefficient inverse Mills ratio given Heckman model. fact statistically different zero consistent idea selection bias serious problem case.estimated coefficient inverse Mills ratio Heckman model statistically different zero, selection bias serious problem.","code":"\nlibrary(sampleSelection)## Loading required package: maxLik## Warning: package 'maxLik' was built under R version 4.0.5## Loading required package: miscTools## \n## Please cite the 'maxLik' package as:\n## Henningsen, Arne and Toomet, Ott (2011). maxLik: A package for maximum likelihood estimation in R. Computational Statistics 26(3), 443-458. DOI 10.1007/s00180-010-0217-1.\n## \n## If you have questions, suggestions, or comments regarding the 'maxLik' package, please use a forum or 'tracker' at maxLik's R-Forge site:\n## https://r-forge.r-project.org/projects/maxlik/\nlibrary(dplyr)## Warning: package 'dplyr' was built under R version 4.0.5## \n## Attaching package: 'dplyr'## The following object is masked from 'package:car':\n## \n##     recode## The following objects are masked from 'package:stats':\n## \n##     filter, lag## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\ndata(\"Mroz87\") #1975 data on married women’s pay and labor-force participation from the Panel Study of Income Dynamics (PSID)\nhead(Mroz87)##   lfp hours kids5 kids618 age educ   wage repwage hushrs husage huseduc huswage\n## 1   1  1610     1       0  32   12 3.3540    2.65   2708     34      12  4.0288\n## 2   1  1656     0       2  30   12 1.3889    2.65   2310     30       9  8.4416\n## 3   1  1980     1       3  35   12 4.5455    4.04   3072     40      12  3.5807\n## 4   1   456     0       3  34   12 1.0965    3.25   1920     53      10  3.5417\n## 5   1  1568     1       2  31   14 4.5918    3.60   2000     32      12 10.0000\n## 6   1  2032     0       0  54   12 4.7421    4.70   1040     57      11  6.7106\n##   faminc    mtr motheduc fatheduc unem city exper  nwifeinc wifecoll huscoll\n## 1  16310 0.7215       12        7  5.0    0    14 10.910060    FALSE   FALSE\n## 2  21800 0.6615        7        7 11.0    1     5 19.499981    FALSE   FALSE\n## 3  21040 0.6915       12        7  5.0    0    15 12.039910    FALSE   FALSE\n## 4   7300 0.7815        7        7  5.0    0     6  6.799996    FALSE   FALSE\n## 5  27300 0.6215       12       14  9.5    1     7 20.100058     TRUE   FALSE\n## 6  19495 0.6915       14        7  7.5    1    33  9.859054    FALSE   FALSE\nMroz87 = Mroz87 %>%\n        mutate(kids = kids5+kids618)\n\nlibrary(nnet)## Warning: package 'nnet' was built under R version 4.0.5\nlibrary(ggplot2)## Warning: package 'ggplot2' was built under R version 4.0.5\nlibrary(reshape2)\n# OLS: log wage regression on LF participants only\nols1 = lm(log(wage) ~ educ + exper + I( exper^2 ) + city, data=subset(Mroz87, lfp==1))\n# Heckman's Two-step estimation with LFP selection equation\nheck1 = heckit( lfp ~ age + I( age^2 ) + kids + huswage + educ, # the selection process, lfp = 1 if the woman is participating in the labor force \n                 log(wage) ~ educ + exper + I( exper^2 ) + city, data=Mroz87 )\n# ML estimation of selection model\nml1 = selection( lfp ~ age + I( age^2 ) + kids + huswage + educ,\n                    log(wage) ~ educ + exper + I( exper^2 ) + city, data=Mroz87 ) \nlibrary(\"stargazer\")## \n## Please cite as:##  Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables.##  R package version 5.2.2. https://CRAN.R-project.org/package=stargazer\nlibrary(\"Mediana\")\nlibrary(\"plm\")## \n## Attaching package: 'plm'## The following objects are masked from 'package:dplyr':\n## \n##     between, lag, lead\n# function to calculate corrected SEs for regression \ncse = function(reg) {\n  rob = sqrt(diag(vcovHC(reg, type = \"HC1\")))\n  return(rob)\n}\n\n# stargazer table\nstargazer(ols1, heck1, ml1,    \n          se=list(cse(ols1),NULL,NULL), \n          title=\"Married women's wage regressions\", type=\"text\", \n          df=FALSE, digits=4, selection.equation = T)## \n## Married women's wage regressions\n## ==============================================================\n##                                Dependent variable:            \n##                     ------------------------------------------\n##                     log(wage)                lfp              \n##                        OLS         Heckman        selection   \n##                                   selection                   \n##                        (1)           (2)             (3)      \n## --------------------------------------------------------------\n## age                               0.1861***       0.1842***   \n##                                   (0.0652)        (0.0658)    \n##                                                               \n## I(age2)                          -0.0024***      -0.0024***   \n##                                   (0.0008)        (0.0008)    \n##                                                               \n## kids                             -0.1496***      -0.1488***   \n##                                   (0.0383)        (0.0385)    \n##                                                               \n## huswage                          -0.0430***      -0.0434***   \n##                                   (0.0122)        (0.0123)    \n##                                                               \n## educ                0.1057***     0.1250***       0.1256***   \n##                      (0.0130)     (0.0228)        (0.0229)    \n##                                                               \n## exper               0.0411***                                 \n##                      (0.0154)                                 \n##                                                               \n## I(exper2)            -0.0008*                                 \n##                      (0.0004)                                 \n##                                                               \n## city                  0.0542                                  \n##                      (0.0653)                                 \n##                                                               \n## Constant            -0.5308***   -4.1815***      -4.1484***   \n##                      (0.2032)     (1.4024)        (1.4109)    \n##                                                               \n## --------------------------------------------------------------\n## Observations           428           753             753      \n## R2                    0.1581       0.1582                     \n## Adjusted R2           0.1501       0.1482                     \n## Log Likelihood                                    -914.0777   \n## rho                                0.0830      0.0505 (0.2317)\n## Inverse Mills Ratio            0.0551 (0.2099)                \n## Residual Std. Error   0.6667                                  \n## F Statistic         19.8561***                                \n## ==============================================================\n## Note:                              *p<0.1; **p<0.05; ***p<0.01"},{"path":"endogeneity.html","id":"example-2-2","chapter":"21 Endogeneity","heading":"21.4.1.2 Example 2","text":"code R package sampleSelectionwithout exclusion restriction, generate yo using xs instead xo.can see estimates still unbiased standard errors substantially larger. exclusion restriction (.e., independent information selection process) certain identifying power desire. Hence, ’s better different set variable selection process interested equation. Without exclusion restriction, solely rely functional form identification.","code":"\nset.seed(0)\nlibrary(\"sampleSelection\")\nlibrary(\"mvtnorm\")## Warning: package 'mvtnorm' was built under R version 4.0.5\neps <- rmvnorm(500, c(0,0), matrix(c(1,-0.7,-0.7,1), 2, 2)) # bivariate normal disturbances\nxs <- runif(500)# uniformly distributed explanatory variable (vectors of explanatory variables for the selection )\nys <- xs + eps[,1] > 0 # probit data generating process\nxo <- runif(500) # vectors of explanatory variables for outcome equation \nyoX <- xo + eps[,2] # latent outcome\nyo <- yoX*(ys > 0) # observable outcome\n# true intercepts = 0 and our true slopes = 1\n# xs and xo are independent. Hence, exclusion restriction is fulfilled\nsummary( selection(ys~xs, yo ~xo))## --------------------------------------------\n## Tobit 2 model (sample selection model)\n## Maximum Likelihood estimation\n## Newton-Raphson maximisation, 5 iterations\n## Return code 1: gradient close to zero (gradtol)\n## Log-Likelihood: -712.3163 \n## 500 observations (172 censored and 328 observed)\n## 6 free parameters (df = 494)\n## Probit selection equation:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  -0.2228     0.1081  -2.061   0.0399 *  \n## xs            1.3377     0.2014   6.642 8.18e-11 ***\n## Outcome equation:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -0.0002265  0.1294178  -0.002    0.999    \n## xo           0.7299070  0.1635925   4.462 1.01e-05 ***\n##    Error terms:\n##       Estimate Std. Error t value Pr(>|t|)    \n## sigma   0.9190     0.0574  16.009  < 2e-16 ***\n## rho    -0.5392     0.1521  -3.544 0.000431 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## --------------------------------------------\nyoX <- xs + eps[,2]\nyo <- yoX*(ys > 0)\nsummary(selection(ys ~ xs, yo ~ xs))## --------------------------------------------\n## Tobit 2 model (sample selection model)\n## Maximum Likelihood estimation\n## Newton-Raphson maximisation, 14 iterations\n## Return code 8: successive function values within relative tolerance limit (reltol)\n## Log-Likelihood: -712.8298 \n## 500 observations (172 censored and 328 observed)\n## 6 free parameters (df = 494)\n## Probit selection equation:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  -0.1984     0.1114  -1.781   0.0756 .  \n## xs            1.2907     0.2085   6.191 1.25e-09 ***\n## Outcome equation:\n##             Estimate Std. Error t value Pr(>|t|)   \n## (Intercept)  -0.5499     0.5644  -0.974  0.33038   \n## xs            1.3987     0.4482   3.120  0.00191 **\n##    Error terms:\n##       Estimate Std. Error t value Pr(>|t|)    \n## sigma  0.85091    0.05352  15.899   <2e-16 ***\n## rho   -0.13226    0.72684  -0.182    0.856    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## --------------------------------------------"},{"path":"endogeneity.html","id":"tobit-5","chapter":"21 Endogeneity","heading":"21.4.2 Tobit-5","text":"Also known switching regression model\nCondition: least one variable X selection process included observed process. Used separate models participants, non-participants.exclusion restriction fulfilled x’s independent.estimates close true values.Example functional form misspecificationAlthough still exclusion restriction (xo1 xo2 independent), now problems intercepts (.e., statistically significantly different true values zero), convergence problems.don’t exclusion restriction, larger variance xsUsually converge. Even , results may seriously biased.NoteThe log-likelihood function models might globally concave. Hence, might converge, converge local maximum. combat , can useDifferent starting valueDifferent maximization methods.refer Non-linear Least Squares suggestions.","code":"\nset.seed(0)\nvc <- diag(3)\nvc[lower.tri(vc)] <- c(0.9, 0.5, 0.1)\nvc[upper.tri(vc)] <- vc[lower.tri(vc)]\neps <- rmvnorm(500, c(0,0,0), vc) # 3 disturbance vectors by a 3-dimensional normal distribution\nxs <- runif(500) # uniformly distributed on [0, 1]\nys <- xs + eps[,1] > 0\nxo1 <- runif(500) # uniformly distributed on [0, 1]\nyo1 <- xo1 + eps[,2]\nxo2 <- runif(500) # uniformly distributed on [0, 1]\nyo2 <- xo2 + eps[,3]\nsummary(selection(ys~xs, list(yo1 ~ xo1, yo2 ~ xo2))) # one selection equation and a list of two outcome equations## --------------------------------------------\n## Tobit 5 model (switching regression model)\n## Maximum Likelihood estimation\n## Newton-Raphson maximisation, 11 iterations\n## Return code 1: gradient close to zero (gradtol)\n## Log-Likelihood: -895.8201 \n## 500 observations: 172 selection 1 (FALSE) and 328 selection 2 (TRUE)\n## 10 free parameters (df = 490)\n## Probit selection equation:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  -0.1550     0.1051  -1.474    0.141    \n## xs            1.1408     0.1785   6.390 3.86e-10 ***\n## Outcome equation 1:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  0.02708    0.16395   0.165    0.869    \n## xo1          0.83959    0.14968   5.609  3.4e-08 ***\n## Outcome equation 2:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)   0.1583     0.1885   0.840    0.401    \n## xo2           0.8375     0.1707   4.908 1.26e-06 ***\n##    Error terms:\n##        Estimate Std. Error t value Pr(>|t|)    \n## sigma1  0.93191    0.09211  10.118   <2e-16 ***\n## sigma2  0.90697    0.04434  20.455   <2e-16 ***\n## rho1    0.88988    0.05353  16.623   <2e-16 ***\n## rho2    0.17695    0.33139   0.534    0.594    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## --------------------------------------------\nset.seed(5)\neps <- rmvnorm(1000, rep(0, 3), vc)\neps <- eps^2 - 1 # subtract 1 in order to get the mean zero disturbances\nxs <- runif(1000, -1, 0) # interval [−1, 0] to get an asymmetric distribution over observed choices\nys <- xs + eps[,1] > 0\nxo1 <- runif(1000)\nyo1 <- xo1 + eps[,2]\nxo2 <- runif(1000)\nyo2 <- xo2 + eps[,3]\nsummary(selection(ys~xs, list(yo1 ~ xo1, yo2 ~ xo2), iterlim=20))## Warning in sqrt(diag(vc)): NaNs produced\n\n## Warning in sqrt(diag(vc)): NaNs produced## Warning in sqrt(diag(vcov(object, part = \"full\"))): NaNs produced## --------------------------------------------\n## Tobit 5 model (switching regression model)\n## Maximum Likelihood estimation\n## Newton-Raphson maximisation, 4 iterations\n## Return code 3: Last step could not find a value above the current.\n## Boundary of parameter space?  \n## Consider switching to a more robust optimisation method temporarily.\n## Log-Likelihood: -1665.936 \n## 1000 observations: 760 selection 1 (FALSE) and 240 selection 2 (TRUE)\n## 10 free parameters (df = 990)\n## Probit selection equation:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -0.53698    0.05808  -9.245  < 2e-16 ***\n## xs           0.31268    0.09395   3.328 0.000906 ***\n## Outcome equation 1:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -0.70679    0.03573  -19.78   <2e-16 ***\n## xo1          0.91603    0.05626   16.28   <2e-16 ***\n## Outcome equation 2:\n##             Estimate Std. Error t value Pr(>|t|)  \n## (Intercept)   0.1446         NA      NA       NA  \n## xo2           1.1196     0.5014   2.233   0.0258 *\n##    Error terms:\n##        Estimate Std. Error t value Pr(>|t|)    \n## sigma1  0.67770    0.01760   38.50   <2e-16 ***\n## sigma2  2.31432    0.07615   30.39   <2e-16 ***\n## rho1   -0.97137         NA      NA       NA    \n## rho2    0.17039         NA      NA       NA    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## --------------------------------------------\nset.seed(6)\nxs <- runif(1000, -1, 1)\nys <- xs + eps[,1] > 0\nyo1 <- xs + eps[,2]\nyo2 <- xs + eps[,3]\nsummary(tmp <- selection(ys~xs, list(yo1 ~ xs, yo2 ~ xs), iterlim=20))## --------------------------------------------\n## Tobit 5 model (switching regression model)\n## Maximum Likelihood estimation\n## Newton-Raphson maximisation, 16 iterations\n## Return code 8: successive function values within relative tolerance limit (reltol)\n## Log-Likelihood: -1936.431 \n## 1000 observations: 626 selection 1 (FALSE) and 374 selection 2 (TRUE)\n## 10 free parameters (df = 990)\n## Probit selection equation:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  -0.3528     0.0424  -8.321 2.86e-16 ***\n## xs            0.8354     0.0756  11.050  < 2e-16 ***\n## Outcome equation 1:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) -0.55448    0.06339  -8.748   <2e-16 ***\n## xs           0.81764    0.06048  13.519   <2e-16 ***\n## Outcome equation 2:\n##             Estimate Std. Error t value Pr(>|t|)\n## (Intercept)   0.6457     0.4994   1.293    0.196\n## xs            0.3520     0.3197   1.101    0.271\n##    Error terms:\n##        Estimate Std. Error t value Pr(>|t|)    \n## sigma1  0.59187    0.01853  31.935   <2e-16 ***\n## sigma2  1.97257    0.07228  27.289   <2e-16 ***\n## rho1    0.15568    0.15914   0.978    0.328    \n## rho2   -0.01541    0.23370  -0.066    0.947    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## --------------------------------------------"},{"path":"endogeneity.html","id":"pattern-mixture-models","chapter":"21 Endogeneity","heading":"21.4.2.0.1 Pattern-Mixture Models","text":"compared Heckman’s model assumes value missing data predetermined, pattern-mixture models assume missingness affect distribution variable interest (e.g., Y)read , can check NCSU, stefvanbuuren.","code":""},{"path":"mediation.html","id":"mediation","chapter":"22 Mediation","heading":"22 Mediation","text":"","code":""},{"path":"mediation.html","id":"traditional","chapter":"22 Mediation","heading":"22.1 Traditional","text":"(Baron Kenny 1986) outdated step 1, still see original idea.3 regressionsStep 1: \\(X \\Y\\)Step 1: \\(X \\Y\\)Step 2: \\(X \\M\\)Step 2: \\(X \\M\\)Step 3: \\(X + M \\Y\\)Step 3: \\(X + M \\Y\\)\\(X\\) = independent variable\\(X\\) = independent variable\\(Y\\) = dependent variable\\(Y\\) = dependent variable\\(M\\) = mediating variable\\(M\\) = mediating variableOriginally, first path \\(X \\Y\\) suggested (Baron Kenny 1986) needs signficaint. cases indirect \\(X\\) \\(Y\\) without significant direct effect \\(X\\) \\(Y\\) (e.g., effect absorbed M, two counteracting effects \\(M_1, M_2\\) cancel effect).Mathematically,\\[\nY = b_0 + b_1 X + \\epsilon\n\\]\\(b_1\\) need significant.examine effect \\(X\\) \\(M\\). step requires significant effect \\(X\\) \\(M\\) continue analysisMathematically,\\[\nM = b_0 + b_2 X + \\epsilon\n\\]\\(b_2\\) needs significant.<br>step, want effect \\(M\\) \\(Y\\) “absorbs” direct effect \\(X\\) \\(Y\\) (least makes effect smaller).Mathematically,\\[\nY = b_0 + b_4 X + b_3 M + \\epsilon\n\\]\\(b_4\\) needs either smaller insignificant.Examine mediation effect (.e., whether significant)Fist approach: Sobel’s test (Sobel 1982)Fist approach: Sobel’s test (Sobel 1982)Second approach: bootstrapping (Preacher Hayes 2004) (preferable)Second approach: bootstrapping (Preacher Hayes 2004) (preferable)details can found ","code":""},{"path":"mediation.html","id":"example-1-mediation-traditional","chapter":"22 Mediation","heading":"22.1.1 Example 1","text":"Virginia’s libraryTotal Effect = 0.3961 = \\(b_1\\) (step 1) = total effect \\(X\\) \\(Y\\) without \\(M\\)Total Effect = 0.3961 = \\(b_1\\) (step 1) = total effect \\(X\\) \\(Y\\) without \\(M\\)Direct Effect = ADE = 0.0396 = \\(b_4\\) (step 3) = direct effect \\(X\\) \\(Y\\) accounting indirect effect \\(M\\)Direct Effect = ADE = 0.0396 = \\(b_4\\) (step 3) = direct effect \\(X\\) \\(Y\\) accounting indirect effect \\(M\\)ACME = Average Causal Mediation Effects = \\(b_1 - b_4\\) = 0.3961 - 0.0396 = 0.3565 = \\(b_2 \\times b_3\\) = 0.56102 * 0.6355 = 0.3565ACME = Average Causal Mediation Effects = \\(b_1 - b_4\\) = 0.3961 - 0.0396 = 0.3565 = \\(b_2 \\times b_3\\) = 0.56102 * 0.6355 = 0.3565Using mediation package suggested (Imai, Keele, Tingley 2010) (Imai, Keele, Yamamoto 2010). details package can found here2 types Inference package:Model-based inference:\nAssumptions:\nTreatment randomized (use matching methods achieve ).\nSequential Ignorability: conditional covariates, confounders affect relationship (1) treatment-mediator, (2) treatment-outcome, (3) mediator-outcome. Typically hard argue observational data. assumption identification ACME (.e., average causal mediation effects).\n\nModel-based inference:Assumptions:\nTreatment randomized (use matching methods achieve ).\nSequential Ignorability: conditional covariates, confounders affect relationship (1) treatment-mediator, (2) treatment-outcome, (3) mediator-outcome. Typically hard argue observational data. assumption identification ACME (.e., average causal mediation effects).\nAssumptions:Treatment randomized (use matching methods achieve ).Treatment randomized (use matching methods achieve ).Sequential Ignorability: conditional covariates, confounders affect relationship (1) treatment-mediator, (2) treatment-outcome, (3) mediator-outcome. Typically hard argue observational data. assumption identification ACME (.e., average causal mediation effects).Sequential Ignorability: conditional covariates, confounders affect relationship (1) treatment-mediator, (2) treatment-outcome, (3) mediator-outcome. Typically hard argue observational data. assumption identification ACME (.e., average causal mediation effects).Design-based inferenceDesign-based inferenceNotations: stay consistent package instruction\\(M_i(t)\\) = mediator\\(M_i(t)\\) = mediator\\(T_i\\) = treatment status (0,1)\\(T_i\\) = treatment status (0,1)\\(Y_i(t,m)\\) = outcome t = treatment, m = mediating variables.\\(Y_i(t,m)\\) = outcome t = treatment, m = mediating variables.\\(X_i\\) = vector observed pre-treatment confounders\\(X_i\\) = vector observed pre-treatment confoundersTreatment effect (per unit \\(\\)) = \\(\\tau_i = Y_i(1,M_i(1)) - Y_i (0,M_i(0))\\) 2 effects\nCausal mediation effects: \\(\\delta_i (t) \\equiv Y_i (t,M_i(1)) - Y_i(t,M_i(0))\\)\nDirect effects: \\(\\zeta (t) \\equiv Y_i (1, M_i(1)) - Y_i(0, M_i(0))\\)\nsumming treatment effect: \\(\\tau_i = \\delta_i (t) + \\zeta_i (1-t)\\)\nTreatment effect (per unit \\(\\)) = \\(\\tau_i = Y_i(1,M_i(1)) - Y_i (0,M_i(0))\\) 2 effectsCausal mediation effects: \\(\\delta_i (t) \\equiv Y_i (t,M_i(1)) - Y_i(t,M_i(0))\\)Causal mediation effects: \\(\\delta_i (t) \\equiv Y_i (t,M_i(1)) - Y_i(t,M_i(0))\\)Direct effects: \\(\\zeta (t) \\equiv Y_i (1, M_i(1)) - Y_i(0, M_i(0))\\)Direct effects: \\(\\zeta (t) \\equiv Y_i (1, M_i(1)) - Y_i(0, M_i(0))\\)summing treatment effect: \\(\\tau_i = \\delta_i (t) + \\zeta_i (1-t)\\)summing treatment effect: \\(\\tau_i = \\delta_i (t) + \\zeta_i (1-t)\\)sequential ignorability\\[\n\\{ Y_i (t', m) , M_i (t) \\} \\perp T_i |X_i = x \n\\]\\[\nY_i(t',m) \\perp M_i(t) | T_i = t, X_i = x\n\\]\\(0 < P(T_i = t | X_i = x)\\)\\(0 < P(T_i = t | X_i = x)\\)\\(0 < P(M_i = m | T_i = t , X_i =x)\\)\\(0 < P(M_i = m | T_i = t , X_i =x)\\)First condition standard strong ignorability condition treatment assignment random conditional pre-treatment confounders.Second condition stronger mediators also random given observed treatment pre-treatment confounders. condition satisfied unobserved pre-treatment confounders, post-treatment confounders, multiple mediators correlated.understanding moment write note, way test sequential ignorability assumption. Hence, researchers can sensitivity analysis argue result.","code":"\nmyData <-\n    read.csv('http://static.lib.virginia.edu/statlab/materials/data/mediationData.csv')\n\n# Step 1 (no longer necessary)\nmodel.0 <- lm(Y ~ X, myData)\n\n# Step 2\nmodel.M <- lm(M ~ X, myData)\n\n# Step 3\nmodel.Y <- lm(Y ~ X + M, myData)\n\n# Step 4 (boostrapping)\nlibrary(mediation)## Warning: package 'mediation' was built under R version 4.0.5## Warning: package 'MASS' was built under R version 4.0.5## Warning: package 'Matrix' was built under R version 4.0.5## Warning: package 'mvtnorm' was built under R version 4.0.5\nresults <- mediate(\n    model.M,\n    model.Y,\n    treat = 'X',\n    mediator = 'M',\n    boot = TRUE,\n    sims = 500\n)\nsummary(results)## \n## Causal Mediation Analysis \n## \n## Nonparametric Bootstrap Confidence Intervals with the Percentile Method\n## \n##                Estimate 95% CI Lower 95% CI Upper p-value    \n## ACME             0.3565       0.2078         0.54  <2e-16 ***\n## ADE              0.0396      -0.2052         0.27    0.81    \n## Total Effect     0.3961       0.1429         0.64  <2e-16 ***\n## Prop. Mediated   0.9000       0.5181         2.10  <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Sample Size Used: 100 \n## \n## \n## Simulations: 500"},{"path":"mediation.html","id":"model-based-causal-mediation-analysis","chapter":"22 Mediation","heading":"22.2 Model-based causal mediation analysis","text":"put understanding model-based causal mediation analysis encounter design-based . Maybe future use , start reading .Fit 2 modelsmediator model: conditional distribution mediators \\(M_i | T_i, X_i\\)mediator model: conditional distribution mediators \\(M_i | T_i, X_i\\)Outcome model: conditional distribution \\(Y_i | T_i, M_i, X_i\\)Outcome model: conditional distribution \\(Y_i | T_i, M_i, X_i\\)mediation can accommodate almost types model mediator model outcome model except Censored mediator model.update estimation ACME rely product difference coefficients (see 22.1.1 ,requires strict assumption: (1) linear regression models mediator outcome, (2) \\(T_i\\) \\(M_i\\) effects additive interactionNonparametric bootstrap versionIf theoretically understanding suggests treatment mediator interactionmediation can used conjunction imputation packages.can also handle mediated moderation non-binary treatment variables, multi-level dataSensitivity Analysis sequential ignorabilitytest unobserved pre-treatment covariatestest unobserved pre-treatment covariates\\(\\rho\\) = correlation residuals mediator outcome regressions.\\(\\rho\\) = correlation residuals mediator outcome regressions.\\(\\rho\\) significant, evidence violation sequential ignorability (.e., unobserved pre-treatment confounders).\\(\\rho\\) significant, evidence violation sequential ignorability (.e., unobserved pre-treatment confounders).ACME confidence intervals contains 0 \\(\\rho \\(0.3,0.4)\\)Alternatively, using \\(R^2\\) interpretation, need specify direction confounder affects mediator outcome variables plot using sign.prod = \"positive\" (.e., direction) sign.prod = \"negative\" (.e., opposite direction).","code":"\nlibrary(mediation)\nset.seed(2014)\ndata(\"framing\", package = \"mediation\")\n\nmed.fit <-\n    lm(emo ~ treat + age + educ + gender + income, data = framing)\nout.fit <-\n    glm(\n        cong_mesg ~ emo + treat + age + educ + gender + income,\n        data = framing,\n        family = binomial(\"probit\")\n    )\n\n# Quasi-Bayesian Monte Carlo \nmed.out <-\n    mediate(\n        med.fit,\n        out.fit,\n        treat = \"treat\",\n        mediator = \"emo\",\n        robustSE = TRUE,\n        sims = 1000 # should be 10000 in practice\n    )\nsummary(med.out)## \n## Causal Mediation Analysis \n## \n## Quasi-Bayesian Confidence Intervals\n## \n##                          Estimate 95% CI Lower 95% CI Upper p-value    \n## ACME (control)             0.0826       0.0356         0.14  <2e-16 ***\n## ACME (treated)             0.0831       0.0348         0.14  <2e-16 ***\n## ADE (control)              0.0137      -0.0967         0.13    0.82    \n## ADE (treated)              0.0142      -0.1101         0.14    0.82    \n## Total Effect               0.0968      -0.0290         0.23    0.14    \n## Prop. Mediated (control)   0.7706      -6.3968         4.70    0.14    \n## Prop. Mediated (treated)   0.7938      -5.7506         4.52    0.14    \n## ACME (average)             0.0829       0.0351         0.14  <2e-16 ***\n## ADE (average)              0.0140      -0.1047         0.13    0.82    \n## Prop. Mediated (average)   0.7822      -6.0737         4.61    0.14    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Sample Size Used: 265 \n## \n## \n## Simulations: 1000\nmed.out <-\n    mediate(\n        med.fit,\n        out.fit,\n        boot = TRUE,\n        treat = \"treat\",\n        mediator = \"emo\",\n        sims = 1000, # should be 10000 in practice\n        boot.ci.type = \"bca\" # bias-corrected and accelerated intervals\n    )## Running nonparametric bootstrap\nsummary(med.out)## \n## Causal Mediation Analysis \n## \n## Nonparametric Bootstrap Confidence Intervals with the BCa Method\n## \n##                          Estimate 95% CI Lower 95% CI Upper p-value   \n## ACME (control)             0.0833       0.0386         0.15   0.002 **\n## ACME (treated)             0.0844       0.0374         0.15   0.002 **\n## ADE (control)              0.0114      -0.0875         0.13   0.792   \n## ADE (treated)              0.0125      -0.1033         0.14   0.792   \n## Total Effect               0.0958      -0.0291         0.23   0.124   \n## Prop. Mediated (control)   0.8696     -97.4552         1.00   0.126   \n## Prop. Mediated (treated)   0.8806     -82.9081         1.02   0.126   \n## ACME (average)             0.0839       0.0381         0.15   0.002 **\n## ADE (average)              0.0120      -0.0961         0.14   0.792   \n## Prop. Mediated (average)   0.8751     -90.6217         1.01   0.126   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Sample Size Used: 265 \n## \n## \n## Simulations: 1000\nmed.fit <-\n    lm(emo ~ treat + age + educ + gender + income, data = framing)\nout.fit <-\n    glm(\n        cong_mesg ~ emo * treat + age + educ + gender + income,\n        data = framing,\n        family = binomial(\"probit\")\n    )\nmed.out <-\n    mediate(\n        med.fit,\n        out.fit,\n        treat = \"treat\",\n        mediator = \"emo\",\n        robustSE = TRUE,\n        sims = 100\n    )\nsummary(med.out)## \n## Causal Mediation Analysis \n## \n## Quasi-Bayesian Confidence Intervals\n## \n##                           Estimate 95% CI Lower 95% CI Upper p-value    \n## ACME (control)            0.079925     0.035230         0.14  <2e-16 ***\n## ACME (treated)            0.097504     0.045279         0.17  <2e-16 ***\n## ADE (control)            -0.000865    -0.107228         0.11    0.98    \n## ADE (treated)             0.016714    -0.121163         0.14    0.76    \n## Total Effect              0.096640    -0.046523         0.23    0.26    \n## Prop. Mediated (control)  0.672278    -5.266859         3.40    0.26    \n## Prop. Mediated (treated)  0.860650    -6.754965         3.60    0.26    \n## ACME (average)            0.088715     0.040207         0.15  <2e-16 ***\n## ADE (average)             0.007925    -0.111833         0.14    0.88    \n## Prop. Mediated (average)  0.766464    -5.848496         3.43    0.26    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Sample Size Used: 265 \n## \n## \n## Simulations: 100\ntest.TMint(med.out, conf.level = .95) # test treatment-mediator interaction effect ## \n##  Test of ACME(1) - ACME(0) = 0\n## \n## data:  estimates from med.out\n## ACME(1) - ACME(0) = 0.017579, p-value = 0.44\n## alternative hypothesis: true ACME(1) - ACME(0) is not equal to 0\n## 95 percent confidence interval:\n##  -0.02676143  0.06257828\nplot(med.out)\nmed.fit <-\n    lm(emo ~ treat + age + educ + gender + income, data = framing)\nout.fit <-\n    glm(\n        cong_mesg ~ emo + treat + age + educ + gender + income,\n        data = framing,\n        family = binomial(\"probit\")\n    )\nmed.out <-\n    mediate(\n        med.fit,\n        out.fit,\n        treat = \"treat\",\n        mediator = \"emo\",\n        robustSE = TRUE,\n        sims = 100\n    )\nsens.out <-\n    medsens(med.out,\n            rho.by = 0.1, # \\rho varies from -0.9 to 0.9 by 0.1\n            effect.type = \"indirect\", # sensitivity on ACME\n            # effect.type = \"direct\", # sensitivity on ADE\n            # effect.type = \"both\", # sensitivity on ACME and ADE\n            sims = 100)## Warning in rho^2 * (1 - r.sq.m) * (1 - r.sq.y): Recycling array of length 1 in vector-array arithmetic is deprecated.\n##   Use c() or as.vector() instead.## Warning in err.cr.d^2 * (1 - r.sq.m) * (1 - r.sq.y): Recycling array of length 1 in vector-array arithmetic is deprecated.\n##   Use c() or as.vector() instead.\nsummary(sens.out)## \n## Mediation Sensitivity Analysis: Average Mediation Effect\n## \n## Sensitivity Region: ACME for Control Group\n## \n##      Rho ACME(control) 95% CI Lower 95% CI Upper R^2_M*R^2_Y* R^2_M~R^2_Y~\n## [1,] 0.3        0.0061      -0.0070       0.0163         0.09       0.0493\n## [2,] 0.4       -0.0081      -0.0254       0.0043         0.16       0.0877\n## \n## Rho at which ACME for Control Group = 0: 0.3\n## R^2_M*R^2_Y* at which ACME for Control Group = 0: 0.09\n## R^2_M~R^2_Y~ at which ACME for Control Group = 0: 0.0493 \n## \n## \n## Sensitivity Region: ACME for Treatment Group\n## \n##      Rho ACME(treated) 95% CI Lower 95% CI Upper R^2_M*R^2_Y* R^2_M~R^2_Y~\n## [1,] 0.3        0.0069      -0.0085       0.0197         0.09       0.0493\n## [2,] 0.4       -0.0099      -0.0304       0.0054         0.16       0.0877\n## \n## Rho at which ACME for Treatment Group = 0: 0.3\n## R^2_M*R^2_Y* at which ACME for Treatment Group = 0: 0.09\n## R^2_M~R^2_Y~ at which ACME for Treatment Group = 0: 0.0493\nplot(sens.out, sens.par = \"rho\", main = \"Anxiety\", ylim = c(-0.2, 0.2))\nplot(sens.out, sens.par = \"R2\", r.type = \"total\", sign.prod = \"positive\")## Warning in (1 - x$r.square.y) * seq(0, 1 - x$rho.by, 0.01): Recycling array of length 1 in array-vector arithmetic is deprecated.\n##   Use c() or as.vector() instead."},{"path":"report.html","id":"report","chapter":"23 Report","heading":"23 Report","text":"StructureExploratory analysis\nplots\npreliminary results\ninteresting structure/features data\noutliers\nExploratory analysisplotspreliminary resultsinteresting structure/features dataoutliersModel\nAssumptions\nmodel/ model best one?\nConsideration: interactions, collinearity, dependence\nModelAssumptionsWhy model/ model best one?Consideration: interactions, collinearity, dependenceModel Fit\nwell fit?\nmodel assumptions met?\nResidual analysis\n\nModel FitHow well fit?well fit?model assumptions met?\nResidual analysis\nmodel assumptions met?Residual analysisInference/ Prediction\ndifferent way support inference?\nInference/ PredictionAre different way support inference?Conclusion\nRecommendation\nLimitation analysis\ncorrect future\nConclusionRecommendationRecommendationLimitation analysisLimitation analysisHow correct futureHow correct futureThis chapter based jtools package. information can found .","code":""},{"path":"report.html","id":"one-summary-table","chapter":"23 Report","heading":"23.1 One summary table","text":"Packages reporting:Summary Statistics Table:qwraps2vtablegtsummaryapaTablesstargazerRegression TablegtsummarysjPlot,sjmisc, sjlabelledstargazer: recommended (Example)modelsummaryModel Equation\\[\n\\operatorname{metascore} = \\alpha + \\beta_{1}(\\operatorname{budget}) + \\beta_{2}(\\operatorname{us\\_gross}) + \\beta_{3}(\\operatorname{year}) + \\epsilon\n\\]\\[\n\\operatorname{\\widehat{metascore}} = 52.06 + 0(\\operatorname{budget}) + 0(\\operatorname{us\\_gross}) + 0.01(\\operatorname{year})\n\\]","code":"\nlibrary(jtools)## Warning: package 'jtools' was built under R version 4.0.5\ndata(movies)\nfit <- lm(metascore ~ budget + us_gross + year, data = movies)\nsumm(fit)\nsumm(fit, scale = TRUE, vifs = TRUE, part.corr = TRUE, confint = TRUE, pvals = FALSE) #notice that scale here is TRUE\n#obtain clsuter-robust SE\ndata(\"PetersenCL\", package = \"sandwich\")\nfit2 <- lm(y ~ x, data = PetersenCL)\nsumm(fit2, robust = \"HC3\", cluster = \"firm\") \n# install.packages(\"equatiomatic\")\nfit <- lm(metascore ~ budget + us_gross + year, data = movies)\n# show the theoretical model\nequatiomatic::extract_eq(fit)## Registered S3 methods overwritten by 'broom':\n##   method            from  \n##   tidy.glht         jtools\n##   tidy.summary.glht jtools\n# display the actual coefficients\nequatiomatic::extract_eq(fit, use_coefs = TRUE)"},{"path":"report.html","id":"model-comparison","chapter":"23 Report","heading":"23.2 Model Comparison","text":"Table 23.1:  Another package stargazerCorrelation Table","code":"\nfit <- lm(metascore ~ log(budget), data = movies)\nfit_b <- lm(metascore ~ log(budget) + log(us_gross), data = movies)\nfit_c <- lm(metascore ~ log(budget) + log(us_gross) + runtime, data = movies)\ncoef_names <- c(\"Budget\" = \"log(budget)\", \"US Gross\" = \"log(us_gross)\",\n                \"Runtime (Hours)\" = \"runtime\", \"Constant\" = \"(Intercept)\")\nexport_summs(fit, fit_b, fit_c, robust = \"HC3\", coefs = coef_names)\nlibrary(\"stargazer\")## \n## Please cite as:##  Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables.##  R package version 5.2.2. https://CRAN.R-project.org/package=stargazer\nstargazer(attitude)## \n## % Table created by stargazer v.5.2.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu\n## % Date and time: Mon, Nov 22, 2021 - 2:40:16 PM\n## \\begin{table}[!htbp] \\centering \n##   \\caption{} \n##   \\label{} \n## \\begin{tabular}{@{\\extracolsep{5pt}}lccccccc} \n## \\\\[-1.8ex]\\hline \n## \\hline \\\\[-1.8ex] \n## Statistic & \\multicolumn{1}{c}{N} & \\multicolumn{1}{c}{Mean} & \\multicolumn{1}{c}{St. Dev.} & \\multicolumn{1}{c}{Min} & \\multicolumn{1}{c}{Pctl(25)} & \\multicolumn{1}{c}{Pctl(75)} & \\multicolumn{1}{c}{Max} \\\\ \n## \\hline \\\\[-1.8ex] \n## rating & 30 & 64.633 & 12.173 & 40 & 58.8 & 71.8 & 85 \\\\ \n## complaints & 30 & 66.600 & 13.315 & 37 & 58.5 & 77 & 90 \\\\ \n## privileges & 30 & 53.133 & 12.235 & 30 & 45 & 62.5 & 83 \\\\ \n## learning & 30 & 56.367 & 11.737 & 34 & 47 & 66.8 & 75 \\\\ \n## raises & 30 & 64.633 & 10.397 & 43 & 58.2 & 71 & 88 \\\\ \n## critical & 30 & 74.767 & 9.895 & 49 & 69.2 & 80 & 92 \\\\ \n## advance & 30 & 42.933 & 10.289 & 25 & 35 & 47.8 & 72 \\\\ \n## \\hline \\\\[-1.8ex] \n## \\end{tabular} \n## \\end{table}\n## 2 OLS models\nlinear.1 <- lm(rating ~ complaints + privileges + learning + raises + critical,data = attitude)\nlinear.2 <- lm(rating ~ complaints + privileges + learning, data = attitude)\n## create an indicator dependent variable, and run a probit model\nattitude$high.rating <- (attitude$rating > 70)\nprobit.model <-\n    glm(\n        high.rating ~ learning + critical + advance,\n        data = attitude,\n        family = binomial(link = \"probit\")\n    )\nstargazer(linear.1,\n          linear.2,\n          probit.model,\n          title = \"Results\",\n          align = TRUE)## \n## % Table created by stargazer v.5.2.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu\n## % Date and time: Mon, Nov 22, 2021 - 2:40:16 PM\n## % Requires LaTeX packages: dcolumn \n## \\begin{table}[!htbp] \\centering \n##   \\caption{Results} \n##   \\label{} \n## \\begin{tabular}{@{\\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } \n## \\\\[-1.8ex]\\hline \n## \\hline \\\\[-1.8ex] \n##  & \\multicolumn{3}{c}{\\textit{Dependent variable:}} \\\\ \n## \\cline{2-4} \n## \\\\[-1.8ex] & \\multicolumn{2}{c}{rating} & \\multicolumn{1}{c}{high.rating} \\\\ \n## \\\\[-1.8ex] & \\multicolumn{2}{c}{\\textit{OLS}} & \\multicolumn{1}{c}{\\textit{probit}} \\\\ \n## \\\\[-1.8ex] & \\multicolumn{1}{c}{(1)} & \\multicolumn{1}{c}{(2)} & \\multicolumn{1}{c}{(3)}\\\\ \n## \\hline \\\\[-1.8ex] \n##  complaints & 0.692^{***} & 0.682^{***} &  \\\\ \n##   & (0.149) & (0.129) &  \\\\ \n##   & & & \\\\ \n##  privileges & -0.104 & -0.103 &  \\\\ \n##   & (0.135) & (0.129) &  \\\\ \n##   & & & \\\\ \n##  learning & 0.249 & 0.238^{*} & 0.164^{***} \\\\ \n##   & (0.160) & (0.139) & (0.053) \\\\ \n##   & & & \\\\ \n##  raises & -0.033 &  &  \\\\ \n##   & (0.202) &  &  \\\\ \n##   & & & \\\\ \n##  critical & 0.015 &  & -0.001 \\\\ \n##   & (0.147) &  & (0.044) \\\\ \n##   & & & \\\\ \n##  advance &  &  & -0.062 \\\\ \n##   &  &  & (0.042) \\\\ \n##   & & & \\\\ \n##  Constant & 11.011 & 11.258 & -7.476^{**} \\\\ \n##   & (11.704) & (7.318) & (3.570) \\\\ \n##   & & & \\\\ \n## \\hline \\\\[-1.8ex] \n## Observations & \\multicolumn{1}{c}{30} & \\multicolumn{1}{c}{30} & \\multicolumn{1}{c}{30} \\\\ \n## R$^{2}$ & \\multicolumn{1}{c}{0.715} & \\multicolumn{1}{c}{0.715} &  \\\\ \n## Adjusted R$^{2}$ & \\multicolumn{1}{c}{0.656} & \\multicolumn{1}{c}{0.682} &  \\\\ \n## Log Likelihood &  &  & \\multicolumn{1}{c}{-9.087} \\\\ \n## Akaike Inf. Crit. &  &  & \\multicolumn{1}{c}{26.175} \\\\ \n## Residual Std. Error & \\multicolumn{1}{c}{7.139 (df = 24)} & \\multicolumn{1}{c}{6.863 (df = 26)} &  \\\\ \n## F Statistic & \\multicolumn{1}{c}{12.063$^{***}$ (df = 5; 24)} & \\multicolumn{1}{c}{21.743$^{***}$ (df = 3; 26)} &  \\\\ \n## \\hline \n## \\hline \\\\[-1.8ex] \n## \\textit{Note:}  & \\multicolumn{3}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\\\ \n## \\end{tabular} \n## \\end{table}\n# Latex\nstargazer(\n    linear.1,\n    linear.2,\n    probit.model,\n    title = \"Regression Results\",\n    align = TRUE,\n    dep.var.labels = c(\"Overall Rating\", \"High Rating\"),\n    covariate.labels = c(\n        \"Handling of Complaints\",\n        \"No Special Privileges\",\n        \"Opportunity to Learn\",\n        \"Performance-Based Raises\",\n        \"Too Critical\",\n        \"Advancement\"\n    ),\n    omit.stat = c(\"LL\", \"ser\", \"f\"),\n    no.space = TRUE\n)\n# ASCII text output\nstargazer(\n    linear.1,\n    linear.2,\n    type = \"text\",\n    title = \"Regression Results\",\n    dep.var.labels = c(\"Overall Rating\", \"High Rating\"),\n    covariate.labels = c(\n        \"Handling of Complaints\",\n        \"No Special Privileges\",\n        \"Opportunity to Learn\",\n        \"Performance-Based Raises\",\n        \"Too Critical\",\n        \"Advancement\"\n    ),\n    omit.stat = c(\"LL\", \"ser\", \"f\"),\n    ci = TRUE,\n    ci.level = 0.90,\n    single.row = TRUE\n)## \n## Regression Results\n## ========================================================================\n##                                        Dependent variable:              \n##                          -----------------------------------------------\n##                                          Overall Rating                 \n##                                    (1)                     (2)          \n## ------------------------------------------------------------------------\n## Handling of Complaints   0.692*** (0.447, 0.937) 0.682*** (0.470, 0.894)\n## No Special Privileges    -0.104 (-0.325, 0.118)  -0.103 (-0.316, 0.109) \n## Opportunity to Learn      0.249 (-0.013, 0.512)   0.238* (0.009, 0.467) \n## Performance-Based Raises -0.033 (-0.366, 0.299)                         \n## Too Critical              0.015 (-0.227, 0.258)                         \n## Advancement              11.011 (-8.240, 30.262) 11.258 (-0.779, 23.296)\n## ------------------------------------------------------------------------\n## Observations                       30                      30           \n## R2                                0.715                   0.715         \n## Adjusted R2                       0.656                   0.682         \n## ========================================================================\n## Note:                                        *p<0.1; **p<0.05; ***p<0.01\nstargazer(\n    linear.1,\n    linear.2,\n    probit.model,\n    title = \"Regression Results\",\n    align = TRUE,\n    dep.var.labels = c(\"Overall Rating\", \"High Rating\"),\n    covariate.labels = c(\n        \"Handling of Complaints\",\n        \"No Special Privileges\",\n        \"Opportunity to Learn\",\n        \"Performance-Based Raises\",\n        \"Too Critical\",\n        \"Advancement\"\n    ),\n    omit.stat = c(\"LL\", \"ser\", \"f\"),\n    no.space = TRUE\n)\ncorrelation.matrix <- cor(attitude[,c(\"rating\",\"complaints\",\"privileges\")])\nstargazer(correlation.matrix, title=\"Correlation Matrix\")"},{"path":"report.html","id":"changes-in-an-estimate","chapter":"23 Report","heading":"23.3 Changes in an estimate","text":"","code":"\ncoef_names <- coef_names[1:3] # Dropping intercept for plots\nplot_summs(fit, fit_b, fit_c, robust = \"HC3\", coefs = coef_names)\nplot_summs(fit_c, robust = \"HC3\", coefs = coef_names, plot.distributions = TRUE)"},{"path":"appendix.html","id":"appendix","chapter":"A Appendix","heading":"A Appendix","text":"","code":""},{"path":"appendix.html","id":"git","chapter":"A Appendix","heading":"A.1 Git","text":"Cheat SheetCheat Sheet different languagesLearn GitInteractive Cheat SheetUltimate Guide Git GitHub R userSetting Git: git config --global option configure user name, email, editor, etc.Setting Git: git config --global option configure user name, email, editor, etc.Creating repository: git init initialize repo. Git stores repo data .git directory.Creating repository: git init initialize repo. Git stores repo data .git directory.Tracking changes:\ngit status shows status repo\nFile stored project’s working directory (users see)\nstaging area (next commit built)\nlocal repo commits permanently recorded\n\ngit add put files staging area\ngit commit saves staged content new commit local repo.\ngit commit -m \"message\" give messages purpose commit.\n\nTracking changes:git status shows status repo\nFile stored project’s working directory (users see)\nstaging area (next commit built)\nlocal repo commits permanently recorded\ngit status shows status repoFile stored project’s working directory (users see)File stored project’s working directory (users see)staging area (next commit built)staging area (next commit built)local repo commits permanently recordedlocal repo commits permanently recordedgit add put files staging areagit add put files staging areagit commit saves staged content new commit local repo.\ngit commit -m \"message\" give messages purpose commit.\ngit commit saves staged content new commit local repo.git commit -m \"message\" give messages purpose commit.History\ngit diff shows differences commits\ngit checkout recovers old version fields\ngit checkout HEAD go last commit\ngit checkout <unique ID commit> go commit\n\nHistorygit diff shows differences commitsgit diff shows differences commitsgit checkout recovers old version fields\ngit checkout HEAD go last commit\ngit checkout <unique ID commit> go commit\ngit checkout recovers old version fieldsgit checkout HEAD go last commitgit checkout HEAD go last commitgit checkout <unique ID commit> go commitgit checkout <unique ID commit> go commitIgnoring\n.gitignore file tells Git files ignore\ncat . gitignore *.dat results/ ignore files ending “dat” folder “results.”\nIgnoring.gitignore file tells Git files ignore.gitignore file tells Git files ignorecat . gitignore *.dat results/ ignore files ending “dat” folder “results.”cat . gitignore *.dat results/ ignore files ending “dat” folder “results.”Remotes GitHub\nlocal git repo can connected one remote repos.\nUse HTTPS protocol connect remote repos\ngit push copies changes local repo remote repo\ngit pull copies changes remote repo local repo\nRemotes GitHubA local git repo can connected one remote repos.local git repo can connected one remote repos.Use HTTPS protocol connect remote reposUse HTTPS protocol connect remote reposgit push copies changes local repo remote repogit push copies changes local repo remote repogit pull copies changes remote repo local repogit pull copies changes remote repo local repoCollaborating\ngit clone copies remote repo create local repo remote called origin automatically set \nCollaboratinggit clone copies remote repo create local repo remote called origin automatically set upBranching\ngit check - b <new-branch-name\ngit checkout master switch master branch.\nBranchinggit check - b <new-branch-namegit check - b <new-branch-namegit checkout master switch master branch.git checkout master switch master branch.Conflicts\noccur 2 people change lines file\nversion control system allow overwrite ’s changes blindly, highlights conflicts can resolved.\nConflictsoccur 2 people change lines fileoccur 2 people change lines filethe version control system allow overwrite ’s changes blindly, highlights conflicts can resolved.version control system allow overwrite ’s changes blindly, highlights conflicts can resolved.Licensing\nPeople incorporate General Public License (GPL’d) software won software must make software also open GPL license; open licenses require .\nCreative Commons family licenses allow people mix match requirements restrictions attribution, creation derivative works, sharing commercialization.\nLicensingPeople incorporate General Public License (GPL’d) software won software must make software also open GPL license; open licenses require .People incorporate General Public License (GPL’d) software won software must make software also open GPL license; open licenses require .Creative Commons family licenses allow people mix match requirements restrictions attribution, creation derivative works, sharing commercialization.Creative Commons family licenses allow people mix match requirements restrictions attribution, creation derivative works, sharing commercialization.Citation:\nAdd CITATION file repo explain want others cite work.\nCitation:Add CITATION file repo explain want others cite work.Hosting\nRules regarding intellectual property storage sensitive info apply matter code data hosted.\nHostingRules regarding intellectual property storage sensitive info apply matter code data hosted.","code":""},{"path":"appendix.html","id":"short-cut","chapter":"A Appendix","heading":"A.2 Short-cut","text":"shortcuts probably remember working R. Even though might take bit time learn use second nature, save lot time.\nJust like learning another language, speak practice , comfortable speaking . Sometimes can’t stage folder ’s large. case, use Terminal pane Rstudio type git add -stage changes commit push like usual.","code":""},{"path":"appendix.html","id":"function-short-cut","chapter":"A Appendix","heading":"A.3 Function short-cut","text":"apply one function data create new variable: mutate(mod=map(data,function))\ninstead using 1:length(object): (seq_along(object))\napply multiple function: map_dbl\napply multiple function multiple variables:map2autoplot(data) plot times series datamod_tidy = linear(reg) %>% set_engine('lm') %>% fit(price ~ ., data=data) fit lm model. also fit models (stan, spark, glmnet, keras)Sometimes, data-masking able recognize whether ’re calling environment data variables. bypass , use .data$variable .env$variable. example data %>% mutate(x=.env$variable/.data$variableProblems data-masking:Unexpected masking data-var: Use .data .env disambiguate\nData-var cant get :\nTunnel data-var {{}} + Subset .data [[]]\nUnexpected masking data-var: Use .data .env disambiguateData-var cant get :Tunnel data-var {{}} + Subset .data [[]]Passing Data-variables argumentsTrouble selection:","code":"\nlibrary(\"dplyr\")\nmean_by <- function(data,by,var){\n    data %>%\n        group_by({{{by}}}) %>%\n        summarise(\"{{var}}\":=mean({{var}})) # new name for each var will be created by tunnel data-var inside strings\n}\n\nmean_by <- function(data,by,var){\n    data %>%\n        group_by({{{by}}}) %>%\n        summarise(\"{var}\":=mean({{var}})) # use single {} to glue the string, but hard to reuse code in functions\n}\nlibrary(\"purrr\")\nname <- c(\"mass\",\"height\")\nstarwars %>% select(name) # Data-var. Here you are referring to variable named \"name\"\n\nstarwars %>% select(all_of((name))) # use all_of() to disambiguate when \n\naverages <- function(data,vars){ # take character vectors with all_of()\n    data %>%\n        select(all_of(vars)) %>%\n        map_dbl(mean,na.rm=TRUE)\n} \n\nx = c(\"Sepal.Length\",\"Petal.Length\")\niris %>% averages(x)\n\n\n# Another way\naverages <- function(data,vars){ # Tunnel selectiosn with {{}}\n    data %>%\n        select({{vars}}) %>%\n        map_dbl(mean,na.rm=TRUE)\n} \n\nx = c(\"Sepal.Length\",\"Petal.Length\")\niris %>% averages(x)"},{"path":"appendix.html","id":"citation","chapter":"A Appendix","heading":"A.4 Citation","text":"include citation [@Farjam_2015]cite packages used sessionpackage=ls(sessionInfo()$loadedOnly) (package){print(toBibtex(citation()))}","code":"\npackage=ls(sessionInfo()$loadedOnly) \nfor (i in package){\n    print(toBibtex(citation(i)))\n    }"},{"path":"bookdown-cheat-sheet.html","id":"bookdown-cheat-sheet","chapter":"B Bookdown cheat sheet","heading":"B Bookdown cheat sheet","text":"","code":"\n# to see non-scientific notation a result\nformat(12e-17, scientific = FALSE)## [1] \"0.00000000000000012\""},{"path":"bookdown-cheat-sheet.html","id":"operation","chapter":"B Bookdown cheat sheet","heading":"B.1 Operation","text":"R commands derivatives defined function Taking derivatives R involves using expression, D, eval functions. wrap function want take derivative expression(), use D, eval follows.simple exampleEvaluate\n* first argument passed eval expression want evaluate * second list containing values quantities defined elsewhere.","code":"\n#define a function\nf=expression(sqrt(x))\n\n#take the first derivative\ndf.dx=D(f,'x')\ndf.dx## 0.5 * x^-0.5\n#take the second derivative\nd2f.dx2=D(D(f,'x'),'x')\nd2f.dx2## 0.5 * (-0.5 * x^-1.5)\n#evaluate the function at a given x\neval(f,list(x=3))## [1] 1.732051\n#evaluate the first derivative at a given x\neval(df.dx,list(x=3))## [1] 0.2886751\n#evaluate the second derivative at a given x\neval(d2f.dx2,list(x=3))## [1] -0.04811252"},{"path":"bookdown-cheat-sheet.html","id":"math-expresssion-syntax","chapter":"B Bookdown cheat sheet","heading":"B.2 Math Expresssion/ Syntax","text":"Full listAligning equations\\[\n\\begin{aligned}\n& = b \\\\\nX &\\sim {\\sf Norm}(10, 3) \\\\\n5 & \\le 10\n\\end{aligned}\n\\]\nCross-reference equation\\[\\begin{equation}\n\\tag{B.1}\n=b\n\\end{equation}\\]refer sentence (B.1) (\\@ref(eq:1))Limit P(\\lim_{n\\\\infty}\\bar{X}_n =\\mu) =1\\[\nP(\\lim_{n\\\\infty}\\bar{X}_n =\\mu) =1\n\\]Matrices\\[\n\\begin{array}\n{rrr}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{array}\n\\]\\[\n\\mathbf{X} = \\left[\\begin{array}\n{rrr}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{array}\\right]\n\\]Aligning EquationsAligning Equations Comments\\[\n\\begin{aligned}\n    3+x &=4 && \\text{(Solve } x \\text{.)}\\\\\n    x &=4-3 && \\text{(Subtract 3 sides.)}\\\\\n    x &=1   && \\text{(Yielding solution.)}\n\\end{aligned}\n\\]","code":"\\begin{aligned}\na & = b \\\\\nX &\\sim {\\sf Norm}(10, 3) \\\\\n5 & \\le 10\n\\end{aligned}\\begin{equation}\n(\\#eq:1)\na=b\n\\end{equation}$$\\begin{array}\n{rrr}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{array}\n$$$$\\mathbf{X} = \\left[\\begin{array}\n{rrr}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{array}\\right]\n$$\\begin{aligned}\n    3+x &=4 && \\text{(Solve for} x \\text{.)}\\\\\n    x &=4-3 && \\text{(Subtract 3 from both sides.)}\\\\\n    x &=1   && \\text{(Yielding the solution.)}\n\\end{aligned}"},{"path":"bookdown-cheat-sheet.html","id":"statistics-notation","chapter":"B Bookdown cheat sheet","heading":"B.2.1 Statistics Notation","text":"\\[\nf(y|N,p) = \\frac{N!}{y!(N-y)!}\\cdot p^y \\cdot (1-p)^{N-y} = {{N}\\choose{y}} \\cdot p^y \\cdot (1-p)^{N-y}\n\\]\\[\n\\begin{cases}\n\\frac{1}{b-} & \\text{} x\\[,b]\\\\\n0 & \\text{otherwise}\\\\\n\\end{cases}\n\\]","code":"$$\nf(y|N,p) = \\frac{N!}{y!(N-y)!}\\cdot p^y \\cdot (1-p)^{N-y} = {{N}\\choose{y}} \\cdot p^y \\cdot (1-p)^{N-y}\n$$\\begin{cases}\n\\frac{1}{b-a}&\\text{for $x\\in[a,b]$}\\\\\n0&\\text{otherwise}\\\\\n\\end{cases}"},{"path":"bookdown-cheat-sheet.html","id":"table","chapter":"B Bookdown cheat sheet","heading":"B.3 Table","text":"built-wrapperbright colorcures scurvytasty\\((\\mathbf{x}^T\\mathbf{x})^{-1}\\mathbf{x}^T\\mathbf{y}\\)","code":"+---------------+---------------+--------------------+\n| Fruit         | Price         | Advantages         |\n+===============+===============+====================+\n| *Bananas*     | $1.34         | - built-in wrapper |\n|               |               | - bright color     |\n+---------------+---------------+--------------------+\n| Oranges       | $2.10         | - cures scurvy     |\n|               |               | - **tasty**        |\n+---------------+---------------+--------------------+(\\mathbf{x}^T\\mathbf{x})^{-1}\\mathbf{x}^T\\mathbf{y}"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
