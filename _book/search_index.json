[["index.html", "A Guide on Data Analysis Chapter 1 Prerequisites", " A Guide on Data Analysis Mike Nguyen 2020-11-29 Chapter 1 Prerequisites This chapter is just a quick review of Matrix Theory and Probability Theory If you feel you do not need to brush up on these theories, you can jump right into Introduction "],["general-math.html", "1.1 General Math", " 1.1 General Math Maclaurin series expansion for \\[ e^z = 1 + z + \\frac{z^2}{2!} + \\frac{z^3}{3!} + ... \\] Geometric series: \\[ s_n=\\sum_{k=1}^{n}ar^{n-1}=\\frac{a(1-r^n)}{1-r} \\] if |r| &lt; 1 \\[ s=\\sum_{k=1}^{\\infty}ar^{n-1}=\\frac{a}{1-r} \\] "],["matrix-theory.html", "1.2 Matrix Theory", " 1.2 Matrix Theory \\[\\begin{equation} \\begin{split} A= \\left[\\begin{array}{c} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\\\ \\end{array} \\right] \\end{split} \\end{equation}\\] \\[\\begin{equation} \\begin{split} A&#39; = \\left[\\begin{array}{c} a_{11} &amp; a_{21} \\\\ a_{12} &amp; a_{22} \\\\ \\end{array} \\right] \\end{split} \\end{equation}\\] \\[ \\mathbf{(ABC)&#39;=C&#39;B&#39;A&#39;} \\] \\[\\begin{equation} \\begin{split} \\mathbf{A} &amp;= \\left(\\begin{array}{cccc} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ \\end{array}\\right) \\left(\\begin{array}{c} b_{11} &amp; b_{12} &amp; b_{13} \\\\ b_{21} &amp; b_{22} &amp; b_{23} \\\\ b_{31} &amp; b_{32} &amp; b_{33} \\\\ \\end{array}\\right) \\\\ &amp;= \\left(\\begin{array}{c} a_{11}b_{11}+a_{12}b_{21}+a_{13}b_{31} &amp; \\sum_{i=1}^{3}a_{1i}b_{i2} &amp; \\sum_{i=1}^{3}a_{1i}b_{i3} \\\\ \\sum_{i=1}^{3}a_{2i}b_{i1} &amp; \\sum_{i=1}^{3}a_{2i}b_{i2} &amp; \\sum_{i=1}^{3}a_{2i}b_{i3} \\\\ \\end{array}\\right) \\end{split} \\end{equation}\\] Let \\(\\mathbf{a}\\) be a 3 x 1 vector, then the quadratic form is \\[ \\mathbf{a&#39;Ba} = \\sum_{i=1}^{3}\\sum_{i=1}^{3}a_i b_{ij} a_{j} \\] 1.2.1 Rank Dimension of space spanned by its columns (or its rows). Number of linearly indepdent columns/rows For a n x k matrix A and k x k matrix B \\(rank(A)\\leq min(n,k)\\) \\(rank(A) = rank(A&#39;) = rank(A&#39;A)=rank(AA&#39;)\\) \\(rank(AB)=min(rank(A),rank(B))\\) B is invertible if and only if rank(B) = k (non-singular) 1.2.2 Inverse A non-singular square matrix A is invertible if there exists a non-singular square matrix B such that, \\[AB=I\\] Then \\(A^{-1}=B\\). For a 2x2 matrix, \\[ A = \\left(\\begin{array}{c} a &amp; b \\\\ c &amp; d \\\\ \\end{array} \\right) \\] \\[ A^{-1}= \\frac{1}{ad-bc} \\left(\\begin{array}{c} d &amp; -b \\\\ -c &amp; a \\\\ \\end{array} \\right) \\] For the partition matrix, \\[\\begin{equation} \\begin{split} \\left[\\begin{array}{c} A &amp; B \\\\ C &amp; D \\\\ \\end{array} \\right]^{-1} = \\left[\\begin{array}{c} \\mathbf{(A-BD^{-1}C)^{-1}} &amp; \\mathbf{-(A-BD^{-1}C)^{-1}BD^-1}\\\\ \\mathbf{-DC(A-BD^{-1}C)^{-1}} &amp; \\mathbf{D^{-1}+D^{-1}C(A-BD^{-1}C)^{-1}BD^{-1}}\\ \\\\ \\end{array} \\right] \\end{split} \\end{equation}\\] Properties for a non-singular square matrix \\(\\mathbf{A^{-1}}=A\\) for a non-zero scalar b, \\(\\mathbf{(bA)^{-1}=b^{-1}A^{-1}}\\) for a matrix B, \\(\\mathbf(BA)^{-1}=B^{-1}A^{-1}\\) only if B is non-singular \\(\\mathbf{(A^{-1})&#39;=(A&#39;)^{-1}}\\) Never notate \\(\\mathbf{1/A}\\) 1.2.3 Definiteness A symmetric square k x k matrix, \\(\\mathbf{A}\\), is Positive Semi-Definite if for any non-zero k x 1 vector \\(\\mathbf{x}\\), \\[\\mathbf{x&#39;Ax \\geq 0 }\\] A symmetric square k x k matrix, \\(\\mathbf{A}\\), is Negative Semi-Definite if for any non-zero k x 1 vector \\(\\mathbf{x}\\) \\[\\mathbf{x&#39;Ax \\leq 0 }\\] \\(\\mathbf{A}\\) is indefinite if it is neither positive semi-definite or negative semi-definite. The identity matrix is positive definite Example Let \\(\\mathbf{x} =(x_1 x_2)&#39;\\), then for a 2 x 2 identity matrix, \\[\\begin{equation} \\begin{split} \\mathbf{x&#39;Ix} &amp;= (x_1 x_2) \\left(\\begin{array}{c} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{array} \\right) \\left(\\begin{array}{c} x_1 \\\\ x_2 \\\\ \\end{array} \\right) \\\\ &amp;= (x_1 x_2) \\left(\\begin{array}{c} x_1 \\\\ x_2 \\\\ \\end{array} \\right) \\\\ &amp;= x_1^2 + x_2^2 &gt;0 \\end{split} \\end{equation}\\] Definiteness gives us the ability to compare matrices \\(\\mathbf{A-B}\\) is PSD This property also helps us show efficiency (which variance covariance matrix of one estimator is smaller than another) Properties any variance matrix is PSD a matrix \\(\\mathbf{A}\\) is PSD if and only if there exists a matrix \\(\\mathbf{B}\\) such that \\(\\mathbf{A=B&#39;B}\\) if \\(\\mathbf{A}\\) is PSD, then \\(\\mathbf{B&#39;AB}\\) is PSD if A and C are non-singular, then A-C is PSD if and only if \\(\\mathbf{C^{-1}-A^{-1}}\\) if A is PD (ND) then \\(A^{-1}\\) is PD (ND) 1.2.4 Matrix Calculus \\(y=f(x_1,x_2,...,x_k)=f(x)\\) where x is a 1 x k row vector. The Gradient (first order derivative with respect to a vector) is, \\[ \\frac{\\partial{f(x)}}{\\partial{x}}= \\left(\\begin{array}{c} \\frac{\\partial{f(x)}}{\\partial{x_1}} \\\\ \\frac{\\partial{f(x)}}{\\partial{x_2}} \\\\ ... \\\\ \\frac{\\partial{f(x)}}{\\partial{x_k}} \\end{array} \\right) \\] The Hessian (second order derivative with respect to a vector) is, \\[ \\frac{\\partial^2{f(x)}}{\\partial{x}\\partial{x&#39;}}= \\left(\\begin{array}{c} \\frac{\\partial^2{f(x)}}{\\partial{x_1}\\partial{x_1}} &amp; \\frac{\\partial^2{f(x)}}{\\partial{x_1}\\partial{x_2}} &amp; ... &amp; \\frac{\\partial^2{f(x)}}{\\partial{x_1}\\partial{x_k}} \\\\ \\frac{\\partial^2{f(x)}}{\\partial{x_1}\\partial{x_2}} &amp; \\frac{\\partial^2{f(x)}}{\\partial{x_2}\\partial{x_2}} &amp; ... &amp; \\frac{\\partial^2{f(x)}}{\\partial{x_2}\\partial{x_k}} \\\\ ... &amp; ...&amp; &amp; ...\\\\ \\frac{\\partial^2{f(x)}}{\\partial{x_k}\\partial{x_1}} &amp; \\frac{\\partial^2{f(x)}}{\\partial{x_k}\\partial{x_2}} &amp; ... &amp; \\frac{\\partial^2{f(x)}}{\\partial{x_k}\\partial{x_k}} \\end{array} \\right) \\] 1.2.5 Optimization Scalar Optimization Vector Optimization First Order Condition \\[\\frac{\\partial{f(x_0)}}{\\partial{x}}=0\\] \\[\\frac{\\partial{f(x_0)}}{\\partial{x}}=\\left(\\begin{array}{c}0 \\\\ .\\\\ .\\\\ .\\\\ 0\\end{array}\\right)\\] Second Order Condition Convex \\(\\rightarrow\\) Min \\[\\frac{\\partial^2{f(x_0)}}{\\partial{x^2}} &gt; 0\\] \\[\\frac{\\partial^2{f(x_0)}}{\\partial{xx&#39;}}&gt;0\\] Concave \\(\\rightarrow\\) Max \\[\\frac{\\partial^2{f(x_0)}}{\\partial{x^2}} &lt; 0\\] \\[\\frac{\\partial^2{f(x_0)}}{\\partial{xx&#39;}}&lt;0\\] "],["probability-theory.html", "1.3 Probability Theory", " 1.3 Probability Theory 1.3.1 Axiom and Theorems of Probability Let S denote a sample space of an experiment P[S]=1 \\(P[A] \\ge 0\\) for every event A Let \\(A_1,A_2,A_3,...\\) be a finite or an infinite collection of mutually exclusive events. Then \\(P[A_1\\cup A_2 \\cup A_3 ...]=P[A_1]+P[A_2]+P[A_3]+...\\) \\(P[\\emptyset]=0\\) \\(P[A&#39;]=1-P[A]\\) \\(P[A_1 \\cup A_2] = P[A_1] + P[A_2] - P[A_1 \\cap A_2]\\) Conditional Probability \\[ P[A|B]=\\frac{A \\cap B}{P[B]} \\] Independent Events Two events A and B are independent if and only if: \\(P[A\\cap B]=P[A]P[B]\\) \\(P[A|B]=P[A]\\) \\(P[B|A]=P[B]\\) A finite collection of events \\(A_1, A_2, ..., A_n\\) is independent if and only if any subcollection is independent. Multiplication Rule \\(P[A \\cap B] = P[A|B]P[B] = P[B|A]P[A]\\) Bayes Theorem Let \\(A_1, A_2, ..., A_n\\) be a collection of mutually exclusive events whose union is S. Let b be an event such that \\(P[B]\\neq0\\) Then for any of the events \\(A_j\\), j = 1,2,,n \\[ P[A_|B]=\\frac{P[B|A_j]P[A_j]}{\\sum_{i=1}^{n}P[B|A_j]P[A_i]} \\] Jensens Inequality If g(x) is convex \\(E(g(X)) \\ge g(E(X))\\) If g(x) is concave \\(E(g(X)) \\le g(E(X))\\) 1.3.1.1 Law of Iterated Expectations \\(E(Y)=E(E(Y|X))\\) 1.3.1.2 Correlation and Independence Independence \\(f(x,y)=f_X(x)f_Y(y)\\) \\(f_{Y|X}(y|x)=f_Y(y)\\) and \\(f_{X|Y}(x|y)=f_X(x)\\) \\(E(g_1(X)g_2(Y))=E(g_1(X))E(g_2(Y))\\) Mean Independence (implied by independence) Y is mean independent of X if and only if \\(E(Y|X)=E(Y)\\) \\(E(Xg(Y))=E(X)E(g(Y))\\) Uncorrelated (implied by independence and mean independence) \\(Cov(X,Y)=0\\) \\(Var(X+Y)=Var(X) + Var(Y)\\) \\(E(XY)=E(X)E(Y)\\) \\[ Strongest \\\\ \\downarrow \\\\ Independence \\\\ \\downarrow \\\\ Mean Independence \\\\ \\downarrow \\\\ Uncorrelated \\\\ \\downarrow \\\\ Weakest \\] 1.3.2 Random variable Discrete Variable Continuous Variable Definition A random variable is discrete if it can assume at most a finite or countably infinite number of possible values A random variable is continuous if it can assume any value in some interval or intervals of real numbers and the probability that it assumes any specific value is 0 Density Function A function f is called a density for X if: (1) \\(f(x) \\ge 0\\) (2) \\(\\sum_{all~x}f(x)=1\\) (3) \\(f(x)=P(X=x)\\) for x real A function f is called a density for X if: (1) \\(f(x) \\ge 0\\) for x real (2) \\(\\int_{-\\infty}^{\\infty} f(x) \\; dx=1\\) (3) \\(P[a \\le X \\le] =\\int_{a}^{b} f(x) \\; dx\\) for a and b real Cumulative Distribution Function for x real \\(F(x)=P[X \\le x]\\) \\(F(x)=P[X \\le x]=\\int_{-\\infty}^{\\infty}f(t)dt\\) \\(E[H(X)]\\) \\(\\sum_{all ~x}H(x)f(x)\\) \\(\\int_{-\\infty}^{\\infty}H(x)f(x)\\) \\(\\mu=E[X]\\) \\(\\sum_{all ~ x}xf(x)\\) \\(\\int_{-\\infty}^{\\infty}xf(x)\\) Ordinary Moments the kth ordinary moment for variable X is defined as: \\(E[X^k]\\) \\(\\sum_{all ~ x \\in X}(x^kf(x))\\) \\(\\int_{-\\infty}^{\\infty}(x^kf(x))\\) Moment generating function (mgf) \\(m_X(t)=E[e^{tX}]\\) \\(\\sum_{all ~ x \\in X}(e^{tx}f(x))\\) \\(\\int_{-\\infty}^{\\infty}(e^{tx}f(x)dx)\\) Expected value Properties: E[c] = c for any constant c E[cX] = cE[X] for any constant c E[X+Y] = E[X] = E[Y] E[XY] = E[X].E[Y] (if X and Y are independent) Expected Variance Properties: \\(Var(c) = 0\\) for any constant c \\(Var(cX) = c^2Var(X)\\) for any constant c \\(Var(X) \\ge 0\\) \\(Var(X) = E(X^2) - (E(X))^2\\) \\(Var(X+c)=Var(X)\\) \\(Var (X+Y) = Var(X) + Var(Y)\\) (if X and Y are independent) Standard deviation \\(\\sigma=\\sqrt(\\sigma^2)=\\sqrt(Var X)\\) Moment generating function properties: \\(\\frac{d^k(m_X(t))}{dt^k}|_{t=0}=E[X^k]\\) \\(\\mu=E[X]=m_X&#39;(0)\\) \\(E[X^2]=m_X&#39;&#39;(0)\\) 1.3.3 Moment Moment Uncentered Centered 1st \\(E(X)=\\mu=Mean(X)\\) 2nd \\(E(X^2)\\) \\(E((X-\\mu)^2)=Var(X)=\\sigma^2\\) 3rd \\(E(X^3)\\) \\(E((X-\\mu)^3)\\) 4th \\(E(X^4)\\) \\(E((X-\\mu)^4)\\) Skewness(X) = \\(E((X-\\mu)^3)/\\sigma^3\\) Kurtosis(X) = \\(E((X-\\mu)^4)/\\sigma^4\\) Conditional Moments \\[ E(Y|X=x)= \\begin{cases} \\sum_yyf_Y(y|x) &amp; \\text{for discrete RV}\\\\ \\int_yyf_Y(y|x)dy &amp; \\text{for continous RV}\\\\ \\end{cases} \\] \\[ Var(Y|X=x)= \\begin{cases} \\sum_y(y-E(Y|x))^2f_Y(y|x) &amp; \\text{for discrete RV}\\\\ \\int_y(y-E(Y|x))^2f_Y(y|x)dy &amp; \\text{for continous RV}\\\\ \\end{cases} \\] 1.3.3.1 Multivariate Moments \\[ \\begin{equation} E= \\left( \\begin{array}{c} X \\\\ Y \\\\ \\end{array} \\right) = \\left( \\begin{array}{c} E(X) \\\\ E(Y) \\\\ \\end{array} \\right) = \\left( \\begin{array}{c} \\mu_X \\\\ \\mu_Y \\\\ \\end{array} \\right) \\end{equation} \\] \\[ \\begin{equation} \\begin{split} Var \\left( \\begin{array}{c} X \\\\ Y \\\\ \\end{array} \\right) &amp;= \\left( \\begin{array}{c} Var(X) &amp; Cov(X,Y) \\\\ Cov(X,Y) &amp; Var(Y) \\\\ \\end{array} \\right) \\\\ &amp;= \\left( \\begin{array}{c} E((X-\\mu_X)^2) &amp; E((X-\\mu_X)(Y-\\mu_Y)) \\\\ E((X-\\mu_X)(Y-\\mu_Y)) &amp; E((Y-\\mu_Y)^2) \\\\ \\end{array} \\right) \\end{split} \\end{equation} \\] Properties \\(E(aX + bY + c)=aE(X) +bE(Y) + c\\) \\(Var(aX + bY + c) = a^2Var(X) + b^2Var(Y) + 2abCov(X,Y)\\) \\(Cov(aX + bY, cX + bY) =acVar(X)+bdVar(Y) + (ad+bc)Cov(X,Y)\\) Correlation: \\(\\rho_{XY} = \\frac{Cov(X,Y)}{\\sigma_X\\sigma_Y}\\) 1.3.4 Distributions Conditional Distributions \\[ f_{X|Y}(X|Y=y)=\\frac{f(X,Y)}{f_Y(y)} \\] \\(f_{X|Y}(X|Y=y)=f_X(X)\\) if X and Y are independent 1.3.4.1 Discrete Distribution Density CDF MGF Mean Variance Geometric \\(f(x)=pq^{x-1}\\) \\(F(x)=1-q^{[x]}\\) \\(m_X(t) = \\frac{pe^t}{1-qe^t}\\) for t &lt; -ln(q) \\(\\frac{1}{p}\\) \\(\\frac{q}{p^2}\\) Binomial \\(f(x)={{n}\\choose{x}}p^xq^{n-x}\\) Table \\(m_X(t) =(q+pe^t)^n\\) \\(np\\) \\(npq\\) Hypergeometric \\(f(x)=\\frac{{{r}\\choose{x}}{{N-r}\\choose{n-x}}}{{{N}\\choose{n}}}\\) where \\(max[0,n-(N-r)] \\le x \\le min(n,r)\\) \\(\\frac{nr}{N}\\) n Poisson \\(f(x) = \\frac{e^{-k}k^x}{x!}\\),k &gt; 0, x =0,1, Use table \\(m_X(t)=e^{k(e^t-1)}\\) k k CDF: Cumulative Density Function MGF: Moment Generating Function 1.3.4.1.1 Bernoulli \\(Bernoulli(p)\\) 1.3.4.1.2 Binomial \\(B(n,p)\\) 1.3.4.1.3 Poisson \\(Pois(\\lambda)\\) 1.3.4.1.4 1.3.4.1.5 1.3.4.1.6 1.3.4.1.7 1.3.4.1.8 1.3.4.1.9 1.3.4.1.10 1.3.4.1.11 1.3.4.1.12 Properties: Geometric The experiment consists of a series of trails. The outcome of each trial can be classed as being either a success (s) or failure (f). (This is called a Bernoulli trial). The trials are identical and independent in the sense that the outcome of one trial has no effect on the outcome of any other. The probability of success (p) and probability of failure (q=1-p) remains the same from trial to trial. lack of memory X: the number of trials needed to obtain the first success. Binomial The experiment consists of a fixed number (n) of Bernoulli trials, each of which results in a success (s) or failure (f) The trials are identical and independent. The probability of success (p) and probability of failure (q=1-p) remains the same from trial to trial. The random variable X denotes the number of successes obtained in the n trails. Hypergeometric The experiment consists of drawing a random sample of size n without replacement and without regard to order from a collection of N objects. Of the N objects, r have a trait of interest; N-r do not have the trait X is the number of objects in the sample with the trait. Poisson in connection with a Poisson process, which involves observing discrete events in a continuous interval of time, length, or space. X : the number of occurrences of the event within an interval of s units The parameter \\(\\lambda\\) is the average number of occurrences of the event per measurement unit. For the distribution, we use the parameter \\(k=\\lambda s\\) 1.3.4.2 Continuous Distribution Density CDF MGF Mean Variance |Uniform|\\(f(x)=\\frac{1}{b-a}\\) for a &lt; x &lt; b | \\[\\begin{cases}0&amp;\\text{if x &lt;a }\\frac{x-a}{b-a}&amp;\\text{if $a \\le x \\le b$ }\\\\1&amp;\\text{if x &gt;b}\\\\\\end{cases}\\] | \\[\\begin{cases}\\frac{e^{tb} - e^{ta}}{t(b-a)}&amp;\\text{ if $t \\neq 0$}\\\\1&amp;\\text{if $ t \\neq 0$}\\\\\\end{cases}\\] | | 1.3.4.2.1 Normal \\(N(\\mu,\\sigma^2)\\) 1.3.4.2.2 Logistic \\(Logistic(\\mu,s)\\) 1.3.4.2.3 Lognomral \\(lognormal(\\mu,\\sigma^2)\\) 1.3.4.2.4 Exponential \\(Exp(\\lambda)\\) 1.3.4.2.5 Chi-squared \\(\\chi^2=\\chi^2(k)\\) 1.3.4.2.6 Student T \\(T(v)\\) 1.3.4.2.7 F-Distribution \\(F(d_1,d_2)\\) 1.3.4.2.8 1.3.4.2.9 1.3.4.2.10 1.3.4.2.11 1.3.4.2.12 "],["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction This guide is an attempt to streamline and demystify the data analysis process. By no mean this is an ultimate guide, or I am a great source of knowledge, or I claim myself as a statistician/ data analyst/ econometrician (or any fancy name we have now), but I am a strong proponent of learning by teaching, and doing. Hence, this is more like a learning experience for both you and me. Since the beginning of the century, we have been bombarded with amazing advancements and inventions, especially in the field of statistics, information technology, and computer science. However, I believe the downside of this introduction is that we use big and trendy words too often (i.e., big data, machine learning, deep learning). Its all fun and exciting when I learned these new tools. But I have to admit that I hardly retain any of these new inventions.However, writing down from the beginning till the end of a data analysis process is the solution that I came up with. Accordingly, lets dive right in. Some general recommendation: The more you practice/habituate/condition, more line of codes that you write, more function that you memorize, I think the more you will like this journey. Readers can follow this book several ways: If you are interested in particular methods/tools, you can jump to that section by clicking the section name. If you want to follow a traditional path of data analysis, read the Regression Analysis section. If you want to create your experiment and test your hypothesis, read the Experimental Design section. Alternatively, if you rather see the application of models, and disregard any theory or underlying mechanisms, you can skip to summary portion of each section. If you dont understand a part, search the title of that part of that part on Google, and read more into that subject. This is just a general guide. If you want to customize your code beyond the ones provided in this book, run in the console help(code) or ?code. For example, I want more information on hist function, Ill type in the console ?hist or help(hist). Another way is that you can search on Google. Different people will use different packages to achieve the same result in R. Accordingly, if you want to create a histogram, search on Google histogram in R, then you should be able to find multiple ways to create histogram in R. Information in this book are from various sources, but the skeleton is based on several courses that I have taken formally. Id like to give professors credit accordingly. Course Professor Data Analysis I Erin M. Schliep Applied Econometric Alyssa Carlson Tools of statistics Probability Theory Mathematical Analysis Computer Science Numerical Analysis Database Management Setup Working Environment if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) ## Loading required package: pacman if (!require(&quot;devtools&quot;)) install.packages(&quot;devtools&quot;) ## Loading required package: devtools ## Loading required package: usethis library(&quot;pacman&quot;) library(&quot;devtools&quot;) "],["descriptive-stat.html", "Chapter 3 Descriptive Statistics", " Chapter 3 Descriptive Statistics When you have an area of interest that you want to research, a problem that you want to solve, a relationship that you want to investigate, theoretical and empirical processes will help you. Estimand is defined as a quantity of scientific interest that can be calculated in the population and does not change its value depending on the data collection design used to measure it (i.e., it does not vary with sample size and survey design, or the number of nonrespondents, or follow-up efforts). (Rubin 1996) Estimands include: population means Population variances correlations factor loading regression coefficients References "],["numerical-measures.html", "3.1 Numerical Measures", " 3.1 Numerical Measures There is a difference between a population and a sample Measures of Category Population Sample - What is it? Reality A small fraction of reality (inference) - Characteristics described by Parameters Statistics Central Tendency Mean \\(\\mu = E(Y)\\) \\(\\hat{\\mu} = \\overline{y}\\) Central Tendency Median 50-th percentile \\(y_{(\\frac{n+1}{2})}\\) Dispersion Variance \\(\\sigma^2=var(Y)\\) \\(=E(Y-\\mu)^2\\) \\(s^2=\\frac{1}{n-1} \\sum_{i = 1}^{n} (y_i-\\overline{y})^2\\) \\(=\\frac{1}{n-1} \\sum_{i = 1}^{n} (y_i^2-n\\overline{y}^2)\\) Dispersion Coefficient of Variation \\(\\frac{\\sigma}{\\mu}\\) \\(\\frac{s}{\\overline{y}}\\) Dispersion Interquartile Range difference between 25th and 75th percentiles. Robust to outliers Shape Skewness Standardized 3rd central moment (unitless) \\(g_1=\\frac{\\mu_3}{\\mu_2^{3/2}}\\) \\(\\hat{g_1}=\\frac{m_3}{m_2sqrt(m_2)}\\) Shape Central moments \\(\\mu=E(Y)\\) \\(\\mu_2 = \\sigma^2=E(Y-\\mu)^2\\) \\(\\mu_3 = E(Y-\\mu)^3\\) \\(\\mu_4 = E(Y-\\mu)^4\\) \\(m_2=\\sum_{i=1}^{n}(y_1-\\overline{y})^2/n\\) \\(m_3=\\sum_{i=1}^{n}(y_1-\\overline{y})^3/n\\) Shape Kurtosis (peakedness and tail thickness) Standardized 4th central moment \\(g_2^*=\\frac{E(Y-\\mu)^4}{\\sigma^4}\\) \\(\\hat{g_2}=\\frac{m_4}{m_2^2}-3\\) Note: Order Statistics: \\(y_{(1)},y_{(2)},...,y_{(n)}\\) where \\(y_{(1)}&lt;y_{(2)}&lt;...&lt;y_{(n)}\\) Coefficient of variation: standard deviation over mean. This metric is stable, dimensionless statistic for comparison. Symmetric: mean = median, skewness = 0 Skewed right: mean &gt; median, skewness &gt; 0 Skewed left: mean &lt; median, skewness &lt; 0 Central moments: \\(\\mu=E(Y)\\) , \\(\\mu_2 = \\sigma^2=E(Y-\\mu)^2\\) , \\(\\mu_3 = E(Y-\\mu)^3\\), \\(\\mu_4 = E(Y-\\mu)^4\\) For normal distributions, \\(\\mu_3=0\\), so \\(g_1=0\\) \\(\\hat{g_1}\\) is distributed approximately as N(0,6/n) if sample is from a normal population. (valid when n &gt; 150) For large samples, inferece on skewness can be based on normal tables with 95% confidence itnerval for \\(g_1\\) as \\(\\hat{g_1}\\pm1.96\\sqrt{6/n}\\) For small samples, special tables from Snedecor and Cochran 1989, Table A 19(i) or Monte Carlo test Kurtosis &gt; 0 (leptokurtic) heavier tail compared to a normal distribution with the same \\(\\sigma\\) (e.g., t-distribution) Kurtosis &lt; 0 (platykurtic) lighter tail compared to a normal distribution with the same \\(\\sigma\\) For a normal distribution, \\(g_2^*=3\\). Kurtosis is often redefined as: \\(g_2=\\frac{E(Y-\\mu)^4}{\\sigma^4}-3\\) where the 4th central moment is estimated by \\(m_4=\\sum_{i=1}^{n}(y_i-\\overline{y})^4/n\\) the asymptotic sampling distribution for \\(\\hat{g_2}\\) is approximately N(0,24/n) (with n &gt; 1000) large sample on kurtosis uses standard normal tables small sample uses tables by Snedecor and Cochran, 1989, Table A 19(ii) or Geary 1936 "],["graphical-measures.html", "3.2 Graphical Measures", " 3.2 Graphical Measures 3.2.1 Shape Its a good habit to label your graph, so others can easily follow. data = rnorm(100) # Histogram hist(data,labels = T,col=&quot;grey&quot;,breaks = 12) # Interactive histogram pacman::p_load(&quot;highcharter&quot;) hchart(data) # Box-and-Whisker plot boxplot(count ~ spray, data = InsectSprays,col = &quot;lightgray&quot;,main=&quot;boxplot&quot;) # Notched Boxplot boxplot(len~supp*dose, data=ToothGrowth, notch=TRUE, col=(c(&quot;gold&quot;,&quot;darkgreen&quot;)), main=&quot;Tooth Growth&quot;, xlab=&quot;Suppliment and Dose&quot;) ## Warning in bxp(list(stats = structure(c(8.2, 9.7, 12.25, 16.5, 21.5, 4.2, : some ## notches went outside hinges (&#39;box&#39;): maybe set notch=FALSE # If notches differ -&gt; medians differ # Stem-and-Leaf Plots stem(data) ## ## The decimal point is at the | ## ## -2 | 720 ## -1 | 9987543111111000 ## -0 | 99988877766665554444433333333222210 ## 0 | 000111111222344444555666678889999 ## 1 | 01112233467 ## 2 | 1 ## 3 | 6 # Bagplot - A 2D Boxplot Extension pacman::p_load(aplpack) attach(mtcars) bagplot(wt,mpg, xlab=&quot;Car Weight&quot;, ylab=&quot;Miles Per Gallon&quot;, main=&quot;Bagplot Example&quot;) Others more advanced plots # boxplot.matrix() #library(&quot;sfsmisc&quot;) # boxplot.n() #library(&quot;gplots&quot;) # vioplot() #library(&quot;vioplot&quot;) "],["normality-assessment.html", "3.3 Normality Assessment", " 3.3 Normality Assessment Since Normal (Gaussian) distribution has many applications, we typically want/ wish our data or our variable is normal. Hence, we have to assess the normality based on not only Numerical Measures but also Graphical Measures 3.3.1 Graphical Assessment pacman::p_load(&quot;car&quot;) qqnorm(precip, ylab = &quot;Precipitation [in/yr] for 70 US cities&quot;) qqline(precip) The straight line represents the theoretical line for normally distributed data. The dots represent real empirical data that we are checking. If all the dots fall on the straight line, we can be confident that our data follow a normal distribution. If our data wiggle and deviate from the line, we should be concerned with the normality assumption. 3.3.2 Summary Statistics Sometimes its hard to tell whether your data follow the normal distribution by just looking at the graph. Hence, we often have to conduct statistical test to aid our decision. Common tests are Methods based on normal probability plot Correlation Coefficient with Normal Probability Plots Shapiro-Wilk Test Methods based on empirical cumulative distribution function Anderson-Darling Test Kolmogorov-Smirnov Test Cramer-von Mises Test 3.3.2.1 Methods based on normal probability plot 3.3.2.1.1 Correlation Coefficient with Normal Probability Plots (Looney and Gulledge 1985) (Shapiro and Francia 1972) The correlation coefficient between \\(y_{(i)}\\) and \\(m_i^*\\) as given on the normal probability plot: \\[W^*=\\frac{\\sum_{i=1}^{n}(y_{(i)}-\\bar{y})(m_i^*-0)}{(\\sum_{i=1}^{n}(y_{(i)}-\\bar{y})^2\\sum_{i=1}^{n}(m_i^*-0)^2)^.5}\\] where \\(\\bar{m^*}=0\\) Pearson product moment formula for correlation: \\[\\hat{p}=\\frac{\\sum_{i-1}^{n}(y_i-\\bar{y})(x_i-\\bar{x})}{(\\sum_{i=1}^{n}(y_{i}-\\bar{y})^2\\sum_{i=1}^{n}(x_i-\\bar{x})^2)^.5}\\] When the correlation is 1, the plot is exactly linear and normality is assumed. The closer the correlation is to zero, the more confident we are to reject normality Inference on W* needs to be based on special tables (Looney and Gulledge 1985) 3.3.2.1.2 Shapiro-Wilk Test (Shapiro and Wilk 1965) \\[W=(\\frac{\\sum_{i=1}^{n}a_i(y_{(i)}-\\bar{y})(m_i^*-0)}{(\\sum_{i=1}^{n}a_i^2(y_{(i)}-\\bar{y})^2\\sum_{i=1}^{n}(m_i^*-0)^2)^.5})^2\\] where \\(a_1,..,a_n\\) are weights computed from the covariance matrix for the order statistics. Researchers typically use this test to assess normality. (n &lt; 2000) Under normality, W is close to 1, just like \\(W^*\\). Notice that the only difference between W and W* is the weights. 3.3.2.2 Methods based on empirical cumulative distribution function The formula for the empirical cumulative distribution function (CDF) is: \\(F_n(t)\\) = estimate of probability that an observation \\(\\le\\) t = (number of observation \\(\\le\\) t)/n This method requires large sample sizes. However, it can apply to distributions other than the normal (Gaussian) one. 3.3.2.2.1 Anderson-Darling Test (Anderson and Darling 1952) The Anderson-Darling statistic: \\[A^2=\\int_{-\\infty}^{\\infty}(F_n(t)=F(t))^2\\frac{dF(t)}{F(t)(1-F(t))}\\] a weight average of squared deviations (it weights small and large values of t more) For the normal distribution, \\(A^2 = - (\\sum_{i=1}^{n}(2i-1)(ln(p_i) +ln(1-p_{n+1-i}))/n-n\\) where \\(p_i=\\Phi(\\frac{y_{(i)}-\\bar{y}}{s})\\), the probability that a standard normal variable is less than \\(\\frac{y_{(i)}-\\bar{y}}{s}\\) Reject normal assumption when \\(A^2\\) is too large Evaluate the null hypothesis that the observations are randomly selected from a normal population based on the critical value provided by (Marsaglia and Marsaglia 2004) and (Stephens 1974) This test can be applied to other distributions: Exponential Logistic Gumbel Extreme-value Weibull: log(Weibull) = Gumbel Gamma Logistic Cauchy von Mises Log-normal (two-parameter) Consult (Stephens 1974) for more detailed transformation and critical values. 3.3.2.2.2 Kolmogorov-Smirnov Test Based on the largest absolute difference between empirical and expected cumulative distribution Another deviation of K-S test is Kuipers test 3.3.2.2.3 Cramer-von Mises Test Based on the average squared discrepancy between the empirical distribution and a given theoretical distribution. Each discrepancy is weighted equally (unlike Anderson-Darling test weights end points more heavily) 3.3.2.2.4 JarqueBera Test (Bera and Jarque 1981) Based on the skewness and kurtosis to test normality. \\(JB = \\frac{n}{6}(S^2+(K-3)^2/4)\\) where S is the sample skewness and K is the sample kurtosis \\(S=\\frac{\\hat{\\mu_3}}{\\hat{\\sigma}^3}=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})^3/n}{(\\sum_{i=1}^{n}(x_i-\\bar{x})^2/n)^\\frac{3}{2}}\\) \\(K=\\frac{\\hat{\\mu_4}}{\\hat{\\sigma}^4}=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})^4/n}{(\\sum_{i=1}^{n}(x_i-\\bar{x})^2/n)^2}\\) recall \\(\\hat{\\sigma^2}\\) is the estimate of the second central moment (variance) \\(\\hat{\\mu_3}\\) and \\(\\hat{\\mu_4}\\) are the estimates of third and fourth central moments. If the data comes from a normal distribution, the JB statistic asymptotically has a chi-squared distribution with two degrees of freedom. The null hypothesis is a joint hypothesis of the skewness being zero and the excess kurtosis being zero. References "],["basic-statistical-inference.html", "Chapter 4 Basic Statistical Inference", " Chapter 4 Basic Statistical Inference One Sample Inference Two Sample Inference Categorical Data Analysis "],["one-sample-inference.html", "4.1 One Sample Inference", " 4.1 One Sample Inference \\(Y_i \\sim i.i.d. N(\\mu, \\sigma^2)\\) i.i.d. standards for independent and identically distributed Hence, we have the following model: \\(Y_i=\\mu +\\epsilon_i\\) where \\(\\epsilon_i \\sim{iid}\\) "],["two-sample-inference.html", "4.2 Two Sample Inference", " 4.2 Two Sample Inference "],["categorical-data-analysis.html", "4.3 Categorical Data Analysis", " 4.3 Categorical Data Analysis "],["regression-analysis.html", "Chapter 5 Regression Analysis ", " Chapter 5 Regression Analysis "],["linear-regression.html", "5.1 Linear Regression", " 5.1 Linear Regression 5.1.1 Ordinary Least Squares The most fundamental model in statistics or econometric is a OLS linear regression. OLS = Maximum likelihood when the error term is assumed to be normally distributed. 5.1.1.1 OLS Assumptions 5.1.1.1.1 A1 Linearity \\[ \\begin{equation} A1: y=\\mathbf{x}\\beta + \\epsilon \\tag{5.1} \\end{equation} \\] Not restrictive x can be nonlinear transformation including interactions, natural log, quadratic With A3 (Exogeneity of Independent), linearity can be restrictive 5.1.1.1.1.1 Log-linear 5.1.1.1.1.2 Linear-Log 5.1.1.1.1.3 Log-Log 5.1.1.1.1.4 Higher Orders \\(y=\\beta_0 + x_1\\beta_1 + x_1^2\\beta_2 + \\epsilon\\) \\[ \\frac{\\partial y}{\\partial x_1}=\\beta_1 + 2x_1\\beta_2 \\] The effect of \\(x_1\\) on y depends on the level of \\(x_1\\) The partial effect at the average = \\(\\beta_1+2E(x_1)\\beta_2\\) Average Partial Effect = \\(E(\\beta_1 + 2x_1\\beta_2)\\) 5.1.1.1.1.5 Interactions \\(y=\\beta_0 + x_1\\beta_1 + x_2\\beta_2 + x_1x_2\\beta_3 + \\epsilon\\) \\(\\beta_1\\) is the average effect on y for a unit change in \\(x_1\\) when \\(x_2=0\\) \\(\\beta_1 + x_2\\beta_3\\) is the partial effect of \\(x_1\\) on y which depends on the level of \\(x_2\\) 5.1.1.1.2 A2 Full rank \\[ \\begin{equation} A2: rank(E(x&#39;x))=k \\tag{5.2} \\end{equation} \\] also known as identification condition columns of \\(\\mathbf{x}\\) cannot be written as a linear function of the other columns which ensures that each parameter is unique and exists in the population regression equation 5.1.1.1.3 A3 Exogeneity of Independent Variables \\[ \\begin{equation} A3: E[\\epsilon|x_1,x_2,...,x_k]=E[\\epsilon|\\mathbf{x}]=0 \\tag{5.3} \\end{equation} \\] strict exogeneity also known as mean independence check back on Correlation and Independence by the Law of Iterated Expectations \\(E(\\epsilon)=0\\), which can be satisfied by always including an intercept. independent variables do not carry information for prediction of \\(\\epsilon\\) A3 implies \\(E(y|x)=x\\beta\\), which means the conditional mean function must be a linear function of x A1 Linearity 5.1.1.1.4 A4 Homoskedasticity \\[ \\begin{equation} A4: Var(\\epsilon|x)=Var(\\epsilon)=\\sigma^2 \\tag{5.4} \\end{equation} \\] Variation in the disturbance to be the same over the independent variables 5.1.1.1.5 A5 Data Generation (random Sampling) \\[ \\begin{equation} A5: {y_i,x_{i1},...,x_{ik-1}: i = 1,..., n} \\tag{5.5} \\end{equation} \\] is a random sample random sample mean samples are independent and identically distributed (iid) from a joint distribution of \\((y,\\mathbf{x})\\) with A3 and A4, we have Strict Exogeneity: \\(E(\\epsilon_i|x_1,...,x_n)=0\\). independent variables do not carry information for prediction of \\(\\epsilon\\) Non-autocorrelation: \\(E(\\epsilon_i\\epsilon_j|x_1,...,x_n)=0\\) The error term is uncorrelated across the draws conditional on the independent variables \\(\\rightarrow\\) \\(A4: Var(\\epsilon|\\mathbf{X})=Var(\\epsilon)=\\sigma^2I_n\\) In times series and spatial settings, A5 is less likely to hold. 5.1.1.1.6 A6 Normal Distribution \\[ \\begin{equation} A6: \\epsilon|\\mathbf{x}\\sim N(0,\\sigma^2I_n) \\tag{5.6} \\end{equation} \\] The error term is normally distributed From A1-A3, we have identification (also known as Orthogonality Condition) of the population parameter \\(\\beta\\) \\[\\begin{align} y &amp;= {x}\\beta + \\epsilon &amp;&amp; \\text{A1} \\\\ x&#39;y &amp;= x&#39;x\\beta + x&#39;\\epsilon &amp;&amp; \\text{} \\\\ E(x&#39;y) &amp;= E(x&#39;x)\\beta + E(x&#39;\\epsilon) &amp;&amp; \\text{} \\\\ E(x&#39;y) &amp;= E(x&#39;x)\\beta &amp;&amp; \\text{A3} \\\\ [E(x&#39;x)]^{-1}E(x&#39;y) &amp;= [E(x&#39;x)]^{-1}E(x&#39;x)\\beta &amp;&amp; \\text{A2} \\\\ [E(x&#39;x)]^{-1}E(x&#39;y) &amp;= \\beta \\end{align}\\] is the row vector of parameters that produces the best predictor of y we choose the min of : \\[ \\underset{\\gamma}{\\operatorname{argmin}}E((y-x\\gamma)^2) \\] First Order Condition \\[ \\begin{split} \\frac{\\partial((y-x\\gamma)^2)}{\\partial\\gamma}&amp;=0 \\\\ -2E(x&#39;(y-x\\gamma))&amp;=0 \\\\ E(x&#39;y)-E(x&#39;x\\gamma) &amp;=0 \\\\ E(x&#39;y) &amp;= E(x&#39;x)\\gamma \\\\ (E(x&#39;x))^{-1}E(x&#39;y) &amp;= \\gamma \\end{split} \\] Second Order Conditon \\[ \\begin{split} \\frac{\\partial^2E((y-x\\gamma)^2)}{}&amp;=0 \\\\ E(\\frac{\\partial(y-x\\partial)^2)}{\\partial\\gamma\\partial\\gamma&#39;}) &amp;= 2E(x&#39;x) \\end{split} \\] If A3 holds, then \\(2E(x&#39;x)\\) is PSD \\(\\rightarrow\\) minimum 5.1.1.2 Frisch-Waugh-Lovell Theorem \\[ \\mathbf{y=X\\beta + \\epsilon=X_1\\beta_1+X_2\\beta_2 +\\epsilon} \\] Equivalently, \\[ \\left( \\begin{array}{c} X_1&#39;X_1 &amp; X_1&#39;X_2 \\\\ X_2&#39;X_1 &amp; X_2&#39;X_2 \\end{array} \\right) \\left( \\begin{array}{c} \\hat{\\beta_1} \\\\ \\hat{\\beta_2} \\end{array} \\right) = \\left( \\begin{array}{c} X_1&#39;y \\\\ X_2&#39;y \\end{array} \\right) \\] Hence, \\[ \\mathbf{\\hat{\\beta_1}=(X_1&#39;X_1)^{-1}X_1&#39;y - (X_1&#39;X_1)^{-1}X_1&#39;X_2\\hat{\\beta_2}} \\] Betas from the multiple regression are not the same as the betas from each of the individual simple regression Different set of X will affect all the coefficient estimates. If \\(X_1&#39;X_2 = 0\\) or $=0, then 1 and 2 do not hold. Hierarchy of OLS Assumptions Identification Data Description Unbiasedness Consistency Gauss-Markov(BLUE) Asymptotic Inference (z and Chi-squared) Classical LM (BUE) Small-sample Inference (t and F) Variation in X Variation in X Variation in X Variation in X Random Sampling Random Sampling Random Sampling Linearity in Parameters Linearity in Parameters Linearity in Parameters Zero Conditional Mean Zero Conditional Mean Zero Conditional Mean Homoskedasticity Homoskedasticity Normality of Errors 5.1.2 Feasible Generalized Least Squares Motivation for a more efficient estimator Gauss Markov Theorem holds under A1-A4 A4: \\(Var(\\epsilon| \\mathbf{X} )=\\sigma^2I_n\\) Heteroskedasticity: \\(Var(\\epsilon_i|\\mathbf{X}) \\neq \\sigma^2I_n\\) Serial Correlation: \\(Cov(\\epsilon_i,\\epsilon_j|\\mathbf{X}) \\neq 0\\) Without A4, how can we know which unbiased estimator is the most efficient? Original (unweighted) model: \\[ \\mathbf{y=X\\beta+ \\epsilon} \\] Suppose A1-A3 hold, but A4 does not hold, \\[ \\mathbf{Var(\\epsilon|X)=\\Omega \\neq \\sigma^2 I_n} \\] We will try to use OLS to estimate the transformed (weighted) model \\[ \\mathbf{wy=wX\\beta + w\\epsilon} \\] We need to choose \\(\\mathbf{w}\\) so that \\[ \\mathbf{w&#39;w = \\Omega^{-1}} \\] then \\(\\mathbf{w}\\) (full-rank matrix) is the Cholesky decomposition of \\(\\mathbf{\\Omega^{-1}}\\) (full-rank matrix) In other words, \\(\\mathbf{w}\\) is the squared root of \\(\\Omega\\) (squared root version in matrix) \\[ \\Omega = var(\\epsilon | X) \\\\ \\Omega^{-1} = var(\\epsilon | X)^{-1} \\] Then, the transformed equation (IGLS) will have the following properties. \\[\\begin{equation} \\begin{split} \\mathbf{\\hat{\\beta}_{IGLS}} &amp;= \\mathbf{(X&#39;w&#39;wX)^{-1}X&#39;w&#39;wy} \\\\ &amp; = \\mathbf{(X&#39;\\Omega^{-1}X)^{-1}X&#39;\\Omega^{-1}y} \\\\ &amp; = \\mathbf{\\beta + X&#39;\\Omega^{-1}X&#39;\\Omega^{-1}\\epsilon} \\end{split} \\end{equation}\\] Since A1-A3 hold for the unweighted model \\[\\begin{equation} \\begin{split} \\mathbf{E(\\hat{\\beta}_{IGLS}|X)} &amp; = E(\\mathbf{\\beta + (X&#39;\\Omega^{-1}X&#39;\\Omega^{-1}\\epsilon)}|X)\\\\ &amp; = \\mathbf{\\beta + E(X&#39;\\Omega^{-1}X&#39;\\Omega^{-1}\\epsilon)|X)} \\\\ &amp; = \\mathbf{\\beta + X&#39;\\Omega^{-1}X&#39;\\Omega^{-1}E(\\epsilon|X)} &amp;&amp; \\text{since A3: $E(\\epsilon|X)=0$} \\\\ &amp; = \\mathbf{\\beta} \\end{split} \\end{equation}\\] \\(\\rightarrow\\) IGLS estimator is unbiased \\[\\begin{equation} \\begin{split} \\mathbf{Var(w\\epsilon|X)} &amp;= \\mathbf{wVar(\\epsilon|X)w&#39;} \\\\ &amp; = \\mathbf{w\\Omega w&#39;} \\\\ &amp; = \\mathbf{w(w&#39;w)^{-1}w&#39;} &amp;&amp; \\text{since w is a full-rank matrix}\\\\ &amp; = \\mathbf{ww^{-1}(w&#39;)^{-1}w&#39;} \\\\ &amp; = \\mathbf{I_n} \\end{split} \\end{equation}\\] \\(\\rightarrow\\) A4 holds for the transformed (weighted) equation Then, the variance for the estimator is \\[\\begin{equation} \\begin{split} Var(\\hat{\\beta}_{IGLS}|\\mathbf{X}) &amp; = \\mathbf{Var(\\beta + (X&#39;\\Omega ^{-1}X)^{-1}X&#39;\\Omega^{-1}\\epsilon|X)} \\\\ &amp;= \\mathbf{Var((X&#39;\\Omega ^{-1}X)^{-1}X&#39;\\Omega^{-1}\\epsilon|X)} \\\\ &amp;= \\mathbf{(X&#39;\\Omega ^{-1}X)^{-1}X&#39;\\Omega^{-1} Var(\\epsilon|X) \\Omega^{-1}X(X&#39;\\Omega ^{-1}X)^{-1}} &amp;&amp; \\text{because A4 holds}\\\\ &amp;= \\mathbf{(X&#39;\\Omega ^{-1}X)^{-1}X&#39;\\Omega^{-1} \\Omega \\Omega^{-1} \\Omega^{-1}X(X&#39;\\Omega ^{-1}X)^{-1}} \\\\ &amp;= \\mathbf{(X&#39;\\Omega ^{-1}X)^{-1}} \\end{split} \\end{equation}\\] Let \\(A = \\mathbf{(X&#39;X)^{-1}X&#39;-(X&#39;\\Omega ^{-1} X)X&#39; \\Omega^{-1}}\\) then \\[ Var(\\hat{\\beta}_{OLS}|X)- Var(\\hat{\\beta}_{IGLS}|X) = A\\Omega A&#39; \\] And \\(\\Omega\\) is Positive Semi Definite, then \\(A\\Omega A&#39;\\) also PSD, then IGLS is more efficient The name Infeasible comes from the fact that it is impossible to compute this estimator. \\[\\begin{equation} \\mathbf{w} = \\left( \\begin{array}{c} w_{11} &amp; 0 &amp; 0 &amp; ... &amp; 0 \\\\ w_{21} &amp; w_{22} &amp; 0 &amp; ... &amp; 0 \\\\ w_{31} &amp; w_{32} &amp; w_{33} &amp; ... &amp; ... \\\\ w_{n1} &amp; w_{n2} &amp; w_{n3} &amp; ... &amp; w_{nn} \\\\ \\end{array} \\right) \\end{equation}\\] With \\(n(n+1)/2\\) number of elements and n observations \\(\\rightarrow\\) infeasible to estimate. (number of equation &gt; data) Hence, we need to make assumption on \\(\\Omega\\) to make it feasible to estimate \\(\\mathbf{w}\\): Heteroskedasticity : multiplicative exponential model Serial Correlation: AR(1) Serial Correlation: Cluster 5.1.2.1 Heteroskedasticity \\[\\begin{equation} \\begin{split} Var(\\epsilon_i |x_i) &amp; = E(\\epsilon^2|x_i) \\neq \\sigma^2 \\\\ &amp; = h(x_i) = \\sigma_i^2 \\text{(variance of the error term is a function of x)} \\end{split} \\tag{5.7} \\end{equation}\\] For our model, \\[ y_i = x_i\\beta + \\epsilon_i \\\\ (1/\\sigma_i)y_i = (1/\\sigma_i)x_i\\beta + (1/\\sigma_i)\\epsilon_i \\] then, from (5.7) \\[\\begin{equation} \\begin{split} Var((1/\\sigma_i)\\epsilon_i|X) &amp;= (1/\\sigma_i^2) Var(\\epsilon_i|X) \\\\ &amp;= (1/\\sigma_i^2)\\sigma_i^2 \\\\ &amp;= 1 \\end{split} \\end{equation}\\] then the 5.1.3 Weighted Least Squares 5.1.4 Generalized Least Squares 5.1.5 Maximum Likelihood Premise: find values of the parameters that maximize the probability of observing the data In other words, we try to maximize the value of theta in the likelihood function \\[ L(\\theta)=\\prod_{i=1}^{n}f(y_i|\\theta) \\] \\(f(y|\\theta)\\) is the probability density of observing a single value of Y given some value of \\(\\theta\\) \\(f(y|\\theta)\\) can be specify as various type of distributions. You can review back section Distributions. For example If y is a dichotomous variable, then \\[ L(\\theta)=\\prod_{i=1}^{n}\\theta^{y_i}(1-\\theta)^{1-y_i} \\] Assumption: observations are independent and have the same density function. Under multivariate normal assumption, ML yields consistent estimates of the means and the covariance matrix for multivariate distribution with finite fourth moments (Little and Smith 1987) Properties (EJD, Agresti, and Finlay 1998) Consistent: estimates are approximately unbiased in large samples Asymptotically efficient: approximately smaller standard errors compared to other estimator Asymptotically normal: with repeated sampling, the estimates will have an approximately normal distribution. References "],["non-linear-regression.html", "5.2 Non-linear Regression", " 5.2 Non-linear Regression "],["quantile-regression.html", "5.3 Quantile Regression", " 5.3 Quantile Regression Linear Regression is based on the conditional mean function \\(E(y|x)\\) In Quantile regression, we can view each points in the conditional distribution of y. Quantile regression estimates the conditional median or any other quantile of Y. In the case that were interested in the 50th percentile, quantile regression is median regression, also known as least-absolute-deviations (LAD) regression, minimizes \\(\\sum_{i}|e_i|\\) Properties of estimators \\(\\beta\\) Asymptotically normally distributed Advantages More robust to outliers compared to OLS In the case the dependent variable has a bimodal or multimodal (multiple humps with multiple modes) distribution, quantile regression can be extremely useful. Avoids parametric distribution assumption of the error process. In another word, no assumptions regarding the distribution of the error term. Better characterization of the data (not just its conditional mean) is invariant to monotonic transformations (such as log) while OLS is not. In another word, \\(E(g(y))=g(E(y))\\) Disadvantages The dependent variable needs to be continuous with no zeroes or too many repeated values. \\[ y_i = x_i&#39;\\beta_q + e_i \\] Let \\(e(x) = y -\\hat{y}(x)\\), then \\(L(e(x)) = L(y -\\hat{y}(x))\\) is the loss function of the error term. If \\(L(e) = |e|\\) (called absolute-error loss function) then \\(\\hat{\\beta}\\) can be estimated by minimizing \\(\\sum_{i}|y_i-x_i&#39;\\beta|\\) More specifically, the objective function is \\[ Q(\\beta_q)=\\sum_{i:y_i \\ge x_i&#39;\\beta}^{N} q|y_i - x_i&#39;\\beta_q| + \\sum_{i:y_i &lt; x_i&#39;\\beta}^{N} (1-q)|y_i-x_i&#39;\\beta_q \\] where \\(0&lt;q&lt;1\\) The sum penalizes \\(q|e_i|\\) for under-prediction and \\((1-q)|e_i|\\) for over-prediction We use simplex method to minimize this function (cannot use analytical solution since its non-differentiable). Standard errors can be estimated by bootstrap. The absolute-error loss function is symmetric. Interpretation For the jth regressor (\\(x_j\\)), the marginal effect is the coefficient for the qth quantile \\[ \\frac{\\partial Q_q(y|x)}{\\partial x_j} = \\beta_{qj} \\] At the quantile q of the dependent variable y, \\(\\beta_q\\) represents a one unit change in the independent variable \\(x_j\\) on the dependent variable y. In other words, at the qth percentile, a one unit change in x results in \\(\\beta_q\\) unit change in y. 5.3.1 Application # generate data with non-constant variance x &lt;- seq(0,100,length.out = 100) # independent variable sig &lt;- 0.1 + 0.05*x # non-constant variance b_0 &lt;- 3 # true intercept b_1 &lt;- 0.05 # true slope set.seed(1) # reproducibility e &lt;- rnorm(100,mean = 0, sd = sig) # normal random error with non-constant variance y &lt;- b_0 + b_1*x + e # dependent variable dat &lt;- data.frame(x,y) hist(y) library(ggplot2) ## ## Attaching package: &#39;ggplot2&#39; ## The following object is masked from &#39;mtcars&#39;: ## ## mpg ggplot(dat, aes(x,y)) + geom_point() ggplot(dat, aes(x,y)) + geom_point() + geom_smooth(method=&quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; We follow (Koenker 1996) to estimate quantile regression library(quantreg) ## Loading required package: SparseM ## ## Attaching package: &#39;SparseM&#39; ## The following object is masked from &#39;package:base&#39;: ## ## backsolve qr &lt;- rq(y ~ x, data=dat, tau = 0.5) # tau: quantile of interest. Here we have it at 50th percentile. summary(qr) ## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique ## ## Call: rq(formula = y ~ x, tau = 0.5, data = dat) ## ## tau: [1] 0.5 ## ## Coefficients: ## coefficients lower bd upper bd ## (Intercept) 3.02410 2.80975 3.29408 ## x 0.05351 0.03838 0.06690 adding the regression line ggplot(dat, aes(x,y)) + geom_point() + geom_abline(intercept=coef(qr)[1], slope=coef(qr)[2]) To have R estimate multiple quantile at once qs &lt;- 1:9/10 qr1 &lt;- rq(y ~ x, data=dat, tau = qs) #check for its coefficients coef(qr1) ## tau= 0.1 tau= 0.2 tau= 0.3 tau= 0.4 tau= 0.5 tau= 0.6 ## (Intercept) 2.95735740 2.93735462 3.19112214 3.08146314 3.02409828 3.16840820 ## x -0.01203696 0.01942669 0.02394535 0.04208019 0.05350556 0.06507385 ## tau= 0.7 tau= 0.8 tau= 0.9 ## (Intercept) 3.09507770 3.10539343 3.041681 ## x 0.07783556 0.08782548 0.111254 # plot ggplot(dat, aes(x,y)) + geom_point() + geom_quantile(quantiles = qs) ## Smoothing formula not specified. Using: y ~ x To examine if the quantile regression is appropriate, we can see its plot compared to least squares regression plot(summary(qr1), parm=&quot;x&quot;) ## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique where red line is the least squares estimates, and its confidence interval. x-axis is the quantile y-axis is the value of the quantile regression coefficients at different quantile If the error term is normally distributed, the quantile regression line will fall inside the coefficient interval of least squares regression. # generate data with constant variance x &lt;- seq(0, 100, length.out = 100) # independent variable b_0 &lt;- 3 # true intercept b_1 &lt;- 0.05 # true slope set.seed(1) # reproducibility e &lt;- rnorm(100, mean = 0, sd = 1) # normal random error with constant variance y &lt;- b_0 + b_1 * x + e # dependent variable dat2 &lt;- data.frame(x, y) qr2 = rq(y ~ x, data = dat2, tau = qs) plot(summary(qr2), parm = &quot;x&quot;) ## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique ## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be nonunique References "],["imputation-missing-data.html", "Chapter 6 Imputation (Missing Data)", " Chapter 6 Imputation (Missing Data) Imputation is usually seen as the illegitimate child of statistical analysis. Several reasons that contribute to this negative views could be: Peopled hardly do imputation correctly Imputation can only be applied to a small range of problems correctly If you have missing data on y (dependent variable), you probability would not be able to do any imputation appropriately. However, if you have certain type of missing data (e.g., non-random missing data) in the xs variable (independent variables), then you can still salvage your collected data points with imputation. We also need to talk why you would want to do imputation in the first place. If your purpose is inference/ explanation (valid statistical inference not optimal point prediction), then imputation would not offer much help (Rubin 1996). However, if your purpose is prediction, you would want your standard error to be reduced by including information (non-missing data) on other variables of a data point. Then imputation could be the tool that youre looking for. For most software packages, it will use listwise deletion or casewise deletion to have complete case analysis (analysis with only observations with all information). Not until recently that statistician can propose some methods that are a bit better than listwise deletion which are maximum likelihood and multiple imputation. References "],["assumptions.html", "6.1 Assumptions", " 6.1 Assumptions 6.1.1 Missing Completely at Random (MCAR) The probability of missing data on a variable is unrelated to the value of it or to the values of any other variables in the data set. Note: the missingness on Y can be correlated with the missingness on X We can compare the value of other variables for the observations with missing data, and observations without missing data. If we reject the t-test for mean difference, we can say there is evidence that the data are not MCAR. But we cannot say that our data are MCAR if we fail to reject the t-test. 6.1.2 Missing at Random (MAR) MAR is weaker than MCAR \\[ P(Y_{missing}|Y,X)= P(Y_{missing}|X) \\] The probability of Y missing given Y and X equal to the probability of of Y missing given X. However, it is impossible to provide evidence to the MAR condition. 6.1.3 Ignorable The missing data mechanism is ignorable when The data are MAR the parameters in the function of the missing data process are unrelated to the parameters (of interest) that need to be estimated. In this case, you actually dont need to model the missing data mechanisms unless you would like to improve on your accuracy, in which case you still need to be very rigorous about your approach to improve efficiency in your parameters. 6.1.4 Nonignorable Hence, in the case of nonignorable, the data are not MAR. Then, your parameters of interest will be biased if you do not model the missing data mechanism. One of the most widely used approach for nonignorable missing data is (Heckman 1976) References "],["solutions-to-missing-data.html", "6.2 Solutions to Missing data", " 6.2 Solutions to Missing data 6.2.1 Listwise Deletion Advantages: Can be applied to any statistical test (SEM, multi-level regression, etc.) In the case of MCAR, both the parameters estimates and its standard errors are unbiased. In the case of MAR among independent variables (not depend on the values of dependent variables), then listwise deletion parameter estimates can still be unbiased. (Little 1992) For example, you have a model \\(y=\\beta_{0}+\\beta_1X_1 + \\beta_2X_2 +\\epsilon\\) if the probability of missing data on X1 is independent of Y, but dependent on the value of X1 and X2, then the model estimates are still unbiased. The missing data mechanism the depends on the values of the independent variables are the same as stratified sampling. And stratified sampling does not bias your estimates In the case of logistic regression, if the probability of missing data on any variable depends on the value of the dependent variable, but independent of the value of the independent variables, then the listwise deletion will yield biased intercept estimate, but consistent estimates of the slope and their standard errors (Vach 1994). However, logistic regression will still fail if the probability of missing data is dependent on both the value of the dependent and independent variables. Under regression analysis, listwise deletion is more robust than maximum likelihood and multiple imputation when MAR assumption is violated. Disadvantages: It will yield a larger standard errors than other more sophisticated methods discussed later. If the data are not MCAR, but MAR, then your listwise deletion can yield biased estimates. In other cases than regression analysis, other sophisticated methods can yield better estimates compared to listwise deletion. 6.2.2 Pairwise Deletion This method could only be used in the case of linear models such as linear regression, factor analysis, or SEM. The premise of this method based on that the coefficient estimates are calculated based on the means, standard deviations, and correlation matrix. Compared to listwise deletion, we still utilized as many correlation between variables as possible to compute the correlation matrix. Advantages: If the true missing data mechanism is MCAR, pair wise deletion will yield consistent estimates, and unbiased in large samples Compared to listwise deletion: (Glasser 1964) If the correlation among variables are low, pairwise deletion is more efficient estimates than listwise If the correlations among variables are high, listwise deletion is more efficient than pairwise. Disadvantages: If the data mechanism is MAR, pairwise deletion will yield biased estimates. In small sample, sometimes covariance matrix might not be positive definite, which means coefficients estimates cannot be calculated. Note: You need to read carefully on how your software specify the sample size because it will alter the standard errors. 6.2.3 Dummy Variable Adjustment Also known as Missing Indicator Method or Proxy Variable Add another variable in the database to indicate whether a value is missing. Create 2 variables \\[\\begin{equation} D= \\begin{cases} 1 &amp; \\text{data on X are missing} \\\\ 0 &amp; \\text{otherwise}\\\\ \\end{cases} \\end{equation}\\] \\[\\begin{equation} X^* = \\begin{cases} X &amp; \\text{data are available} \\\\ c &amp; \\text{data are missing}\\\\ \\end{cases} \\end{equation}\\] Note: A typical choice for c is usually the mean of X Interpretation: Coefficient of D is the the difference in the expected value of Y between the group with data and the group without data on X. Coefficient of X* is the effect of the group with data on Y Disadvantages: This method yields bias estimates of the coefficient even in the case of MCAR (Jones 1996) 6.2.4 Imputation 6.2.4.1 Maximum Likelihood When missing data are MAR and monotonic (such as in the case of panel studies), ML can be adequately in estimating coefficients. Monotonic means that if you are missing data on X1, then that observation also has missing data on all other variables that come after it. ML can generally handle linear models, log-linear model, but beyond that, ML still lacks both theory and software to implement. 6.2.4.1.1 Expectation-Maximization Algorithm (EM Algorithm) An iterative process: Other variables are used to impute a value (Expectation). Check whether the value is most likely (Maximization). If not, it re-imputes a more likely value. You start your regression with your estimates based on either listwise deletion or pairwise deletion. After regressing missing variables on available variables, you obtain a regression model. Plug the missing data back into the original model, with modified variances and covariances For example, if you have missing data on \\(X_{ij}\\) you would regress it on available data of \\(X_{i(j)}\\), then plug the expected value of \\(X_{ij}\\) back with its \\(X_{ij}^2\\) turn into \\(X_{ij}^2 + s_{j(j)}^2\\) where \\(s_{j(j)}^2\\) stands for the residual variance from regressing \\(X_{ij}\\) on \\(X_{i(j)}\\) With the new estimated model, you rerun the process until the estimates converge. Advantages: easy to use preserves the relationship with other variables Disadvantages: Standard errors of the coefficients are incorrect (biased usually downward) Models with overidentification, the estimates will not be efficient. 6.2.4.1.2 Direct ML (raw maximum likelihood) Advantages efficient estimates and correct standard errors. Disadvantages: Hard to implements 6.2.4.2 Multiple Imputation MI is designed to use the Bayesian model-based approach to create procedures, and the frequentist (randomization-based approach) to evaluate procedures. (Rubin 1996) MI estimates have the same properties as ML when the data is MAR Consistent Asymptotically efficient Asymptotically normal MI can be applied to any type of model, unlike Maximum Likelihood that is only limited to a small set of models. A drawback of MI is that it will produce slightly different estimates every time you run it. To avoid such problem, you can set seed when doing your analysis to ensure its reproducibility. 6.2.4.2.1 Single Random Imputaiton Random draws form the residual distribution of each imputed variable and add those random numbers to the imputed values. For example, if we have missing data on X, and its MCAR, then regress X on Y (Listwise Deletion method) to get its residual distribution. For every missing value on X, we substitute with \\(\\tilde{x_i}=\\hat{x_i} + \\rho u_i\\) where \\(u_i\\) is a random draw from a standard normal distribution \\(x_i\\) is the predicted value from the regression of X and Y \\(\\rho\\) is the standard deviation of the residual distribution of X regressed on Y. However, the model you run with the imputed data still thinks that your data are collected, not imputed, which leads your standard error estimates to be too low and test statistics too high. To address this problem, we need to repeat the imputation process which leads us to repeated imputation or multiple random imputation. 6.2.4.2.2 Repeated Imputation Repeated imputations are draws from the posterior predictive distribution of the missing values under a specific model , a particular Bayesian model for both the data and the missing mechanism.(Rubin 1996) Repeated imputation, also known as, multiple random imputation, allows us to have multiple completed data sets. The variability across imputations will adjust the standard errors upward. 6.2.4.3 Mean, Mode, Median Imputation Bad: Mean imputation does not preserve the relationships among variables Mean imputation leads to An Underestimate of Standard Errors  youre making Type I errors without realizing it. Biased estimates of variances and covariances (Haitovsky 1968) 6.2.4.4 Hot Deck Imputation Randomly choose value from other observations that share similar values on other variables. Good: Constrained to only possible values. Since the value is picked at random, it adds some variability, which might come in handy when calculating standard errors. 6.2.4.5 Cold Deck Imputation Contrary to Hot Deck, Cold Deck choose value systematically from an observation that has similar values on other variables, which remove the random variation that we want. 6.2.4.6 Regression Imputation Also known as conditional mean imputation Missing value is based (regress) on other variables. Good: Maintain the relationship with other variables If the data are MCAR, least-squares coefficients estimates will be consistent, and approximately unbiased in large samples (Gourieroux and Monfort 1981) Can have improvement on efficiency by using weighted least squares (Beale and Little 1975) or generalized least squares (Gourieroux and Monfort 1981). Bad: No variability left. treated data as if they were collected. Underestimate the standard errors and overestimate test statistics 6.2.4.7 Stochatistc Imputation Regression imputation + random residual = Stochastic Imputation Most multiple imputation is based off of some form of stochastic regression imputation. 6.2.4.8 Interpolation and Extrapolation An estimated value from other observations from the same individual. It usually only works in longitudinal data. 6.2.4.9 K-nearest neighbour (KNN) imputation The above methods are model-based imputation (regression). This is an example of neighbor-based imputation (K-nearest neighbor). For a discrete variable, it uses the most frequent value among the k nearest neighbors. Distance metrics: Hamming distance. For a continuous variable, it uses the mean or mode. Distance metrics: Euclidean Mahalanobis Manhattan 6.2.4.10 Bayesian Ridge regression implementation 6.2.5 Criteria for Choosing an Effective Approach Criteria for an ideal technique in treating missing data: Unbiased parameter estimates Adequate power Accurate standard errors (p-values, confidence intervals) The Multiple Imputation and Full Information Maximum Likelihood are the the most ideal candidate. Single imputation will generally lead to underestimation of standard errors. 6.2.6 Another Perspective In the imputation world, you can go for either single or multiple imputation. Most of the imputation methods listed above are single imputation. A downfall of single imputation is its underestimation towards standard error. Model bias can arisen from various factors including: Imputation method Missing data mechanism (MCAR vs. MAR) Proportion of the missing data Information available in the data set Since the imputed observations are themselves estimates, their values have corresponding random error. But when you put in that estimate as a data point, your software doesnt know that. So it overlooks the extra source of error, resulting in too-small standard errors and too-small p-values. So multiple imputation comes up with multiple estimates. Two of the methods listed above work as the imputation method in multiple imputationhot deck and stochastic regression. Because these two methods have a random component, the multiple estimates are slightly different. This re-introduces some variation that your software can incorporate in order to give your model accurate estimates of standard error. Multiple imputation was a huge breakthrough in statistics about 20 years ago. It solves a lot of problems with missing data (though, unfortunately not all) and if done well, leads to unbiased parameter estimates and accurate standard errors. If your rate of missing data is very, very small, it honestly doesnt matter what technique you use. Im talking very, very, very small (2-3%). Missing Data Mechanisms (1) Missing Completely at Random (MCAR) - the propensity for a data point to be missing is completely random. - Theres no relationship between whether a data point is missing and any values in the data set, missing or observed. - The missing data are just a random subset of the data. (2) Missing at Random. (MAR) - the propensity for a data point to be missing is not related to the missing data, but it is related to some of the observed data. In another word, there is a systematic relationship between the propensity of missing values and the observed data, but not the missing data. - For example, if men are more likely to tell you their weight than women, weight is MAR - MAR requires that the cause of the missing data is unrelated to the missing values but may be related to the observed values of other variables. - MAR means that the missing values are related to observed values on other variables. As an example of CD missing data, missing income data may be unrelated to the actual income values but are related to education. Perhaps people with more education are less likely to reveal their income than those with less education (3) Non-Ignorable (NI) - Another name: Missing Not at Random (MNAR): there is a relationship between the propensity of a value to be missing and its values - For example, people with low education will be less likely to report it. - We need to model why the data are missing and what the likely values are. - the missing data mechanism is related to the missing values - It commonly occurs when people do not want to reveal something very personal or unpopular about themselves - Complete case analysis can give highly biased results for NI missing data. If proportionally more low and moderate income individuals are left in the sample because high income people are missing, an estimate of the mean income will be lower than the actual population mean. Remember that there are three goals of multiple imputation, or any missing data technique: Unbiased parameter estimates in the final analysis (regression coefficients, group means, odds ratios, etc.); accurate standard errors of those parameter estimates, and therefore, accurate p-values in the analysis; and adequate power to find meaningful parameter values significant. 1. Dont round off imputations for dummy variables. Many common imputation techniques, like MCMC, require normally distributed variables. Suggestions for imputing categorical variables were to dummy code them, impute them, then round off imputed values to 0 or 1. Recent research, however, has found that rounding off imputed values actually leads to biased parameter estimates in the analysis model. You actually get better results by leaving the imputed values at impossible values, even though its counter-intuitive. Dont transform skewed variables. Likewise, when you transform a variable to meet normality assumptions before imputing, you not only are changing the distribution of that variable but the relationship between that variable and the others you use to impute. Doing so can lead to imputing outliers, creating more bias than just imputing the skewed variable. Use more imputations. The advice for years has been that 5-10 imputations are adequate. And while this is true for unbiasedness, you can get inconsistent results if you run the multiple imputation more than once. Bodner (2008) recommends having as many imputations as the percentage of missing data. Since running more imputations isnt any more work for the data analyst, theres no reason not to. Bodner, T. E. 2008. What Improves with Increased Missing Data Imputations? Structural Equation Modeling 15(4):65175. Create multiplicative terms before imputing. When the analysis model contains a multiplicative term, like an interaction term or a quadratic, create the multiplicative terms first, then impute. Imputing first, and then creating the multiplicative terms actually biases the regression parameters of the multiplicative term (von Hippel, 2009). von Hippel, P.T. (2009). How To Impute Squares, Interactions, and Other Transformed Variables. Sociological Methodology 39. Alternatives to multiple imputation arent usually better. Multiple imputation assumes the data are missing at random. In most tests, if an assumption is not met, there are better alternativesa nonparametric test or an alternative type of model. This is often not true with missing data. Alternatives like listwise deletion (a.k.a. ignoring it) have more stringent assumptions. So do nonignorable missing data techniques like Heckmans selection models. Do not use these ad-hoc: Pairwise Deletion: use the available data for each part of an analysis. This has been shown to result in correlations beyond the 0,1 range and other fun statistical impossibilities. Mean Imputation: substitute the mean of the observed values for all missing data. There are so many problems, its difficult to list them all, but suffice it to say, this technique never meets the above 3 criteria. Dummy Variable: create a dummy variable that indicates whether a data point is missing, then substitute any arbitrary value for the missing data in the original variable. Use both variables in the analysis. While it does help the loss of power, it usually leads to biased results. Multiple Imputation of categorical variables is bad Paul Allison, one of my favorite authors of statistical information for researchers, did a study that showed that the most common method actually gives worse results that listwise deletion. (Did I mention Ive used it myself?) What is the bad method? 1. Dummy code the variable 2. Impute a continuous value. This will generally be between 0 and 1. 3. Round off to either 0 or 1, based on whether the imputed value is below or above .5. As Allison discovered, this method generally leads to biased results, and incorrect standard errors What to do instead? Allison compared this approach to four others, each of which generally gave more accurate results, at least under some conditions. 1. Listwise deletion 2. Imputation of the continuous variable without rounding (just leave off step 3). 3. Logistic Regression imputation 4. Discriminant Analysis imputation These last two generally performed best, but only work in limited situations. How to Diagnose the Missing Data Mechanism by Karen Grace-Martin 4 Comments One important consideration in choosing a missing data approach is the missing data mechanismdifferent approaches have different assumptions about the mechanism. Each of the three mechanisms describes one possible relationship between the propensity of data to be missing and values of the data, both missing and observed. The Missing Data Mechanisms Missing Completely at Random, MCAR, means there is no relationship between the missingness of the data and any values, observed or missing. Those missing data points are a random subset of the data. There is nothing systematic going on that makes some data more likely to be missing than others. Missing at Random, MAR, means there is a systematic relationship between the propensity of missing values and the observed data, but not the missing data. Whether an observation is missing has nothing to do with the missing values, but it does have to do with the values of an individuals observed variables. So, for example, if men are more likely to tell you their weight than women, weight is MAR. Missing Not at Random, MNAR, means there is a relationship between the propensity of a value to be missing and its values. This is a case where the people with the lowest education are missing on education or the sickest people are most likely to drop out of the study. MNAR is called non-ignorable because the missing data mechanism itself has to be modeled as you deal with the missing data. You have to include some model for why the data are missing and what the likely values are. Missing Completely at Random and Missing at Random are both considered ignorable because we dont have to include any information about the missing data itself when we deal with the missing data. Why you need to know the mechanism you have Multiple imputation and Maximum Likelihood assume the data are at least missing at random. So the important distinction here is whether the data are MAR as opposed to MNAR. Listwise deletion, however, requires the data are MCAR in order to not introduce bias in the results. As long as the distribution and percentage of missing data is no so great that it negatively affects power, listwise deletion can be a good choice for MCAR missing data. So the important distinction here is whether the data are MCAR as opposed to MAR. Keep in mind that in most data sets, more than one variable will have missing data, and they may not all have the same mechanism. Its worthwhile diagnosing the mechanism for each variable with missing data before choosing an approach. I use the term diagnosing rather than testing, because youre not going to get a straight answer without knowing the values of the missing data. Of course, if you knew those, you wouldnt be doing any of this. Its like checking for multicollinearity or testing assumptions. Each piece of information tells you something, but there is no definitive answer. You have to get at the mechanism in a number of ways and then decide if making the assumption about the mechanism is reasonable. 6.2.6.0.1 Diagnosing the Mechanism MAR vs. MNAR The only true way to distinguish between MNAR and MAR is to measure some of that missing data. Its a common practice among professional surveyors to, for example, follow-up on a paper survey with phone calls to a group of the non-respondents and ask a few key survey items. This allows you to compare respondents to non-respondents. If their responses on those key items differ by very much, thats good evidence that the data are MNAR. However in most missing data situations, we dont have the luxury of getting a hold of the missing data. So while we cant test it directly, we can examine patterns in the data get an idea of whats the most likely mechanism. The first thing in diagnosing randomness of the missing data is to use your substantive scientific knowledge of the data and your field. The more sensitive the issue, the less likely people are to tell you. Theyre not going to tell you as much about their cocaine usage as they are about their phone usage. Likewise, many fields have common research situations in which non-ignorable data is common. Educate yourself in your fields literature. MCAR vs. MAR There is a very useful test for MCAR, Littles test. A second technique is to create dummy variables for whether a variable is missing. 1 = missing 0 = observed You can then run t-tests and chi-square tests between this variable and other variables in the data set to see if the missingness on this variable is related to the values of other variables. For example, if women really are less likely to tell you their weight than men, a chi-square test will tell you that the percentage of missing data on the weight variable is higher for women than men. Judging the quality of missing data procedures by their ability to recreate the individual missing values (according to hit rate, mean square error, etc) does not lead to choosing procedures that result in valid inference, (Rubin 1996) References "],["experimental-design.html", "Chapter 7 Experimental Design ", " Chapter 7 Experimental Design "],["analysis-of-variance-anova.html", "7.1 Analysis of Variance (ANOVA)", " 7.1 Analysis of Variance (ANOVA) ANOVA is using the same underlying mechanism as linear regression. However, the angle that ANOVA chooses to look at is slightly different from the traditional linear regression. It can be more useful in the case with qualitative variables and designed experiments. Experimental Design Factor: explanatory or predictor variable to be studied in an investigation Treatment (or Factor Level): value of a factor applied to the experimental unit Experimental Unit: person, animal, piece of material, etc. that is subjected to treatment(s) and provides a response Single Factor Experiment: one explanatory variable considered Multifactor Experiment: more than one explanatory variable Classification Factor: A factor that is not under the control of the experimenter (observational data) Experimental Factor: assigned by the experimenter Basics of experimental design: Choices that a statistician has to make: set of treatments set of experimental units treatment assignment (selection bias) measurement (measurement bias, blind experiments) Advancements in experimental design: Factorial Experiments: consider multiple factors at the same time (interaction) Replication: repetition of experiment assess mean squared error control over precision of experiment (power) Randomization Before R.A. Fisher (1900s), treatments were assigned systematically or subjectively randomization: assign treatments to experimental units at random, which averages out systematic effects that cannot be control by the investigator Local control: Blocking or Stratification Reduce experimental errors and increase power by placing restrictions on the randomization of treatments to experimental units. Randomization may also eliminate correlations due to time and space. 7.1.1 Completely Randomized Design (CRD) Treatment factor A with \\(a\\ge2\\) treatments levels. Experimental units are randomly assinged to each treatment. The number of experiemntal units in each group can be equal (balanced): n unequal (unbalanced): \\(n_i\\) for the i-th group (i = 1,,a). The total sample size is \\(N=\\sum_{i=1}^{a}n_i\\) Possible assignments of units to treatments are \\(k=\\frac{N!}{n_1!n_2!...n_a!}\\) Each has probability 1/k of being selected. Each experimental unit is measured with a response \\(Y_{ij}\\), in which j denotes unit and i denotes treatment. Treatment 1 2  a \\(Y_{11}\\) \\(Y_{21}\\)  \\(Y_{a1}\\) \\(Y_{12}\\)        Sample Mean \\(\\bar{Y_{1.}}\\) \\(\\bar{Y_{2.}}\\)  \\(\\bar{Y_{a.}}\\) Sample SD \\(s_1\\) \\(s_2\\)  \\(s_a\\) where \\(\\bar{Y_{i.}}=\\frac{1}{n_i}\\sum_{j=1}^{n_i}Y_{ij}\\) \\(s_i^2=\\frac{1}{n_i-1}\\sum_{j=1}^{n_i}(Y_{ij}-\\bar{Y_i})^2\\) And the grand mean is \\(\\bar{Y_{..}}=\\frac{1}{N}\\sum_{i}\\sum_{j}Y_{ij}\\) 7.1.1.1 Single Factor (One-Way) ANOVA Partitioning the Variance The total variability of the \\(Y_{ij}\\) observation can be measured as the deviation of \\(Y_{ij}\\) around the overall mean \\(\\bar{Y_{..}}\\): \\(Y_{ij} - \\bar{Y_{..}}\\) This can be rewritten as: \\[ \\begin{split} Y_{ij} - \\bar{Y_{..}}&amp;=Y_{ij} - \\bar{Y_{..}} + \\bar{Y_{i.}} - \\bar{Y_{i.}} \\\\ &amp;= (\\bar{Y_{i.}}-\\bar{Y_{..}})+(Y_{ij}-\\bar{Y_{i.}}) \\end{split} \\] where the first term is the between treatment differences (i.e., the deviation of the treatment mean from the overall mean) the second term is within treatment differences (i.e., the deviation of the observation around its treatment mean) \\[ \\begin{split} \\sum_{i}\\sum_{j}(Y_{ij} - \\bar{Y_{..}})^2 &amp;= \\sum_{i}n_i(\\bar{Y_{i.}}-\\bar{Y_{..}})^2+\\sum_{i}\\sum_{j}(Y_{ij}-\\bar{Y_{i.}})^2 \\\\ SSTO &amp;= SSTR + SSE \\\\ total~SS &amp;= treatment~SS + error~SS \\\\ (N-1)~d.f. &amp;= (a-1)~d.f. + (N - a) ~ d.f. \\end{split} \\] we lose a d.f. for the total corrected SSTO because of the estimation of the mean (\\(\\sum_{i}\\sum_{j}(Y_{ij} - \\bar{Y_{..}})=0\\)) And, for the SSTR \\(\\sum_{i}n_i(\\bar{Y_{i.}}-\\bar{Y_{..}})=0\\) Accordingly, \\(MSTR= \\frac{SST}{a-1}\\) and \\(MSR=\\frac{SSE}{N-a}\\) ANOVA table Source of Variation SS df MS Between Treatments \\(\\sum_{i}n_i(\\bar{Y_{i.}}-\\bar{Y_{..}})^2\\) a-1 SSTR/(a-1) Error (within treatments) \\(\\sum_{i}\\sum_{j}(Y_{ij}-\\bar{Y_{i.}})^2\\) N-a SSE/(N-a) Total (corrected) \\(\\sum_{i}n_i(\\bar{Y_{i.}}-\\bar{Y_{..}})^2\\) N-1 Linear Model Explanation of ANOVA 7.1.1.1.1 Cell means model \\(Y_{ij}=\\mu_i+\\epsilon_{ij}\\) where * \\(Y_{ij}\\) response variable in j-th subject for the i-th treatment * \\(\\mu_i\\): parameters (fixed) representing the unknown population mean for the i-th treatment * \\(\\epsilon_{ij}\\) independent \\(N(0,\\sigma^2)\\) errors \\(E(Y_{ij})=\\mu_i\\) \\(var(Y_{ij})=var(\\epsilon_{ij})=\\sigma^2\\) All observations have the same variance Example: a = 3 (3 treatments) \\(n_1=n_2=n_3=2\\) \\[ \\begin{split} \\left(\\begin{array}{c} Y_{11}\\\\ Y_{12}\\\\ Y_{21}\\\\ Y_{22}\\\\ Y_{31}\\\\ Y_{32}\\\\ \\end{array}\\right) &amp;= \\left(\\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\end{array}\\right) \\left(\\begin{array}{c} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\\\ \\end{array}\\right) + \\left(\\begin{array}{c} \\epsilon_{11} \\\\ \\epsilon_{12} \\\\ \\epsilon_{21} \\\\ \\epsilon_{22} \\\\ \\epsilon_{31} \\\\ \\epsilon_{32} \\\\ \\end{array}\\right)\\\\ \\mathbf{y} &amp;= \\mathbf{X\\beta} +\\mathbf{\\epsilon} \\end{split} \\] \\(X_{k,ij}=1\\) if the k-th treatment is used \\(X_{k,ij}=0\\) Otherwise Note: no intercept term. \\[\\begin{equation} \\begin{split} \\mathbf{b}= \\left[\\begin{array}{c} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\\\ \\end{array}\\right] &amp;= (\\mathbf{x}&#39;\\mathbf{x})^{-1}\\mathbf{x}&#39;\\mathbf{y} \\\\ &amp; = \\left[\\begin{array}{ccc} n_1 &amp; 0 &amp; 0\\\\ 0 &amp; n_2 &amp; 0\\\\ 0 &amp; 0 &amp; n_3 \\\\ \\end{array}\\right]^{-1} \\left[\\begin{array}{c} Y_1\\\\ Y_2\\\\ Y_3\\\\ \\end{array}\\right] \\\\ &amp; = \\left[\\begin{array}{c} \\bar{Y_1}\\\\ \\bar{Y_2}\\\\ \\bar{Y_3}\\\\ \\end{array}\\right] \\end{split} \\tag{7.1} \\end{equation}\\] is the BLUE (best linear unbiased estimator) for \\(\\beta=[\\mu_1 \\mu_2\\mu_3]&#39;\\) \\[E(\\mathbf{b})=\\beta\\] \\[ var(\\mathbf{b})=\\sigma^2(\\mathbf{X&#39;X})^{-1}=\\sigma^2 \\left[\\begin{array}{ccc} 1/n_1 &amp; 0 &amp; 0\\\\ 0 &amp; 1/n_2 &amp; 0\\\\ 0 &amp; 0 &amp; 1/n_3\\\\ \\end{array}\\right] \\] \\(var(b_i)=var(\\hat{\\mu_i})=\\sigma^2/n_i\\) where \\(\\mathbf{b} \\sim N(\\beta,\\sigma^2(\\mathbf{X&#39;X})^{-1})\\) \\[ \\begin{split} MSE &amp;= \\frac{1}{N-a} \\sum_{i}\\sum_{j}(Y_{ij}-\\bar{Y_{i.}})^2 \\\\ &amp;= \\frac{1}{N-a} \\sum_{i}[(n_i-1)\\frac{\\sum_{i}(Y_{ij}-\\bar{Y_{i.}})^2}{n_i-1}] \\\\ &amp;= \\frac{1}{N-a} \\sum_{i}(n_i-1)s_1^2 \\end{split} \\] We have \\(E(s_i^2)=\\sigma^2\\) \\(E(MSE)=\\frac{1}{N-a}\\sum_{i}(n_i-1)\\sigma^2=\\sigma^2\\) Hence, MSE is an unbiased estimator of \\(\\sigma^2\\), regardless of whether the treatment means are equal or not. \\(E(MSTR)=\\sigma^2+\\frac{\\sum_{i}n_i(\\mu_i-\\mu_.)^2}{a-1}\\) where \\(\\mu_.=\\frac{\\sum_{i=1}^{a}n_i\\mu_i}{\\sum_{i=1}^{a}n_i}\\) If all treatment means are equals (=\\(\\mu_.\\)), \\(E(MSTR)=\\sigma^2\\). Then we can use an F-test for teh equality of all treatment means: \\[H_0:\\mu_1=\\mu_2=..=\\mu_a\\] \\[H_a: not~al l~ \\mu_i ~ are ~ equal \\] \\(F=\\frac{MSTR}{MSE}\\) where large values of F support \\(H_a\\) (since MSTR will tend to exceed MSE when \\(H_a\\) holds) and F near 1 support \\(H_0\\) (upper tail test) Equivalently, when \\(H_0\\) is true, \\(F \\sim f_{(a-1,N-a)}\\) If \\(F \\leq f_{(a-1,N-a;1-\\alpha)}\\), we cannot reject \\(H_0\\) If \\(F \\geq f_{(a-1,N-a;1-\\alpha)}\\), we reject \\(H_0\\) Note: If a = 2 (2 treatments), F-test = two sample t-test 7.1.1.1.2 Treatment Effects (Factor Effects) Besides Cell means model, we have another way to formalize one-way ANOVA: \\[Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\] where \\(Y_{ij}\\) is the j-th response for the i-th treatment \\(\\tau_i\\) i-th treatment effect \\(\\mu\\) constant component, common to all observations \\(\\epsilon_{ij}\\) independent random errors ~ \\(N(0,\\sigma^2)\\) For example, a = 3, \\(n_1=n_2=n_3=2\\) \\[\\begin{equation} \\begin{split} \\left(\\begin{array}{c} Y_{11}\\\\ Y_{12}\\\\ Y_{21}\\\\ Y_{22}\\\\ Y_{31}\\\\ Y_{32}\\\\ \\end{array}\\right) &amp;= \\left(\\begin{array}{cccc} 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{array}\\right) \\left(\\begin{array}{c} \\mu \\\\ \\tau_1 \\\\ \\tau_2 \\\\ \\tau_3\\\\ \\end{array}\\right) + \\left(\\begin{array}{c} \\epsilon_{11} \\\\ \\epsilon_{12} \\\\ \\epsilon_{21} \\\\ \\epsilon_{22} \\\\ \\epsilon_{31} \\\\ \\epsilon_{32} \\\\ \\end{array}\\right)\\\\ \\mathbf{y} &amp;= \\mathbf{X\\beta} +\\mathbf{\\epsilon} \\end{split} \\tag{7.2} \\end{equation}\\] However, \\[ \\mathbf{X&#39;X} = \\left(\\begin{array}{cccc} \\sum_{i}n_i &amp; n_1 &amp; n_2 &amp; n_3 \\\\ n_1 &amp; n_1 &amp; 0 &amp; 0 \\\\ n_2 &amp; 0 &amp; n_2 &amp; 0 \\\\ n_3 &amp; 0 &amp; 0 &amp; n_3 \\\\ \\end{array}\\right) \\] is singular thus does not exist, \\(\\mathbf{b}\\) is insolvable (infinite solutions) Hence, we have to impose restrictions on the parameters to a model matrix \\(\\mathbf{X}\\) of full rank. Whatever restriction we use, we still have: \\(E(Y_{ij})=\\mu + \\tau_i = \\mu_i = mean ~ response ~ for ~ i-th ~ treatment\\) 7.1.1.1.2.1 Restriction on sum of tau \\(\\sum_{i=1}^{a}\\tau_i=0\\) implies \\[ \\mu= \\mu +\\frac{1}{a}\\sum_{i=1}^{a}(\\mu+\\tau_i) \\] is the average of the treatment mean (grand mean) (overall mean) \\[ \\begin{split} \\tau_i &amp;=(\\mu+\\tau_i) -\\mu = \\mu_i-\\mu \\\\ &amp;= (treatment ~ mean) -(grand~mean) \\\\ &amp;= treatment ~ effect \\end{split} \\] \\[ \\tau_a=-\\tau_1-\\tau_2-...-\\tau_{a-1} \\] Hence, the mean for the a-th treatment is \\[ \\mu_a=\\mu+\\tau_a=\\mu-\\tau_1-\\tau_2-...-\\tau_{a-1} \\] Hence, the model need only a parameters: \\[ \\mu,\\tau_1,\\tau_2,..,\\tau_{a-1} \\] Equation \\tag{7.2} becomes \\[\\begin{equation} \\begin{split} \\left(\\begin{array}{c} Y_{11}\\\\ Y_{12}\\\\ Y_{21}\\\\ Y_{22}\\\\ Y_{31}\\\\ Y_{32}\\\\ \\end{array}\\right) &amp;= \\left(\\begin{array}{cccc} 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; -1 &amp; -1 \\\\ 1 &amp; -1 &amp; -1 \\\\ \\end{array}\\right) \\left(\\begin{array}{c} \\mu \\\\ \\tau_1 \\\\ \\tau_2 \\\\ \\end{array}\\right) + \\left(\\begin{array}{c} \\epsilon_{11} \\\\ \\epsilon_{12} \\\\ \\epsilon_{21} \\\\ \\epsilon_{22} \\\\ \\epsilon_{31} \\\\ \\epsilon_{32} \\\\ \\end{array}\\right)\\\\ \\mathbf{y} &amp;= \\mathbf{X\\beta} +\\mathbf{\\epsilon} \\end{split} \\end{equation}\\] where \\(\\beta\\equiv[\\mu,\\tau_1,\\tau_2]&#39;\\) Equation (7.1) with \\(\\sum_{i}\\tau_i=0\\) becomes \\[\\begin{equation} \\begin{split} \\mathbf{b}= \\left[\\begin{array}{c} \\hat{\\mu} \\\\ \\hat{\\tau_1} \\\\ \\hat{\\tau_2} \\\\ \\end{array}\\right] &amp;= (\\mathbf{x}&#39;\\mathbf{x})^{-1}\\mathbf{x}&#39;\\mathbf{y} \\\\ &amp; = \\left[\\begin{array}{ccc} \\sum_{i}n_i &amp; n_1-n_3 &amp; n_2-n_3\\\\ n_1-n_3 &amp; n_1+n_3 &amp; n_3\\\\ n_2-n_3 &amp; n_3 &amp; n_2-n_3 \\\\ \\end{array}\\right]^{-1} \\left[\\begin{array}{c} Y_{..}\\\\ Y_{1.}-Y_{3.}\\\\ Y_{2.}-Y_{3.}\\\\ \\end{array}\\right] \\\\ &amp; = \\left[\\begin{array}{c} \\frac{1}{3}\\sum_{i=1}^{3}\\bar{Y_{i.}}\\\\ \\bar{Y_{1.}}-\\frac{1}{3}\\sum_{i=1}^{3}\\bar{Y_{i.}}\\\\ \\bar{Y_{2.}}-\\frac{1}{3}\\sum_{i=1}^{3}\\bar{Y_{i.}}\\\\ \\end{array}\\right]\\\\ &amp; = \\left[\\begin{array}{c} \\hat{\\mu}\\\\ \\hat{\\tau_1}\\\\ \\hat{\\tau_2}\\\\ \\end{array}\\right] \\end{split} \\end{equation}\\] and \\(\\hat{\\tau_3}=-\\hat{\\tau_1}-\\hat{\\tau_2}=\\bar{Y_3}-\\frac{1}{3} \\sum_{i}\\bar{Y_{i.}}\\) In R, lm() uses the restriction \\(\\tau_1=0\\) 7.1.1.2 Two Factor Fixed Effect ANOVA 7.1.1.2.1 Balanced "],["deep-learning.html", "Chapter 8 Deep Learning ", " Chapter 8 Deep Learning "],["overview.html", "8.1 Overview", " 8.1 Overview This section is based on (Allaire 2018) What is deep learning? Input to output via layers of representation What are layers? A layer is a geometric transformation function on the data that goes through it Weights determine the data transformation behavior of a layer Learning Representation Transforming input data into useful representation The deep in deep learning multiple layers Other possibly more appropriate names for the field: Layered representations learning Hierarchical representations learning Chained geometric transformation learning New problem domains for R: Computer vision Computer speech recognition Reinforcement learning applications How do we train deep learning models? Basic of machine learning algorithms Machine learning vs. statistical modeling MNIST example Model definition in R Layers of representation The training loop Machine learning algorithms Learning model parameters via exposure to many example data points Statistics: Often focused on inferring the process by which data is generated Machine Learning: Principally focused on predicting future data Deep learning frontiers Computer vision Natural language processing Time series Biomedical References "],["tensor-flow-and-r.html", "8.2 Tensor Flow and R", " 8.2 Tensor Flow and R TensorFlow APIs Keras API Estimator API Core API 8.2.1 R packages TensorFlow APIs * keras: Interface for neural networks, with a focus on enabling fast experimentation. * tfestimators: Implementations of common model types such as regressors and classifiers. * tensorflow: Low level interface to the TensorFlow computational graph. tfdatasets: Scalable input pipelines for TensorFlow models. Supporting Tools * tfruns : Track , visualize, and manage TensorFlow training runs and experiments * tfdeploy : Tools designed to make exporting and serving TensorFlow models straightforward. * cloudml : R interface to Google Cloud Machine Learning Engine. R interface to Keras Step by step example Keras layers Compiling models Losses, Optimizers, and Metrics More examples 8.2.2 Keras layers 65 layers available Dense layers: classic fully connected neural network layers Convolutional layers: \"Filters for learning local patterns in data Recurrent layers: Layers that maintain state based on on previously seen data Embedding layers: Vectorization of text that reflects semantic relationships between words "],["compiling-models.html", "8.3 Compiling models", " 8.3 Compiling models Model compilation prepares the model for training by: 1. Converting the layers into a TensorFlow graph 2. Applying the specified loss function and optimizer 3. Arranging for the collection of metrics during training "],["losses-optimizers-and-metrics.html", "8.4 Losses, Optimizers, and Metrics", " 8.4 Losses, Optimizers, and Metrics All available at Keras for R cheatsheet Example of Image classificaiton "],["research-methods.html", "Chapter 9 Research Methods ", " Chapter 9 Research Methods "],["but-for-world.html", "9.1 But-for World", " 9.1 But-for World (Mahajan, Sharma, and Buzzell 1993) (Landsman and Stremersch 2020) References "],["causality.html", "Chapter 10 Causality", " Chapter 10 Causality After all of the mumbo jumbo that we have learned so far, I want to now talk about the concept of causality. We usually say that correlation is not causation. Then, what is causation? One of my favorite books has explained this concept beautifully (Mackenzie and Pearl 2018). And I am just going to quickly summarize the gist of it from my understanding. I hope that it can give you an initial grasp on the concept so that later you can continue to read up and develop a deeper understanding. Its important to have a deep understanding regarding the method research. However, one needs to be aware of its limitation and compliment with conceptual understanding. The aspect of concepts is typically referred in statistics when as expert knowledge. As mentioned in various sections throughout the book, we see that we need to ask experts for number as our baseline or visit literature to gain insight from past research. Here, we dive in a more conceptual side statistical analysis as a whole, regardless of particular approach. References "],["appendix.html", "Chapter 11 Appendix ", " Chapter 11 Appendix "],["short-cut.html", "11.1 Short-cut", " 11.1 Short-cut These are shortcuts that you probably you remember when working with R. Even though it might take a bit of time to learn and use them as your second nature, but they will save you a lot of time. Just like learning another language, the more you speak and practice it, the more comfortable you are speaking it. function short-cut navigate folders in console \" \" + tab pull up short-cut cheat sheet ctrl + shift + k go to file/function (everything in your project) ctrl + . search everything cmd + shift + f navigate between tabs Crtl + shift + . type function faster snip + shift + tab type faster use tab for fuzzy match cmd + up ctrl + . Sometimes you cant stage a folder because its too large. In such case, use Terminal pane in Rstudio then type git add -A to stage all changes then commit and push like usual. "],["function-short-cut.html", "11.2 Function short-cut", " 11.2 Function short-cut apply one function to your data to create a new variable: mutate(mod=map(data,function)) instead of using i in 1:length(object): for (i in seq_along(object)) apply multiple function: map_dbl apply multiple function to multiple variables:map2 autoplot(data) plot times series data mod_tidy = linear(reg) %&gt;% set_engine('lm') %&gt;% fit(price ~ ., data=data) fit lm model. It could also fit other models (stan, spark, glmnet, keras) Sometimes, data-masking will not be able to recognize whether youre calling from environment or data variables. To bypass this, we use .data$variable or .env$variable. For example data %&gt;% mutate(x=.env$variable/.data$variable Problems with data-masking: + Unexpected masking by data-var: Use .data and .env to disambiguate + Data-var cant get through: + Tunnel data-var with {{}} + Subset .data with [[]] Passing Data-variables through arguments library(&quot;dplyr&quot;) mean_by &lt;- function(data,by,var){ data %&gt;% group_by({{{by}}}) %&gt;% summarise(&quot;{{var}}&quot;:=mean({{var}})) # new name for each var will be created by tunnel data-var inside strings } mean_by &lt;- function(data,by,var){ data %&gt;% group_by({{{by}}}) %&gt;% summarise(&quot;{var}&quot;:=mean({{var}})) # use single {} to glue the string, but hard to reuse code in functions } Trouble with selection: library(&quot;purrr&quot;) name &lt;- c(&quot;mass&quot;,&quot;height&quot;) starwars %&gt;% select(name) # Data-var. Here you are referring to variable named &quot;name&quot; starwars %&gt;% select(all_of((name))) # use all_of() to disambiguate when averages &lt;- function(data,vars){ # take character vectors with all_of() data %&gt;% select(all_of(vars)) %&gt;% map_dbl(mean,na.rm=TRUE) } x = c(&quot;Sepal.Length&quot;,&quot;Petal.Length&quot;) iris %&gt;% averages(x) # Another way averages &lt;- function(data,vars){ # Tunnel selectiosn with {{}} data %&gt;% select({{vars}}) %&gt;% map_dbl(mean,na.rm=TRUE) } x = c(&quot;Sepal.Length&quot;,&quot;Petal.Length&quot;) iris %&gt;% averages(x) "],["citation.html", "11.3 Citation", " 11.3 Citation include a citation by [@Farjam_2015] cite packages used in this session package=ls(sessionInfo()$loadedOnly) for (i in package){print(toBibtex(citation(i)))} package=ls(sessionInfo()$loadedOnly) for (i in package){ print(toBibtex(citation(i))) } "],["bookdown-cheat-sheet.html", "Chapter 12 Bookdown cheat sheet", " Chapter 12 Bookdown cheat sheet Took from professor Jenny to Heres where I park little examples for myself about bookdown mechanics that I keep forgetting. The bookdown book: https://bookdown.org/yihui/bookdown/ "],["math-expresssion-syntax.html", "12.1 Math Expresssion/ Syntax", " 12.1 Math Expresssion/ Syntax Full list Aligning equations \\begin{align*} a &amp; = b \\\\ X &amp;\\sim {\\sf Norm}(10, 3) \\\\ 5 &amp; \\le 10 \\end{align*} \\[\\begin{align*} a &amp; = b \\\\ X &amp;\\sim {\\sf Norm}(10, 3) \\\\ 5 &amp; \\le 10 \\end{align*}\\] Syntax Notation Math $\\pm$ \\(\\pm\\) $\\ge$ \\(\\ge\\) $\\le$ \\(\\le\\) $\\neq$ \\(\\neq\\) $\\equiv$ \\(\\equiv\\) $^\\circ$ \\(^\\circ\\) $\\times$ \\(\\times\\) $\\cdot$ \\(\\cdot\\) $\\leq$ \\(\\leq\\) $\\geq$ \\(\\geq\\) $\\subset$ \\(\\subset\\) $\\subseteq$ \\(\\subseteq\\) $\\leftarrow$ \\(\\leftarrow\\) $\\rightarrow$ \\(\\rightarrow\\) $\\Leftarrow$ \\(\\Leftarrow\\) $\\Rightarrow$ \\(\\Rightarrow\\) $\\approx$ \\(\\approx\\) $\\mathbb{R}$ \\(\\mathbb{R}\\) $\\sum_{n=1}^{10} n^2$ \\(\\sum_{n=1}^{10} n^2\\) $$\\sum_{n=1}^{10} n^2$$ \\[\\sum_{n=1}^{10} n^2\\] $x^{n}$ \\(x^{n}\\) $x_{n}$ \\(x_{n}\\) $\\overline{x}$ \\(\\overline{x}\\) $\\hat{x}$ \\(\\hat{x}\\) $\\tilde{x}$ \\(\\tilde{x}\\) \\underset{\\gamma}{\\operatorname{argmin}} \\(\\underset{\\gamma}{\\operatorname{argmin}}\\) $\\frac{a}{b}$ \\(\\frac{a}{b}\\) $\\frac{a}{b}$ \\(\\frac{a}{b}\\) $\\displaystyle \\frac{a}{b}$ \\(\\displaystyle \\frac{a}{b}\\) $\\binom{n}{k}$ \\(\\binom{n}{k}\\) $x_{1} + x_{2} + \\cdots + x_{n}$ \\(x_{1} + x_{2} + \\cdots + x_{n}\\) $x_{1}, x_{2}, \\dots, x_{n}$ \\(x_{1}, x_{2}, \\dots, x_{n}\\) \\mathbf{x} = \\langle x_{1}, x_{2}, \\dots, x_{n}\\rangle$ \\(\\mathbf{x} = \\langle x_{1}, x_{2}, \\dots, x_{n}\\rangle\\) $x \\in A$ \\(x \\in A\\) $|A|$ \\(|A|\\) $x \\in A$ \\(x \\in A\\) $x \\subset B$ \\(x \\subset B\\) $x \\subseteq B$ \\(x \\subseteq B\\) $A \\cup B$ \\(A \\cup B\\) $A \\cap B$ \\(A \\cap B\\) $X \\sim {\\sf Binom}(n, \\pi)$ \\(X \\sim {\\sf Binom}(n, \\pi)\\) $\\mathrm{P}(X \\le x) = {\\tt pbinom}(x, n, \\pi)$ \\(\\mathrm{P}(X \\le x) = {\\tt pbinom}(x, n, \\pi)\\) $P(A \\mid B)$ \\(P(A \\mid B)\\) $\\mathrm{P}(A \\mid B)$ \\(\\mathrm{P}(A \\mid B)\\) $\\{1, 2, 3\\}$ \\(\\{1, 2, 3\\}\\) $\\sin(x)$ \\(\\sin(x)\\) $\\log(x)$ \\(\\log(x)\\) $\\int_{a}^{b}$ \\(\\int_{a}^{b}\\) $\\left(\\int_{a}^{b} f(x) \\; dx\\right)$ \\(\\left(\\int_{a}^{b} f(x) \\; dx\\right)\\) $\\left[\\int_{\\-infty}^{\\infty} f(x) \\; dx\\right]$ \\(\\left[\\int_{-\\infty}^{\\infty} f(x) \\; dx\\right]\\) $\\left. F(x) \\right|_{a}^{b}$ \\(\\left. F(x) \\right|_{a}^{b}\\) $\\sum_{x = a}^{b} f(x)$ \\(\\sum_{x = a}^{b} f(x)\\) $\\prod_{x = a}^{b} f(x)$ \\(\\prod_{x = a}^{b} f(x)\\) $\\lim_{x \\to \\infty} f(x)$ \\(\\lim_{x \\to \\infty} f(x)\\) $\\displaystyle \\lim_{x \\to \\infty} f(x)$ \\(\\displaystyle \\lim_{x \\to \\infty} f(x)\\) Greek Letters $\\alpha A$ \\(\\alpha A\\) $\\beta B$ \\(\\beta B\\) $\\gamma \\Gamma$ \\(\\gamma \\Gamma\\) $\\delta \\Delta$ \\(\\delta \\Delta\\) $\\epsilon \\varepsilon E$ \\(\\epsilon \\varepsilon E\\) $\\zeta Z \\sigma \\,\\!$ \\(\\zeta Z \\sigma \\,\\!\\) $\\eta H$ \\(\\eta H\\) $\\theta \\vartheta \\Theta$ \\(\\theta \\vartheta \\Theta\\) $\\iota I$ \\(\\iota I\\) $\\kappa K$ \\(\\kappa K\\) $\\lambda \\Lambda$ \\(\\lambda \\Lambda\\) $\\mu M$ \\(\\mu M\\) $\\nu N$ \\(\\nu N\\) $\\xi\\Xi$ \\(\\xi\\Xi\\) $o O$ \\(o O\\) $\\pi \\Pi$ \\(\\pi \\Pi\\) $\\rho\\varrho P$ \\(\\rho\\varrho P\\) $\\sigma \\Sigma$ \\(\\sigma \\Sigma\\) $\\tau T$ \\(\\tau T\\) $\\upsilon \\Upsilon$ \\(\\upsilon \\Upsilon\\) $\\phi \\varphi \\Phi$ \\(\\phi \\varphi \\Phi\\) $\\chi X$ \\(\\chi X\\) $\\psi \\Psi$ \\(\\psi \\Psi\\) $\\omega \\Omega$ \\(\\omega \\Omega\\) Matrices $$\\begin{array} {rrr} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{array} $$ \\[\\begin{array} {rrr} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{array} \\] $$\\mathbf{X} = \\left[\\begin{array} {rrr} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{array}\\right] $$ \\[\\mathbf{X} = \\left[\\begin{array} {rrr} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{array}\\right] \\] Aligning Equations Aligning Equations with Comments \\begin{align} 3+x &amp;=4 &amp;&amp; \\text{(Solve for} x \\text{.)}\\\\ x &amp;=4-3 &amp;&amp; \\text{(Subtract 3 from both sides.)}\\\\ x &amp;=1 &amp;&amp; \\text{(Yielding the solution.)} \\end{align} \\[\\begin{align} 3+x &amp;=4 &amp;&amp; \\text{(Solve for} x \\text{.)}\\\\ x &amp;=4-3 &amp;&amp; \\text{(Subtract 3 from both sides.)}\\\\ x &amp;=1 &amp;&amp; \\text{(Yielding the solution.)} \\end{align}\\] 12.1.1 Statistics Notation $$f(y|N,p) = \\frac{N!}{y!(N-y)!}\\cdot p^y \\cdot (1-p)^{N-y} = {{N}\\choose{y}} \\cdot p^y \\cdot (1-p)^{N-y}$$ \\[f(y|N,p) = \\frac{N!}{y!(N-y)!}\\cdot p^y \\cdot (1-p)^{N-y} = {{N}\\choose{y}} \\cdot p^y \\cdot (1-p)^{N-y}\\] \\begin{cases} \\frac{1}{b-a}&amp;\\text{for $x\\in[a,b]$}\\\\ 0&amp;\\text{otherwise}\\\\ \\end{cases} \\[\\begin{cases} \\frac{1}{b-a}&amp;\\text{for $x\\in[a,b]$}\\\\ 0&amp;\\text{otherwise}\\\\ \\end{cases}\\] "],["table.html", "12.2 Table", " 12.2 Table +---------------+---------------+--------------------+ | Fruit | Price | Advantages | +===============+===============+====================+ | *Bananas* | $1.34 | - built-in wrapper | | | | - bright color | +---------------+---------------+--------------------+ | Oranges | $2.10 | - cures scurvy | | | | - **tasty** | +---------------+---------------+--------------------+ Fruit Price Advantages Bananas $1.34 built-in wrapper bright color Oranges $2.10 cures scurvy tasty (\\mathbf{x}^T\\mathbf{x})^{-1}\\mathbf{x}^T\\mathbf{y} \\((\\mathbf{x}^T\\mathbf{x})^{-1}\\mathbf{x}^T\\mathbf{y}\\) "],["heading.html", "12.3 heading", " 12.3 heading "],["id-example.html", "12.4 About labelling things", " 12.4 About labelling things You can label chapter and section titles using {#label} after them, e.g., we can reference Section 12.4. If you do not manually label them, there will be automatic labels anyway, e.g., this reference to the unlabelled heading 12.3 uses the automatically generated label \\@ref(heading). "],["cross-references.html", "12.5 Cross-references", " 12.5 Cross-references Add an explicit label by adding {#label} to the end of the section header. If you know youre going to refer to something, this is probably a good idea. To refer to in a chapter- or section-number-y way, use \\@ref(label). \\@ref(install-git) example: In chapter 2 If you are happy with the section header as the link text, use it inside a single set of square brackets: [A picture is worth a thousand words]: example A picture is worth a thousand words via [A picture is worth a thousand words] There are two ways to specify custom link text: [link text][Section header text], e.g., pic = 1000 words via [pic = 1000 words][A picture is worth a thousand words] [link text](#label), e.g., RStudio, meet Git via RStudio, meet Git The Pandoc documentation provides more details on automatic section IDs and implicit header references. "],["figures-tables-citations.html", "12.6 Figures, tables, citations", " 12.6 Figures, tables, citations Figures and tables with captions will be placed in figure and table environments, respectively. You can write citations, too. For example, we are using the bookdown package (Xie 2020) in this sample book, which was built on top of R Markdown and knitr (???). References "],["how-the-square-bracket-links-work.html", "12.7 How the square bracket links work", " 12.7 How the square bracket links work There are three ways to address a section when creating links within your book: Explicit identifier: In # My header {#foo} the explicit identifier is foo. Automatically generated identifier: my-header is the auto-identifier for # My header. Pandoc creates auto-identifiers according to rules laid out in Extension: auto_identifiers. The header text, e.g., My header be used verbatim as an implicit header reference. See Extension: implicit_header_references for more. All 3 forms can be used to create cross-references but you build the links differently. Advantage of explicit identification: You are less likely to update the section header and then forget to make matching edits to references elsewhere in the book. How to make text-based links using explicit identifiers, automatic identifiers, and implicit references: Use implicit reference alone to get a link where the text is exactly the section header: [Introduce yourself to Git] [Introduce yourself to Git] [Success and operating systems] [Success and operating systems] You can provide custom text for the link with all 3 methods of addressing a section: Implicit header reference: [link text][Recommended Git clients] [link text][Recommended Git clients] Explicit identifier: [hello git! I'm Jenny](#hello-git) hello git! Im Jenny Automatic identifier: [Any text you want](#recommended-git-clients) Any text you want "],["references.html", "References", " References "]]
