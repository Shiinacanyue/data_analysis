[["intro.html", "A Guide on Data Analysis Chapter 1 Introduction 1.1 Tools of statistics 1.2 Setup Working Environment", " A Guide on Data Analysis Mike Nguyen 2020-11-15 Chapter 1 Introduction This guide is an attempt to streamline and demystify the data analysis process. By no mean this is an ultimate guide, or I am a great source of knowledge, or I claim myself as a statistician/ data analyst/ econometrician (or any fancy name we have now), but I am a strong proponent of learning by teaching, and doing. Hence, this is more like a learning experience for both you and me. Since the beginning of the century, we have been bombarded with amazing advancements and inventions, especially in the field of statistics, information technology, and computer science. However, I believe the downside of this introduction is that we use big and trendy words too often (i.e., big data, machine learning, deep learning). Its all fun and exciting when I learned these new tools. But I have to admit that I hardly retain any of these new inventions.However, writing down from the beginning till the end of a data analysis process is the solution that I came up with. Accordingly, lets dive right in. Some general recommendation: * The more you practice, more line of codes that you write, more function that you memorize, I think the more you will like this journey. * Readers can follow this book several ways: + If you are interested in particular methods/tools, you can jump to that section by clicking the section name. + If you want to follow a traditional path of data analysis, read the [Regression Analysis] section. + If you want to explore new and upcoming method, read the [Advanced Method] section. + If you want to create your experiment and test your hypothesis, read the [Experimental Design] section. If you dont understand a part, search the title of that part of that part on Google, and read more into that subject. This is just a general guide. If you want to customize your code beyond the ones provided in this book, run in the console help(code) or ?code. For example, I want more information on hist function, Ill type in the console ?hist or help(hist). Another way is that you can search on Google. Different people will use different packages to achieve the same result in R. Accordingly, if you want to create a histogram, search on Google histogram in R, then you should be able to find multiple ways to create histogram in R. Information in this book are from various sources, but the skeleton is based on several courses that I have taken formally. Id like to give professors credit accordingly. Course Professor Data Analysis I Erin M. Schliep Applied Econometric Alyssa Carlson Bayesian Sounak Chakraborty 1.1 Tools of statistics Probability Theory Mathematical Analysis Computer Science Numerical Analysis Database Management 1.2 Setup Working Environment if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) ## Loading required package: pacman if (!require(&quot;devtools&quot;)) install.packages(&quot;devtools&quot;) ## Loading required package: devtools ## Warning: package &#39;devtools&#39; was built under R version 4.0.3 ## Loading required package: usethis ## Warning: package &#39;usethis&#39; was built under R version 4.0.3 library(&quot;pacman&quot;) library(&quot;devtools&quot;) "],["descriptive-stat.html", "Chapter 2 Descriptive Statistics 2.1 Numerical Measures 2.2 Graphical Measures 2.3 Normality Assessment", " Chapter 2 Descriptive Statistics When you have an area of interest that you to research, a problem that you want to solve, a relationship that you want to investigate, theoretical and empirical processes will help you. Discrete Variable Continuous Variable \\(E(Y)\\) \\(\\sum_{i=1}^k y_i p_i\\) \\(\\int_{-\\infty}^{\\infty} f(y) dy\\) \\(\\overline{y}\\) \\(\\frac{1}{n} \\sum_{i = 1}^{n} y_i\\) \\(\\frac{1}{n} \\sum_{i = 1}^{n} y_i\\) 2.1 Numerical Measures There is a difference between a population and a sample Measures of Category Population Sample - What is it? Reality A small fraction of reality (inference) - Characteristics described by Parameters Statistics Central Tendency Mean \\(\\mu = E(Y)\\) \\(\\hat{\\mu} = \\overline{y}\\) Central Tendency Median 50-th percentile \\(y_{(\\frac{n+1}{2})}\\) Dispersion Variance \\(\\sigma^2=var(Y)\\) \\(=E(Y-\\mu)^2\\) \\(s^2=\\frac{1}{n-1} \\sum_{i = 1}^{n} (y_i-\\overline{y})^2\\) \\(=\\frac{1}{n-1} \\sum_{i = 1}^{n} (y_i^2-n\\overline{y}^2)\\) Dispersion Coefficient of Variation \\(\\frac{\\sigma}{\\mu}\\) \\(\\frac{s}{\\overline{y}}\\) Dispersion Interquartile Range difference between 25th and 75th percentiles. Robust to outliers Shape Skewness Standardized 3rd central moment (unitless) \\(g_1=\\frac{\\mu_3}{\\mu_2^{3/2}}\\) \\(\\hat{g_1}=\\frac{m_3}{m_2sqrt(m_2)}\\) Shape Central moments \\(\\mu=E(Y)\\) \\(\\mu_2 = \\sigma^2=E(Y-\\mu)^2\\) \\(\\mu_3 = E(Y-\\mu)^3\\) \\(\\mu_4 = E(Y-\\mu)^4\\) \\(m_2=\\sum_{i=1}^{n}(y_1-\\overline{y})^2/n\\) \\(m_3=\\sum_{i=1}^{n}(y_1-\\overline{y})^3/n\\) Shape Kurtosis (peakedness and tail thickness) Standardized 4th central moment \\(g_2^*=\\frac{E(Y-\\mu)^4}{\\sigma^4}\\) \\(\\hat{g_2}=\\frac{m_4}{m_2^2}-3\\) Note: Order Statistics: \\(y_{(1)},y_{(2)},...,y_{(n)}\\) where \\(y_{(1)}&lt;y_{(2)}&lt;...&lt;y_{(n)}\\) Coefficient of variation: standard deviation over mean. This metric is stable, dimensionless statistic for comparison. Symmetric: mean = median, skewness = 0 Skewed right: mean &gt; median, skewness &gt; 0 Skewed left: mean &lt; median, skewness &lt; 0 Central moments: \\(\\mu=E(Y)\\) , \\(\\mu_2 = \\sigma^2=E(Y-\\mu)^2\\) , \\(\\mu_3 = E(Y-\\mu)^3\\), \\(\\mu_4 = E(Y-\\mu)^4\\) For normal distributions, \\(\\mu_3=0\\), so \\(g_1=0\\) \\(\\hat{g_1}\\) is distributed approximately as N(0,6/n) if sample is from a normal population. (valid when n &gt; 150) For large samples, inferece on skewness can be based on normal tables with 95% confidence itnerval for \\(g_1\\) as \\(\\hat{g_1}\\pm1.96\\sqrt{6/n}\\) For small samples, special tables from Snedecor and Cochran 1989, Table A 19(i) or Monte Carlo test Kurtosis &gt; 0 (leptokurtic) heavier tail compared to a normal distribution with the same \\(\\sigma\\) (e.g., t-distribution) Kurtosis &lt; 0 (platykurtic) lighter tail compared to a normal distribution with the same \\(\\sigma\\) For a normal distribution, \\(g_2^*=3\\). Kurtosis is often redefined as: \\(g_2=\\frac{E(Y-\\mu)^4}{\\sigma^4}-3\\) where the 4th central moment is estimated by \\(m_4=\\sum_{i=1}^{n}(y_i-\\overline{y})^4/n\\) the asymptotic sampling distribution for \\(\\hat{g_2}\\) is approximately N(0,24/n) (with n &gt; 1000) large sample on kurtosis uses standard normal tables small sample uses tables by Snedecor and Cochran, 1989, Table A 19(ii) or Geary 1936 2.2 Graphical Measures 2.2.1 Shape Its a good habit to label your graph, so others can easily follow. data = rnorm(100) # Histogram hist(data,labels = T,col=&quot;grey&quot;,breaks = 12) # Interactive histogram pacman::p_load(&quot;highcharter&quot;) hchart(data) # Box-and-Whisker plot boxplot(count ~ spray, data = InsectSprays,col = &quot;lightgray&quot;,main=&quot;boxplot&quot;) # Notched Boxplot boxplot(len~supp*dose, data=ToothGrowth, notch=TRUE, col=(c(&quot;gold&quot;,&quot;darkgreen&quot;)), main=&quot;Tooth Growth&quot;, xlab=&quot;Suppliment and Dose&quot;) ## Warning in bxp(list(stats = structure(c(8.2, 9.7, 12.25, 16.5, 21.5, 4.2, : some ## notches went outside hinges (&#39;box&#39;): maybe set notch=FALSE # If notches differ -&gt; medians differ # Stem-and-Leaf Plots stem(data) ## ## The decimal point is at the | ## ## -2 | 10 ## -1 | 977655 ## -1 | 4432222000 ## -0 | 9999988888865555 ## -0 | 4444433333333221111000 ## 0 | 01111222223334 ## 0 | 55566777788 ## 1 | 00111222222233 ## 1 | 6679 ## 2 | 4 # Bagplot - A 2D Boxplot Extension pacman::p_load(aplpack) attach(mtcars) bagplot(wt,mpg, xlab=&quot;Car Weight&quot;, ylab=&quot;Miles Per Gallon&quot;, main=&quot;Bagplot Example&quot;) Others more advanced plots # boxplot.matrix() #library(&quot;sfsmisc&quot;) # boxplot.n() #library(&quot;gplots&quot;) # vioplot() #library(&quot;vioplot&quot;) 2.3 Normality Assessment Since Normal (Gaussian) distribution has many applications, we typically want/ wish our data or our variable is normal. Hence, we have to assess the normality based on not only Numerical Measures but also Graphical Measures 2.3.1 Graphical Assessment pacman::p_load(&quot;car&quot;) qqnorm(precip, ylab = &quot;Precipitation [in/yr] for 70 US cities&quot;) qqline(precip) The straight line represents the theoretical line for normally distributed data. The dots represent real empirical data that we are checking. If all the dots fall on the straight line, we can be confident that our data follow a normal distribution. If our data wiggle and deviate from the line, we should be concerned with the normality assumption. 2.3.2 Summary Statistics Sometimes its hard to tell whether your data follow the normal distribution by just looking at the graph. Hence, we often have to conduct statistical test to aid our decision. Common tests are Methods based on normal probability plot Correlation Coefficient with Normal Probability Plots Shapiro-Wilk Test Methods based on empirical cumulative distribution function Anderson-Darling Test Kolmogorov-Smirnov Test Cramer-von Mises Test 2.3.2.1 Methods based on normal probability plot 2.3.2.1.1 Correlation Coefficient with Normal Probability Plots (Looney and Gulledge 1985) (Shapiro and Francia 1972) The correlation coefficient between \\(y_{(i)}\\) and \\(m_i^*\\) as given on the normal probability plot: \\(W^*=\\frac{\\sum_{i=1}^{n}(y_{(i)}-\\bar{y})(m_i^*-0)}{(\\sum_{i=1}^{n}(y_{(i)}-\\bar{y})^2\\sum_{i=1}^{n}(m_i^*-0)^2)^.5}\\) where \\(\\bar{m^*}=0\\) Pearson product moment formula for correlation: \\(\\hat{p}=\\frac{\\sum_{i-1}^{n}(y_i-\\bar{y})(x_i-\\bar{x})}{(\\sum_{i=1}^{n}(y_{i}-\\bar{y})^2\\sum_{i=1}^{n}(x_i-\\bar{x})^2)^.5}\\) When the correlation is 1, the plot is exactly linear and normality is assumed. The closer the correlation is to zero, the more confident we are to reject normality Inference on W* needs to be based on special tables (Looney and Gulledge 1985) 2.3.2.1.2 Shapiro-Wilk Test (Shapiro and Wilk 1965) \\(W=(\\frac{\\sum_{i=1}^{n}a_i(y_{(i)}-\\bar{y})(m_i^*-0)}{(\\sum_{i=1}^{n}a_i^2(y_{(i)}-\\bar{y})^2\\sum_{i=1}^{n}(m_i^*-0)^2)^.5})^2\\) where \\(a_1,..,a_n\\) are weights computed from the covariance matrix for the order statistics. Researchers typically use this test to assess normality. (n &lt; 2000) Under normality, W is close to 1, just like \\(W^*\\). Notice that the only difference between W and W* is the weights. 2.3.2.2 Methods based on empirical cumulative distribution function The formula for the empirical cumulative distribution function (CDF) is: \\(F_n(t)\\) = estimate of probability that an observation \\(\\le\\) t = (number of observation \\(\\let\\) t)/n This method requires large sample sizes. However, it can apply to distributions other than the normal (Gaussian) one. 2.3.2.2.1 Anderson-Darling Test (Anderson and Darling 1952) The Anderson-Darling statistic: \\(A^2=\\int_{-\\infty}^{\\infty}(F_n(t)=F(t))^2\\frac{dF(t)}{F(t)(1-F(t))}\\) a weight average of squared deviations (it weights small and large values of t more) For the normal distribution, \\(A^2 = - (\\sum_{i=1}^{n}(2i-1)(ln(p_i) +ln(1-p_{n+1-i}))/n-n\\) where \\(p_i=\\Phi(\\frac{y_{(i)}-\\bar{y}}{s})\\), the probability that a standard normal variable is less than \\(\\frac{y_{(i)}-\\bar{y}}{s}\\) Reject normal assumption when \\(A^2\\) is too large Evaluate the null hypothesis that the observations are randomly selected from a normal population based on the critical value provided by (Marsaglia and Marsaglia 2004) and (Stephens 1974) This test can be applied to other distributions: Exponential Logistic Gumbel Extreme-value Weibull: log(Weibull) = Gumbel Gamma Logistic Cauchy von Mises Log-normal (two-parameter) Consult (Stephens 1974) for more detailed transformation and critical values. 2.3.2.2.2 Kolmogorov-Smirnov Test Based on the largest absolute difference between empirical and expected cumulative distribution Another deviation of K-S test is Kuipers test 2.3.2.2.3 Cramer-von Mises Test Based on the average squared discrepancy between the empirical distribution and a given theoretical distribution. Each discrepancy is weighted equally (unlike Anderson-Darling test weights end points more heavily) 2.3.2.2.4 JarqueBera Test (Bera and Jarque 1981) Based on the skewness and kurtosis to test normality. \\(JB = \\frac{n}{6}(S^2+(K-3)^2/4)\\) where S is the sample skewness and K is the sample kurtosis \\(S=\\frac{\\hat{\\mu_3}}{\\hat{\\sigma}^3}=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})^3/n}{(\\sum_{i=1}^{n}(x_i-\\bar{x})^2/n)^\\frac{3}{2}}\\) \\(K=\\frac{\\hat{\\mu_4}}{\\hat{\\sigma}^4}=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})^4/n}{(\\sum_{i=1}^{n}(x_i-\\bar{x})^2/n)^2}\\) recall \\(\\hat{\\sigma^2}\\) is the estimate of the second central moment (variance) \\(\\hat{\\mu_3}\\) and \\(\\hat{\\mu_4}\\) are the estimates of third and fourth central moments. If the data comes from a normal distribution, the JB statistic asymptotically has a chi-squared distribution with two degrees of freedom. The null hypothesis is a joint hypothesis of the skewness being zero and the excess kurtosis being zero. References "],["regression-analysis.html", "Chapter 3 Regression Analysis 3.1 Linear Regression 3.2 Generalized Least Squares 3.3 Non-linear Regression", " Chapter 3 Regression Analysis 3.1 Linear Regression 3.1.1 Ordinary Least Squares The most fundamental model in statistics or econometric is a OLS linear regression. 3.1.2 Weighted Least Squares 3.2 Generalized Least Squares 3.3 Non-linear Regression "],["experimental-design.html", "Chapter 4 Experimental Design 4.1 Analysis of Variance (ANOVA)", " Chapter 4 Experimental Design 4.1 Analysis of Variance (ANOVA) ANOVA is using the same underlying mechanism as linear regression. However, the angle that ANOVA chooses to look at is slight different from the traditional linear regression. It can be more useful in the case with qualitative variables and designed experiments. Experimental Design - Factor: explanatory or predictor variable to be studied in an investigation - Treatment (or Factor Level): value of a factor applied to the experimental unit - Experimental Unit: person, animal, piece of material, etc. that is subjected to treatment(s) and provides a response - Single Factor Experiment: one explanatory variable considered - Multifactor Experiment: more than one explanatory variable - Classification Factor: A factor that is not under the control of the experimenter (observational data) - Experimental Factor: assigned by the experimenter Basics of experimental design: - Choices that a statistician has to make: - set of treatments - set of experimental units - treatment assignment (selection bias) - measurement (measurement bias, blind experiments) - Advancements in experimental design: 1. Factorial Experiments: consider multiple factors at the same time (interaction) date: &quot;2020-11-15&quot; "],["advanced-methods.html", "Chapter 5 Advanced Methods 5.1 Imputation (Missing Data) 5.2 Deep Learning", " Chapter 5 Advanced Methods 5.1 Imputation (Missing Data) 5.1.1 Types of Imputation 5.1.1.1 Mean, Mode, Median Imputation Bad: Mean imputation does not preserve the relationships among variables Mean imputation leads to An Underestimate of Standard Errors  youre making Type I errors without realizing it. 5.1.1.2 Hot Deck Imputation Randomly choose value from other observations that share similar values on other variables. Good: Constrained to only possible values. Since the value is picked at random, it adds some variability, which might come in handy when calculating standard errors. 5.1.1.3 Cold Deck Imputation Contrary to Hot Deck, Cold Deck choose value systematically from an observation that has similar values on other variables, which remove the random variation that we want. 5.1.1.4 Regression Imputation Missing value is based (regress) on other variables. Good: Maintain the relationship with other variables Bad: No variability left. 5.1.1.5 Stochatistc Imputation Regression imputation + random residual = Stochastic Imputation Most multiple imputation is based off of some form of stochastic regression imputation. 5.1.1.6 Interpolation and Extrapolation An estimated value from other observations from the same individual. It usually only works in longitudinal data. 5.1.1.7 E-M (Expectation-Maximization) Algorithm An iterative process: Other variables are used to impute a value (Expectation). Check whether the value is most likely (Maximization). If not, it re-imputes a more likely value. E-M also preserves the relationship with other variables. However, it still underestimates standard error. 5.1.1.8 K-nearest neighbour (KNN) imputation The above methods are model-based imputation (regression). This is an example of neighbor-based imputation (K-nearest neighbor). For a discrete variable, it uses the most frequent value among the k nearest neighbors. Distance metrics: Hamming distance. For a continuous variable, it uses the mean or mode. Distance metrics: Euclidean Mahalanobis Manhattan 5.1.1.9 Bayesian Ridge regression implementation 5.1.2 Another Perspective In the imputation world, you can go for either single or multiple imputation. Most of the imputation methods listed above are single imputation. A downfall of single imputation is its underestimation towards standard error. Model bias can arisen from various factors including: Imputation method Missing data mechanism (MCAR vs. MAR) Proportion of the missing data Information available in the data set Since the imputed observations are themselves estimates, their values have corresponding random error. But when you put in that estimate as a data point, your software doesnt know that. So it overlooks the extra source of error, resulting in too-small standard errors and too-small p-values. So multiple imputation comes up with multiple estimates. Two of the methods listed above work as the imputation method in multiple imputationhot deck and stochastic regression. Because these two methods have a random component, the multiple estimates are slightly different. This re-introduces some variation that your software can incorporate in order to give your model accurate estimates of standard error. Multiple imputation was a huge breakthrough in statistics about 20 years ago. It solves a lot of problems with missing data (though, unfortunately not all) and if done well, leads to unbiased parameter estimates and accurate standard errors. If your rate of missing data is very, very small, it honestly doesnt matter what technique you use. Im talking very, very, very small (2-3%). Missing Data Mechanisms (1) Missing Completely at Random (MCAR) - the propensity for a data point to be missing is completely random. - Theres no relationship between whether a data point is missing and any values in the data set, missing or observed. - The missing data are just a random subset of the data. (2) Missing at Random. (MAR) - the propensity for a data point to be missing is not related to the missing data, but it is related to some of the observed data - MAR requires that the cause of the missing data is unrelated to the missing values but may be related to the observed values of other variables. - MAR means that the missing values are related to observed values on other variables. As an example of CD missing data, missing income data may be unrelated to the actual income values but are related to education. Perhaps people with more education are less likely to reveal their income than those with less education (3) Non-Ignorable (NI) - the missing data mechanism is related to the missing values - It commonly occurs when people do not want to reveal something very personal or unpopular about themselves - Complete case analysis can give highly biased results for NI missing data. If proportionally more low and moderate income individuals are left in the sample because high income people are missing, an estimate of the mean income will be lower than the actual population mean. Remember that there are three goals of multiple imputation, or any missing data technique: Unbiased parameter estimates in the final analysis (regression coefficients, group means, odds ratios, etc.); accurate standard errors of those parameter estimates, and therefore, accurate p-values in the analysis; and adequate power to find meaningful parameter values significant. 1. Dont round off imputations for dummy variables. Many common imputation techniques, like MCMC, require normally distributed variables. Suggestions for imputing categorical variables were to dummy code them, impute them, then round off imputed values to 0 or 1. Recent research, however, has found that rounding off imputed values actually leads to biased parameter estimates in the analysis model. You actually get better results by leaving the imputed values at impossible values, even though its counter-intuitive. Dont transform skewed variables. Likewise, when you transform a variable to meet normality assumptions before imputing, you not only are changing the distribution of that variable but the relationship between that variable and the others you use to impute. Doing so can lead to imputing outliers, creating more bias than just imputing the skewed variable. Use more imputations. The advice for years has been that 5-10 imputations are adequate. And while this is true for unbiasedness, you can get inconsistent results if you run the multiple imputation more than once. Bodner (2008) recommends having as many imputations as the percentage of missing data. Since running more imputations isnt any more work for the data analyst, theres no reason not to. Bodner, T. E. 2008. What Improves with Increased Missing Data Imputations? Structural Equation Modeling 15(4):65175. Create multiplicative terms before imputing. When the analysis model contains a multiplicative term, like an interaction term or a quadratic, create the multiplicative terms first, then impute. Imputing first, and then creating the multiplicative terms actually biases the regression parameters of the multiplicative term (von Hippel, 2009). von Hippel, P.T. (2009). How To Impute Squares, Interactions, and Other Transformed Variables. Sociological Methodology 39. Alternatives to multiple imputation arent usually better. Multiple imputation assumes the data are missing at random. In most tests, if an assumption is not met, there are better alternativesa nonparametric test or an alternative type of model. This is often not true with missing data. Alternatives like listwise deletion (a.k.a. ignoring it) have more stringent assumptions. So do nonignorable missing data techniques like Heckmans selection models. Do not use these ad-hoc: Pairwise Deletion: use the available data for each part of an analysis. This has been shown to result in correlations beyond the 0,1 range and other fun statistical impossibilities. Mean Imputation: substitute the mean of the observed values for all missing data. There are so many problems, its difficult to list them all, but suffice it to say, this technique never meets the above 3 criteria. Dummy Variable: create a dummy variable that indicates whether a data point is missing, then substitute any arbitrary value for the missing data in the original variable. Use both variables in the analysis. While it does help the loss of power, it usually leads to biased results. Multiple Imputation of categorical variables is bad Paul Allison, one of my favorite authors of statistical information for researchers, did a study that showed that the most common method actually gives worse results that listwise deletion. (Did I mention Ive used it myself?) What is the bad method? 1. Dummy code the variable 2. Impute a continuous value. This will generally be between 0 and 1. 3. Round off to either 0 or 1, based on whether the imputed value is below or above .5. As Allison discovered, this method generally leads to biased results, and incorrect standard errors What to do instead? Allison compared this approach to four others, each of which generally gave more accurate results, at least under some conditions. 1. Listwise deletion 2. Imputation of the continuous variable without rounding (just leave off step 3). 3. Logistic Regression imputation 4. Discriminant Analysis imputation These last two generally performed best, but only work in limited situations. 5.2 Deep Learning 5.2.1 Overview This section is based on (Allaire 2018) What is deep learning? Input to output via layers of representation What are layers? A layer is a geometric transformation function on the data that goes through it Weights determine the data transformation behavior of a layer Learning Representation Transforming input data into useful representation The deep in deep learning multiple layers Other possibly more appropriate names for the field: Layered representations learning Hierarchical representations learning Chained geometric transformation learning New problem domains for R: Computer vision Computer speech recognition Reinforcement learning applications How do we train deep learning models? Basic of machine learning algorithms Machine learning vs. statistical modeling MNIST example Model definition in R Layers of representation The training loop Machine learning algorithms Learning model parameters via exposure to many example data points Statistics: Often focused on inferring the process by which data is generated Machine Learning: Principally focused on predicting future data Deep learning frontiers Computer vision Natural language processing Time series Biomedical 5.2.2 Tensor Flow and R TensorFlow APIs Keras API Estimator API Core API 5.2.2.1 R packages TensorFlow APIs * keras: Interface for neural networks, with a focus on enabling fast experimentation. * tfestimators: Implementations of common model types such as regressors and classifiers. * tensorflow: Low level interface to the TensorFlow computational graph. tfdatasets: Scalable input pipelines for TensorFlow models. Supporting Tools * tfruns : Track , visualize, and manage TensorFlow training runs and experiments * tfdeploy : Tools designed to make exporting and serving TensorFlow models straightforward. * cloudml : R interface to Google Cloud Machine Learning Engine. R interface to Keras Step by step example Keras layers Compiling models Losses, Optimizers, and Metrics More examples 5.2.2.2 Keras layers 65 layers available Dense layers: classic fully connected neural network layers Convolutional layers: \"Filters for learning local patterns in data Recurrent layers: Layers that maintain state based on on previously seen data Embedding layers: Vectorization of text that reflects semantic relationships between words 5.2.3 Compiling models Model compilation prepares the model for training by: 1. Converting the layers into a TensorFlow graph 2. Applying the specified loss function and optimizer 3. Arranging for the collection of metrics during training 5.2.4 Losses, Optimizers, and Metrics All available at Keras for R cheatsheet Example of Image classificaiton References "],["appendix.html", "Chapter 6 Appendix 6.1 Short-cut 6.2 Function short-cut 6.3 Citation", " Chapter 6 Appendix 6.1 Short-cut These are shortcuts that you probably you remember when working with R. Even though it might take a bit of time to learn and use them as your second nature, but they will save you a lot of time. Just like learning another language, the more you speak and practice it, the more comfortable you are speaking it. function short-cut navigate folders in console \" \" + tab pull up short-cut cheat sheet ctrl + shift + k go to file/function (everything in your project) ctrl + . search everything cmd + shift + f navigate between tabs Crtl + shift + . type function faster snip + shift + tab type faster use tab for fuzzy match cmd + up ctrl + . 6.2 Function short-cut apply one function to your data to create a new variable: mutate(mod=map(data,function)) instead of using i in 1:length(object): for (i in seq_along(object)) apply multiple function: map_dbl apply multiple function to multiple variables:map2 autoplot(data) plot times series data mod_tidy = linear(reg) %&gt;% set_engine('lm') %&gt;% fit(price ~ ., data=data) fit lm model. It could also fit other models (stan, spark, glmnet, keras) Sometimes, data-masking will not be able to recognize whether youre calling from environment or data variables. To bypass this, we use .data$variable or .env$variable. For example data %&gt;% mutate(x=.env$variable/.data$variable 6.3 Citation include a citation by [@Farjam_2015] cite packages used in this session package=ls(sessionInfo()$loadedOnly) for (i in package){print(toBibtex(citation(i)))} package=ls(sessionInfo()$loadedOnly) for (i in package){ print(toBibtex(citation(i))) } "],["bookdown-cheat-sheet.html", "Chapter 7 Bookdown cheat sheet 7.1 Heading blah blah 7.2 About labelling things 7.3 Cross-references 7.4 Figures, tables, citations 7.5 How the square bracket links work", " Chapter 7 Bookdown cheat sheet Took from professor Jenny to Heres where I park little examples for myself about bookdown mechanics that I keep forgetting. The bookdown book: https://bookdown.org/yihui/bookdown/ 7.1 Heading blah blah 7.2 About labelling things You can label chapter and section titles using {#label} after them, e.g., we can reference Section 7.2. If you do not manually label them, there will be automatic labels anyway, e.g., this reference to the unlabelled heading 7.1 uses the automatically generated label \\@ref(heading-blah-blah). 7.3 Cross-references Add an explicit label by adding {#label} to the end of the section header. If you know youre going to refer to something, this is probably a good idea. To refer to in a chapter- or section-number-y way, use \\@ref(label). \\@ref(install-git) example: In chapter 1 If you are happy with the section header as the link text, use it inside a single set of square brackets: [A picture is worth a thousand words]: example A picture is worth a thousand words via [A picture is worth a thousand words] There are two ways to specify custom link text: [link text][Section header text], e.g., pic = 1000 words via [pic = 1000 words][A picture is worth a thousand words] [link text](#label), e.g., RStudio, meet Git via RStudio, meet Git The Pandoc documentation provides more details on automatic section IDs and implicit header references. 7.4 Figures, tables, citations Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 7.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 7.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 7.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 7.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2020) in this sample book, which was built on top of R Markdown and knitr (???). 7.5 How the square bracket links work Context: you prefer to link with text, not a chapter or section number. GOOD! Heres a link to [Contributors]. BAD. You can see contributors in ??. Facts and vocabulary Each chapter is a file. These files should begin with the chapter title using a level-one header, e.g., # Chapter Title. A chapter can be made up of sections, indicated by lower-level headers, e.g., ## A section within the chapter. There are three ways to address a section when creating links within your book: Explicit identifier: In # My header {#foo} the explicit identifier is foo. Automatically generated identifier: my-header is the auto-identifier for # My header. Pandoc creates auto-identifiers according to rules laid out in Extension: auto_identifiers. The header text, e.g., My header be used verbatim as an implicit header reference. See Extension: implicit_header_references for more. All 3 forms can be used to create cross-references but you build the links differently. Advantage of explicit identification: You are less likely to update the section header and then forget to make matching edits to references elsewhere in the book. How to make text-based links using explicit identifiers, automatic identifiers, and implicit references: Use implicit reference alone to get a link where the text is exactly the section header: [Introduce yourself to Git] [Introduce yourself to Git] [Success and operating systems] [Success and operating systems] You can provide custom text for the link with all 3 methods of addressing a section: Implicit header reference: [link text][Recommended Git clients] [link text][Recommended Git clients] Explicit identifier: [hello git! I'm Jenny](#hello-git) hello git! Im Jenny Automatic identifier: [Any text you want](#recommended-git-clients) Any text you want References "],["references.html", "References", " References "]]
