[["index.html", "A Guide on Data Analysis Chapter 1 Introduction", " A Guide on Data Analysis Mike Nguyen 2021-01-09 Chapter 1 Introduction This guide is an attempt to streamline and demystify the data analysis process. By no mean this is an ultimate guide, or I am a great source of knowledge, or I claim myself as a statistician/ data analyst/ econometrician (or any fancy name we have now), but I am a strong proponent of learning by teaching, and doing. Hence, this is more like a learning experience for both you and me. Since the beginning of the century, we have been bombarded with amazing advancements and inventions, especially in the field of statistics, information technology, and computer science. However, I believe the downside of this introduction is that we use big and trendy words too often (i.e., big data, machine learning, deep learning). Its all fun and exciting when I learned these new tools. But I have to admit that I hardly retain any of these new inventions.However, writing down from the beginning till the end of a data analysis process is the solution that I came up with. Accordingly, lets dive right in. Some general recommendation: The more you practice/habituate/condition, more line of codes that you write, more function that you memorize, I think the more you will like this journey. Readers can follow this book several ways: If you are interested in particular methods/tools, you can jump to that section by clicking the section name. If you want to follow a traditional path of data analysis, read the Regression Analysis section. If you want to create your experiment and test your hypothesis, read the Analysis of Variance (ANOVA) section. Alternatively, if you rather see the application of models, and disregard any theory or underlying mechanisms, you can skip to summary and application portion of each section. If you dont understand a part, search the title of that part of that part on Google, and read more into that subject. This is just a general guide. If you want to customize your code beyond the ones provided in this book, run in the console help(code) or ?code. For example, I want more information on hist function, Ill type in the console ?hist or help(hist). Another way is that you can search on Google. Different people will use different packages to achieve the same result in R. Accordingly, if you want to create a histogram, search on Google histogram in R, then you should be able to find multiple ways to create histogram in R. Information in this book are from various sources, but the outline is based on several courses that I have taken formally. Id like to give professors credit accordingly. Course Professor Data Analysis I Erin M. Schliep Applied Econometric Alyssa Carlson Tools of statistics Probability Theory Mathematical Analysis Computer Science Numerical Analysis Database Management Setup Working Environment if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) if (!require(&quot;devtools&quot;)) install.packages(&quot;devtools&quot;) library(&quot;pacman&quot;) library(&quot;devtools&quot;) "],["prerequisites.html", "Chapter 2 Prerequisites", " Chapter 2 Prerequisites This chapter is just a quick review of Matrix Theory and Probability Theory If you feel you do not need to brush up on these theories, you can jump right into Descriptive Statistics "],["matrix-theory.html", "2.1 Matrix Theory", " 2.1 Matrix Theory \\[\\begin{equation} \\begin{split} A= \\left[\\begin{array}{c} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\\\ \\end{array} \\right] \\end{split} \\end{equation}\\] \\[\\begin{equation} \\begin{split} A&#39; = \\left[\\begin{array}{c} a_{11} &amp; a_{21} \\\\ a_{12} &amp; a_{22} \\\\ \\end{array} \\right] \\end{split} \\end{equation}\\] \\[ \\mathbf{(ABC)&#39;=C&#39;B&#39;A&#39;} \\] \\[\\begin{equation} \\begin{split} \\mathbf{A} &amp;= \\left(\\begin{array}{cccc} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ \\end{array}\\right) \\left(\\begin{array}{c} b_{11} &amp; b_{12} &amp; b_{13} \\\\ b_{21} &amp; b_{22} &amp; b_{23} \\\\ b_{31} &amp; b_{32} &amp; b_{33} \\\\ \\end{array}\\right) \\\\ &amp;= \\left(\\begin{array}{c} a_{11}b_{11}+a_{12}b_{21}+a_{13}b_{31} &amp; \\sum_{i=1}^{3}a_{1i}b_{i2} &amp; \\sum_{i=1}^{3}a_{1i}b_{i3} \\\\ \\sum_{i=1}^{3}a_{2i}b_{i1} &amp; \\sum_{i=1}^{3}a_{2i}b_{i2} &amp; \\sum_{i=1}^{3}a_{2i}b_{i3} \\\\ \\end{array}\\right) \\end{split} \\end{equation}\\] Let \\(\\mathbf{a}\\) be a 3 x 1 vector, then the quadratic form is \\[ \\mathbf{a&#39;Ba} = \\sum_{i=1}^{3}\\sum_{i=1}^{3}a_i b_{ij} a_{j} \\] 2.1.1 Rank Dimension of space spanned by its columns (or its rows). Number of linearly indepdent columns/rows For a n x k matrix A and k x k matrix B \\(rank(A)\\leq min(n,k)\\) \\(rank(A) = rank(A&#39;) = rank(A&#39;A)=rank(AA&#39;)\\) \\(rank(AB)=min(rank(A),rank(B))\\) B is invertible if and only if rank(B) = k (non-singular) 2.1.2 Inverse In scalar, a = 0 then 1/a does not exist. In matrix, a matrix is invertible when its a non-zero matrix. A non-singular square matrix A is invertible if there exists a non-singular square matrix B such that, \\[AB=I\\] Then \\(A^{-1}=B\\). For a 2x2 matrix, \\[ A = \\left(\\begin{array}{c} a &amp; b \\\\ c &amp; d \\\\ \\end{array} \\right) \\] \\[ A^{-1}= \\frac{1}{ad-bc} \\left(\\begin{array}{c} d &amp; -b \\\\ -c &amp; a \\\\ \\end{array} \\right) \\] For the partition matrix, \\[\\begin{equation} \\begin{split} \\left[\\begin{array}{c} A &amp; B \\\\ C &amp; D \\\\ \\end{array} \\right]^{-1} = \\left[\\begin{array}{c} \\mathbf{(A-BD^{-1}C)^{-1}} &amp; \\mathbf{-(A-BD^{-1}C)^{-1}BD^-1}\\\\ \\mathbf{-DC(A-BD^{-1}C)^{-1}} &amp; \\mathbf{D^{-1}+D^{-1}C(A-BD^{-1}C)^{-1}BD^{-1}}\\ \\\\ \\end{array} \\right] \\end{split} \\end{equation}\\] Properties for a non-singular square matrix \\(\\mathbf{A^{-1}}=A\\) for a non-zero scalar b, \\(\\mathbf{(bA)^{-1}=b^{-1}A^{-1}}\\) for a matrix B, \\(\\mathbf(BA)^{-1}=B^{-1}A^{-1}\\) only if B is non-singular \\(\\mathbf{(A^{-1})&#39;=(A&#39;)^{-1}}\\) Never notate \\(\\mathbf{1/A}\\) 2.1.3 Definiteness A symmetric square k x k matrix, \\(\\mathbf{A}\\), is Positive Semi-Definite if for any non-zero k x 1 vector \\(\\mathbf{x}\\), \\[\\mathbf{x&#39;Ax \\geq 0 }\\] A symmetric square k x k matrix, \\(\\mathbf{A}\\), is Negative Semi-Definite if for any non-zero k x 1 vector \\(\\mathbf{x}\\) \\[\\mathbf{x&#39;Ax \\leq 0 }\\] \\(\\mathbf{A}\\) is indefinite if it is neither positive semi-definite or negative semi-definite. The identity matrix is positive definite Example Let \\(\\mathbf{x} =(x_1 x_2)&#39;\\), then for a 2 x 2 identity matrix, \\[\\begin{equation} \\begin{split} \\mathbf{x&#39;Ix} &amp;= (x_1 x_2) \\left(\\begin{array}{c} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{array} \\right) \\left(\\begin{array}{c} x_1 \\\\ x_2 \\\\ \\end{array} \\right) \\\\ &amp;= (x_1 x_2) \\left(\\begin{array}{c} x_1 \\\\ x_2 \\\\ \\end{array} \\right) \\\\ &amp;= x_1^2 + x_2^2 &gt;0 \\end{split} \\end{equation}\\] Definiteness gives us the ability to compare matrices \\(\\mathbf{A-B}\\) is PSD This property also helps us show efficiency (which variance covariance matrix of one estimator is smaller than another) Properties any variance matrix is PSD a matrix \\(\\mathbf{A}\\) is PSD if and only if there exists a matrix \\(\\mathbf{B}\\) such that \\(\\mathbf{A=B&#39;B}\\) if \\(\\mathbf{A}\\) is PSD, then \\(\\mathbf{B&#39;AB}\\) is PSD if A and C are non-singular, then A-C is PSD if and only if \\(\\mathbf{C^{-1}-A^{-1}}\\) if A is PD (ND) then \\(A^{-1}\\) is PD (ND) Note Indefinite A is neither PSD nor NSD. There is no comparable concept in scalar. If a square matrix is PSD and invertible then it is PD Example: Invertible / Indefinite \\[ \\left[ \\begin{array}{a} -1 &amp; 0 \\\\ 0 &amp; 10 \\\\ \\end{array} \\right] \\] Non-invertible/ Indefinite \\[ \\left[ \\begin{array}{a} 0 &amp; 1 \\\\ 0 &amp; 0 \\\\ \\end{array} \\right] \\] Invertible / PSD \\[ \\left[ \\begin{array}{a} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{array} \\right] \\] Non-Invertible / PSD \\[ \\left[ \\begin{array}{a} 0 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{array} \\right] \\] 2.1.4 Matrix Calculus \\(y=f(x_1,x_2,...,x_k)=f(x)\\) where x is a 1 x k row vector. The Gradient (first order derivative with respect to a vector) is, \\[ \\frac{\\partial{f(x)}}{\\partial{x}}= \\left(\\begin{array}{c} \\frac{\\partial{f(x)}}{\\partial{x_1}} \\\\ \\frac{\\partial{f(x)}}{\\partial{x_2}} \\\\ ... \\\\ \\frac{\\partial{f(x)}}{\\partial{x_k}} \\end{array} \\right) \\] The Hessian (second order derivative with respect to a vector) is, \\[ \\frac{\\partial^2{f(x)}}{\\partial{x}\\partial{x&#39;}}= \\left(\\begin{array}{c} \\frac{\\partial^2{f(x)}}{\\partial{x_1}\\partial{x_1}} &amp; \\frac{\\partial^2{f(x)}}{\\partial{x_1}\\partial{x_2}} &amp; ... &amp; \\frac{\\partial^2{f(x)}}{\\partial{x_1}\\partial{x_k}} \\\\ \\frac{\\partial^2{f(x)}}{\\partial{x_1}\\partial{x_2}} &amp; \\frac{\\partial^2{f(x)}}{\\partial{x_2}\\partial{x_2}} &amp; ... &amp; \\frac{\\partial^2{f(x)}}{\\partial{x_2}\\partial{x_k}} \\\\ ... &amp; ...&amp; &amp; ...\\\\ \\frac{\\partial^2{f(x)}}{\\partial{x_k}\\partial{x_1}} &amp; \\frac{\\partial^2{f(x)}}{\\partial{x_k}\\partial{x_2}} &amp; ... &amp; \\frac{\\partial^2{f(x)}}{\\partial{x_k}\\partial{x_k}} \\end{array} \\right) \\] 2.1.5 Optimization Scalar Optimization Vector Optimization First Order Condition \\[\\frac{\\partial{f(x_0)}}{\\partial{x}}=0\\] \\[\\frac{\\partial{f(x_0)}}{\\partial{x}}=\\left(\\begin{array}{c}0 \\\\ .\\\\ .\\\\ .\\\\ 0\\end{array}\\right)\\] Second Order Condition Convex \\(\\rightarrow\\) Min \\[\\frac{\\partial^2{f(x_0)}}{\\partial{x^2}} &gt; 0\\] \\[\\frac{\\partial^2{f(x_0)}}{\\partial{xx&#39;}}&gt;0\\] Concave \\(\\rightarrow\\) Max \\[\\frac{\\partial^2{f(x_0)}}{\\partial{x^2}} &lt; 0\\] \\[\\frac{\\partial^2{f(x_0)}}{\\partial{xx&#39;}}&lt;0\\] "],["probability-theory.html", "2.2 Probability Theory", " 2.2 Probability Theory 2.2.1 Axiom and Theorems of Probability Let S denote a sample space of an experiment P[S]=1 \\(P[A] \\ge 0\\) for every event A Let \\(A_1,A_2,A_3,...\\) be a finite or an infinite collection of mutually exclusive events. Then \\(P[A_1\\cup A_2 \\cup A_3 ...]=P[A_1]+P[A_2]+P[A_3]+...\\) \\(P[\\emptyset]=0\\) \\(P[A&#39;]=1-P[A]\\) \\(P[A_1 \\cup A_2] = P[A_1] + P[A_2] - P[A_1 \\cap A_2]\\) Conditional Probability \\[ P[A|B]=\\frac{A \\cap B}{P[B]} \\] Independent Events Two events A and B are independent if and only if: \\(P[A\\cap B]=P[A]P[B]\\) \\(P[A|B]=P[A]\\) \\(P[B|A]=P[B]\\) A finite collection of events \\(A_1, A_2, ..., A_n\\) is independent if and only if any subcollection is independent. Multiplication Rule \\(P[A \\cap B] = P[A|B]P[B] = P[B|A]P[A]\\) Bayes Theorem Let \\(A_1, A_2, ..., A_n\\) be a collection of mutually exclusive events whose union is S. Let b be an event such that \\(P[B]\\neq0\\) Then for any of the events \\(A_j\\), j = 1,2,,n \\[ P[A_|B]=\\frac{P[B|A_j]P[A_j]}{\\sum_{i=1}^{n}P[B|A_j]P[A_i]} \\] Jensens Inequality If g(x) is convex \\(E(g(X)) \\ge g(E(X))\\) If g(x) is concave \\(E(g(X)) \\le g(E(X))\\) 2.2.1.1 Law of Iterated Expectations \\(E(Y)=E(E(Y|X))\\) 2.2.1.2 Correlation and Independence Independence \\(f(x,y)=f_X(x)f_Y(y)\\) \\(f_{Y|X}(y|x)=f_Y(y)\\) and \\(f_{X|Y}(x|y)=f_X(x)\\) \\(E(g_1(X)g_2(Y))=E(g_1(X))E(g_2(Y))\\) Mean Independence (implied by independence) Y is mean independent of X if and only if \\(E(Y|X)=E(Y)\\) \\(E(Xg(Y))=E(X)E(g(Y))\\) Uncorrelated (implied by independence and mean independence) \\(Cov(X,Y)=0\\) \\(Var(X+Y)=Var(X) + Var(Y)\\) \\(E(XY)=E(X)E(Y)\\) \\[ Strongest \\\\ \\downarrow \\\\ Independence \\\\ \\downarrow \\\\ Mean Independence \\\\ \\downarrow \\\\ Uncorrelated \\\\ \\downarrow \\\\ Weakest\\\\ \\] 2.2.2 Central Limit Theorem Let \\(X_1, X_2,...,X_n\\) be a random sample of size n from a distribution (not necessarily normal) X with mean \\(\\mu\\) and variance \\(\\sigma^2\\). then for large n (\\(n \\ge 25\\)), \\(\\bar{X}\\) is approximately normal with with mean \\(\\mu_{\\bar{X}}=\\mu\\) and variance \\(\\sigma^2_{\\bar{X}} = Var(\\bar{X})= \\frac{\\sigma^2}{n}\\) \\(\\hat{p}\\)is approximately normal with \\(\\mu_{\\hat{p}} = p, \\sigma^2_{\\hat{p}} = \\frac{p(1-p)}{n}\\) \\(\\hat{p_1} - \\hat{p_2}\\) is approximately normal with \\(\\mu_{\\hat{p_1} - \\hat{p_2}} = p_1 - p_2, \\sigma^2_{\\hat{p_1} - \\hat{p_2}}=\\frac{p_1(1-p)}{n_1} + \\frac{p_2(1-p)}{n_2}\\) \\(\\bar{X_1} - \\bar{X_2}\\) is approximately normal with \\(\\mu_{\\bar{X_1} - \\bar{X_2}} = \\mu_1 - \\mu_2, \\sigma^2_{\\bar{X_1} - \\bar{X_2}} = \\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}\\) The following random variables are approximately standard normal: + \\(\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\) + \\(\\frac{\\hat{p}-p}{\\sqrt{\\frac{p(1-p)}{n}}}\\) + \\(\\frac{(\\hat{p_1}-\\hat{p_2})-(p_1-p_2)}{\\sqrt{\\frac{p_1(1-p_1)}{n_1}-\\frac{p_2(1-p_2)}{n_2}}}\\) + \\(\\frac{(\\bar{X_1}-\\bar{X_2})-(\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\sigma^2_1}{n_1}-\\frac{\\sigma^2_2}{n_2}}}\\) If \\(\\{x_i\\}_{i=1}^{n}\\) is an iid random sample from a probability distribution with finite mean \\(\\mu\\) and finite variance \\(\\sigma^2\\) then the sample mean \\(\\bar{x}=n^{-1}\\sum_{i=1}^{n}x_i\\) scaled by \\(\\sqrt{n}\\) has the following limiting distribution \\[ \\sqrt{n}(\\bar{x}-\\mu) \\to^d N(0,\\sigma^2) \\] or if we were to standardize the sample mean, \\[ \\frac{\\sqrt{n}(\\bar{x}-\\mu)}{\\sigma} \\to^d N(0,1) \\] holds for most random sample from any distribution (continuous, discrete, unknown). extends to multivariate case: random sample of a random vector converges to a multivariate normal. Variance from the limiting distribution is the asymptotic variance (Avar) \\[ Avar(\\sqrt{n}(\\bar{x}-\\mu)) = \\sigma^2 \\\\ \\lim_{n \\to \\infty} Var(\\sqrt{n}(\\bar{x}-\\mu)) = \\sigma^2 \\\\ Avar(.) \\neq lim_{n \\to \\infty} Var(.) \\] 2.2.3 Random variable Discrete Variable Continuous Variable Definition A random variable is discrete if it can assume at most a finite or countably infinite number of possible values A random variable is continuous if it can assume any value in some interval or intervals of real numbers and the probability that it assumes any specific value is 0 Density Function A function f is called a density for X if: (1) \\(f(x) \\ge 0\\) (2) \\(\\sum_{all~x}f(x)=1\\) (3) \\(f(x)=P(X=x)\\) for x real A function f is called a density for X if: (1) \\(f(x) \\ge 0\\) for x real (2) \\(\\int_{-\\infty}^{\\infty} f(x) \\; dx=1\\) (3) \\(P[a \\le X \\le] =\\int_{a}^{b} f(x) \\; dx\\) for a and b real Cumulative Distribution Function for x real \\(F(x)=P[X \\le x]\\) \\(F(x)=P[X \\le x]=\\int_{-\\infty}^{\\infty}f(t)dt\\) \\(E[H(X)]\\) \\(\\sum_{all ~x}H(x)f(x)\\) \\(\\int_{-\\infty}^{\\infty}H(x)f(x)\\) \\(\\mu=E[X]\\) \\(\\sum_{all ~ x}xf(x)\\) \\(\\int_{-\\infty}^{\\infty}xf(x)\\) Ordinary Moments the kth ordinary moment for variable X is defined as: \\(E[X^k]\\) \\(\\sum_{all ~ x \\in X}(x^kf(x))\\) \\(\\int_{-\\infty}^{\\infty}(x^kf(x))\\) Moment generating function (mgf) \\(m_X(t)=E[e^{tX}]\\) \\(\\sum_{all ~ x \\in X}(e^{tx}f(x))\\) \\(\\int_{-\\infty}^{\\infty}(e^{tx}f(x)dx)\\) Expected Value Properties: E[c] = c for any constant c E[cX] = cE[X] for any constant c E[X+Y] = E[X] = E[Y] E[XY] = E[X].E[Y] (if X and Y are independent) Expected Variance Properties: \\(Var(c) = 0\\) for any constant c \\(Var(cX) = c^2Var(X)\\) for any constant c \\(Var(X) \\ge 0\\) \\(Var(X) = E(X^2) - (E(X))^2\\) \\(Var(X+c)=Var(X)\\) \\(Var (X+Y) = Var(X) + Var(Y)\\) (if X and Y are independent) Standard deviation \\(\\sigma=\\sqrt(\\sigma^2)=\\sqrt(Var X)\\) Moment generating function properties: \\(\\frac{d^k(m_X(t))}{dt^k}|_{t=0}=E[X^k]\\) \\(\\mu=E[X]=m_X&#39;(0)\\) \\(E[X^2]=m_X&#39;&#39;(0)\\) mgf Theorems Let \\(X_1,X_2,...X_n,Y\\) be random variables with moment-generating functions \\(m_{X_1}(t),m_{X_2}(t),...,m_{X_n}(t),m_{Y}(t)\\) If \\(m_{X_1}(t)=m_{X_2}(t)\\) for all t in some open interval about 0, then \\(X_1\\) and \\(X_2\\) have the same distribution If \\(Y = \\alpha + \\beta X_1\\), then \\(m_{Y}(t)= e^{\\alpha t}m_{X_1}(\\beta t)\\) If \\(X_1,X_2,...X_n\\) are independent and \\(Y = \\alpha_0 + \\alpha_1 X_1 + \\alpha_2 X_2 + ... + \\alpha_n X_n\\) (where \\(\\alpha_0, ... ,\\alpha_n\\) are real numbers), then \\(m_{Y}(t)=e^{\\alpha_0 t}m_{X_1}(\\alpha_1t)m_{X_2}(\\alpha_2 t)...m_{X_n}(\\alpha_nt)\\) Suppose \\(X_1,X_2,...X_n\\) are independent normal random variables with means \\(\\mu_1,\\mu_2,...\\mu_n\\) and variances \\(\\sigma^2_1,\\sigma^2_2,...,\\sigma^2_n\\). If \\(Y = \\alpha_0 + \\alpha_1 X_1 + \\alpha_2 X_2 + ... + \\alpha_n X_n\\) (where \\(\\alpha_0, ... ,\\alpha_n\\) are real numbers), then Y is normally distributed with mean \\(\\mu_Y = \\alpha_0 + \\alpha_1 \\mu_1 +\\alpha_2 \\mu_2 + ... + \\alpha_n \\mu_n\\) and variance \\(\\sigma^2_Y = \\alpha_1^2 \\sigma_1^2 + \\alpha_2^2 \\sigma_2^2 + ... + \\alpha_n^2 \\sigma_n^2\\) 2.2.4 Moment Moment Uncentered Centered 1st \\(E(X)=\\mu=Mean(X)\\) 2nd \\(E(X^2)\\) \\(E((X-\\mu)^2)=Var(X)=\\sigma^2\\) 3rd \\(E(X^3)\\) \\(E((X-\\mu)^3)\\) 4th \\(E(X^4)\\) \\(E((X-\\mu)^4)\\) Skewness(X) = \\(E((X-\\mu)^3)/\\sigma^3\\) Kurtosis(X) = \\(E((X-\\mu)^4)/\\sigma^4\\) Conditional Moments \\[ E(Y|X=x)= \\begin{cases} \\sum_yyf_Y(y|x) &amp; \\text{for discrete RV}\\\\ \\int_yyf_Y(y|x)dy &amp; \\text{for continous RV}\\\\ \\end{cases} \\] \\[ Var(Y|X=x)= \\begin{cases} \\sum_y(y-E(Y|x))^2f_Y(y|x) &amp; \\text{for discrete RV}\\\\ \\int_y(y-E(Y|x))^2f_Y(y|x)dy &amp; \\text{for continous RV}\\\\ \\end{cases} \\] 2.2.4.1 Multivariate Moments \\[ \\begin{equation} E= \\left( \\begin{array}{c} X \\\\ Y \\\\ \\end{array} \\right) = \\left( \\begin{array}{c} E(X) \\\\ E(Y) \\\\ \\end{array} \\right) = \\left( \\begin{array}{c} \\mu_X \\\\ \\mu_Y \\\\ \\end{array} \\right) \\end{equation} \\] \\[ \\begin{equation} \\begin{split} Var \\left( \\begin{array}{c} X \\\\ Y \\\\ \\end{array} \\right) &amp;= \\left( \\begin{array}{c} Var(X) &amp; Cov(X,Y) \\\\ Cov(X,Y) &amp; Var(Y) \\\\ \\end{array} \\right) \\\\ &amp;= \\left( \\begin{array}{c} E((X-\\mu_X)^2) &amp; E((X-\\mu_X)(Y-\\mu_Y)) \\\\ E((X-\\mu_X)(Y-\\mu_Y)) &amp; E((Y-\\mu_Y)^2) \\\\ \\end{array} \\right) \\end{split} \\end{equation} \\] Properties \\(E(aX + bY + c)=aE(X) +bE(Y) + c\\) \\(Var(aX + bY + c) = a^2Var(X) + b^2Var(Y) + 2abCov(X,Y)\\) \\(Cov(aX + bY, cX + bY) =acVar(X)+bdVar(Y) + (ad+bc)Cov(X,Y)\\) Correlation: \\(\\rho_{XY} = \\frac{Cov(X,Y)}{\\sigma_X\\sigma_Y}\\) 2.2.5 Distributions Conditional Distributions \\[ f_{X|Y}(X|Y=y)=\\frac{f(X,Y)}{f_Y(y)} \\] \\(f_{X|Y}(X|Y=y)=f_X(X)\\) if X and Y are independent 2.2.5.1 Discrete CDF: Cumulative Density Function MGF: Moment Generating Function 2.2.5.1.1 Bernoulli \\(Bernoulli(p)\\) PDF hist(mc2d::rbern(100000, prob=.5)) 2.2.5.1.2 Binomial \\(B(n,p)\\) the experiment consists of a fixed number (n) of Bernoulli trials, each of which results in a success (s) or failure (f). The trials are identical and independent, and probability of success (p) and probability of failure (q = 1- p) remains the same for all trials. The random variable X denotes the number of successes obtained in the n trials. Density \\[ f(x)={{n}\\choose{x}}p^xq^{n-x} \\] CDF You have to use table PDF # Histogram of 100000 random values from a sample of 100 with probability of 0.5 hist(rbinom(100000, size = 100, prob = 0.5)) MGF \\[ m_X(t) =(q+pe^t)^n \\] Mean \\[ \\mu = E(x) = np \\] Variance \\[ \\sigma^2 =Var(X) = npq \\] 2.2.5.1.3 Poisson \\(Pois(\\lambda)\\) Arises with Poisson process, which involves observing discrete events in a continuous interval of time, length, or space. The random variable X is the number of occurrences of the event within an interval of s units The parameter \\(\\lambda\\) is the average number of occurrences of the event in question per measurement unit. For the distribution, we use the parameter \\(k=\\lambda s\\) Density \\[ f(x) = \\frac{e^{-k}k^x}{x!} \\] ,k &gt; 0, x =0,1, CDF Use table PDF # Poisson dist with mean of 5 or Poisson(5) hist(rpois(10000, lambda = 5)) MGF \\[ m_X(t)=e^{k(e^t-1)} \\] Mean \\[ \\mu = E(X) = k \\] Variance \\[ \\sigma^2 = Var(X) = k \\] 2.2.5.1.4 Geometric The experiment consists of a series of trails. The outcome of each trial can be classed as being either a success (s) or failure (f). (This is called a Bernoulli trial). The trials are identical and independent in the sense that the outcome of one trial has no effect on the outcome of any other. The probability of success (p) and probability of failure (q=1-p) remains the same from trial to trial. lack of memory X: the number of trials needed to obtain the first success. Density \\[ f(x)=pq^{x-1} \\] CDF \\[ F(x) = 1- q^x \\] PDF # hist of Geometric distribution with probability of success = 0.5 hist(rgeom(n = 10000, prob = 0.5)) MGF \\[ m_X(t) = \\frac{pe^t}{1-qe^t} \\] for \\(t &lt; -ln(q)\\) Mean \\[ \\mu = \\frac{1}{p} \\] Variance \\[ \\sigma^2 = Var(X) = \\frac{q}{p^2} \\] 2.2.5.1.5 Hypergeometric The experiment consists of drawing a random sample of size n without replacement and without regard to order from a collection of N objects. Of the N objects, r have a trait of interest; N-r do not have the trait X is the number of objects in the sample with the trait. Density \\[ f(x)=\\frac{{{r}\\choose{x}}{{N-r}\\choose{n-x}}}{{{N}\\choose{n}}} \\] where \\(max[0,n-(N-r)] \\le x \\le min(n,r)\\) PDF # hist of hypergeometric distribution with the number of white balls = 50, and the number of black balls = 20, and number of balls drawn = 30. hist(rhyper(nn = 10000 , m=50, n=20, k=30)) Mean \\[ \\mu = E(x)= \\frac{nr}{N} \\] Variance \\[ \\sigma^2 = var(X) = n (\\frac{r}{N})(\\frac{N-r}{N})(\\frac{N-n}{N-1}) \\] Note For large N (if \\(\\frac{n}{N} \\le 0.05\\)), this distribution can be approximated using a Binomial distribution with \\(p = \\frac{r}{N}\\) 2.2.5.1.6 2.2.5.1.7 2.2.5.1.8 2.2.5.1.9 2.2.5.1.10 2.2.5.1.11 2.2.5.1.12 2.2.5.2 Continuous 2.2.5.2.1 Uniform Defined over an interval (a,b) in which the probabilities are equally likely for subintervals of equal length. Density \\[ f(x)=\\frac{1}{b-a} \\] for a &lt; x &lt; b CDF \\[\\begin{cases} 0 &amp; \\text{if x &lt;a } \\\\ \\frac{x-a}{b-a} &amp; \\text{if $a \\le x \\le b$ }\\\\ 1 &amp; \\text{if x &gt;b}\\\\ \\end{cases}\\] PDF hist(runif(100000, min = 0, max = 1)) MGF \\[\\begin{cases} \\frac{e^{tb} - e^{ta}}{t(b-a)}&amp;\\text{ if $t \\neq 0$}\\\\ 1&amp;\\text{if $ t \\neq 0$}\\\\ \\end{cases}\\] Mean \\[ \\mu = E(X) = \\frac{a +b}{2} \\] Variance \\[ \\sigma^2 = Var(X) = \\frac{(b-a)^2}{12} \\] 2.2.5.2.2 Gamma is used to define the exponential and chi-squared distributions The gamma function is defined as: \\[ \\Gamma(\\alpha) = \\int_0^{\\infty} z^{\\alpha-1}e^{-z}dz \\] where \\(\\alpha &gt; 0\\) Properties of The Gamma function: + \\(\\Gamma(1) = 1\\) + For \\(\\alpha &gt;1\\), \\(\\Gamma(\\alpha)=(\\alpha-1)\\Gamma(\\alpha-1)\\) + If n is an integer and \\(n&gt;1\\), then \\(\\Gamma(n) = (n-1)!\\) Density \\[ f(x) = \\frac{1}{\\Gamma(\\alpha)\\beta^{\\alpha}}x^{\\alpha-1}e^{-x/\\beta} \\] CDF \\[ F(x,n,\\beta) = 1 -\\sum_{k=0}^{n-1} \\frac{(\\frac{x}{\\beta})^k e^{-x/\\beta}}{k!} \\] for x&gt;0, and \\(\\alpha = n\\) (a positive integer) PDF hist(rgamma(n = 10000, shape = 5, rate = 1)) MGF \\[ m_X(t) = (1-\\beta t)^{-\\alpha} \\] where \\(t &lt; \\frac{1}{\\beta}\\) Mean \\[ \\mu = E(X) = \\alpha \\beta \\] Variance \\[ \\sigma^2 = Var(X) = \\alpha \\beta^2 \\] 2.2.5.2.3 Normal \\(N(\\mu,\\sigma^2)\\) is symmetric, bell-shaped curve with parameters \\(\\mu\\) and \\(\\sigma^2\\) also known as Gaussian. Density \\[ f(x) = \\frac{1}{\\sqrt{2\\pi \\sigma}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2} \\] for \\(-\\infty &lt; x, \\mu&lt; \\infty, \\sigma &gt; 0\\) CDF Use table PDF hist(rnorm(1000000, mean = 0, sd = 1)) MGF \\[ m_X(t) = e^{\\mu t + \\frac{\\sigma^2 t^2}{2}} \\] Mean \\[ \\mu = E(X) \\] Variance \\[ \\sigma^2 = Var(X) \\] Standard Normal Random Variable The normal random variable Z with mean \\(\\mu = 0\\) and standard deviation \\(\\sigma =1\\) is called standard normal Any normal random variable X with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) can be converted to the standard normal random variable \\(Z = \\frac{X-\\mu}{\\sigma}\\) Normal Approximation to the Binomial Distribution Let X be binomial with parameters n and p. For large n (so that \\((A)p \\le .5\\) and \\(np &gt; 5\\) or (B) \\(p&gt;.5\\) and \\(nq&gt;5\\)), X is approximately normally distributed with mean \\(\\mu = np\\) and standard deviation \\(\\sigma = \\sqrt{npq}\\) When using the normal approximation, add or subtract 0.5 as needed for the continuity correction Discrete Approximate Normal (corrected) P(X = c) P(c -0.5 &lt; Y &lt; c + 0.5) P(X &lt; c) P(Y &lt; c - 0.5) P(X c) P(Y &lt; c + 0.5) P(X &gt; c) P(Y &gt; c + 0.5) P(X c) P(Y &gt; c - 0.5) Normal Probability Rule If X is normally distributed with parameters \\(\\mu\\) and \\(\\sigma\\), then * \\(P(-\\sigma &lt; X - \\mu &lt; \\sigma) \\approx .68\\) * \\(P(-2\\sigma &lt; X - \\mu &lt; 2\\sigma) \\approx .95\\) * \\(P(-3\\sigma &lt; X - \\mu &lt; 3\\sigma) \\approx .997\\) 2.2.5.2.4 Logistic \\(Logistic(\\mu,s)\\) PDF hist(rlogis(n = 100000, location = 0, scale = 1)) 2.2.5.2.5 Lognomral \\(lognormal(\\mu,\\sigma^2)\\) PDF hist(rlnorm(n = 10000, meanlog = 0, sdlog = 1)) 2.2.5.2.6 Exponential \\(Exp(\\lambda)\\) A special case of the gamma distribution with \\(\\alpha = 1\\) Lack of memory \\(\\lambda\\) = rate Within a Poisson process with parameter \\(\\lambda\\), if W is the waiting tine until the occurrence of the first event, then W has an exponential distribution with \\(\\beta = 1/\\alpha\\) Density \\[ f(x) = \\frac{1}{\\beta} e^{-x/\\beta} \\] for \\(x,\\beta &gt; 0\\) CDF \\[\\begin{equation} F(x) = \\begin{cases} 0 &amp; \\text{ if $x \\le 0$}\\\\ 1 - e^{-x/\\beta} &amp; \\text{if $x &gt; 0$}\\\\ \\end{cases} \\end{equation}\\] PDF hist(rexp(n = 100000, rate = 1)) MGF \\[ m_X(t) = (1-\\beta t)^{-1} \\] for \\(t &lt; 1/\\beta\\) Mean \\[ \\mu = E(X) = \\beta \\] Variance \\[ \\sigma^2 = Var(X) =\\beta^2 \\] 2.2.5.2.7 Chi-squared \\(\\chi^2=\\chi^2(k)\\) A special case of the gamma distribution with \\(\\beta =2\\), and \\(\\alpha = \\gamma /2\\) for a positive integer \\(\\gamma\\) The random variable X is denoted \\(\\chi_{\\gamma}^2\\) and is said to have a chi-squared distribution with \\(\\gamma\\) degrees of freedom. Density Use density for Gamma Distribution with \\(\\beta = 2\\) and \\(\\alpha = \\gamma/2\\) CDF Use table PDF hist(rchisq(n = 10000, df=2, ncp = 0)) MGF \\[ m_X(t) = (1-2t)^{-\\gamma/2} \\] Mean \\[ \\mu = E(X) = \\gamma \\] Variance \\[ \\sigma^2 = Var(X) = 2\\gamma \\] 2.2.5.2.8 Student T \\(T(v)\\) \\(T=\\frac{Z}{\\sqrt{\\chi_{\\gamma}^2/\\gamma}}\\), where Z is standard normal follows a student-t distribution with \\(\\gamma\\) dof The distribution is symmetric, bell-shaped , with a mean of \\(\\mu=0\\) hist(rt(n = 100000, df=2, ncp =1)) 2.2.5.2.9 F-Distribution \\(F(d_1,d_2)\\) F distribution is strictly positive \\(F=\\frac{\\chi_{\\gamma_1}^2/\\gamma_1}{\\chi_{\\gamma_2^2}/\\gamma_2}\\) follows an F distribution with dof \\(\\gamma_1\\) and \\(\\gamma_2\\), where \\(\\chi_{\\gamma_1}^2\\) and \\(\\chi_{\\gamma_2}^2\\) are independent chi-squared random variables. The distribution is asymmetric and never negative. PDF hist(rf(n = 100000, df1=2, df2=3, ncp=1)) 2.2.5.2.10 Cauchy Central Limit Theorem and Weak Law do not apply to Cauchy because it does not have finite mean and finite variance PDF hist(rcauchy(n = 100000, location = 0, scale = 1)) 2.2.5.2.11 2.2.5.2.12 2.2.5.2.13 2.2.5.2.14 "],["general-math.html", "2.3 General Math", " 2.3 General Math Chebyshevs Inequality Let X be a random variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). Then for any postiive number k: \\[ P(|X-\\mu| &lt; k\\sigma) \\ge 1 - \\frac{1}{k^2} \\] Chebyshevs Inequality does not require that X be normally distributed Maclaurin series expansion for \\[ e^z = 1 + z + \\frac{z^2}{2!} + \\frac{z^3}{3!} + ... \\] Geometric series: \\[ s_n=\\sum_{k=1}^{n}ar^{n-1}=\\frac{a(1-r^n)}{1-r} \\] if |r| &lt; 1 \\[ s=\\sum_{k=1}^{\\infty}ar^{n-1}=\\frac{a}{1-r} \\] 2.3.1 Law of large numbers Let \\(X_1,X_2,...\\) be an infinite sequence of independent and identically distributed (i.i.d) Then, the sample average is \\[ \\bar{X}_n =\\frac{1}{n} (X_1 + ... + X_n) \\] converges to the expected value (\\(\\bar{X}_n \\rightarrow \\mu\\)) as \\(n \\rightarrow \\infty\\) \\[ Var(X_i) = Var(\\frac{1}{n}(X_1 + ... + X_n)) = \\frac{1}{n^2}Var(X_1 + ... + X_n)= \\frac{n\\sigma^2}{n^2}=\\frac{\\sigma^2}{n} \\] The difference between Weak Law and Strong Law regards the mode of convergence 2.3.1.1 Weak Law The sample average converges in probability towards the expected value \\[ \\bar{X}_n \\rightarrow^{p} \\mu \\] when \\(n \\rightarrow \\infty\\) \\[ \\lim_{n\\to \\infty}P(|\\bar{X}_n - \\mu| &gt; \\epsilon) = 0 \\] The sample mean from a iid random sample (\\(\\{ x_i \\}_{i=1}^n\\)) from any population with a finite mean and finite variance \\(\\sigma^2\\) is ca consistent estimation for the population mean \\(\\mu\\) \\[ plim(\\bar{x})=plim(n^{-1}\\sum_{i=1}^{n}x_i) =\\mu \\] 2.3.1.2 Strong Law The sample average converges almost surely to the expected value \\[ \\bar{X}_n \\rightarrow^{a.s} \\mu \\] when \\(n \\rightarrow \\infty\\) Equivalently, \\[ P(\\lim_{n\\to \\infty}\\bar{X}_n =\\mu) =1 \\] 2.3.2 Law of Iterated Expectation Let X, Y be random variables. Then, \\[ E(X) = E(E(X|Y)) \\] means that the expected value of X can be calculated from the probability distribution of X|Y and Y 2.3.3 Convergence 2.3.3.1 Convergence in Probability \\(n \\rightarrow \\infty\\), an estimator (random variable) that is close to the true value. The random variable \\(\\theta_n\\) converges in probability to a constant c if \\[ \\lim_{n\\to \\infty}P(|\\theta_n - c| \\ge \\epsilon) = 0 \\] for any positive \\(\\epsilon\\) Notation \\[ plim(\\theta_n)=c \\] Equivalently, \\[ \\theta_n \\rightarrow^p c \\] Properties of Convergence in Probability Slutskys Theorem: for a continuous function g(.), if \\(plim(\\theta_n)= \\theta\\) then \\(plim(g(\\theta_n)) = g(\\theta)\\) if \\(\\gamma_n \\rightarrow^p \\gamma\\) then + \\(plim(\\theta_n + \\gamma_n)=\\theta + \\gamma\\) + \\(plim(\\theta_n \\gamma_n) = \\theta \\gamma\\) + \\(plim(\\theta_n/\\gamma_n) = \\theta/\\gamma\\) if \\(\\gamma \\neq 0\\) Also hold for random vectors/ matrices 2.3.3.2 Convergence in Distribution As \\(n \\rightarrow \\infty\\), the distribution of a random variable may converge towards another (fixed) distribution. The random variable \\(X_n\\) with CDF \\(F_n(x)\\) converges in distribution to a random variable X with CDF \\(F(X)\\) if \\[ \\lim_{n\\to \\infty}|F_n(x) - F(x)| = 0 \\] at all points of continuity of \\(F(X)\\) Notation F(x) is the limiting distribution of \\(X_n\\) or \\(X_n \\rightarrow^d X\\) E(X) is the limiting mean (asymptotic mean) Var(X) is the limiting variance (asymptotic variance) Note \\[ E(X) \\neq \\lim_{n\\to \\infty}E(X_n) \\\\ Avar(X_n) \\neq \\lim_{n\\to \\infty}Var(X_n) \\] Properties of Convergence in Distribution Continuous Mapping Theorem: for a continuous function g(.), if \\(X_n \\to^{d} g(X)\\) then \\(g(X_n) \\to^{d} g(X)\\) if \\(Y_n\\to^{d} c\\), then + $X_n + Y_n \\to^{d} X + c$ + $Y_nX_n \\to^{d} cX$ + $X_nY_n \\to^{d} X/c$ if $c \\neq 0$ also hold for random vectors/matrices 2.3.3.3 Summary Properties of Convergence Probability Distribution Slutskys Theorem: for a continuous function g(.), if \\(plim(\\theta_n)= \\theta\\) then \\(plim(g(\\theta_n)) = g(\\theta)\\) Continuous Mapping Theorem: for a continuous function g(.), if \\(X_n \\to^{d} g(X)\\) then \\(g(X_n) \\to^{d} g(X)\\) if \\(\\gamma_n \\rightarrow^p \\gamma\\) then if \\(Y_n\\to^{d} c\\), then \\(plim(\\theta_n + \\gamma_n)=\\theta + \\gamma\\) \\(X_n + Y_n \\to^{d} X + c\\) \\(plim(\\theta_n \\gamma_n) = \\theta \\gamma\\) \\(Y_nX_n \\to^{d} cX\\) \\(plim(\\theta_n/\\gamma_n) = \\theta/\\gamma\\) if \\(\\gamma \\neq 0\\) \\(X_nY_n \\to^{d} X/c\\) if \\(c \\neq 0\\) Convergence in Probability is stronger than Convergence in Distribution. However, Convergence in Distribution does not guarantee [Convergence in Probability] 2.3.4 Sufficient Statistics Likelihood describes the extent to which the sample provides support for any particular parameter value. Higher support corresponds to a higher value for the likelihood The exact value of any likelihood is meaningless, The relative value, (i.e., comparing two values of \\(\\theta\\)), is informative. \\[ L(\\theta_0; y) = P(Y = y | \\theta = \\theta_0) = f_Y(y;\\theta_0) \\] Likelihood Ratio \\[ \\frac{L(\\theta_0;y)}{L(\\theta_1;y)} \\] Likelihood Function For a given sample, you can create likelihoods for all possible values of \\(\\theta\\), which is called likelihood function \\[ L(\\theta) = L(\\theta; y) = f_Y(y;\\theta) \\] In a sample of size n, the likelihood function takes the form of a product \\[ L(\\theta) = \\prod_{i=1}^{n}f_i (y_i;\\theta) \\] Equivalently, the log likelihood function \\[ l(\\theta) = \\sum_{i=1}^{n} logf_i(y_i;\\theta) \\] Sufficient statistics A statistic, T(y), is any quantity that can be calculated purely from a sample (independent of \\(\\theta\\)) A statistic is sufficient if it conveys all the available information about the parameter. \\[ L(\\theta; y) = c(y)L^*(\\theta;T(y)) \\] Nuisance parameters If we are interested in a parameter (e.g., mean). Other parameters requiring estimation (e.g., standard deviation) are nuisance parameters. We can replace nuisance parameters in likelihood function with their estimates to create a **profile likelihood*. 2.3.5 Parameter transformations log-odds transformation \\[ Log odds = g(\\theta)= ln[\\frac{\\theta}{1-\\theta}] \\] log transformation $$ $$ "],["methods.html", "2.4 Methods", " 2.4 Methods Trade-off between parametric and non-parametric "],["descriptive-stat.html", "Chapter 3 Descriptive Statistics", " Chapter 3 Descriptive Statistics When you have an area of interest that you want to research, a problem that you want to solve, a relationship that you want to investigate, theoretical and empirical processes will help you. Estimand is defined as a quantity of scientific interest that can be calculated in the population and does not change its value depending on the data collection design used to measure it (i.e., it does not vary with sample size and survey design, or the number of nonrespondents, or follow-up efforts). (Rubin 1996) Estimands include: population means Population variances correlations factor loading regression coefficients References "],["numerical-measures.html", "3.1 Numerical Measures", " 3.1 Numerical Measures There are differences between a population and a sample Measures of Category Population Sample - What is it? Reality A small fraction of reality (inference) - Characteristics described by Parameters Statistics Central Tendency Mean \\(\\mu = E(Y)\\) \\(\\hat{\\mu} = \\overline{y}\\) Central Tendency Median 50-th percentile \\(y_{(\\frac{n+1}{2})}\\) Dispersion Variance \\(\\sigma^2=var(Y)\\) \\(=E(Y-\\mu)^2\\) \\(s^2=\\frac{1}{n-1} \\sum_{i = 1}^{n} (y_i-\\overline{y})^2\\) \\(=\\frac{1}{n-1} \\sum_{i = 1}^{n} (y_i^2-n\\overline{y}^2)\\) Dispersion Coefficient of Variation \\(\\frac{\\sigma}{\\mu}\\) \\(\\frac{s}{\\overline{y}}\\) Dispersion Interquartile Range difference between 25th and 75th percentiles. Robust to outliers Shape Skewness Standardized 3rd central moment (unitless) \\(g_1=\\frac{\\mu_3}{\\mu_2^{3/2}}\\) \\(\\hat{g_1}=\\frac{m_3}{m_2sqrt(m_2)}\\) Shape Central moments \\(\\mu=E(Y)\\) \\(\\mu_2 = \\sigma^2=E(Y-\\mu)^2\\) \\(\\mu_3 = E(Y-\\mu)^3\\) \\(\\mu_4 = E(Y-\\mu)^4\\) \\(m_2=\\sum_{i=1}^{n}(y_1-\\overline{y})^2/n\\) \\(m_3=\\sum_{i=1}^{n}(y_1-\\overline{y})^3/n\\) Shape Kurtosis (peakedness and tail thickness) Standardized 4th central moment \\(g_2^*=\\frac{E(Y-\\mu)^4}{\\sigma^4}\\) \\(\\hat{g_2}=\\frac{m_4}{m_2^2}-3\\) Note: Order Statistics: \\(y_{(1)},y_{(2)},...,y_{(n)}\\) where \\(y_{(1)}&lt;y_{(2)}&lt;...&lt;y_{(n)}\\) Coefficient of variation: standard deviation over mean. This metric is stable, dimensionless statistic for comparison. Symmetric: mean = median, skewness = 0 Skewed right: mean &gt; median, skewness &gt; 0 Skewed left: mean &lt; median, skewness &lt; 0 Central moments: \\(\\mu=E(Y)\\) , \\(\\mu_2 = \\sigma^2=E(Y-\\mu)^2\\) , \\(\\mu_3 = E(Y-\\mu)^3\\), \\(\\mu_4 = E(Y-\\mu)^4\\) For normal distributions, \\(\\mu_3=0\\), so \\(g_1=0\\) \\(\\hat{g_1}\\) is distributed approximately as N(0,6/n) if sample is from a normal population. (valid when n &gt; 150) For large samples, inferece on skewness can be based on normal tables with 95% confidence itnerval for \\(g_1\\) as \\(\\hat{g_1}\\pm1.96\\sqrt{6/n}\\) For small samples, special tables from Snedecor and Cochran 1989, Table A 19(i) or Monte Carlo test Kurtosis &gt; 0 (leptokurtic) heavier tail compared to a normal distribution with the same \\(\\sigma\\) (e.g., t-distribution) Kurtosis &lt; 0 (platykurtic) lighter tail compared to a normal distribution with the same \\(\\sigma\\) For a normal distribution, \\(g_2^*=3\\). Kurtosis is often redefined as: \\(g_2=\\frac{E(Y-\\mu)^4}{\\sigma^4}-3\\) where the 4th central moment is estimated by \\(m_4=\\sum_{i=1}^{n}(y_i-\\overline{y})^4/n\\) the asymptotic sampling distribution for \\(\\hat{g_2}\\) is approximately N(0,24/n) (with n &gt; 1000) large sample on kurtosis uses standard normal tables small sample uses tables by Snedecor and Cochran, 1989, Table A 19(ii) or Geary 1936 data = rnorm(100) library(e1071) skewness(data,type = 1) ## [1] 0.1982115 kurtosis(data, type = 1) ## [1] -0.4677954 "],["graphical-measures.html", "3.2 Graphical Measures", " 3.2 Graphical Measures 3.2.1 Shape Its a good habit to label your graph, so others can easily follow. data = rnorm(100) # Histogram hist(data,labels = T,col=&quot;grey&quot;,breaks = 12) # Interactive histogram pacman::p_load(&quot;highcharter&quot;) hchart(data) # Box-and-Whisker plot boxplot(count ~ spray, data = InsectSprays,col = &quot;lightgray&quot;,main=&quot;boxplot&quot;) # Notched Boxplot boxplot(len~supp*dose, data=ToothGrowth, notch=TRUE, col=(c(&quot;gold&quot;,&quot;darkgreen&quot;)), main=&quot;Tooth Growth&quot;, xlab=&quot;Suppliment and Dose&quot;) # If notches differ -&gt; medians differ # Stem-and-Leaf Plots stem(data) ## ## The decimal point is at the | ## ## -3 | 6 ## -2 | 60 ## -1 | 955432221000 ## -0 | 9887666555444432222222222211110 ## 0 | 00011111223334444455666667778888899 ## 1 | 00112234456666679 ## 2 | 13 # Bagplot - A 2D Boxplot Extension pacman::p_load(aplpack) attach(mtcars) bagplot(wt,mpg, xlab=&quot;Car Weight&quot;, ylab=&quot;Miles Per Gallon&quot;, main=&quot;Bagplot Example&quot;) Others more advanced plots # boxplot.matrix() #library(&quot;sfsmisc&quot;) # boxplot.n() #library(&quot;gplots&quot;) # vioplot() #library(&quot;vioplot&quot;) "],["normality-assessment.html", "3.3 Normality Assessment", " 3.3 Normality Assessment Since Normal (Gaussian) distribution has many applications, we typically want/ wish our data or our variable is normal. Hence, we have to assess the normality based on not only Numerical Measures but also Graphical Measures 3.3.1 Graphical Assessment pacman::p_load(&quot;car&quot;) qqnorm(precip, ylab = &quot;Precipitation [in/yr] for 70 US cities&quot;) qqline(precip) The straight line represents the theoretical line for normally distributed data. The dots represent real empirical data that we are checking. If all the dots fall on the straight line, we can be confident that our data follow a normal distribution. If our data wiggle and deviate from the line, we should be concerned with the normality assumption. 3.3.2 Summary Statistics Sometimes its hard to tell whether your data follow the normal distribution by just looking at the graph. Hence, we often have to conduct statistical test to aid our decision. Common tests are Methods based on normal probability plot Correlation Coefficient with Normal Probability Plots Shapiro-Wilk Test Methods based on empirical cumulative distribution function Anderson-Darling Test Kolmogorov-Smirnov Test Cramer-von Mises Test JarqueBera Test 3.3.2.1 Methods based on normal probability plot 3.3.2.1.1 Correlation Coefficient with Normal Probability Plots (Looney and Gulledge 1985) (Shapiro and Francia 1972) The correlation coefficient between \\(y_{(i)}\\) and \\(m_i^*\\) as given on the normal probability plot: \\[W^*=\\frac{\\sum_{i=1}^{n}(y_{(i)}-\\bar{y})(m_i^*-0)}{(\\sum_{i=1}^{n}(y_{(i)}-\\bar{y})^2\\sum_{i=1}^{n}(m_i^*-0)^2)^.5}\\] where \\(\\bar{m^*}=0\\) Pearson product moment formula for correlation: \\[\\hat{p}=\\frac{\\sum_{i-1}^{n}(y_i-\\bar{y})(x_i-\\bar{x})}{(\\sum_{i=1}^{n}(y_{i}-\\bar{y})^2\\sum_{i=1}^{n}(x_i-\\bar{x})^2)^.5}\\] When the correlation is 1, the plot is exactly linear and normality is assumed. The closer the correlation is to zero, the more confident we are to reject normality Inference on W* needs to be based on special tables (Looney and Gulledge 1985) library(&quot;EnvStats&quot;) gofTest(data,test=&quot;ppcc&quot;)$p.value #Probability Plot Correlation Coefficient ## [1] 0.0657035 3.3.2.1.2 Shapiro-Wilk Test (Shapiro and Wilk 1965) \\[W=(\\frac{\\sum_{i=1}^{n}a_i(y_{(i)}-\\bar{y})(m_i^*-0)}{(\\sum_{i=1}^{n}a_i^2(y_{(i)}-\\bar{y})^2\\sum_{i=1}^{n}(m_i^*-0)^2)^.5})^2\\] where \\(a_1,..,a_n\\) are weights computed from the covariance matrix for the order statistics. Researchers typically use this test to assess normality. (n &lt; 2000) Under normality, W is close to 1, just like \\(W^*\\). Notice that the only difference between W and W* is the weights. gofTest(data,test=&quot;sw&quot;)$p.value #Shapiro-Wilk is the default. ## [1] 0.1241497 3.3.2.2 Methods based on empirical cumulative distribution function The formula for the empirical cumulative distribution function (CDF) is: \\(F_n(t)\\) = estimate of probability that an observation \\(\\le\\) t = (number of observation \\(\\le\\) t)/n This method requires large sample sizes. However, it can apply to distributions other than the normal (Gaussian) one. # Empirical CDF hand-code plot.ecdf(data,verticals = T, do.points=F) 3.3.2.2.1 Anderson-Darling Test (Anderson and Darling 1952) The Anderson-Darling statistic: \\[A^2=\\int_{-\\infty}^{\\infty}(F_n(t)=F(t))^2\\frac{dF(t)}{F(t)(1-F(t))}\\] a weight average of squared deviations (it weights small and large values of t more) For the normal distribution, \\(A^2 = - (\\sum_{i=1}^{n}(2i-1)(ln(p_i) +ln(1-p_{n+1-i}))/n-n\\) where \\(p_i=\\Phi(\\frac{y_{(i)}-\\bar{y}}{s})\\), the probability that a standard normal variable is less than \\(\\frac{y_{(i)}-\\bar{y}}{s}\\) Reject normal assumption when \\(A^2\\) is too large Evaluate the null hypothesis that the observations are randomly selected from a normal population based on the critical value provided by (Marsaglia and Marsaglia 2004) and (Stephens 1974) This test can be applied to other distributions: Exponential Logistic Gumbel Extreme-value Weibull: log(Weibull) = Gumbel Gamma Logistic Cauchy von Mises Log-normal (two-parameter) Consult (Stephens 1974) for more detailed transformation and critical values. gofTest(data,test=&quot;ad&quot;)$p.value #Anderson-Darling ## [1] 0.4842151 3.3.2.2.2 Kolmogorov-Smirnov Test Based on the largest absolute difference between empirical and expected cumulative distribution Another deviation of K-S test is Kuipers test gofTest(data,test=&quot;ks&quot;)$p.value #Komogorov-Smirnov ## [1] 0.6000563 3.3.2.2.3 Cramer-von Mises Test Based on the average squared discrepancy between the empirical distribution and a given theoretical distribution. Each discrepancy is weighted equally (unlike Anderson-Darling test weights end points more heavily) gofTest(data,test=&quot;cvm&quot;)$p.value #Cramer-von Mises ## [1] 0.4976998 3.3.2.2.4 JarqueBera Test (Bera and Jarque 1981) Based on the skewness and kurtosis to test normality. \\(JB = \\frac{n}{6}(S^2+(K-3)^2/4)\\) where S is the sample skewness and K is the sample kurtosis \\(S=\\frac{\\hat{\\mu_3}}{\\hat{\\sigma}^3}=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})^3/n}{(\\sum_{i=1}^{n}(x_i-\\bar{x})^2/n)^\\frac{3}{2}}\\) \\(K=\\frac{\\hat{\\mu_4}}{\\hat{\\sigma}^4}=\\frac{\\sum_{i=1}^{n}(x_i-\\bar{x})^4/n}{(\\sum_{i=1}^{n}(x_i-\\bar{x})^2/n)^2}\\) recall \\(\\hat{\\sigma^2}\\) is the estimate of the second central moment (variance) \\(\\hat{\\mu_3}\\) and \\(\\hat{\\mu_4}\\) are the estimates of third and fourth central moments. If the data comes from a normal distribution, the JB statistic asymptotically has a chi-squared distribution with two degrees of freedom. The null hypothesis is a joint hypothesis of the skewness being zero and the excess kurtosis being zero. References "],["basic-statistical-inference.html", "Chapter 4 Basic Statistical Inference", " Chapter 4 Basic Statistical Inference One Sample Inference Two Sample Inference Categorical Data Analysis Random sample of size n: A collection of n independent random variables taken from the distribution X, each with the same distribution as X. Sample mean \\[ \\bar{X}= (\\sum_{i=1}^{n}X_i)/n \\] Sample Median \\(\\tilde{x}\\) = the middle observation in a sample of observation order from smallest to largest (or vice versa). If n is odd, \\(\\tilde{x}\\) is the middle observation, If n is even, \\(\\tilde{x}\\) is the average of the two middle observations. Sample variance \\[ S^2 = \\frac{\\sum_{i=1}^{n}(X_i = \\bar{X})^2}{n-1}= \\frac{n\\sum_{i=1}^{n}X_i^2 -(\\sum_{i=1}^{n}X_i)^2}{n(n-1)} \\] Sample standard deviation \\[ S = \\sqrt{S^2} \\] Sample proportions \\[ \\hat{p} = \\frac{X}{n} = \\frac{\\text{number in the sample with trait}}{\\text{sample size}} \\] \\[ \\widehat{p_1-p_2} = \\hat{p_1}-\\hat{p_2} = \\frac{X_1}{n_1} - \\frac{X_2}{n_2} = \\frac{n_2X_1 = n_1X_2}{n_1n_2} \\] Estimators Point Estimator \\(\\hat{\\theta}\\) is a statistic used to approximate a population parameter \\(\\theta\\) Point estimate The numerical value assumed by \\(\\hat{\\theta}\\) when evaluated for a given sample Unbiased estimator If \\(E(\\hat{\\theta}) = \\theta\\), then \\(\\hat{\\theta}\\) is an unbiased estimator for \\(\\theta\\) \\(\\bar{X}\\) is an unbiased estimator for \\(\\mu\\) \\(S^2\\) is an unbiased estimator for \\(\\sigma^2\\) \\(\\hat{p}\\) is an unbiased estimator for p \\(\\widehat{p_1-P_2}\\) is an unbiased estimator for \\(p_1- p_2\\) \\(\\bar{X_1} - \\bar{X_2}\\) is an unbiased estimator for \\(\\mu_1 - \\mu_2\\) Note: \\(S\\) is a biased estimator for \\(\\sigma\\) Distribution of the sample mean If \\(\\bar{X}\\) is the sample mean based on a random sample of size n drawn from a normal distribution X with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the \\(\\bar{X}\\) is normally distributed, with mean \\(\\mu_{\\bar{X}} = \\mu\\) and variance \\(\\sigma_{\\bar{X}}^2 = Var(\\bar{X}) = \\frac{\\sigma^2}{n}\\). Then the standard error of the mean is: \\(\\sigma_{\\bar{X}}= \\frac{\\sigma}{\\sqrt{n}}\\) "],["one-sample-inference.html", "4.1 One Sample Inference", " 4.1 One Sample Inference \\(Y_i \\sim i.i.d. N(\\mu, \\sigma^2)\\) i.i.d. standards for independent and identically distributed Hence, we have the following model: \\(Y_i=\\mu +\\epsilon_i\\) where \\(\\epsilon_i \\sim^{iid} N(0,\\sigma^2)\\) \\(E(Y_i)=\\mu\\) \\(Var(Y_i)=\\sigma^2\\) \\(\\bar{y} \\sim N(\\mu,\\sigma^2/n)\\) 4.1.1 Interval Estimation of the Mean When \\(\\sigma^2\\) is estimated by \\(s^2\\), then \\[ \\frac{\\bar{y}-\\mu}{s/\\sqrt{n}} \\sim t_{n-1} \\] Then, a \\(100(1-\\alpha) \\%\\) confidence interval for \\(\\mu\\) is obtained from: \\[ 1 - \\alpha = P(-t_{\\alpha/2;n-1} \\le \\frac{\\bar{y}-\\mu}{s/\\sqrt{n}} \\le t_{\\alpha/2;n-1}) \\\\ = P(\\bar{y} - (t_{\\alpha/2;n-1})s/\\sqrt{n} \\le \\mu \\le \\bar{y} + (t_{\\alpha/2;n-1})s/\\sqrt{n}) \\] And the interval is \\[ \\bar{y} \\pm (t_{\\alpha/2;n-1})s/\\sqrt{n} \\] and \\(s/\\sqrt{n}\\) is the standard error of \\(\\bar{y}\\) If the experiment were repeated many times, \\(100(1-\\alpha) \\%\\) of these intervals would contain \\(\\mu\\) 4.1.2 Interval Estimation for the Variance \\[ 1 - \\alpha = P( \\chi_{1-\\alpha/2;n-1}^2) \\le (n-1)s^2/\\sigma^2 \\le \\chi_{\\alpha/2;n-1}^2) \\\\ = P(\\frac{(n-1)s^2}{\\chi_{\\alpha/2}^2} \\le \\sigma^2 \\le \\frac{(n-1)s^2}{\\chi_{1-\\alpha/2}^2}) \\] and a \\(100(1-\\alpha) \\%\\) confidence interval for \\(\\sigma^2\\) is: \\[ (\\frac{(n-1)s^2}{\\chi_{\\alpha/2;n-1}^2},\\frac{(n-1)s^2}{\\chi_{1-\\alpha/2;n-1}^2}) \\] Confidence limits for \\(\\sigma^2\\) are obtained by computing the positive square roots of these limits 4.1.3 Power Formally, power (for the test of the mean) is given by: \\[ \\pi(\\mu) = 1 - \\beta = P(\\text{test rejects } H_0|\\mu) \\] To evaluate the power, one needs to know the distribution of the test statistic if the null hypothesis is false. For 1-sided z-test where \\(H_0: \\mu \\le \\mu_0 \\\\ H_A: \\mu &gt;0\\) The power is: \\[ \\begin{align} \\pi(\\mu) &amp;= P(\\bar{y} &gt; \\mu_0 + z_{\\alpha} \\sigma/\\sqrt{n}|\\mu) \\\\ &amp;= P(Z = \\frac{\\bar{y} - \\mu}{\\sigma / \\sqrt{n}} &gt; z_{\\alpha} + \\frac{\\mu_0 - \\mu}{\\sigma/ \\sqrt{n}}|\\mu) \\\\ &amp;= 1 - \\Phi(z_{\\alpha} + \\frac{(\\mu_0 - \\mu)\\sqrt{n}}{\\sigma}) \\\\ &amp;= \\Phi(-z_{\\alpha}+\\frac{(\\mu -\\mu_0)\\sqrt{n}}{\\sigma}) \\end{align} \\] where \\(1-\\Phi(x) = \\Phi(-x)\\) since the normal pdf is symmetric Power is correlated to the difference in \\(\\mu - \\mu_0\\), sample size n, variance \\(\\sigma^2\\), and the \\(\\alpha\\)-level of the test (through \\(z_{\\alpha}\\)) Equivalently, power can be increased by making \\(\\alpha\\) large, \\(\\sigma^2\\) smaller, or n larger. For 2-sided z-test is: \\[ \\pi(\\mu) = \\Phi(-z_{\\alpha/2} + \\frac{(\\mu_0 - \\mu)\\sqrt{n}}{\\sigma}) + \\Phi(-z_{\\alpha/2}+\\frac{(\\mu - \\mu_0)\\sqrt{n}}{\\sigma}) \\] 4.1.4 Sample Size 4.1.4.1 1-sided Z-test Example: to show that the mean response \\(\\mu\\) under the treatment is higher than the mean response \\(\\mu_0\\) without treatment (show that the treatment effect \\(\\delta = \\mu -\\mu_0\\) is large) Because power is an increasing function of \\(\\mu - \\mu_0\\), it is only necessary to find n that makes the power equal to \\(1- \\beta\\) at \\(\\mu = \\mu_0 + \\delta\\) Hence, we have \\[ \\pi(\\mu_0 + \\delta) = \\Phi(-z_{\\alpha} + \\frac{\\delta \\sqrt{n}}{\\sigma}) = 1 - \\beta \\] Since \\(\\Phi (z_{\\beta})= 1-\\beta\\), we have \\[ -z_{\\alpha} + \\frac{\\delta \\sqrt{n}}{\\sigma} = z_{\\beta} \\] Then n is \\[ n = (\\frac{(z_{\\alpha}+z_{\\beta})\\sigma}{\\delta})^2 \\] Then, we need larger samples, when the sample variability is large (\\(\\sigma\\) is large) \\(\\alpha\\) is small (\\(z_{\\alpha}\\) is large) power \\(1-\\beta\\) is large (\\(z_{\\beta}\\) is large) The magnitude of the effect is smaller (\\(\\delta\\) is small) Since we dont know \\(\\delta\\) and \\(\\sigma\\). We can base \\(\\sigma\\) on previous studies, pilot studies. Or, obtain an estimate of \\(\\sigma\\) by anticipating the range of the observation (without outliers). divide this range by 4 and use the resulting number as an approximate estimate of \\(\\sigma\\). For normal (distribution) data, this is reasonable. 4.1.4.2 2-sided Z-test We want to know the min n, required to guarantee \\(1-\\beta\\) power when the treatment effect \\(\\delta = |\\mu - \\mu_0|\\) is at least greater than 0. Since the power function for the 2-sided is increasing and symmetric in \\(|\\mu - \\mu_0|\\), we only need to find n that makes the power equal to \\(1-\\beta\\) when \\(\\mu = \\mu_0 + \\delta\\) \\[ n = (\\frac{(z_{\\alpha/2} + z_{\\beta}) \\sigma}{\\delta})^2 \\] We could also use the confidence interval apporach. If we reuqire that an \\(\\alpha\\)-level two-cided CI for \\(\\mu\\) be \\[ \\bar{y} \\pm D \\] where \\(D = z_{\\alpha/2}\\sigma/\\sqrt{n}\\) gives \\[ n = (\\frac{z_{\\alpha/2}\\sigma}{D})^2 \\] (round up to the nearest integer) data = rnorm(100) t.test(data, conf.level=0.95) ## ## One Sample t-test ## ## data: data ## t = 0.014583, df = 99, p-value = 0.9884 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -0.1893285 0.1921320 ## sample estimates: ## mean of x ## 0.001401736 \\[ H_0: \\mu \\ge 30 \\\\ H_a: \\mu &lt; 30 \\] t.test(data, mu=30,alternative=&quot;less&quot;) ## ## One Sample t-test ## ## data: data ## t = -312.08, df = 99, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is less than 30 ## 95 percent confidence interval: ## -Inf 0.1610047 ## sample estimates: ## mean of x ## 0.001401736 4.1.5 Note For t-tests, the sample and power are not as easy as z-test. \\[ \\pi(\\mu) = P(\\frac{\\bar{y}-\\mu_0}{s/\\sqrt{n}}&gt; t_{n-1;\\alpha}|\\mu) \\] when \\(\\mu &gt; \\mu_0\\) (i.e., \\(\\mu - \\mu_0 = \\delta\\)), the random variable \\((\\bar{y} - \\mu_0)/(s/\\sqrt{n})\\) does not have a Students t distribution, but rather is distributed as a non-central t-distribution with non-centrality parameter \\(\\delta \\sqrt{n}/\\sigma\\) and d.f. of \\(n-1\\) The power is an increasing function of this non-centrality parameter (note, when \\(\\delta = 0\\) the distribution is usual Students t-distribution). To evaluate power, one must consider numerical procedure or use special charts Approximate Sample Size Adjustment for t-test. We use an adjustment to the z-test determination for sample size. Let \\(v = n-1\\), where n is sample size derived based on the z-test power. Then the 2-sided t-test sample size (apporximate) is given: \\[ n^* = \\frac{(t_{v;\\alpha/2}+t_{v;\\beta})^2 \\sigma^2}{\\delta^2} \\] 4.1.6 One-sample Non-parametric Methods lecture.data=c(0.76, 0.82, 0.80, 0.79, 1.06, 0.83, -0.43, -0.34, 3.34, 2.33) 4.1.6.1 Sign Test If we want to test \\(H_0: \\mu_{(0.5)} = 0; H_a: \\mu_{(0.5)} &gt;0\\) where \\(\\mu_{(0.5)}\\) is the population median. We can Count the number of observation (\\(y_i\\)s) that exceed 0. Denote this number by \\(s_+\\), called the number of plus signs. Let \\(s_- = n - s_+\\), which is the number of minus signs. Reject \\(H_0\\) if \\(s_+\\) is large or equivalently, if \\(s_-\\) is small. To determine how large \\(s_+\\) must be to reject \\(H_0\\) at a given significance level, we need to know the distribution of the corresponding random variable \\(S_+\\) under the null hypothesis, which is a binomial with p = 1/2,w hen the null is true. To work out the null distribution using the binomial formula, we have \\(\\alpha\\)-level test rejects \\(H_0\\) if \\(s_+ \\ge b_{n,\\alpha}\\), where \\(b_{n,\\alpha}\\) is the upper \\(\\alpha\\) critical point of the \\(Bin(n,1/2)\\) distribution. Both \\(S_+\\) and \\(S_-\\) have this same distribution (\\(S = S_+ = S_-\\)). \\[ \\text{p-value} = P(S \\ge s_+) = \\sum_{i = s_+}^{n} {{n}\\choose{i}} (\\frac{1}{2})^n \\] equivalently, \\[ P(S \\le s_-) = \\sum_{i=0}^{s_-}{{n}\\choose{i}} (\\frac{1}{2})^2 \\] For large sample sizes, we could use the normal approximation for the binomial, in which case reject \\(H_0\\) if \\[ s_+ \\ge n/2 + 1/2 + z_{\\alpha}\\sqrt{n/4} \\] For the 2-sided test, we use the tests statistic \\(s_{max} = max(s_+,s_-)\\) or \\(s_{min} = min(s_+, s_-)\\). An \\(\\alpha\\)-level test rejects \\(H_0\\) if the p-value is \\(\\le \\alpha\\), where the p-value is computed from: \\[ p-value = 2 \\sum_{i=s_{max}}^{n} {{n}\\choose{i}} (\\frac{1}{2})^n = s \\sum_{i=0}^{s_{min}} {{n}\\choose{i}} (\\frac{1}{2})^n \\] Equivalently, rejecting \\(H_0\\) if \\(s_{max} \\ge b_{n,\\alpha/2}\\) A large sample normal approximation can be used, where \\[ z = \\frac{s_{max}- n/2 -1/2}{\\sqrt{n/4}} \\] and reject \\(H_0\\) at \\(\\alpha\\) if \\(z \\ge z_{\\alpha/2}\\) However, treatment of 0 is problematic for this test. Solution 1: randomly assign 0 to the positive or negative (2 researchers might get different results). Solution 2: count each 0 as a contribution 1/2 toward \\(s_+\\) and \\(s_-\\) (but then could not apply the binomial distribution) Solution 3: ignore 0 (reduces the power of test due to decreased sample size). binom.test(sum(lecture.data &gt; 0), length(lecture.data)) # alternative = &quot;greater&quot; or alternative = &quot;less&quot; ## ## Exact binomial test ## ## data: sum(lecture.data &gt; 0) and length(lecture.data) ## number of successes = 8, number of trials = 10, p-value = 0.1094 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.4439045 0.9747893 ## sample estimates: ## probability of success ## 0.8 4.1.6.2 Wilcoxon Signed Rank Test Since the Sign Test could not consider the magnitude of each observation from 0, the Wilcoxon Signed Rank Test improves by taking account the ordered magnitudes of the observation, but it will impose the requirement of symmetric to this test (while Sign Test does not) \\[ H_0: \\mu_{0.5} = 0 \\\\ H_a: \\mu_{0.5} &gt; 0 \\] (assume no ties or same observations) The signed rank test procedure: rank order the observation \\(y_i\\) in terms of their absolute values. Let \\(r_i\\) be the rank of \\(y_i\\) in this ordering. Since we assume no ties, the ranks \\(r_i\\) are uniquely determined and are a permutation of the integers 1,2,,n. Calculate \\(w_+\\), which is the sum of the ranks of the positive values, and \\(w_-\\), which is the sum of the ranks of the negative values. Note that \\(w_+ + w_- = r_1 + r_2 + ... = 1 + 2 + ... + n = n(n+1)/2\\) Reject \\(H_0\\) if \\(w_+\\) is large (or if \\(w_-\\) is small) To know what is large or small with regard to \\(w_+\\) and \\(w_-\\), we need the distribution of \\(W_+\\) and \\(W_-\\) when the null is true. Since these null distributions are identical and symmetric, the p-value is \\(P(W \\ge w_+) = P(W \\le w_-)\\) An \\(\\alpha\\)-level test rejects the null if the p-value is \\(\\le \\alpha\\), or if \\(w_+ \\ge w_{n,\\alpha}\\), where \\(w_{n,\\alpha}\\) is the upper \\(\\alpha\\) critical point of the null distribution of W. This distribution of W has a special table. For large n, the distribution of W is approximately normal. \\[ z = \\frac{w_+ - n(n+1) /4 -1/2}{\\sqrt{n(n+1)(2n+1)/24}} \\] The test rejcets \\(H_0\\) at level \\(\\alpha\\) if \\[ w_+ \\ge n(n+1)/4 +1/2 + z_{\\alpha}\\sqrt{n(n+1)(2n+1)/24} \\approx w_{n,\\alpha} \\] For the 2-sided test, we use \\(w_{max}=max(w_+,w_-)\\) or \\(w_{min}=min(w_+,w_-)\\), with p-value given by: \\[ p-value = 2P(W \\ge w_{max}) = 2P(W \\le w_{min}) \\] Same as Sign Test,we ignore 0. In some cases where some of the \\(|y_i|\\)s may be tied for the same rank, we simply assign each of the tied ranks the average rank (or midrank). Example, if \\(y_1 = -1\\), \\(y_3 = 3\\) and \\(y_3 = -3\\), and \\(y_4 =5\\), then \\(r_1 = 1\\), \\(r_2 = r_3=(2+3)/2 = 2.5\\), \\(r_4 = 4\\) wilcox.test(lecture.data) #does not use normal approximation (using the underlying W distribution) ## ## Wilcoxon signed rank exact test ## ## data: lecture.data ## V = 52, p-value = 0.009766 ## alternative hypothesis: true location is not equal to 0 wilcox.test(lecture.data,exact=F) #uses normal approximation ## ## Wilcoxon signed rank test with continuity correction ## ## data: lecture.data ## V = 52, p-value = 0.01443 ## alternative hypothesis: true location is not equal to 0 "],["two-sample-inference.html", "4.2 Two Sample Inference", " 4.2 Two Sample Inference 4.2.1 Means Suppose we have 2 sets of observations, \\(y_1,..., y_{n_y}\\) \\(x_1,...,x_{n_x}\\) that are random samples from two independent populations with means \\(\\mu_y\\) and \\(\\mu_x\\) and variances \\(\\sigma^2_y\\),\\(\\sigma^2_x\\). Our goal is to compare \\(\\mu_x\\) and \\(\\mu_y\\) or \\(\\sigma^2_y = \\sigma^2_x\\) 4.2.1.1 Large Sample Tests Assume that \\(n_y\\) and \\(n_x\\) are large (\\(\\ge 30\\)). Then, \\[ E(\\bar{y} - \\bar{x}) = \\mu_y - \\mu_x \\\\ Var(\\bar{y} - \\bar{x}) = \\sigma^2_y /n_y + \\sigma^2_x/n_x \\] Then, \\[ Z = \\frac{\\bar{y}-\\bar{x} - (\\mu_y - \\mu_x)}{\\sqrt{\\sigma^2_y /n_y + \\sigma^2_x/n_x}} \\sim N(0,1) \\] (according to Central Limit Theorem). For large samples, we can replace variances by their unbiased estimators (\\(s^2_y,s^2_x\\)), and get the same large sample distribution. An approximate \\(100(1-\\alpha) \\%\\) CI for \\(\\mu_y - \\mu_x\\) is given by: \\[ \\bar{y} - \\bar{x} \\pm z_{\\alpha/2}\\sqrt{s^2_y/n_y + s^2_x/n_x} \\] \\[ H_0: \\mu_y - \\mu_x = \\delta_0 \\\\ H_A: \\mu_y - \\mu_x \\neq \\delta_0 \\] at the \\(\\alpha\\)-level with the statistic: \\[ z = \\frac{\\bar{y}-\\bar{x} - \\delta_0}{\\sqrt{s^2_y /n_y + s^2_x/n_x}} \\] and reject \\(H_0\\) if \\(|z| &gt; z_{\\alpha/2}\\) If \\(\\delta = )\\), it means that we are testing whether two means are equal. 4.2.1.2 Small Sample Tests If the two samples are from normal distribution, iid \\(N(\\mu_y,\\sigma^2_y)\\) and iid \\(N(\\mu_x,\\sigma^2_x)\\) and the two samples are independent, we can do inference based on the t-distribution Then we have 2 cases Equal Variance Unequal Variance 4.2.1.2.1 Equal variance Assumptions iid: so that \\(var(\\bar{y}) = \\sigma^2_y / n_y ; var(\\bar{x}) = \\sigma^2_x / n_x\\) Independence between samples: No observation from one sample can influence any observation from the other sample, to have \\[ \\begin{align} var(\\bar{y} - \\bar{x}) &amp;= var(\\bar{y}) + var{\\bar{x}} - 2cov(\\bar{y},\\bar{x}) \\\\ &amp;= var(\\bar{y}) + var{\\bar{x}} \\\\ &amp;= \\sigma^2_y / n_y + \\sigma^2_x / n_x \\end{align} \\] Normality: Justifies the use of the t-distribution Let \\(\\sigma^2 = \\sigma^2_y = \\sigma^2_x\\). Then, \\(s^2_y\\) and \\(s^2_x\\) are both unbiased estimators of \\(\\sigma^2\\). We then can pool them. Then the pooled variance estimate is \\[ s^2 = \\frac{(n_y - 1)s^2_y + (n_x - 1)s^2_x}{(n_y-1)+(n_x-1)} \\] has \\(n_y + n_x -2\\) df. Then the test statistic \\[ T = \\frac{\\bar{y}- \\bar{x} -(\\mu_y - \\mu_x)}{s\\sqrt{1/n_y + 1/n_x}} \\sim t_{n_y + n_x -2} \\] \\(100(1 - \\alpha) \\%\\) CI for \\(\\mu_y - \\mu_x\\) is \\[ \\bar{y} - \\bar{x} \\pm (t_{n_y + n_x -2})s\\sqrt{1/n_y + 1/n_x} \\] Hypothesis testing: \\[ H_0: \\mu_y - \\mu_x = \\delta_0 \\\\ H_1: \\mu_y - \\mu_x \\neq \\delta_0 \\] we reject \\(H_0\\) if \\(|t| &gt; t_{n_y + n_x -2;\\alpha/2}\\) 4.2.1.2.2 Unequal Variance Assumptions Two samples are independent 1. Scatter plots 2. Correlation coefficient (if normal) Independence of observation in each sample 1. Test for serial correlation For each sample, homogeneity of variance 1. Scatter plots 2. Formal tests Normality Equality of variances (homogeneity of variance between samples) 1. F-test 2. Barlett test 3. [Modified Levene Test] To compare 2 normal \\(\\sigma^2_y \\neq \\sigma^2_x\\), we use the test statistic: \\[ T = \\frac{\\bar{y}- \\bar{x} -(\\mu_y - \\mu_x)}{\\sqrt{s^2_y/n_y + s^2_x/n_x}} \\] In this case, T does not follow the t-distribution (its distribution depends on the ratio of the unknown variances \\(\\sigma^2_y,\\sigma^2_x\\)). In the case of small sizes, we can can approximate tests by using the Welch-Satterthwaite method (Satterthwaite 1946). We assume T can be approximated by a t-distribution, and adjust the degrees of freedom. Let \\(w_y = s^2_y /n_y\\) and \\(w_x = s^2_x /n_x\\) (the ws are the square of the respective standard errors) Then, the degrees of freedom are \\[ v = \\frac{(w_y + w_x)^2}{w^2_y / (n_y-1) + w^2_x / (n_x-1)} \\] Since v is usually fractional, we truncate down to the nearest integer. \\(100 (1-\\alpha) \\%\\) CI for \\(\\mu_y - \\mu_x\\) is \\[ \\bar{y} - \\bar{x} \\pm t_{v,\\alpha/2} \\sqrt{s^2_y/n_y + s^2_x /n_x} \\] Reject \\(H_0\\) if \\(|t| &gt; t_{v,\\alpha/2}\\), where \\[ t = \\frac{\\bar{y} - \\bar{x}-\\delta_0}{\\sqrt{s^2_y/n_y + s^2_x /n_x}} \\] 4.2.2 Variances 4.2.2.1 F-test Test \\[ H_0: \\sigma^2_y = \\sigma^2_x \\\\ H_a: \\sigma^2_y \\neq \\sigma^2_x \\] Consider the test statistic, \\[ F= \\frac{s^2_y}{s^2_x} \\] Reject \\(H_0\\) if \\(F&gt;f_{n_y -1,n_x -1,\\alpha/2}\\) or \\(F&lt;f_{n_y -1,n_x -1,1-\\alpha/2}\\) Where \\(F&gt;f_{n_y -1,n_x -1,\\alpha/2}\\) and \\(F&lt;f_{n_y -1,n_x -1,1-\\alpha/2}\\) are the upper and lower \\(\\alpha/2\\) critical points of an F-distribution, with a \\(n_y-1\\) and \\(n_x-1\\) degrees of freedom. Note This test depends heavily on the assumption Normality. In particular, it could give to many significant results when observations come from long-tailed distributions (i.e., positive kurtosis). If we cannot find support for normality, then we can use nonparametric tests such as the [Modified Levene Test] data(iris) irisVe=iris$Petal.Width[iris$Species==&quot;versicolor&quot;] irisVi=iris$Petal.Width[iris$Species==&quot;virginica&quot;] var.test(irisVe,irisVi) ## ## F test to compare two variances ## ## data: irisVe and irisVi ## F = 0.51842, num df = 49, denom df = 49, p-value = 0.02335 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.2941935 0.9135614 ## sample estimates: ## ratio of variances ## 0.5184243 4.2.2.2 Modified Levene Test (Brown-Forsythe Test) considers averages of absolute deviations rather than squared deviations. Hence, less sensitive to long-tailed distributions. This test is still good for normal data For each sample, we consider the absolute deviation of each observation form the median: \\[ d_{y,i} = |y_i - y_{.5}| \\\\ d_{x,i} = |x_i - x_{.5}| \\] Then, \\[ t_L^* = \\frac{\\bar{d}_y-\\bar{d}_x}{s \\sqrt{1/n_y + 1/n_x}} \\] The pooled variance \\(s^2\\) is given by: \\[ s^2 = \\frac{\\sum_i^{n_y}(d_{y,i}-\\bar{d}_y)^2 + \\sum_j^{n_x}(d_{x,i}-\\bar{d}_x)^2}{n_y + n_x -2} \\] If the error terms have constant variance and \\(n_y\\) and \\(n_x\\) are not extremely small, then \\(t_L^* \\sim t_{n_x + n_y -2}\\) We reject the null hypothesis when \\(|t_L^*| &gt; t_{n_y + n_x -2;\\alpha/2}\\) This is just the two-sample t-test applied to the absolute deviations. dVe=abs(irisVe-median(irisVe)) dVi=abs(irisVi-median(irisVi)) t.test(dVe,dVi,var.equal=T) ## ## Two Sample t-test ## ## data: dVe and dVi ## t = -2.5584, df = 98, p-value = 0.01205 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.12784786 -0.01615214 ## sample estimates: ## mean of x mean of y ## 0.154 0.226 # small samples t-test t.test(irisVe,irisVi,var.equal=F) ## ## Welch Two Sample t-test ## ## data: irisVe and irisVi ## t = -14.625, df = 89.043, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.7951002 -0.6048998 ## sample estimates: ## mean of x mean of y ## 1.326 2.026 4.2.3 Power Consider \\(\\sigma^2_y = \\sigma^2_x = \\sigma^2\\) Under the assumption of equal variances, we take size samples from both groups (\\(n_y = n_x = n\\)) For 1-sided testing, \\[ H_0: \\mu_y - \\mu_x \\le 0 \\\\ H_a: \\mu_y - \\mu_x &gt; 0 \\] \\(\\alpha\\)-level z-test rejects \\(H_0\\) if \\[ z = \\frac{\\bar{y} - \\bar{x}}{\\sigma \\sqrt{2/n}} &gt; z_{\\alpha} \\] \\[ \\pi(\\mu_y - \\mu_x) = \\Phi(-z_{\\alpha} + \\frac{\\mu_y -\\mu_x}{\\sigma}\\sqrt{n/2}) \\] We need sample size n that gie at least \\(1-\\beta\\) power when \\(\\mu_y - \\mu_x = \\delta\\), where \\(\\delta\\) is the smallest difference that we want to see. Power is given by: \\[ \\Phi(-z_{\\alpha} + \\frac{\\delta}{\\sigma}\\sqrt{n/2}) = 1 - \\beta \\] 4.2.4 Sample Size Then, the sample size is \\[ n = 2(\\frac{\\sigma (z_{\\alpha} + z_{\\beta}}{\\delta})^2 \\] For 2-sided test, replace \\(z_{\\alpha}\\) with \\(z_{\\alpha/2}\\). As with the one-sample case, to perform an exact 2-sample t-test sample size calculation, we must use a non-central t-distribution. A correction that gives the approximate t-test sample size can be obtained by using the z-test n value in the formula: \\[ n^* = 2(\\frac{\\sigma (t_{2n-2;\\alpha} + t_{2n-2;\\beta})}{\\delta})^2 \\] where we use \\(alpha/2\\) for the two-sided test 4.2.5 Matched Pair Designs We have two treatments Subject Treatment A Treatment B Difference 1 \\(y_1\\) \\(x_1\\) \\(d_1 = y_1 - x_1\\) 2 \\(y_2\\) \\(x_2\\) \\(d_2 = y_2 - x_2\\) . . . . n \\(y_n\\) \\(x_n\\) \\(d_n = y_n - x_n\\) we assume \\(y_i \\sim^{iid} N(\\mu_y, \\sigma^2_y)\\) and \\(x_i \\sim^{iid} N(\\mu_x,\\sigma^2_x)\\), but since \\(y_i\\) and \\(x_i\\) are measured on the same subject, they are correlated. Let \\[ \\mu_D = E(y_i - x_i) = \\mu_y -\\mu_x \\\\ \\sigma^2_D = var(y_i - x_i) = Var(y_i) + Var(x_i) -2cov(y_i,x_i) \\] If the matching induces positive correlation, then the variance of the difference of the measurements is reduced as compared to the independent case. This is the point of Matched Pair Designs. Although covariance can be negative, giving a larger variance of the difference than the independent sample case, usually the covariance is positive. This means both \\(y_i\\) and \\(x_i\\) are large for many of the same subjects, and for others, both measurement are small. (we still assume that various subjects respond independently of each other, which is necessary for the iid assumption within groups). Let \\(d_i = y_i - x_i\\), then \\(\\bar{d} = \\bar{y}-\\bar{x}\\) is the sample mean of the \\(d_i\\) \\(s_d^2=\\frac{1}{n-1}\\sum_{i=1}^n (d_i - \\bar{d})^2\\) is the sample variance of the difference Once the data are converted to differences, we are back to One Sample Inference and can use its tests and CIs. 4.2.6 Nonparametric Tests for Two Samples For Matched Pair Designs, we can use the One-sample Non-parametric Methods. Assume that Y and X are random variables with CDF \\(F_y\\) and \\(F_x\\). then, Y is stochastically larger than X for all real number u, \\(P(Y &gt; u) \\ge P(X &gt; u)\\). Equivalently, \\(P(Y \\le u) \\le P(X \\le u)\\), which is \\(F_Y(u) \\le F_X(u)\\), same thing as \\(F_Y &lt; F_X\\) If two distributions are identical, except that one is shifted relative to the other, then each of distribution can be indexed by a location parameter, say \\(\\theta_y\\) and \\(\\theta_x\\). In this case, \\(Y&gt;X\\) if \\(\\theta_y &gt; \\theta_x\\) Consider the hypotheses, \\[ H_0: F_Y = F_X \\\\ H_a: F_Y &lt; F_X \\] where the alternative is an upper one-sided alternative. We can also consider the lower one-sided alternative \\[ H_a: F_Y &gt; F_X \\text{ or} \\\\ H_a: F_Y &lt; F_X \\text{ or } F_Y &gt; F_X \\] In this case, we dont use \\(H_a: F_Y \\neq F_X\\) as that allows arbitrary differences between the distributions, without requiring one be stochastically larger than the other. If the distributions only differ in terms of their location parameters, we can focus hypothesis tests on the parameters (e.g., \\(H_0: \\theta_y = \\theta_x\\) vs. \\(\\theta_y &gt; \\theta_x\\)) We have 2 equivalent nonparametric tests that consider the hypothesis mentioned above Wilcoxon rank test Mann-Whitney U test 4.2.6.1 Wilcoxon rank test Combine all \\(n= n_y + n_x\\) observations and rank them in ascending order. Sum the ranks of the ys and xs separately. Let \\(w_y\\) and \\(w_x\\) be these sums. (\\(w_y + w_x = 1 + 2 + ... + n = n(n+1)/2\\)) Reject \\(H_0\\) if \\(w_y\\) is large (equivalently, \\(w_x\\) is small) Under \\(H_0\\), any arrangement of the ys and xs is equally likely to occur, and there are \\((n_y + n_x)!/(n_y! n_x!)\\) possible arrangements. Technically, for each arrangement we can compute the values of \\(w_y\\) and \\(w_x\\), and thus generate the distribution of the statistic under the null hypothesis. This could lead to computationally intensive. wilcox.test(irisVe,irisVi,alternative=&quot;two.sided&quot;,conf.level=0.95, exact=F,correct=T) ## ## Wilcoxon rank sum test with continuity correction ## ## data: irisVe and irisVi ## W = 49, p-value &lt; 2.2e-16 ## alternative hypothesis: true location shift is not equal to 0 4.2.6.2 Mann-Whitney U test The Mann-Whitney test is computed as follows: Compare each \\(y_i\\) wiht each \\(x_i\\). Let \\(u_y\\) be the number of pairs in which \\(y_i &gt; x_i\\) Let \\(u_x\\) be the number of pairs in which \\(y_i &lt; x_i\\). (assume there are no ties). There are \\(n_y n_x\\) such comparisons and \\(u_y + u_x = n_y n_x\\). Reject \\(H_0\\) if \\(u_y\\) is large (or \\(u_x\\) is small) Mann-Whitney U test and Wilcoxon rank test are related: \\[ u_y = w_y - n_y(n_y+1) /2 \\\\ u_x = w_x - n_x(n_x +1)/2 \\] An \\(\\alpha\\)-level test rejects \\(H_0\\) if \\(u_y \\ge u_{n_y,n_x,\\alpha}\\), where \\(u_{n_y,n_x,\\alpha}\\) is the upper \\(\\alpha\\) critical point of the null distribution of the random variable, U. The p-value is defined to be \\(P(Y \\ge u_y) = P(U \\le u_x)\\). One advantage of Mann-Whitney U test is that we can use either \\(u_y\\) or \\(u_x\\) to carry out the test. For large \\(n_y\\) and \\(n_x\\), the null distribution of U can be well approximated by a normal distribution with mean \\(E(U) = n_y n_x /2\\) and variance \\(var(U) = n_y n_x (n+1)/12\\). A large sample z-test can be based on the statistic: \\[ z = \\frac{u_y - n_y n_x /2 -1/2}{\\sqrt{n_y n_x (n+1)/12}} \\] The test rejects \\(H_0\\) at level \\(\\alpha\\) if \\(z \\ge z_{\\alpha}\\) or if \\(u_y \\ge u_{n_y,n_x,\\alpha}\\) where \\[ u_{n_y, n_x, \\alpha} \\approx n_y n_x /2 + 1/2 + z_{\\alpha}\\sqrt{n_y n_x (n+1)/12} \\] For the 2-sided test, we use the test statistic \\(u_{max} = max(u_y,u_x)\\) and \\(u_{min} = min(u_y, u_x)\\) and p-value is given by \\[ p-value = 2P(U \\ge u_{max}) = 2P(U \\le u_{min}) \\] Since we assume there are no ties (when \\(y_i = x_j\\)), we count 1/2 towards both \\(u_y\\) and \\(u_x\\). Even though the sampling distribution is not the same, but large sample approximation is still reasonable, References "],["categorical-data-analysis.html", "4.3 Categorical Data Analysis", " 4.3 Categorical Data Analysis Categorical Data Analysis when we have categorical outcomes Nominal variables: no logical ordering (e.g., sex) Ordinal variables: logical order, but relative distances between values are not clear (e.g., small, medium, large) The distribution of one variable changes when the level (or values) of the other variable change. The row percentages are different in each column. 4.3.1 Inferences for Small Samples The approximate tests based on the asymptotic normality of \\(\\hat{p}_1 - \\hat{p}_2\\) do not apply for small samples. Using Fishers Exact Test to evaluate \\(H_0: p_1 = p_2\\) Assume \\(X_1\\) and \\(X_2\\) are independent Binomial Let \\(x_1\\) and \\(x_2\\) be the corresponding observed values. Let \\(n= n_1 + n_2\\) be the total sample size \\(m = x_1 + x_2\\) be the observed number of successes. By assuming that m (total successes) is fixed, and conditioning on this value, one can show that the conditional distribution of the number of successes from sample 1 is Hypergeometric If we want to test \\(H_0: p_1 = p_2\\) and \\(H_a: p_1 \\neq p_2\\), we have \\[ Z^2 = (\\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\hat{p}(1-\\hat{p})(1/n_1 + 1/n_2)}})^2 \\sim \\chi_{1,\\alpha}^2 \\] where \\(\\chi_{1,\\alpha}^2\\) is the upper \\(\\alpha\\) percentage point for the central Chi-squared with one d.f. This extends to the contingency table setting: whether the observed frequencies are equal to those expected under a null hypothesis of no association. 4.3.2 Test of Association Pearson Chi-square test statistic is \\[ \\chi^2 = \\sum_{\\text{all categories}} \\frac{(observed - epxected)^2}{expected} \\] Comparison of proportions for several independent surveys or experiments Experiment 1 Experiment 2  Experiment k Number of successes \\(x_1\\) \\(x_2\\)  \\(x_k\\) Number of failures \\(n_1 - x_1\\) \\(n_2 - x_2\\)  \\(n_k - x_k\\) \\(n_1\\) \\(n_2\\)  \\(n_k\\) \\(H_0: p_1 = p_2 = ... = p_k\\) vs. the alternative that the null is not true (at least one pair are not equal). We estimate the common value of the probability of success on a single trial assuming \\(H_0\\) is true: \\[ \\hat{p} = \\frac{x_1 + x_2 + ... + x_k}{n_1 + n_2 + ...+ n_k} \\] we use table of expected counts when \\(H_0\\) is true: success \\(n_1 \\hat{p}\\) \\(n_2 \\hat{p}\\)  \\(n_k \\hat{p}\\) failure \\(n_1(1-\\hat{p})\\) \\(n_2(1-\\hat{p})\\)  \\(n_k (1-\\hat{p})\\) \\(n_1\\) \\(n_2\\)  \\(n_k\\) \\[ \\chi^2 = \\sum_{\\text{all cells in table}} \\frac{(observed - expected)^2}{expected} \\] with k-1 degrees of freedom 4.3.2.1 Two-way Count Data 1 2  j  c Row Total 1 \\(n_{11}\\) \\(n_{12}\\)  \\(n_{1j}\\)  \\(n_{1c}\\) \\(n_{1.}\\) 2 \\(n_{21}\\) \\(n_{22}\\)  \\(n_{2j}\\)  \\(n_{2c}\\) \\(n_{2.}\\) . . . . . . . . r \\(n_{r1}\\) \\(n_{r2}\\)  \\(n_{rj}\\)  \\(n_{rc}\\) \\(n_{r.}\\) Column Total \\(n_{.1}\\) \\(n_{.2}\\)  \\(n_{.j}\\)  \\(n_{.c}\\) \\(n_{}\\) Design 1 total sample size fixed n = constant (e.g., survey on job satisfaction and income); both row and column totals are random variables Design 2 Fix the sample size in each group (in each row) (e.g., Drug treatments success or failure); fixed number of participants for each treatment; independent random samples from the two row populations. These different sampling designs imply two different probability models. 4.3.2.2 Total Sample Size Fixed Design 1 random sample of size n drawn from a single population, and sample units are cross-classified into r row categories and c column This results in an r x c table of observed counts \\(n_{ij} = 1,...,r;j=1,...,c\\) Let \\(p_{ij}\\) be the probability of classification into cell (i,j) and \\(\\sum_{i=1}^r \\sum_{j=1}^c p_{ij} = 1\\). Let \\(N_{ij}\\) be the random variable corresponding to \\(n_{ij}\\) The joint distribution of the \\(N_{ij}\\) is multinomial with unknown parameters \\(p_{ij}\\) Denote the row variable by X and column variable by Y, then \\(p_{ij} = P(X=i,Y = j)\\) and \\(p_{i.} = P(X = i)\\) and \\(p_{.j} = P(Y = j)\\) are the marginal probabilities. The null hypothesis that X and Y are statistically independent (i.e., no association) is just: \\[ H_0: p_{ij} = P(X =i,Y=j) = P(X =i) P(Y =j) = p_{i.}p_{.j} \\\\ H_a: p_{ij} \\neq p_{i.}p_{.j} \\] for all i,j. 4.3.2.3 Row Total Fixed Design 2 Random samples of sizes \\(n_1,...,n_r\\) are drawn independently from \\(r \\ge 2\\) row populations. In this case, the 2-way table row totals are \\(n_{i.} = n_i\\) for \\(i = 1,...,r\\). The counts from each row are modeled by independent multinomial distributions. X is fixed, Y is observed. Then, \\(p_{ij}\\) represent conditional probabilities \\(p_{ij} = P(Y=j|X=i)\\) The null hypothesis is the probability of response j is the same, regardless of the row population (i.e., no association): \\[ H_0: p_{ij} = P(Y = j | X = i) = p_j \\text{for all i,j =1,2,...,c} \\\\ \\text{or } H_0: (p_{i1},p_{i2},...,p_{ic}) = (p_1,p_2,...,p_c) \\text{ for all i} \\\\ H_a: (p_{i1},p_{i2},...,p_{ic}) \\text{ are not the same for all i} \\] Although the hypotheses to be tested are different for two sampling designs, The chi-square test is identical We have estimated expected frequencies: \\[ \\hat{e}_{ij} = \\frac{n_{i.}n_{.j}}{n} \\] The Chi-square statistic is \\[ \\chi^2 = \\sum_{i=1}^{r} \\sum_{j=1}^{c} \\frac{(n_{ij}-\\hat{e}_{ij})^2}{\\hat{e}_{ij}} \\sim \\chi_{(r-1)(c-1)} \\] \\(\\alpha\\)-level test rejects \\(H_0\\) if \\(\\chi^2 &gt; \\chi^2_{(r-1)(c-1),\\alpha}\\) 4.3.2.4 Pearson Chi-square Test Determine whether an association exists Sometimes, \\(H_0\\) represents the model whose validity is to be tested. Contrast this with the conventional formulation of \\(H_0\\) as the hypothesis that is to be disproved. The goal in this case is not to disprove the model, but to see whether data are consistent with the model and if deviation can be attributed to chance. These tests do not measure the strength of an association. These tests depend on and reflect the sample size - double the sample size by copying each observation, double the \\(\\chi^2\\) statistic even thought the strength of the association does not change. The Pearson Chi-square Test is not appropriate when more than about 20% of the cells have an expected cell frequency of less than 5 (large-sample p-values not appropriate). When the sample size is small the exact p-values can be calculated (this is prohibitive for large samples); calculation of the exact p-values assumes that the column totals and row totals are fixed. july.x=480 july.n=1000 sept.x=704 sept.n=1600 \\[ H_0: p_J = 0.5 \\\\ H_a: p_J &lt; 0.5 \\] prop.test(x=july.x,n=july.n,p=0.5,alternative=&quot;less&quot;,correct=F) ## ## 1-sample proportions test without continuity correction ## ## data: july.x out of july.n, null probability 0.5 ## X-squared = 1.6, df = 1, p-value = 0.103 ## alternative hypothesis: true p is less than 0.5 ## 95 percent confidence interval: ## 0.0000000 0.5060055 ## sample estimates: ## p ## 0.48 \\[ H_0: p_J = p_S \\\\ H_a: p_j \\neq p_S \\] prop.test(x=c(july.x,sept.x),n=c(july.n,sept.n),correct=F) ## ## 2-sample test for equality of proportions without continuity ## correction ## ## data: c(july.x, sept.x) out of c(july.n, sept.n) ## X-squared = 3.9701, df = 1, p-value = 0.04632 ## alternative hypothesis: two.sided ## 95 percent confidence interval: ## 0.0006247187 0.0793752813 ## sample estimates: ## prop 1 prop 2 ## 0.48 0.44 4.3.3 Ordinal Association An ordinal association implies that as one variable increases, the other tends to increase or decrease (depending on the nature of the association). For tests for variables with two or more levels, the levels must be in a logical ordering. 4.3.3.1 Mantel-Haenszel Chi-square Test The Mantel-Haenszel Chi-square Test is more powerful for testing ordinal associations, but does not test for the strength of the association. This test is presented in the case where one has a series of 2 x 2 tables that examine the same effects under different conditions (If there are K such tables, we have 2 x 2 x K table) In stratum k, given the marginal totals \\((n_{.1k},n_{.2k},n_{1.k},n_{2.k})\\), the sampling model for cell counts is the Hypergeometric (knowing \\(n_{11k}\\) determines \\((n_{12k},n_{21k},n_{22k})\\), given the marginal totals) Assuming conditional independence, the Hypergeometric mean and variance of \\(n_{11k}\\) are \\[ m_{11k} = E(n_{11k}) = \\frac{n_{1.k} n_{.1k}}{n_{..k}} \\\\ var(n_{11k}) = \\frac{n_{1.k} n_{2.k} n_{.1k} n_{.2k}}{n_{..k}^2(n_{..k}-1)} \\] To test conditional independence, Mantel and Haenszel proposed \\[ M^2 = \\frac{(|\\sum_{k} n_{11k} - \\sum_k m_{11k}| -.5)^2}{\\sum_{k}var(n_{11k})} \\sim \\chi^2_{1} \\] This method can be extended to general I x J x K tables. (2 x 2 x 3) table Bron=array(c(20, 9, 382, 214, 10, 7, 172, 120, 12, 6, 327, 183), dim = c(2, 2, 3), dimnames = list( Particulate = c(&quot;High&quot;, &quot;Low&quot;), Bronchitis = c(&quot;Yes&quot;, &quot;No&quot;), Age = c(&quot;15-24&quot;, &quot;25-39&quot;, &quot;40+&quot;))) margin.table(Bron,c(1,2)) ## Bronchitis ## Particulate Yes No ## High 42 881 ## Low 22 517 # assess whether the relationship between Bronchitis by Particulate Level varies by Age library(samplesizeCMH) marginal_table=margin.table(Bron,c(1,2)) odds.ratio(marginal_table) ## [1] 1.120318 # whether these odds vary by age. The conditional odds can be calculated using the original table. apply(Bron,3,odds.ratio) ## 15-24 25-39 40+ ## 1.2449098 0.9966777 1.1192661 # Mantel-Haenszel Test mantelhaen.test(Bron,correct=T) ## ## Mantel-Haenszel chi-squared test with continuity correction ## ## data: Bron ## Mantel-Haenszel X-squared = 0.11442, df = 1, p-value = 0.7352 ## alternative hypothesis: true common odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.6693022 1.9265813 ## sample estimates: ## common odds ratio ## 1.135546 4.3.3.1.1 McNemars Test special case of Mantel-Haenszel Chi-square Test vote=cbind(c(682,22),c(86,810)) mcnemar.test(vote,correct=T) ## ## McNemar&#39;s Chi-squared test with continuity correction ## ## data: vote ## McNemar&#39;s chi-squared = 36.75, df = 1, p-value = 1.343e-09 4.3.3.2 Spearman Rank Correlation To test for the strength of association between two ordinally scaled variables, we can use Spearman Rank Correlation statistic Let X and Y be two random variables measured on an ordinal scale. Consider n pairs of observations (\\(x_i,y_i\\)), i = 1,,n The Spearman Rank Correlation coefficient (denoted by \\(r_S\\) is calculated using the Pearson correlation formula, but based on the ranks of \\(x_i\\) and \\(y_i\\)). Spearman Rank Correlation be calculated Assign ranks to \\(x_i\\)s and \\(y_i\\)s separately. Let \\(u_i = rank(x_i)\\) and \\(v_i = rank(y_i)\\) Calculate \\(r_S\\) using the formula for the Pearson correlation coefficient, but applied to the ranks: \\[ r_S = \\frac{\\sum_{i=1}^{n}(u_i - \\bar{u})(v_i - \\bar{v})}{\\sqrt{(\\sum_{i = 1}^{n}(u_i - \\bar{u})^2)(\\sum_{i=1}^{n}(v_i - \\bar{v})^2)}} \\] \\(r_S\\) ranges between -1 and +1 , with \\(r_S = -1\\) if there is a perfect negative monotone association \\(r_S = +1\\) if there is a perfect positive monotone association between X and Y. To test \\(H_0:\\) X and Y independent \\(H_a\\): X and Y positively associated For large n (e.g., \\(n \\ge 10\\)), \\[ r_S \\sim N(0,1/(n-1)) \\] Then, \\[ Z = r_s \\sqrt{n-1} \\sim N(0,1) \\] "],["confidence-intervals-hypothesis-testing.html", "Chapter 5 Confidence Intervals &amp; Hypothesis Testing", " Chapter 5 Confidence Intervals &amp; Hypothesis Testing Make inferences (an interpretation) about the true parameter value \\(\\beta\\) based on our estimator/estimate Test whether our underlying assumptions (about the true population parameters, random variables, or model specification) hold true. Testing does not Confirm with 100% a hypothesis is true Confirm with 100% a hypothesis is false Tell you how to interpret the estimate value (Economic vs. Practical vs. Statistical Significance) Hypothesis: Translate an objective in better understanding the results in terms of specifying a value (or sets of values) in which our population parameters should/should not lie. Null hypothesis (\\(H_0\\)): A statement about the population parameter that we take to be true in which we would need the data to provide substantial evidence that against it. Can be either a single value (ex: \\(H_0: \\beta=0\\)) or a set of values (ex: \\(H_0: \\beta_1 \\ge 0\\)) Will generally be the value you would not like the population parameter to be (subjective) \\(H_0: \\beta_1=0\\) means you would like to see a non-zero coefficient \\(H_0: \\beta_1 \\ge 0\\) means you would like to see a negative effect Test of Significance refers to the two-sided test: \\(H_0: \\beta_j=0\\) Alternative hypothesis (\\(H_a\\) or \\(H_1\\)) (Research Hypothesis): All other possible values that the population parameter may be if the null hypothesis does not hold. Type I Error Error made when \\(H_0\\) is rejected when, in fact, \\(H_0\\) is true. The probability of committing a Type I error is \\(\\alpha\\) (known as level of significance of the test) Type I error (\\(\\alpha\\)): probability of rejecting \\(H_0\\) when it is true. Legal analogy: In U.S. law, a defendant is presumed to be innocent until proven guilty. If the null hypothesis is that a person is innocent, the Type I error is the probability that you conclude the person is guilty when he is innocent. Type II Error Type II error level (\\(\\beta\\)): probability that you fail to reject the null hypothesis when it is false. In the legal analogy, this is the probability that you fail to find the person guilty when he or she is guilty. Error made when \\(H_0\\) is not rejected when, in fact, \\(H_1\\) is true The probability of committing a Type II error is \\(\\beta\\) (known as the power of the test) "],["for-the-mean-mu.html", "5.1 For the Mean (\\(\\mu\\))", " 5.1 For the Mean (\\(\\mu\\)) Confidence Interval \\(100(1-\\alpha)%\\) Sample Sizes Confidence \\(\\alpha\\), Error d Hypothesis Testing Test Statistic When \\(\\sigma^2\\) is known, X is normal (or \\(n \\ge 25\\)) \\(\\bar{X} \\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\) \\(n \\approx \\frac{z_{\\alpha/2}^2 \\sigma^2}{d^2}\\) \\(z = \\frac{\\bar{X}-\\mu_0}{\\sigma/\\sqrt{n}}\\) When \\(\\sigma^2\\) is unknown, X is normal (or \\(n \\ge 25\\)) \\(\\bar{X} \\pm t_{\\alpha/2}\\frac{s}{\\sqrt{n}}\\) \\(n \\approx \\frac{z_{\\alpha/2}^2 s^2}{d^2}\\) \\(t = \\frac{\\bar{X}-\\mu_0}{s/\\sqrt{n}}\\) "],["for-single-proportions-p.html", "5.2 For Single Proportions (p)", " 5.2 For Single Proportions (p) Confidence Interval \\(100(1-\\alpha)%\\) Sample Sizes Confidence \\(\\alpha\\), Error d (prior estimate for \\(\\hat{p}\\)) (No prior estimate for \\(\\hat{p}\\)) Hypothesis Testing Test Statistic \\(\\hat{p} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\) \\(n \\approx \\frac{z_{\\alpha/2}^2 \\hat{p}(1-\\hat{p})}{d^2}\\) \\(n \\approx \\frac{z_{\\alpha/2}^2}{4d^2}\\) \\(z = \\frac{\\hat{p}-p_0}{\\sqrt{\\frac{p_0(1-p_0)}{n}}}\\) "],["for-difference-of-two-proportions-p-1-p-2.html", "5.3 For Difference of Two Proportions (\\(p_1 - p_2\\))", " 5.3 For Difference of Two Proportions (\\(p_1 - p_2\\)) Mean \\[ \\hat{p_1}-\\hat{p_2} \\] Variance \\[ \\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2} \\] \\(100(1-\\alpha)%\\) Confidence Interval \\[ \\hat{p_1}-\\hat{p_2} + z_{\\alpha/2}\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}} \\] Sample Sizes, Confidence \\(\\alpha\\), Error d (Prior Estimate fo \\(\\hat{p_1},\\hat{p_2}\\)) \\[ n \\approx \\frac{z_{\\alpha/2}^2[p_1(1-p_1)+p_2(1-p_2)]}{d^2} \\] (No Prior Estimates for \\(\\hat{p}\\)) \\[ n \\approx \\frac{z_{\\alpha/2}^2}{2d^2} \\] Hypothesis Testing - Test Statistics Null Value \\((p_1 - p_2) \\neq 0\\) \\[ z = \\frac{(\\hat{p_1} - \\hat{p_2})-(p_1 - p_2)_0}{\\sqrt{\\frac{p_1(1-p_1)}{n_1} + \\frac{p_2(1-p_2)}{n_2}}} \\] Null Value \\((p_1 - p_2)_0 = 0\\) \\[ z = \\frac{\\hat{p_1} - \\hat{p_2}}{\\sqrt{\\hat{p}(1-\\hat{p})(\\frac{1}{n_1}+\\frac{1}{n_2})}} \\] where \\[ \\hat{p}= \\frac{x_1 + x_2}{n_1 + n_2} = \\frac{n_1 \\hat{p_1} + n_2 \\hat{p_2}}{n_1 + n_2} \\] "],["for-a-signle-variance-sigma2.html", "5.4 For a signle variance (\\(\\sigma^2\\))", " 5.4 For a signle variance (\\(\\sigma^2\\)) \\(100(1-\\alpha)%\\) Confidence Interval \\[ L_1 = \\frac{(n-1)s^2}{\\chi^2_{\\alpha/2}} \\\\ L_1 = \\frac{(n-1)s^2}{\\chi^2_{1-\\alpha/2}} \\] Hypothesis Testing Test Statistic \\[ \\chi^2 = \\frac{(n-1)s^2}{\\sigma^2_0} \\] "],["for-two-variances-sigma2-1sigma2-2.html", "5.5 For Two Variances (\\(\\sigma^2_1,\\sigma^2_2\\))", " 5.5 For Two Variances (\\(\\sigma^2_1,\\sigma^2_2\\)) \\[ F_{ndf,ddf}= \\frac{s^2_1}{s^2_2} \\] where \\(s^2_1&gt;s^2_2, ndf = n_1-1,ddf = n_2-1\\) "],["for-difference-of-means-mu-1-mu-2-independent-samples.html", "5.6 For Difference of Means (\\(\\mu_1-\\mu_2\\)), Independent Samples", " 5.6 For Difference of Means (\\(\\mu_1-\\mu_2\\)), Independent Samples \\(100(1-\\alpha)%\\) Confidence Interval Hypothesis Testing Test Statistic When \\(\\sigma^2\\) is known \\(\\bar{X}_1 - \\bar{X}_2 \\pm z_{\\alpha/2}\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}\\) \\(z= \\frac{(\\bar{X}_1-\\bar{X}_2)-(\\mu_1-\\mu_2)_0}{\\sqrt{\\frac{\\sigma^2_1}{n_1}+\\frac{\\sigma^2_2}{n_2}}}\\) When \\(\\sigma^2\\) is unknown, Variances Assumed EQUAL \\(\\bar{X}_1 - \\bar{X}_2 \\pm t_{\\alpha/2}\\sqrt{s^2_p(\\frac{1}{n_1}+\\frac{1}{n_2})}\\) \\(t = \\frac{(\\bar{X}_1-\\bar{X}_2)-(\\mu_1-\\mu_2)_0}{\\sqrt{s^2_p(\\frac{1}{n_1}+\\frac{1}{n_2})}}\\) Pooled Variance: \\(s_p^2 = \\frac{(n_1 -1)s^2_1 - (n_2-1)s^2_2}{n_1 + n_2 -2}\\) Degrees of Freedom: \\(\\gamma = n_1 + n_2 -2\\) When \\(\\sigma^2\\) is unknown, Variances Assumed UNEQUAL \\(\\bar{X}_1 - \\bar{X}_2 \\pm t_{\\alpha/2}\\sqrt{(\\frac{s^2_1}{n_1}+\\frac{s^2_2}{n_2})}\\) \\(t = \\frac{(\\bar{X}_1-\\bar{X}_2)-(\\mu_1-\\mu_2)_0}{\\sqrt{(\\frac{s^2_1}{n_1}+\\frac{s^2_2}{n_2})}}\\) Degrees of Freedom: \\(\\gamma = \\frac{(\\frac{s_1^2}{n_1}+\\frac{s^2_2}{n_2})^2}{\\frac{(\\frac{s_1^2}{n_1})^2}{n_1-1}+\\frac{(\\frac{s_2^2}{n_2})^2}{n_2-1}}\\) "],["for-difference-of-means-mu-1-mu-2-paired-samples-d-x-y.html", "5.7 For Difference of Means (\\(\\mu_1 - \\mu_2\\)), Paired Samples (D = X-Y)", " 5.7 For Difference of Means (\\(\\mu_1 - \\mu_2\\)), Paired Samples (D = X-Y) \\(100(1-\\alpha)%\\) Confidence Interval \\[ \\bar{D} \\pm t_{\\alpha/2}\\frac{s_d}{\\sqrt{n}} \\] Hypothesis Testing Test Statistic \\[ t = \\frac{\\bar{D}-D_0}{s_d / \\sqrt{n}} \\] "],["regression-analysis.html", "Chapter 6 Regression Analysis", " Chapter 6 Regression Analysis Estimator Desirable Properties Unbiased Consistency \\(plim\\hat{\\beta_n}=\\beta\\) based on the law of large numbers, we can derive consistency More observations means more precise, closer to the true value. Efficiency Minimum variance in comparison to another estimator. OLS is BlUE (best linear unbiased estimator) means that OLS is the most efficient among the class of linear unbiased estimator Gauss-Markov Theorem If we have correct distributional assumptions, then the Maximum Likelihood is asymptotically efficient among consistent estimators. "],["linear-regression.html", "6.1 Linear Regression", " 6.1 Linear Regression 6.1.1 Ordinary Least Squares The most fundamental model in statistics or econometric is a OLS linear regression. OLS = Maximum likelihood when the error term is assumed to be normally distributed. 6.1.1.1 Simple Regression (Basic Model) \\[ Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\] \\(Y_i\\): response (dependent) variable at i-th observation \\(\\beta_0,\\beta_1\\): regression parameters for intercept and slope. \\(X_i\\): known constant (independent or predicotr variable) for i-th observation \\(\\epsilon_i\\): random error term \\[ E(\\epsilon_i) = 0 \\\\ var(\\epsilon_i) = \\sigma^2 \\\\ cov(\\epsilon_i,\\epsilon_j) = 0 \\text{ for all $i \\neq j$} \\] \\(Y_i\\) is random since \\(\\epsilon_i\\) is: \\[ \\begin{align} E(Y_i) &amp;= E(\\beta_0 + \\beta_1 X_i + \\epsilon_i) \\\\ &amp;= E(\\beta_0) + E(\\beta_1 X_i) + E(\\epsilon) \\\\ &amp;= \\beta_0 + \\beta_1 X_i \\end{align} \\] \\[ \\begin{align} var(Y_i) &amp;= var(\\beta_0 + \\beta_1 X_i + \\epsilon_i) \\\\ &amp;= var(\\epsilon_i) \\\\ &amp;= \\sigma^2 \\end{align} \\] Since \\(cov(\\epsilon_i, \\epsilon_j) = 0\\) (uncorrelated), the outcome in any one trail has no effect on the outcome of any other. Hence, \\(Y_i, Y_j\\) are uncorrelated as well (conditioned on the Xs) Note Least Squares does not require a distributional assumption 6.1.1.1.1 Estimation Deviation of \\(Y_i\\) from its expected value: \\[ Y_i - E(Y_i) = Y_i - (\\beta_0 + \\beta_1 X_i) \\] Consider the sum of the square of such deviations: \\[ Q = \\sum_{i=1}^{n} (Y_i - \\beta_0 -\\beta_1 X_i)^2 \\] \\[ b_1 = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2} \\\\ b_0 = \\frac{1}{n}(\\sum_{i=1}^{n}Y_i - b_1\\sum_{i=1}^{n}X_i) = \\bar{Y} - b_1 \\bar{X} \\] 6.1.1.1.2 Properties of Least Least Estimators \\[ E(b_1) = \\beta_1 \\\\ E(b_0) = E(\\bar{Y}) - \\bar{X}\\beta_1 \\\\ E(\\bar{Y}) = \\beta_0 + \\beta_1 \\bar{X} \\\\ E(b_0) = \\beta_0 \\\\ var(b_1) = \\frac{\\sigma^2}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2} \\\\ var(b_0) = \\sigma^2 (\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum (X_i - \\bar{X})^2}) \\] \\(var(b_1)\\) approaches 0 as more measurements are taken at more \\(X_i\\) values (unless \\(X_i\\) is at its mean value) \\(var(b_0)\\) approaches 0 as n increases when the \\(X_i\\) values are judiciously selected. Mean Square Error \\[ MSE = \\frac{SSE}{n-2} = \\frac{\\sum_{i=1}^{n}e_i^2}{n-2} = \\frac{\\sum(Y_i - \\hat{Y_i})^2}{n-2} \\] Unbiased estimator of MSE: \\[ E(MSE) = \\sigma^2 \\] \\[ s^2(b_1) = \\widehat{var(b_1)} = \\frac{MSE}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2} \\\\ s^2(b_0) = \\widehat{var(b_0)} = MSE(\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}) \\] \\[ E(s^2(b_1)) = var(b_1) \\\\ E(s^2(b_0))=var(b_0) \\] 6.1.1.1.3 Residuals \\[ e_i = Y_i - \\hat{Y} = Y_i - (b_0 + b_1 X_i) \\] \\(e_i\\) is an estimate of \\(\\epsilon_i = Y_i - E(Y_i)\\) \\(\\epsilon_i\\) is always unknown since we dont know the true \\(\\beta_0, \\beta_1\\) \\[ \\sum_{i=1}^{n} e_i = 0 \\\\ \\sum_{i=1}^{n} X_i e_i = 0 \\] 6.1.1.1.4 Inference Normality Assumption Least Squares estimation does not require assumptions of normality. However, to do inference on the parameters, we need distributional assumptions. Inference on \\(\\beta_0,\\beta_1\\) and \\(Y_h\\) are not extremely sensitive to moderate departures from normality, especially if the sample size is large Inference on \\(Y_{pred}\\) is very sensitive to the normality assumptions. Normal Error Regression Model \\[ Y_i \\sim N(\\beta_0+\\beta_1X_i, \\sigma^2) \\\\ \\] 6.1.1.1.4.1 \\(\\beta_1\\) Under the normal error model, \\[ b_1 \\sim N(\\beta_1,\\frac{\\sigma^2}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}) \\] A linear combination of independent normal random variable is normally distributed Hence, \\[ \\frac{b_1 - \\beta_1}{s(b_1)} \\sim t_{n-2} \\] A \\((1-\\alpha) 100 \\%\\) confidence interval for \\(\\beta_1\\) is \\[ b_1 \\pm t_{t-\\alpha/2 ; n-2}s(b_1) \\] 6.1.1.1.4.2 \\(\\beta_0\\) Under the normal error model, the sampling distribution for \\(b_0\\) is \\[ b_0 \\sim N(\\beta_0,\\sigma^2(\\frac{1}{n} + \\frac{\\bar{X}^2}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2})) \\] Hence, \\[ \\frac{b_0 - \\beta_0}{s(b_0)} \\sim t_{n-2} \\] A \\((1-\\alpha)100 \\%\\) confidence interval for \\(\\beta_0\\) is \\[ b_0 \\pm t_{1-\\alpha/2;n-2}s(b_0) \\] 6.1.1.1.4.3 Mean Response Let \\(X_h\\) denote the level of X for which we wish to estimate the mean response We denote the mean response when \\(X = X_h\\) by \\(E(Y_h)\\) A point estimator of \\(E(Y_h)\\) is \\(\\hat{Y}_h\\): \\[ \\hat{Y}_h = b_0 + b_1 X_h \\] Note \\[ E(\\bar{Y}_h)= E(b_0 + b_1X_h) \\\\ = \\beta_0 + \\beta_1 X_h \\\\ = E(Y_h) \\] (unbiased estimator) \\[ \\begin{align} var(\\hat{Y}_h) &amp;= var(b_0 + b_1 X_h) \\\\ &amp;= var(\\hat{Y} + b_1 (X_h - \\bar{X})) \\\\ &amp;= var(\\bar{Y}) + (X_h - \\bar{X})^2var(b_1) + 2(X_h - \\bar{X})cov(\\bar{Y},b_1) \\\\ &amp;= \\frac{\\sigma^2}{n} + (X_h - \\bar{X})^2 \\frac{\\sigma^2}{\\sum(X_i - \\bar{X})^2} \\\\ &amp;= \\sigma^2(\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2}) \\end{align} \\] Since \\(cov(\\bar{Y},b_1) = 0\\) due to the iid assumption on \\(\\epsilon_i\\) An estimate of this variance is \\[ s^2(\\hat{Y}_h) = MSE (\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}) \\] the sampling distribution for the mean response is \\[ \\hat{Y}_h \\sim N(E(Y_h),var(\\hat{Y_h})) \\\\ \\frac{\\hat{Y}_h - E(Y_h)}{s(\\hat{Y}_h)} \\sim t_{n-2} \\] A \\(100(1-\\alpha) \\%\\) CI for \\(E(Y_h)\\) is \\[ \\hat{Y}_h \\pm t_{1-\\alpha/2;n-2}s(\\hat{Y}_h) \\] 6.1.1.1.4.4 Prediction of a new observation Regarding the Mean Response, we are interested in estimating mean of the distribution of Y given a certain X. Now, we want to predict an individual outcome for the distribution of Y at a given X. We call \\(Y_{pred}\\) Estimation of mean response versus prediction of a new observation: the point estimates are the same in both cases: \\(\\hat{Y}_{pred} = \\hat{Y}_h\\) It is the variance of the prediction that is different; hence, prediction intervals are different than confidence intervals. The prediction variance must consider: Variation in the mean of the distribution of Y variation within the distribution of Y We want to predict: mean response + error \\[ \\beta_0 + \\beta_1 X_h + \\epsilon \\] Since \\(E(\\epsilon) = 0\\), use the least squares predictor: \\[ \\hat{Y}_h = b_0 + b_1 X_h \\] The variance of the predictor is \\[ \\begin{align} var(b_0 + b_1 X_h + \\epsilon) &amp;= var(b_0 + b_1 X_h) + var(\\epsilon) \\\\ &amp;= \\sigma^2(\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}) + \\sigma^2 \\\\ &amp;= \\sigma^2(1+\\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}) \\end{align} \\] An estimate of the variance is given by \\[ s^2(pred)= MSE (1+ \\frac{1}{n} + \\frac{(X_h - \\bar{X})^2}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}) \\\\ \\frac{Y_{pred}-\\hat{Y}_h}{s(pred)} \\sim t_{n-2} \\] \\(100(1-\\alpha) \\%\\) prediction interval is \\[ \\bar{Y}_h \\pm t_{1-\\alpha/2; n-2}s(pred) \\] The prediction interval is very sensitive to the distributional assumption on the errors, \\(\\epsilon\\) 6.1.1.1.4.5 Confidence Band We want to know the confidence interval for the entire regression line, so we can draw conclusions about any and all mean response fo the entire regression line \\(E(Y) = \\beta_0 + \\beta_1 X\\) rather than for a given response Y Working-Hotelling Confidence Band For a given \\(X_h\\), this band is \\[ \\hat{Y}_h \\pm W s(\\hat{Y}_h) \\] where \\(W^2 = 2F_{1-\\alpha;2,n-2}\\), which is just 2 times the F-stat with 2 and n-2 degrees of freedom the interval width will change with each \\(X_h\\) (since \\(s(\\hat{Y}_h)\\) changes) the boundary values for this confidence band will always define a hyperbole containing the regression line will be smallest at \\(X = \\bar{X}\\) 6.1.1.1.4.6 ANOVA Partitioning the Total Sum of Squares: Consider the corrected Total sum of squres: \\[ SSTO = \\sum_{i=1}^{n} (Y_i -\\bar{Y})^2 \\] Measures the overall dispersion in the response variable We use the term corrected because we correct for mean, the uncorrected total sum of squares is given by \\(\\sum Y_i^2\\) use \\(\\hat{Y}_i = b_0 + b_1 X_i\\) to estimate the conditional mean for Y at \\(X_i\\) \\[ \\begin{align} \\sum_{i=1}^n (Y_i - \\bar{Y})^2 &amp;= \\sum_{i=1}^n (Y_i - \\hat{Y}_i + \\hat{Y}_i - \\bar{Y})^2 \\\\ &amp;= \\sum_{i=1}^n(Y_i - \\hat{Y}_i)^2 + \\sum_{i=1}^n(\\hat{Y}_i - \\bar{Y})^2 + 2\\sum_{i=1}^n(Y_i - \\hat{Y}_i)(\\hat{Y}_i-\\bar{Y}) \\\\ &amp;= \\sum_{i=1}^n(Y_i - \\hat{Y}_i)^2 + \\sum_{i=1}^n(\\bar{Y}_i -\\bar{Y})^2 \\\\ STTO &amp;= SSE + SSR \\\\ \\end{align} \\] where SSR is the regression sum of squares, which measures how the conditional mean varies about a central value. The cross-product term in the decomposition is 0: \\[ \\begin{align} \\sum_{i=1}^n (Y_i - \\hat{Y}_i)(\\hat{Y}_i - \\bar{Y}) &amp;= \\sum_{i=1}^{n}(Y_i - \\bar{Y} -b_1 (X_i - \\bar{X}))(\\bar{Y} + b_1 (X_i - \\bar{X})-\\bar{Y}) \\\\ &amp;= b_1 \\sum_{i=1}^{n} (Y_i - \\bar{Y})(X_i - \\bar{X}) - b_1^2\\sum_{i=1}^{n}(X_i - \\bar{X})^2 \\\\ &amp;= b_1 \\frac{\\sum_{i=1}^{n}(Y_i -\\bar{Y})(X_i - \\bar{X})}{\\sum_{i=1}^{n}(X_i - \\bar{X})^2} \\sum_{i=1}^{n}(X_i - \\bar{X})^2 - b_1^2\\sum_{i=1}^{n}(X_i - \\bar{X})^2 \\\\ &amp;= b_1^2 \\sum_{i=1}^{n}(X_i - \\bar{X})^2 - b_1^2 \\sum_{i=1}^{n}(X_i - \\bar{X})^2 \\\\ &amp;= 0 \\end{align} \\] \\[ \\begin{align} SSTO &amp;= SSR + SSE \\\\ (n-1 d.f) &amp;= (1 d.f.) + (n-2 d.f.) \\end{align} \\] Source of Variation Sum of Squares df Mean Square F Regression (model) SSR 1 MSR = SSR/df MSR/MSE Error SSE n-2 MSE = SSE/df Total (Corrected) SSTO n-1 \\[ E(MSE) = \\sigma^2 \\\\ E(MSR) = \\sigma^2 + \\beta_1^2 \\sum_{i=1}^{n} (X_i - \\bar{X})^2 \\] If \\(\\beta_1 = 0\\), then these two expected values are the same if \\(\\beta_1 \\neq 0\\) then E(MSR) will be larger than E(MSE) which means the ratio of these two quantities, we can infer something about \\(\\beta_1\\) Distribution theory tells us that if \\(\\epsilon_i \\sim iid N(0,\\sigma^2)\\) and assuming \\(H_0: \\beta_1 = 0\\) is true, \\[ \\frac{MSE}{\\sigma^2} \\sim \\chi_{n-2}^2 \\\\ \\frac{MSR}{\\sigma^2} \\sim \\chi_{1}^2 \\text{ if $\\beta_1=0$} \\] where these two chi-square random variables are independent. Since the ratio of 2 independent chi-square random variable follows an F distribution, we consider: \\[ F = \\frac{MSR}{MSE} \\sim F_{1,n-2} \\] when \\(\\beta_1 =0\\). Thus, we reject \\(H_0: \\beta_1 = 0\\) (or \\(E(Y_i)\\) = constant) at \\(\\alpha\\) if \\[ F &gt; F_{1 - \\alpha;1,n-2} \\] this is the only null hypothesis that can be tested with this approach. Coefficient of Determination \\[ R^2 = \\frac{SSR}{SSTO} = 1- \\frac{SSE}{SSTO} \\] where \\(0 \\le R^2 \\le 1\\) Interpretation: The proportionate reduction of the total variation in Y after fitting a linear model in X. It is not really correct to say that \\(R^2\\) is the variation in Y explained by X. \\(R^2\\) is related to the correlation coefficient between Y and X: \\[ R^2 = (r)^2 \\] where \\(r= corr(x,y)\\) is an estimate of the Pearson correlation coefficient. Also, note \\[ b_1 = (\\frac{\\sum_{i=1}^{n}(Y_i - \\bar{Y})^2}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2})^{1/2} \\\\ r = \\frac{s_y}{s_x} r \\] Lack of Fit \\(Y_{11},Y_{21},...,Y_{n_1,1}\\): \\(n_1\\) repeat obs at \\(X_1\\) \\(Y_{1c},Y_{2c},...,Y_{n_c,c}\\): \\(n_c\\) repeat obs at \\(X_c\\) So, there are c distinct X values. Let \\(\\bar{Y}_j\\) be the mean over replicates for \\(X_j\\) Partition the Error Sum of Squares: \\[ \\begin{align} \\sum_{i} \\sum_{j} (Y_{ij} - \\hat{Y}_{ij})^2 &amp;= \\sum_{i} \\sum_{j} (Y_{ij} - \\bar{Y}_j + \\bar{Y}_j + \\hat{Y}_{ij})^2 \\\\ &amp;= \\sum_{i} \\sum_{j} (Y_{ij} - \\bar{Y}_j)^2 + \\sum_{i} \\sum_{j} (\\bar{Y}_j - \\hat{Y}_{ij})^2 + \\text{cross product term} \\\\ &amp;= \\sum_{i} \\sum_{j}(Y_{ij} - \\bar{Y}_j)^2 + \\sum_j n_j (\\bar{Y}_j- \\hat{Y}_{ij})^2 \\\\ SSE &amp;= SSPE + SSLF \\\\ \\end{align} \\] SSPE: pure error sum of squares has n-c degrees of freedom since we need to estimate c means SSLF: lack of fit sum of squares has c - 2 degrees of freedom (the number of unique X values - number of parameters used to specify the conditional mean regression model) \\[ MSPE = \\frac{SSPE}{df_{pe}} = \\frac{SSPE}{n-c} \\\\ MSLF = \\frac{SSLF}{df_{lf}} = \\frac{SSLF}{c-2} \\] The F-test for Lack-of-Fit tests \\[ H_0: Y_{ij} = \\beta_0 + \\beta_1 X_i + \\epsilon_{ij}, \\epsilon_{ij} \\sim iid N(0,\\sigma^2) \\\\ H_a: Y_{ij} = \\alpha_0 + \\alpha_1 X_i + f(X_i, Z_1,...) + \\epsilon_{ij}^*,\\epsilon_{ij}^* \\sim iid N(0, \\sigma^2) \\] \\(E(MSPE) = \\sigma^2\\) under either \\(H_0\\), \\(H_a\\) \\(E(MSLF) = \\sigma^2 + \\frac{\\sum n_j(f(X_i,...))^2}{n-2}\\) in general and \\(E(MSLF) = \\sigma^2\\) when \\(H_0\\) is true We reject \\(H_0\\) (i.e., the model \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\) is not adequate) if \\[ F = \\frac{MSLF}{MSPE} &gt; F_{1-\\alpha;c-2,n-c} \\] Failing to reject \\(H_0\\) does not imply that \\(H_0: Y_{ij} = \\beta_0 + \\beta_1 X_i + \\epsilon_{ij}\\) is exactly true, but it suggests that this model may provide a reasonable approximation to the true model. Source of Variation Sum of Squares df Mean Square F Regression SSR 1 MSR MSR / MSE Error SSE n-2 MSE Lack of fit SSLF c-2 MSLF MSLF / MSPE Pure Error SSPE n-c MSPE Total(Corrected) SSTO n-1 Repeat observations have an effect on \\(R^2\\): It is impossible for \\(R^2\\) to attain 1 when repeat obs. exist (SSE cant be 0) The maximum \\(R^2\\) attainable in this situation: \\[ R^2_{max} = \\frac{SSTo - SSPE}{SSTO} \\] Not all levels of X need have repeat observations. Typically, when \\(H_0\\) is appropriate, one still uses MSE as the estimate for \\(\\sigma^2\\) rather than MSPE, Since MSE has more degrees of freedom, sometimes people will pool these estimates. Joint Inference The confidence coefficient for both \\(\\beta_0\\) and \\(\\beta_1\\) considered simulatneoulsy is \\(\\le \\alpha\\) Let \\(\\bar{A}_1\\) be the event that the first interval covers \\(\\beta_0\\) \\(\\bar{A}_2\\) be the event that teh second interval covers \\(\\beta_1\\) \\[ P(\\bar{A}_1) = 1 - \\alpha \\\\ P(\\bar{A}_2) = 1 - \\alpha \\] The probability that both \\(\\bar{A}_1\\) and \\(\\bar{A}_2\\) \\[ \\begin{align} P(\\bar{A}_1 \\cap \\bar{A}_2) &amp;= 1 - P(\\bar{A}_1 \\cup \\bar{A}_2) \\\\ &amp;= 1 - P(A_1) - P(A_2) + P(A_1 \\cap A_2) \\\\ &amp;\\ge 1 - P(A_1) - P(A_2) \\\\ &amp;= 1 - 2\\alpha \\end{align} \\] If \\(\\beta_0\\) and \\(\\beta_1\\) have separate 95% confidence intervals, the joint (family) confidence coefficient is at least \\(1 - 2(0.05) = 0.9\\). This is called a Bonferroni Inequality We could use a procedure in which we obtained \\(1-\\alpha/2\\) confidence intervals for the two regression parameters separately, then the joint (Bonferroni) family confidence coefficient would be at least \\(1- \\alpha\\) The \\(1-\\alpha\\) joint Bonferroni confidence interval for \\(\\beta_0\\) and \\(\\beta_1\\) is given by calculating: \\[ b_0 \\pm B s(b_0) \\\\ b_1 \\pm B s(b_1) \\] where \\(B= t_{1-\\alpha/4;n-2}\\) Interpretation: If repeated samples were taken and the joint \\((1-\\alpha)\\) intervals for \\(\\beta_0\\) and \\(\\beta_1\\) were obtained, \\((1-\\alpha)100\\)% of the joint intervals would contain the true pair \\((\\beta_0, \\beta_1)\\). That is, in \\(\\alpha \\times 100\\)% of the samples, one or both intervals would not contain the true value. The Bonferroni interval is conservative. It is a lower bound and the joint intervals will tend to be correct more than \\((1-\\alpha)100\\)% of the time (lower power). People usually consider a larger \\(\\alpha\\) for the Bonferroni joint tests (e.g, \\(\\alpha=0.1\\)) The Bonferroni procedure extends to testing more than 2 parameters. Say we are interested in testing \\(\\beta_0,\\beta_1,..., \\beta_{g-1}\\) (g parameters to test). Then, the joint Bonferroni interval is obtained by calculating the $(1-/g)$100% level interval for each separately. For example, if \\(\\alpha = 0.05\\) and \\(g=10\\), each individual test is done at the \\(1- \\frac{.05}{10}\\) level. For 2-sided intervals, this corresponds to using \\(t_{1-\\frac{0.05}{2(10)};n-p}\\) in the CI formula. This procedure works best if g is relatively small, otherwise the intervals for each individual parameter are very wide and teh test is way too conservative. \\(b_0,b_1\\) are usually correlated (negatively if \\(\\bar{X} &gt;0\\) and positively if \\(\\bar{X}&lt;0\\)) Other multiple comparison procedures are available. 6.1.1.2 OLS Assumptions A1 Linearity A2 Full rank A3 Exogeneity of Independent Variables A4 Homoskedasticity A5 Data Generation (random Sampling) A6 Normal Distribution 6.1.1.2.1 A1 Linearity \\[ \\begin{equation} A1: y=\\mathbf{x}\\beta + \\epsilon \\tag{6.1} \\end{equation} \\] Not restrictive x can be nonlinear transformation including interactions, natural log, quadratic With A3 (Exogeneity of Independent), linearity can be restrictive 6.1.1.2.1.1 Log Model Model Form Interpretation of \\(\\beta\\) In words Level-Level \\(y =\\beta_0+\\beta_1x+\\epsilon\\) \\(\\Delta y = \\beta_1 \\Delta x\\) A unit change in x will result in \\(\\beta_1\\) unit change in y Log-Level \\(ln(y)= \\beta_0 + \\beta_1x +epsilon\\) \\(\\% \\Delta y=100 \\beta_1 \\Delta x\\) A unit change in x result in 100 \\(\\beta_1\\) % change in y Level-Log \\(y = beta_0 + \\beta_1 ln(x) + \\epsilon\\) \\(\\Delta y = (\\beta_1/100)\\%\\Delta x\\) One percent change in x result in \\(\\beta_1/100\\) units change in y Log-Log \\(ln(y) = \\beta_0 + \\beta_1 ln(x) +\\epsilon\\) \\(\\% \\Delta y= \\beta_1 \\% \\Delta x\\) One percent change in x result in \\(\\beta_1\\) percent change in y 6.1.1.2.1.2 Higher Orders \\(y=\\beta_0 + x_1\\beta_1 + x_1^2\\beta_2 + \\epsilon\\) \\[ \\frac{\\partial y}{\\partial x_1}=\\beta_1 + 2x_1\\beta_2 \\] The effect of \\(x_1\\) on y depends on the level of \\(x_1\\) The partial effect at the average = \\(\\beta_1+2E(x_1)\\beta_2\\) Average Partial Effect = \\(E(\\beta_1 + 2x_1\\beta_2)\\) 6.1.1.2.1.3 Interactions \\(y=\\beta_0 + x_1\\beta_1 + x_2\\beta_2 + x_1x_2\\beta_3 + \\epsilon\\) \\(\\beta_1\\) is the average effect on y for a unit change in \\(x_1\\) when \\(x_2=0\\) \\(\\beta_1 + x_2\\beta_3\\) is the partial effect of \\(x_1\\) on y which depends on the level of \\(x_2\\) 6.1.1.2.2 A2 Full rank \\[ \\begin{equation} A2: rank(E(x&#39;x))=k \\tag{6.2} \\end{equation} \\] also known as identification condition columns of \\(\\mathbf{x}\\) cannot be written as a linear function of the other columns which ensures that each parameter is unique and exists in the population regression equation 6.1.1.2.3 A3 Exogeneity of Independent Variables \\[\\begin{equation} A3: E[\\epsilon|x_1,x_2,...,x_k]=E[\\epsilon|\\mathbf{x}]=0 \\tag{6.3} \\end{equation}\\] strict exogeneity also known as mean independence check back on Correlation and Independence by the Law of Iterated Expectations \\(E(\\epsilon)=0\\), which can be satisfied by always including an intercept. independent variables do not carry information for prediction of \\(\\epsilon\\) A3 implies \\(E(y|x)=x\\beta\\), which means the conditional mean function must be a linear function of x A1 Linearity 6.1.1.2.3.1 A3a Weaker Exogeneity Assumption Exogeneity of Independent variables A3a: \\(E(\\mathbf{x_i&#39;}\\epsilon_i)=0\\) \\(x_i\\) is uncorrelated with \\(\\epsilon_i\\) Correlation and Independence Weaker than mean independence A3 A3 implies A3a, not the reverse No causality interpretations Cannot test the difference 6.1.1.2.4 A4 Homoskedasticity \\[ \\begin{equation} A4: Var(\\epsilon|x)=Var(\\epsilon)=\\sigma^2 \\tag{6.4} \\end{equation} \\] Variation in the disturbance to be the same over the independent variables 6.1.1.2.5 A5 Data Generation (random Sampling) \\[ \\begin{equation} A5: {y_i,x_{i1},...,x_{ik-1}: i = 1,..., n} \\tag{6.5} \\end{equation} \\] is a random sample random sample mean samples are independent and identically distributed (iid) from a joint distribution of \\((y,\\mathbf{x})\\) with A3 and A4, we have Strict Exogeneity: \\(E(\\epsilon_i|x_1,...,x_n)=0\\). independent variables do not carry information for prediction of \\(\\epsilon\\) Non-autocorrelation: \\(E(\\epsilon_i\\epsilon_j|x_1,...,x_n)=0\\) The error term is uncorrelated across the draws conditional on the independent variables \\(\\rightarrow\\) \\(A4: Var(\\epsilon|\\mathbf{X})=Var(\\epsilon)=\\sigma^2I_n\\) In times series and spatial settings, A5 is less likely to hold. 6.1.1.2.5.1 A5a A stochastic process \\(\\{x_t\\}_{t=1}^T\\) is stationary if for every collection fo time indices \\(\\{t_1,t_2,...,t_m\\}\\), the joint distribution of \\[ x_{t_1},x_{t_2},...,x_{t_m} \\] is the same as the joint distribution of \\[ x_{t_1+h},x_{t_2+h},...,x_{t_m+h} \\] for any \\(h \\ge 1\\) The joint distribution for the first ten observation is the same for the next ten, etc. Independent draws automatically satisfies this A stochastic process \\(\\{x_t\\}_{t=1}^T\\) is weakly stationary if \\(x_t\\) and \\(x_{t+h}\\) are almost independent as h increases without bounds. * two observation that are very far apart should be almost independent Common Weakly Dependent Processes Moving Average process of order 1 (MA(1)) MA(1) means that there is only one period lag. \\[ y_t = u_t + \\alpha_1 u_{t-1} \\\\ E(y_t) = E(u_t) + \\alpha_1E(u_{t-1}) = 0 \\\\ Var(y_t) = var(u_t) + \\alpha_1 var(u_{t-1}) = \\sigma^2 + \\alpha_1^2 \\sigma^2 = \\sigma^2(1+\\alpha_1^2) \\] where \\(u_t\\) is drawn iid over t with variance \\(\\sigma^2\\) An increase in the absolute value of \\(\\alpha_1\\) increases the variance When the MA(1) process can be inverted (\\(|\\alpha|&lt;1\\) then \\[ u_t = y_t - \\alpha_1u_{t-1} \\] called the autoregressive representation (express current observation in term of past observation). We can expand it to more than 1 lag, then we have MA(q) process \\[ y_t = u_t + \\alpha_1 u_{t-1} + ... + \\alpha_q u_{t-q} \\] where \\(u_t \\sim WN(0,\\sigma^2)\\) Covariance stationary: irrespective of the value of the parameters. Invertibility when \\(\\alpha &lt; 1\\) The conditional mean of MA(q) depends on the q lags (long-term memory). In MA(q), all autorcorrealtions beyond q are 0. \\[ \\begin{align} Cov(y_t,y_{t-1}) &amp;= Cov(u_t + \\alpha_1 u_{t-1},u_{t-1}+\\alpha_1u_{t-2}) \\\\ &amp;= \\alpha_1var(u_{t-1}) \\\\ &amp;= \\alpha_1\\sigma^2 \\end{align} \\] \\[ \\begin{align} Cov(y_t,y_{t-2}) &amp;= Cov(u_t + \\alpha_1 u_{t-1},u_{t-2}+\\alpha_{1}u_{t-3}) \\\\ &amp;= 0 \\end{align} \\] An MA models a linear relationship between teh dependent variable and teh current and past values of a stochastic term. Auto regressive process of order 1 (AR(1)) \\[ y_t = \\rho y_{t-1}+ u_t, |\\rho|&lt;1 \\] where \\(u_t\\) is drawn iid over t with variance \\(\\sigma^2\\) \\[ \\begin{align} Cov(y_t,y_{t-1}) &amp;= Cov(\\rho y_{t-1} + u-t,y_{t-1}) \\\\ &amp;= \\rho Var(y_{t-1}) \\\\ &amp;= \\rho \\frac{\\sigma^2}{1-\\rho^2} \\end{align} \\] \\[ \\begin{align} Cov(y_t,y_{t-h}) &amp;= \\rho^h \\frac{\\sigma^2}{1-\\rho^2} \\end{align} \\] Stationarity: in the continuum of t, the distribution of each t is the same \\[ E(y_t) = E(y_{t-1}) = ...= E(y_0) \\\\ y_1 = \\rho y_0 + u_1 \\] where the initial observation \\(y_0=0\\) Assume \\(E(y_t)=0\\) \\[ y_t = \\rho^t y_{t-t} + \\rho^{t-1}u_1 + \\rho^{t-2}u_2 +...+ \\rho u_{t-1} + u_t \\\\ = \\rho^t y_0 + \\rho^{t-1}u_1 + \\rho^{t-2}u_2 +...+ \\rho u_{t-1} + u_t \\] Hence, \\(y_t\\) is the weighted of all of the \\(u_t\\) time observations before. y will be correlated with all the previous observations as well as future observations. \\[ Var(y_t) = Var(\\rho y_{t-1} + u_t) \\\\ = \\rho^2 Var(y_{t-1}) + Var(u_t) + 2\\rho Cov(y_{t-1}u_t) \\\\ = \\rho^2 Var(y_{t-1}) + \\sigma^2 \\] Hence, \\[ Var(y_t) = \\frac{\\sigma^2}{1-\\rho^2} \\] to have Variance constantly over time, then \\(\\rho \\neq 1\\) or \\(-1\\). Then stationarity requires \\(\\rho \\neq 1\\) or -1. weakly dependent process \\(|\\rho|&lt;1\\) To estimate the AR(1) process, we use Yule-Walker Equation \\[ y_t = \\epsilon_t + \\phi y_{t-1} \\\\ y_t y_{t-\\tau} = \\epsilon_t y_{t-\\tau} + \\phi y_{t-1}y_{t-\\tau} \\\\ \\] For \\(\\tau \\ge 1\\), we have \\[ \\gamma \\tau = \\phi \\gamma (\\tau -1) \\\\ \\rho_t = \\phi^t \\] when you generalize to pth order autoregressive process, AR(p): \\[ y_t = \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} + \\epsilon_t \\] AR(p) process is covariance stationary, and decay in autocorrelations. When we combine MA(q) and AR(p), we have ARMA(p,q) process, where you can see seasonality. For example, ARMA(1,1) \\[ y_t = \\phi y_{t-1} + \\epsilon_t + \\alpha \\epsilon_{t-1} \\] Random Walk process \\[ y_t = y_0 + \\sum_{s=1}^{t}u_t \\] not stationary : when \\(y_0 = 0\\) then \\(E(y_t)= 0\\), but \\(Var(y_t)=t\\sigma^2\\). Further along in the spectrum, the variance will be larger not weakly dependent: \\(Cov(\\sum_{s=1}^{t}u_s,\\sum_{s=1}^{t-h}u_s) = (t-h)\\sigma^2\\). So the covariance (fixed) is not diminishing as h increases \\[ Assumption A5a: \\{y_t,x_{t1},..,x_{tk-1} \\} \\] where \\(t=1,...,T\\) are stationary and weakly dependent processes. Alternative Weak Law, Central Limit Theorem If \\(z_t\\) is a weakly dependent stationary process with a finite first absolute moment and \\(E(z_t) = \\mu\\), then \\[ T^{-1}\\sum_{t=1}^{T}z_t \\to^p \\mu \\] If additional regulatory conditions hold (Greene 1990), then \\[ \\sqrt{T}(\\bar{z}-\\mu) \\to^d N(0,B) \\] where \\(B= Var(z_t) + 2\\sum_{h=1}^{\\infty}Cov(z_t,z_{t-h})\\) 6.1.1.2.6 A6 Normal Distribution \\[ \\begin{equation} A6: \\epsilon|\\mathbf{x}\\sim N(0,\\sigma^2I_n) \\tag{6.6} \\end{equation} \\] The error term is normally distributed From A1-A3, we have identification (also known as Orthogonality Condition) of the population parameter \\(\\beta\\) \\[\\begin{align} y &amp;= {x}\\beta + \\epsilon &amp;&amp; \\text{A1} \\\\ x&#39;y &amp;= x&#39;x\\beta + x&#39;\\epsilon &amp;&amp; \\text{} \\\\ E(x&#39;y) &amp;= E(x&#39;x)\\beta + E(x&#39;\\epsilon) &amp;&amp; \\text{} \\\\ E(x&#39;y) &amp;= E(x&#39;x)\\beta &amp;&amp; \\text{A3} \\\\ [E(x&#39;x)]^{-1}E(x&#39;y) &amp;= [E(x&#39;x)]^{-1}E(x&#39;x)\\beta &amp;&amp; \\text{A2} \\\\ [E(x&#39;x)]^{-1}E(x&#39;y) &amp;= \\beta \\end{align}\\] is the row vector of parameters that produces the best predictor of y we choose the min of : \\[ \\underset{\\gamma}{\\operatorname{argmin}}E((y-x\\gamma)^2) \\] First Order Condition \\[ \\begin{split} \\frac{\\partial((y-x\\gamma)^2)}{\\partial\\gamma}&amp;=0 \\\\ -2E(x&#39;(y-x\\gamma))&amp;=0 \\\\ E(x&#39;y)-E(x&#39;x\\gamma) &amp;=0 \\\\ E(x&#39;y) &amp;= E(x&#39;x)\\gamma \\\\ (E(x&#39;x))^{-1}E(x&#39;y) &amp;= \\gamma \\end{split} \\] Second Order Conditon \\[ \\begin{split} \\frac{\\partial^2E((y-x\\gamma)^2)}{}&amp;=0 \\\\ E(\\frac{\\partial(y-x\\partial)^2)}{\\partial\\gamma\\partial\\gamma&#39;}) &amp;= 2E(x&#39;x) \\end{split} \\] If A3 holds, then \\(2E(x&#39;x)\\) is PSD \\(\\rightarrow\\) minimum 6.1.1.3 Theorems 6.1.1.3.1 Frisch-Waugh-Lovell Theorem \\[ \\mathbf{y=X\\beta + \\epsilon=X_1\\beta_1+X_2\\beta_2 +\\epsilon} \\] Equivalently, \\[ \\left( \\begin{array}{c} X_1&#39;X_1 &amp; X_1&#39;X_2 \\\\ X_2&#39;X_1 &amp; X_2&#39;X_2 \\end{array} \\right) \\left( \\begin{array}{c} \\hat{\\beta_1} \\\\ \\hat{\\beta_2} \\end{array} \\right) = \\left( \\begin{array}{c} X_1&#39;y \\\\ X_2&#39;y \\end{array} \\right) \\] Hence, \\[ \\mathbf{\\hat{\\beta_1}=(X_1&#39;X_1)^{-1}X_1&#39;y - (X_1&#39;X_1)^{-1}X_1&#39;X_2\\hat{\\beta_2}} \\] Betas from the multiple regression are not the same as the betas from each of the individual simple regression Different set of X will affect all the coefficient estimates. If \\(X_1&#39;X_2 = 0\\) or $=0, then 1 and 2 do not hold. 6.1.1.3.2 Gauss-Markov Theorem For a linear regression model \\[ \\mathbf{y=X\\beta + \\epsilon} \\] Under A1, A2, A3, A4, OLS estimator defined as \\[ \\hat{\\beta} = \\mathbf{(X&#39;X)^{-1}X&#39;y} \\] is the minimum variance linear (in y) unbiased estimator of \\(\\beta\\) Let \\(\\tilde{\\beta}=\\mathbf{Cy}\\), be another linear estimator where \\(\\mathbf{C}\\) is k x n and only function of X), then for it be unbiased, \\[ \\begin{split} E(\\tilde{\\beta}|\\mathbf{X}) &amp;= E(\\mathbf{Cy|X}) \\\\ &amp;= E(\\mathbf{CX\\beta + C\\epsilon|X}) \\\\ &amp;= \\mathbf{CX\\beta} \\end{split} \\] which equals the true parameter \\(\\beta\\) only if \\(\\mathbf{CX=I}\\) Equivalently, \\(\\tilde{\\beta} = \\beta + \\mathbf{C}\\epsilon\\) and the variance of the estimator is \\(Var(\\tilde{\\beta}|\\mathbf{X}) = \\sigma^2\\mathbf{CC&#39;}\\) To show minimum variance, \\[ \\begin{split} &amp;=\\sigma^2\\mathbf{(C-(X&#39;X)^{-1}X&#39;)(C-(X&#39;X)^{-1}X&#39;)&#39;} \\\\ &amp;= \\sigma^2\\mathbf{(CC&#39; - CX(X&#39;X)^{-1})-(X&#39;X)^{-1}X&#39;C + (X&#39;X)^{-1}X&#39;X(X&#39;X)^{-1})} \\\\ &amp;= \\sigma^2 (\\mathbf{CC&#39; - (X&#39;X)^{-1}-(X&#39;X)^{-1} + (X&#39;X)^{-1}}) \\\\ &amp;= \\sigma^2\\mathbf{CC&#39;} - \\sigma^2(\\mathbf{X&#39;X})^{-1} \\\\ &amp;= Var(\\tilde{\\beta}|\\mathbf{X}) - Var(\\hat{\\beta}|\\mathbf{X}) \\end{split} \\] Hierarchy of OLS Assumptions Identification Data Description Unbiasedness Consistency Gauss-Markov (BLUE) Asymptotic Inference (z and Chi-squared) Classical LM (BUE) Small-sample Inference (t and F) Variation in X Variation in X Variation in X Variation in X Random Sampling Random Sampling Random Sampling Linearity in Parameters Linearity in Parameters Linearity in Parameters Zero Conditional Mean Zero Conditional Mean Zero Conditional Mean Homoskedasticity Homoskedasticity Normality of Errors 6.1.1.4 Finite Sample Properties n is fixed Bias On average, how close is our estimate to the true value \\(Bias = E(\\hat{\\beta}) -\\beta\\) where \\(\\beta\\) is the true parameter value and \\(\\hat{\\beta}\\) is the estimator for \\(\\beta\\) An estimator is unbiased when \\(Bias = E(\\hat{\\beta}) -\\beta = 0\\) or \\(E(\\hat{\\beta})=\\beta\\) means that the estimator will produce estimates that are, on average, equal to the value it it trying to estimate Distribution of an estimator: An estimator is a function of random variables (data) Standard Deviation: the spread of the estimator. OLS Under A1 A2 A3, OLS is unbiased \\[ \\begin{align} E(\\hat{\\beta}) &amp;= E(\\mathbf{(X&#39;X)^{-1}X&#39;y}) &amp;&amp; \\text{A2}\\\\ &amp;= E(\\mathbf{(X&#39;X)^{-1}X&#39;(X\\beta + \\epsilon)}) &amp;&amp; \\text{A1}\\\\ &amp;= E(\\mathbf{(X&#39;X)^{-1}X&#39;X\\beta + (X&#39;X)^{-1}X&#39;\\epsilon}) &amp;&amp; \\text{} \\\\ &amp;= E(\\beta + \\mathbf{(X&#39;X)^{-1}X&#39;\\epsilon}) \\\\ &amp;= \\beta + E(\\mathbf{(X&#39;X^{-1}\\epsilon)}) \\\\ &amp;= \\beta + E(E((\\mathbf{X&#39;X)^{-1}X&#39;\\epsilon|X})) &amp;&amp;\\text{LIE} \\\\ &amp;= \\beta + E((\\mathbf{X&#39;X)^{-1}X&#39;}E\\mathbf{(\\epsilon|X})) \\\\ &amp;= \\beta + E((\\mathbf{X&#39;X)^{-1}X&#39;}0)) &amp;&amp; \\text{A3} \\\\ &amp;= \\beta \\end{align} \\] where LIE stands for Law of Iterated Expectation If A3 does not hold, then OLS will be biased From Frisch-Waugh-Lovell Theorem, if we have the omitted variable \\(\\hat{\\beta}_2 \\neq 0\\) and \\(\\mathbf{X_1&#39;X_2} \\neq 0\\), then the omitted variable will cause OLS estimator to be biased. Under A1 A2 A3 A4, we have the conditional variance of the OLS estimator as follows] \\[ \\begin{align} Var(\\hat{\\beta}|\\mathbf{X}) &amp;= Var(\\beta + \\mathbf{(X&#39;X)^{-1}X&#39;\\epsilon|X}) &amp;&amp; \\text{A1-A2}\\\\ &amp;= Var((\\mathbf{X&#39;X)^{-1}X&#39;\\epsilon|X)} \\\\ &amp;= \\mathbf{X&#39;X^{-1}X&#39;} Var(\\epsilon|\\mathbf{X})\\mathbf{X(X&#39;X)^{-1}} \\\\ &amp;= \\mathbf{X&#39;X^{-1}X&#39;} \\sigma^2I \\mathbf{X(X&#39;X)^{-1}} &amp;&amp; \\text{A4} \\\\ &amp;= \\sigma^2\\mathbf{X&#39;X^{-1}X&#39;} I \\mathbf{X(X&#39;X)^{-1}} \\\\ &amp;= \\sigma^2\\mathbf{(X&#39;X)^{-1}} \\end{align} \\] Sources of variation \\(\\sigma^2=Var(\\epsilon_i|\\mathbf{X})\\) The amount of unexplained variation \\(\\epsilon_i\\) is large relative to the explained \\(\\mathbf{x_i \\beta}\\) variation Small \\(Var(x_{i1}), Var(x_{i1}),..\\) Not a lot of variation in \\(\\mathbf{X}\\) (no information) small sample size Strong correlation between the explanatory variables \\(x_{i1}\\) is highly correlated with a linear combination of 1, \\(x_{i2}\\), \\(x_{i3}\\),  include many irrelevant variables will contribute to this. If \\(x_1\\) is perfectly determined in the regression \\(\\rightarrow\\) Perfect Collinearity \\(\\rightarrow\\) A2 is violated. If \\(x_1\\) is highly correlated with a linear combination of other variables, then we have Multicollinearity 6.1.1.4.1 Check for Multicollinearity Variance Inflation Factor (VIF) Rule of thumb \\(VIF \\ge 10\\) is large \\[ VIF = \\frac{1}{1-R_1^2} \\] 6.1.1.4.2 Standard Errors \\(Var(\\hat{\\beta}|\\mathbf{X})=\\sigma^2\\mathbf{(X&#39;X)^{-1}}\\) is the variance of the estimate \\(\\hat{\\beta}\\) Standard Errors are estimators/estimates of the standard deviation (square root of the variance) of the estimator \\(\\hat{\\beta}\\) Under A1-A5, then we can estimate \\(\\sigma^2=Var(\\epsilon^2|\\mathbf{X})\\) the standard errors as \\[ s^2 = \\frac{1}{n-k}\\sum_{i=1}^{n}e_i^2 \\\\ = \\frac{1}{n-k}SSR \\] degrees of freedom adjustment: because \\(e_i \\neq \\epsilon_i\\) and are estimated using k estimates for \\(\\beta\\), we lose degrees of freedom in our variance estimate. \\(s=\\sqrt{s^2}\\) is a biased estimator for the standard deviation ([Jensens Inequality]) Standard Errors for \\(\\hat{\\beta}\\) \\[ SE(\\hat{\\beta}_{j-1})=s\\sqrt{[(\\mathbf{X&#39;X})^{-1}]_{jj}} \\\\ = \\frac{s}{\\sqrt{SST_{j-1}(1-R_{j-1}^2)}} \\] where \\(SST_{j-1}\\) and \\(R_{j-1}^2\\) from the following regression \\(x_{j-1}\\) on 1, \\(x_1\\), \\(x_{j-2}\\),\\(x_j\\),\\(x_{j+1}\\), , \\(x_{k-1}\\) Summary of Finite Sample Properties Under A1-A3: OLS is unbiased Under A1-A4: The variance of the OLS estimator is \\(Var(\\hat{\\beta}|\\mathbf{X})=\\sigma^2\\mathbf{(X&#39;X)^{-1}}\\) Under A1-A4, A6: OLS estimator \\(\\hat{\\beta} \\sim N(\\beta,\\sigma^2\\mathbf{(X&#39;X)^{-1}})\\) Under A1-A4, Gauss-Markov Theorem holds \\(\\rightarrow\\) OLS is BLUE Under A1-A5, the above standard errors are unbiased estimator of standard deviation for \\(\\hat{\\beta}\\) 6.1.1.5 Large Sample Properties let \\(n \\rightarrow \\infty\\) A perspective that allows us to evaluate the quality of estimators when finite sample properties are not informative, or impossible to compute consistency, asymptotic distribution, asymptotic variance Motivation Finite Sample Properties need strong assumption A1 A3 A4 A6 Other estimation such as GLS, MLE need to be analyzed using Large Sample Properties Let \\(\\mu(\\mathbf{X})=E(y|\\mathbf{X})\\) be the Conditional Expectation Function \\(\\mu(\\mathbf{X})\\) is the minimum mean squared predictor (over all possible functions) \\[ minE((y-f(\\mathbf{X}))^2) \\] under A1 and A3, \\[ \\mu(\\mathbf{X})=\\mathbf{X}\\beta \\] Then the linear projection \\[ L(y|1,\\mathbf{X})=\\gamma_0 + \\mathbf{X}Var(X)^{-1}Cov(X,Y) \\] where \\(\\mathbf{X}Var(X)^{-1}Cov(X,Y)=\\gamma\\) is the minimum mean squared linear approximation to be conditional mean function \\[ (\\gamma_0,\\gamma) = arg min E((E(y|\\mathbf{X})-(a+\\mathbf{Xb})^2) \\] OLS is always consistent for the linear projection, but not necessarily unbiased. Linear projection has no causal interpretation Linear projection does not depend on assumption A1 and A3 Evaluating an estimator using large sample properties: Consistency: measure of centrality Limiting Distribution: the shape of the scaled estimator as the sample size increases Asymptotic variance: spread of the estimator with regards to its limiting distribution. An estimator \\(\\hat{\\theta}\\) is consistent for \\(\\theta\\) if \\(\\hat{\\theta}_n \\to^p \\theta\\) As n increases, the estimator converges to the population parameter value. Unbiased does not imply consistency and consistency does not imply unbiased. Based on Weak Law of Large Numbers \\[ \\begin{split} \\hat{\\beta} &amp;= \\mathbf{(X&#39;X)^{-1}X&#39;y} \\\\ &amp;= \\mathbf{(\\sum_{i=1}^{n}x_i&#39;x_i)^{-1} \\sum_{i=1}^{n}x_i&#39;y_i} \\\\ &amp;= (n^{-1}\\mathbf{\\sum_{i=1}^{n}x_i&#39;x_i)^{-1}} n^{-1}\\mathbf{\\sum_{i=1}^{n}x_i&#39;y_i} \\\\ plim(\\hat{\\beta}) &amp;= plim((n^{-1}\\mathbf{\\sum_{i=1}^{n}x_i&#39;x_i)^{-1}} n^{-1}\\mathbf{\\sum_{i=1}^{n}x_i&#39;y_i}) \\\\ &amp;= plim((n^{-1}\\mathbf{\\sum_{i=1}^{n}x_i&#39;x_i)^{-1}})plim(n^{-1}\\mathbf{\\sum_{i=1}^{n}x_i&#39;y_i}) \\\\ &amp;= (plim(n^{-1}\\mathbf{\\sum_{i=1}^{n}x_i&#39;x_i)^{-1}})plim(n^{-1}\\mathbf{\\sum_{i=1}^{n}x_i&#39;y_i}) &amp;&amp; \\text{ due to A2, A5} \\\\ &amp;= E(\\mathbf{x_i&#39;x_i})^{-1}E(\\mathbf{x_i&#39;y_i}) \\end{split} \\] \\[ E(\\mathbf{x_i&#39;x_i})^{-1}E(\\mathbf{x_i&#39;y_i}) = \\beta + E(\\mathbf{x_i&#39;x_i})^{-1}E(\\mathbf{x_i&#39;\\epsilon_i}) \\] Under A1, A2, A3a, A5 OLS is consistent, but not guarantee unbiased. Under A1, A2, A3a, A5, and \\(\\mathbf{x_i&#39;x_i}\\) has finite first and second moments (CLT), \\(Var(\\mathbf{x_i&#39;}\\epsilon_i)=\\mathbf{B}\\) \\((n^{-1}\\sum_{i=1}^{n}\\mathbf{x_i&#39;x_i})^{-1} \\to^p (E(\\mathbf{x&#39;_ix_i}))^{-1}\\) \\(\\sqrt{n}(n^{-1}\\sum_{i=1}^{n}\\mathbf{x_i&#39;}\\epsilon_i) \\to^d N(0,\\mathbf{B})\\) \\[ \\sqrt{n}(\\hat{\\beta}-\\beta) = (n^{-1}\\sum_{i=1}^{n}\\mathbf{x_i&#39;x_i})^{-1}\\sqrt{n}(n^{-1}\\sum_{i=1}^{n}\\mathbf{x_i&#39;x_i}) \\to^{d} N(0,\\Sigma) \\] where \\(\\Sigma=(E(\\mathbf{x_i&#39;x_i}))^{-1}\\mathbf{B}(E(\\mathbf{x_i&#39;x_i}))^{-1}\\) holds under A3a Do not need A4 and A6 to apply CLT If A4 does not hold, then \\(\\mathbf{B}=Var(\\mathbf{x_i&#39;}\\epsilon_i)=\\sigma^2E(x_i&#39;x_i)\\) which means \\(\\Sigma=\\sigma^2(E(\\mathbf{x_i&#39;x_i}))^{-1}\\), use standard errors Heteroskedasticity can be from Limited dependent variable Dependent variables with large/skewed ranges Solving Asymptotic Variance \\[ \\begin{split} \\Sigma &amp;= (E(\\mathbf{x_i&#39;x_i}))^{-1}\\mathbf{B}(E(\\mathbf{x_i&#39;x_i}))^{-1} \\\\ &amp;= (E(\\mathbf{x_i&#39;x_i}))^{-1}Var(\\mathbf{x_i&#39;}\\epsilon_i)(E(\\mathbf{x_i&#39;x_i}))^{-1} \\\\ &amp;= (E(\\mathbf{x_i&#39;x_i}))^{-1}E[(\\mathbf{x_i&#39;}\\epsilon_i-0)(\\mathbf{x_i&#39;}\\epsilon_i-0)](E(\\mathbf{x_i&#39;x_i}))^{-1} &amp;&amp; \\text{A3a} \\\\ &amp;= (E(\\mathbf{x_i&#39;x_i}))^{-1}E[E(\\mathbf{\\epsilon_i^2|x_i)x_i&#39;x_i]}(E(\\mathbf{x_i&#39;x_i}))^{-1} &amp;&amp; \\text{LIE} \\\\ &amp;= (E(\\mathbf{x_i&#39;x_i}))^{-1}\\sigma^2E(\\mathbf{x_i&#39;x_i})(E(\\mathbf{x_i&#39;x_i}))^{-1} &amp;&amp; \\text{A4} \\\\ &amp;= \\sigma^2(E(\\mathbf{x_i&#39;x_i})) \\end{split} \\] Under A1, A2, A3a, A4, A5: \\[ \\sqrt{n}(\\hat{\\beta}-\\beta) \\to^d N(0,\\sigma^2(E(\\mathbf{x_i&#39;x_i}))^{-1}) \\] The Asymptotic variance is approximation for the variance in the scaled random variable for \\(\\sqrt{n}(\\hat{\\beta}-\\beta)\\) when n is large. use \\(Avar(\\sqrt{n}(\\hat{\\beta}-\\beta))/n\\) as an approximation for finite sample variance for large n: \\[ Avar(\\sqrt{n}(\\hat{\\beta}-\\beta)) \\approx Var(\\sqrt{n}(\\hat{\\beta}-\\beta)) \\\\ Avar(\\sqrt{n}(\\hat{\\beta}-\\beta))/n \\approx Var(\\sqrt{n}(\\hat{\\beta}-\\beta))/n = Var(\\hat{\\beta}) \\] Avar(.) does not behave the same way as Var(.) \\[ Avar(\\sqrt{n}(\\hat{\\beta}-\\beta))/n \\neq Avar(\\sqrt{n}(\\hat{\\beta}-\\beta)/\\sqrt{n}) \\\\ \\neq Avar(\\hat{\\beta}) \\] In Finite Sample Properties, we calculate standard errors as an estimate for the conditional standard deviation: \\[ SE_{fs}(\\hat{\\beta}_{j-1})=\\sqrt{\\hat{Var}}(\\hat{\\beta}_{j-1}|\\mathbf{X}) = \\sqrt{s^2[\\mathbf{(X&#39;X)}^{-1}]_{jj}} \\] In Large Sample Properties, we calculate standard errors as an estimate for the square root of asymptotic variance \\[ SE_{ls}(\\hat{\\beta}_{j-1})=\\sqrt{\\hat{Avar}(\\sqrt{n}\\hat{\\beta}_{j-1})/n} = \\sqrt{s^2[\\mathbf{(X&#39;X)}^{-1}]_{jj}} \\] Hence, the standard error estimator is the same for finite sample and large sample. * Same estimator, but conceptually estimating two different things. * Valid under weaker assumptions: the assumptions needed to produce a consistent estimator for the finite sample conditional variance (A1-A5) are stronger than those needed to produce a consistent estimator for the asymptotic variance (A1,A2,A3a,A4,A5) 6.1.1.6 Application 6.1.2 Feasible Generalized Least Squares Motivation for a more efficient estimator Gauss-Markov Theorem holds under A1-A4 A4: \\(Var(\\epsilon| \\mathbf{X} )=\\sigma^2I_n\\) Heteroskedasticity: \\(Var(\\epsilon_i|\\mathbf{X}) \\neq \\sigma^2I_n\\) Serial Correlation: \\(Cov(\\epsilon_i,\\epsilon_j|\\mathbf{X}) \\neq 0\\) Without A4, how can we know which unbiased estimator is the most efficient? Original (unweighted) model: \\[ \\mathbf{y=X\\beta+ \\epsilon} \\] Suppose A1-A3 hold, but A4 does not hold, \\[ \\mathbf{Var(\\epsilon|X)=\\Omega \\neq \\sigma^2 I_n} \\] We will try to use OLS to estimate the transformed (weighted) model \\[ \\mathbf{wy=wX\\beta + w\\epsilon} \\] We need to choose \\(\\mathbf{w}\\) so that \\[ \\mathbf{w&#39;w = \\Omega^{-1}} \\] then \\(\\mathbf{w}\\) (full-rank matrix) is the Cholesky decomposition of \\(\\mathbf{\\Omega^{-1}}\\) (full-rank matrix) In other words, \\(\\mathbf{w}\\) is the squared root of \\(\\Omega\\) (squared root version in matrix) \\[ \\Omega = var(\\epsilon | X) \\\\ \\Omega^{-1} = var(\\epsilon | X)^{-1} \\] Then, the transformed equation (IGLS) will have the following properties. \\[\\begin{equation} \\begin{split} \\mathbf{\\hat{\\beta}_{IGLS}} &amp;= \\mathbf{(X&#39;w&#39;wX)^{-1}X&#39;w&#39;wy} \\\\ &amp; = \\mathbf{(X&#39;\\Omega^{-1}X)^{-1}X&#39;\\Omega^{-1}y} \\\\ &amp; = \\mathbf{\\beta + X&#39;\\Omega^{-1}X&#39;\\Omega^{-1}\\epsilon} \\end{split} \\end{equation}\\] Since A1-A3 hold for the unweighted model \\[\\begin{equation} \\begin{split} \\mathbf{E(\\hat{\\beta}_{IGLS}|X)} &amp; = E(\\mathbf{\\beta + (X&#39;\\Omega^{-1}X&#39;\\Omega^{-1}\\epsilon)}|X)\\\\ &amp; = \\mathbf{\\beta + E(X&#39;\\Omega^{-1}X&#39;\\Omega^{-1}\\epsilon)|X)} \\\\ &amp; = \\mathbf{\\beta + X&#39;\\Omega^{-1}X&#39;\\Omega^{-1}E(\\epsilon|X)} &amp;&amp; \\text{since A3: $E(\\epsilon|X)=0$} \\\\ &amp; = \\mathbf{\\beta} \\end{split} \\end{equation}\\] \\(\\rightarrow\\) IGLS estimator is unbiased \\[\\begin{equation} \\begin{split} \\mathbf{Var(w\\epsilon|X)} &amp;= \\mathbf{wVar(\\epsilon|X)w&#39;} \\\\ &amp; = \\mathbf{w\\Omega w&#39;} \\\\ &amp; = \\mathbf{w(w&#39;w)^{-1}w&#39;} &amp;&amp; \\text{since w is a full-rank matrix}\\\\ &amp; = \\mathbf{ww^{-1}(w&#39;)^{-1}w&#39;} \\\\ &amp; = \\mathbf{I_n} \\end{split} \\end{equation}\\] \\(\\rightarrow\\) A4 holds for the transformed (weighted) equation Then, the variance for the estimator is \\[\\begin{equation} \\begin{split} Var(\\hat{\\beta}_{IGLS}|\\mathbf{X}) &amp; = \\mathbf{Var(\\beta + (X&#39;\\Omega ^{-1}X)^{-1}X&#39;\\Omega^{-1}\\epsilon|X)} \\\\ &amp;= \\mathbf{Var((X&#39;\\Omega ^{-1}X)^{-1}X&#39;\\Omega^{-1}\\epsilon|X)} \\\\ &amp;= \\mathbf{(X&#39;\\Omega ^{-1}X)^{-1}X&#39;\\Omega^{-1} Var(\\epsilon|X) \\Omega^{-1}X(X&#39;\\Omega ^{-1}X)^{-1}} &amp;&amp; \\text{because A4 holds}\\\\ &amp;= \\mathbf{(X&#39;\\Omega ^{-1}X)^{-1}X&#39;\\Omega^{-1} \\Omega \\Omega^{-1} \\Omega^{-1}X(X&#39;\\Omega ^{-1}X)^{-1}} \\\\ &amp;= \\mathbf{(X&#39;\\Omega ^{-1}X)^{-1}} \\end{split} \\end{equation}\\] Let \\(A = \\mathbf{(X&#39;X)^{-1}X&#39;-(X&#39;\\Omega ^{-1} X)X&#39; \\Omega^{-1}}\\) then \\[ Var(\\hat{\\beta}_{OLS}|X)- Var(\\hat{\\beta}_{IGLS}|X) = A\\Omega A&#39; \\] And \\(\\Omega\\) is Positive Semi Definite, then \\(A\\Omega A&#39;\\) also PSD, then IGLS is more efficient The name Infeasible comes from the fact that it is impossible to compute this estimator. \\[\\begin{equation} \\mathbf{w} = \\left( \\begin{array}{c} w_{11} &amp; 0 &amp; 0 &amp; ... &amp; 0 \\\\ w_{21} &amp; w_{22} &amp; 0 &amp; ... &amp; 0 \\\\ w_{31} &amp; w_{32} &amp; w_{33} &amp; ... &amp; ... \\\\ w_{n1} &amp; w_{n2} &amp; w_{n3} &amp; ... &amp; w_{nn} \\\\ \\end{array} \\right) \\end{equation}\\] With \\(n(n+1)/2\\) number of elements and n observations \\(\\rightarrow\\) infeasible to estimate. (number of equation &gt; data) Hence, we need to make assumption on \\(\\Omega\\) to make it feasible to estimate \\(\\mathbf{w}\\): Heteroskedasticity : multiplicative exponential model AR(1) Cluster 6.1.2.1 Heteroskedasticity \\[\\begin{equation} \\begin{split} Var(\\epsilon_i |x_i) &amp; = E(\\epsilon^2|x_i) \\neq \\sigma^2 \\\\ &amp; = h(x_i) = \\sigma_i^2 \\text{(variance of the error term is a function of x)} \\end{split} \\tag{6.7} \\end{equation}\\] For our model, \\[ y_i = x_i\\beta + \\epsilon_i \\\\ (1/\\sigma_i)y_i = (1/\\sigma_i)x_i\\beta + (1/\\sigma_i)\\epsilon_i \\] then, from (6.7) \\[ \\begin{equation} \\begin{split} Var((1/\\sigma_i)\\epsilon_i|X) &amp;= (1/\\sigma_i^2) Var(\\epsilon_i|X) \\\\ &amp;= (1/\\sigma_i^2)\\sigma_i^2 \\\\ &amp;= 1 \\end{split} \\end{equation} \\] then the weight matrix \\(\\mathbf{w}\\) in the matrix equation \\[ \\mathbf{wy=wX\\beta + w\\epsilon} \\] \\[ \\mathbf{w}= \\left( \\begin{array}{c} 1/\\sigma_1 &amp; 0 &amp; 0 &amp; ... &amp; 0 \\\\ 0 &amp; 1/\\sigma_2 &amp; 0 &amp; ... &amp; 0 \\\\ 0 &amp; 0 &amp; 1/\\sigma_3 &amp; ... &amp; . \\\\ . &amp; . &amp; . &amp; . &amp; 0 \\\\ 0 &amp; 0 &amp; . &amp; . &amp; 1/\\sigma_n \\end{array} \\right) \\] Infeasible Weighted Least Squares Assume we know \\(\\sigma_i^2\\) (Infeasible) The IWLS estimator is obtained as the least squared estimated for the following weighted equation \\[ (1/\\sigma_i)y_i = (1/\\sigma_i)\\mathbf{x}_i\\beta + (1/\\sigma_i)\\epsilon_i \\] Usual standard errors for the weighted equation are valid if \\(Var(\\epsilon | \\mathbf{X}) = \\sigma_i^2\\) If \\(Var(\\epsilon | \\mathbf{X}) \\neq \\sigma_i^2\\) then heteroskedastic robust standard errors are valid. Problem: We do not know \\(\\sigma_i^2=Var(\\epsilon_i|\\mathbf{x_i})=E(\\epsilon_i^2|\\mathbf{x}_i)\\) One observation \\(\\epsilon_i\\) cannot estimate a sample variance estimate \\(\\sigma_i^2\\) Model \\(\\epsilon_i^2\\) as reasonable (strictly positive) function of \\(x_i\\) and independent error \\(v_i\\) (strictly positive) \\[ \\epsilon_i^2=v_i exp(\\mathbf{x_i\\gamma}) \\] Then we can apply a log transformation to recover a linear in parameters model, \\[ ln(\\epsilon_i^2) = \\mathbf{x_i\\gamma} + ln(v_i) \\] where \\(ln(v_i)\\) is independent \\(\\mathbf{x}_i\\) We do not observe \\(\\epsilon_i\\) * OLS residual (\\(e_i\\)) as an approximate 6.1.2.2 Serial Correlation \\[ Cov(\\epsilon_i, \\epsilon_j | \\mathbf{X}) \\neq 0 \\] Under covariance stationary, \\[ Cov(\\epsilon_i,\\epsilon_j|\\mathbf{X}) = Cov(\\epsilon_i, \\epsilon_{i+h}|\\mathbf{x_i,x_{i+h}})=\\gamma_h \\] And the variance covariance matrix is \\[ Var(\\epsilon|\\mathbf{X}) = \\Omega = \\left( \\begin{array}{c} \\sigma^2 &amp; \\gamma_1 &amp; \\gamma_2 &amp; ... &amp; \\gamma_{n-1} \\\\ \\gamma_1 &amp; \\sigma^2 &amp; \\gamma_1 &amp; ... &amp; \\gamma_{n-2} \\\\ \\gamma_2 &amp; \\gamma_1 &amp; \\sigma^2 &amp; ... &amp; ... \\\\ . &amp; . &amp; . &amp; . &amp; \\gamma_1 \\\\ \\gamma_{n-1} &amp; \\gamma_{n-2} &amp; . &amp; \\gamma_1 &amp; \\sigma^2 \\end{array} \\right) \\] There n parameters to estimate - need some sort fo structure to reduce number of parameters to estimate. Time Series Effect of inflation and deficit on Treasury BIll interest rates Cross-sectional Clustering 6.1.2.2.1 AR(1) \\[ y_t= \\beta_0 + x_t\\beta_1 + \\epsilon_t \\\\ \\epsilon_t = \\rho \\epsilon_{t-1} + u_t \\] and the variance covariance matrix is \\[ Var(\\epsilon | \\mathbf{X})= \\frac{\\sigma^2_u}{1-\\rho} \\left( \\begin{array}{c} 1 &amp; \\rho &amp; \\rho^2 &amp; ... &amp; \\rho^{n-1} \\\\ \\rho &amp; 1 &amp; \\rho &amp; ... &amp; \\rho^{n-2} \\\\ \\rho^2 &amp; \\rho &amp; 1 &amp; . &amp; . \\\\ . &amp; . &amp; . &amp; . &amp; \\rho \\\\ \\rho^{n-1} &amp; \\rho^{n-2} &amp; . &amp; \\rho &amp; 1 \\\\ \\end{array} \\right) \\] Hence, there is only 1 parameter to estimate: \\(\\rho\\) Under A1, A2, A3a, A5a, OLS is consistent and asymptotically normal Use Newey West Standard Errors for valid inference. Apply Infeasible Cochrane Orcutt (as if we knew \\(\\rho\\)) Because \\[ u_t = \\epsilon_t - \\rho \\epsilon_{t-1} \\] satisfies A3, A4, A5 wed like to to transform the above equation to one that has \\(u_t\\) as the error. \\[ \\begin{align} y_t - \\rho y_{t-1} &amp;= (\\beta_0 + x\\beta_1 + \\epsilon_t) - \\rho (\\beta_0 + x_{t-1}\\beta_1 + \\epsilon_{t-1}) \\\\ &amp; = (1-\\rho)\\beta_0 + (x_t - \\rho x_{t-1})\\beta_1 + u_t \\end{align} \\] 6.1.2.2.1.1 Infeasible Cochrane Orcutt Assume that we know \\(\\rho\\) (Infeasible) The ICO estimator is obtained as the least squared estimated for the following weighted first difference equation \\[ y_t -\\rho y_{t-1} = (1-\\rho)\\beta_0 + (x_t - \\rho x_{t-1})\\beta_1 + u_t \\] Usual standard errors for the weighted first difference equation are valid if the errors truly follow an AR(1) process If the serial correlation is generated from a more complex dynamic process then Newey-West HAC standard errors are valid Problem We do not know \\(\\rho\\) \\(\\rho\\) is the correlation between \\(\\epsilon_t\\) and \\(\\epsilon_{t-1}\\): estimate using OLS residuals (\\(e_i\\)) as proxy \\[ \\hat{\\rho} = \\frac{\\sum_{t=1}^{T}e_te_{t-1}}{\\sum_{t=1}^{T}e_t^2} \\] which can be obtained from the OLS regression of \\[ e_t = \\rho e_{t-1} + u_t \\] where we suppress the intercept. We are losing an observation By taking the first difference we are dropping the first observation \\[ y_1 = \\beta_0 + x_1 \\beta_1 + \\epsilon_1 \\] + Feasiable Prais Winsten Transformation applies the Infeasible Cochrane Orcutt but includes a weighted version of the first observation \\[ (\\sqrt{1-\\rho^2})y_1 = \\beta_0 + (\\sqrt{1-\\rho^2})x_1 \\beta_1 + (\\sqrt{1-\\rho^2}) \\epsilon_1 \\] 6.1.2.2.2 Cluster \\[ y_{gi} = \\mathbf{x}_{gi}\\beta + \\epsilon_{gi} \\] \\[ Cov(\\epsilon_{gi}, \\epsilon_{hj}) \\begin{cases} = 0 &amp; \\text{for $g \\neq h$ and any pair (i,j)} \\\\ \\neq 0 &amp; \\text{for any (i,j) pair}\\\\ \\end{cases} \\] Intra-group Correlation Each individual in a single group may be correlated but independent across groups. A4 is violated. usual standard errors for OLS are valid. Use cluster robust standard errors for OLS. Suppose there are 3 groups with different n \\[ Var(\\epsilon| \\mathbf{X})= \\Omega = \\left( \\begin{array}{c} \\sigma^2 &amp; \\delta_{12}^1 &amp; \\delta_{13}^1 &amp; 0 &amp; 0 &amp; 0 \\\\ \\delta_{12}^1 &amp; \\sigma^2 &amp; \\delta_{23}^1 &amp; 0 &amp; 0 &amp; 0 \\\\ \\delta_{13}^1 &amp; \\delta_{23}^1 &amp; \\sigma^2 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma^2 &amp; \\delta_{12}^2 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\delta_{12}^2 &amp; \\sigma^2 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma^2 \\end{array} \\right) \\] where \\(Cov(\\epsilon_{gi}, \\epsilon_{gj}) = \\delta_{ij}^g\\) and \\(Cov(\\epsilon_{gi}, \\epsilon_{hj}) = 0\\) for any i and j Infeasible Generalized Least Squares (Cluster) Assume that \\(\\sigma^2\\) and \\(\\delta_{ij}^g\\) are known, plug into \\(\\Omega\\) and solve for the inverse \\(\\Omega^{-1}\\) (infeasible) The Infeasible Generalized Least Squares Estimator is \\[ \\hat{\\beta}_{IGLS} = \\mathbf{(X&#39;\\Omega^{-1}X)^{-1}X&#39;\\Omega^{-1}y} \\] Problem * We do not know \\(\\sigma^2\\) and \\(\\delta_{ij}^g\\) + Can make assumptions about data generating process that is causing the clustering behavior. - Will give structure to \\(Cov(\\epsilon_{gi},\\epsilon_{gj})= \\delta_{ij}^g\\) which makes it feasible to estimate - if the assumptions are wrong then we should use cluster robust standard errors. Solution Assume group level random effects specification in the error \\[ y_{gi} = \\mathbf{g}_i \\beta + c_g + u_{gi} \\\\ Var(c_g|\\mathbf{x}_i) = \\sigma^2_c \\\\ Var(u_{gi}|\\mathbf{x}_i) = \\sigma^2_u \\] where \\(c_g\\) and \\(u_{gi}\\) are independent of each other, and mean independent of \\(\\mathbf{x}_i\\) \\(c_g\\) captures the common group shocks (independent across groups) \\(u_{gi}\\) captures the individual shocks (independent across individuals and groups) Then the error variance is \\[ Var(\\epsilon| \\mathbf{X})= \\Omega = \\left( \\begin{array}{c} \\sigma^2_c + \\sigma^2_u &amp; \\sigma^2_c &amp; \\sigma^2_c &amp; 0 &amp; 0 &amp; 0 \\\\ \\sigma^2_c &amp; \\sigma^2 + \\sigma^2_u &amp; \\sigma^2_c &amp; 0 &amp; 0 &amp; 0 \\\\ \\sigma^2_c &amp; \\sigma^2_c &amp; \\sigma^2+ \\sigma^2_u &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma^2+ \\sigma^2_u &amp; \\sigma^2_c &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; \\sigma^2_c &amp; \\sigma^2+ \\sigma^2_u &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\sigma^2+ \\sigma^2_u \\end{array} \\right) \\] Use Feasible group level Random Effects 6.1.3 Weighted Least Squares Estimate the following equation using OLS \\[ y_i = \\mathbf{x}_i \\beta + \\epsilon_i \\] and obtain the residuals \\(e_i=y_i -\\mathbf{x}_i \\hat{\\beta}\\) Transform the residual and estimate the following by OLS, \\[ ln(e_i^2)= \\mathbf{x}_i\\gamma + ln(v_i) \\] and obtain the predicted values \\(g_i=\\mathbf{x}_i \\hat{\\gamma}\\) The weights will be the untransformed predicted outcome, \\[ \\hat{\\sigma}_i =\\sqrt{exp(g_i)} \\] The FWLS (Feasible WLS) estimator is obtained as the least squared estimated for the following weighted equation \\[ (1/\\hat{\\sigma}_i)y_i = (1/\\hat{\\sigma}_i) \\mathbf{x}_i\\beta + (1/\\hat{\\sigma}_i)\\epsilon_i \\] Properties of the FWLS The infeasible WLS estimator is unbiased under A1-A3 for the unweighted equation. The FWLS estimator is NOT an unbiased estimator. The FWLS estimator is consistent under A1, A2, (for the unweighted equation), A5, and \\(E(\\mathbf{x}_i&#39;\\epsilon_i/\\sigma^2_i)=0\\) A3a is not sufficient for the above equation A3 is sufficient for the above equation. The FWLS estimator is asymptotically more efficient than OLS if the errors have multiplicative exponential heteroskedasticity. If the errors are truly multiplicative exponential heteroskedasticity, then usual standard errors are valid If we believe that there may be some mis-specification with the multiplicative exponential model, then we should report heteroskedastic robust standard errors. 6.1.4 Feasiable Prais Winsten Weighting Matrix \\[ \\mathbf{w} = \\left( \\begin{array}{c} \\sqrt{1- \\hat{\\rho}^2} &amp; 0 &amp; 0 &amp;... &amp; 0 \\\\ -\\hat{\\rho} &amp; 1 &amp; 0 &amp; ... &amp; 0 \\\\ 0 &amp; -\\hat{\\rho} &amp; 1 &amp; &amp; . \\\\ . &amp; . &amp; . &amp; . &amp; 0 \\\\ 0 &amp; . &amp; 0 &amp; -\\hat{\\rho} &amp; 1 \\end{array} \\right) \\] Estimate the following equation using OLS \\[ y_t = \\mathbf{x}_t \\beta + \\epsilon_t \\] and obtain the residuals \\(e_t = y_t - \\mathbf{x}_t \\hat{\\beta}\\) Estimate the correlation coefficient for the AR(1) process by estimating the following by OLS (without no intercept) \\[ e_t = \\rho e_{t-1} + u_t \\] Transform the outcome and independent variables \\(\\mathbf{wy}\\) and \\(\\mathbf{wX}\\) respectively (weight matrix as stated). The FPW estimator is obtained as the least squared estimated for the following weighted equation \\[ \\mathbf{wy = wX\\beta + w\\epsilon} \\] Properties of Feasiable Prais Winsten Estimator The Infeasible PW estimator is under A1-A3 for the unweighted equation The FPW estimator is biased The FPW is consistent under A1 A2 A5 and \\[ E((\\mathbf{x_t - \\rho x_{t-1}})&#39;)(\\epsilon_t - \\rho \\epsilon_{t-1})=0 \\] + A3a is not sufficient for the above equation + A3 is sufficient for the above equation The FPW estimator is asymptotically more efficient than OLS if the errors are truly generated as AR(1) process If the errors are truly generated as AR(1) process then usual standard errors are valid If we are concerned that there may be a more complex dependence structure of heteroskedasticity, then we use Newey West Standard Errors 6.1.5 Feasible group level Random Effects Estimate the following equation using OLS \\[ y_{gi} = \\mathbf{x}_{gi}\\beta + \\epsilon_{gi} \\] and obtain the residuals \\(e_{gi} = y_{gi} - \\mathbf{x}_{gi}\\hat{\\beta}\\) 2. Estimate the variance using the usual $s^2 estimator \\[ s^2 = \\frac{1}{n-k}\\sum_{i=1}^{n}e_i^2 \\] as an estimator for \\(\\sigma^2_c + \\sigma^2_u\\) and estimate the within group correlation, \\[ \\hat{\\sigma}^2_c = \\frac{1}{G} \\sum_{g=1}^{G} (\\frac{1}{\\sum_{i=1}^{n_g-1}i}\\sum_{i\\neq j}\\sum_{j}^{n_g}e_{gi}e_{gj}) \\] and plug in the estimates to obtain \\(\\hat{\\Omega}\\) The feasible group level RE estimator is obtained as \\[ \\hat{\\beta}= \\mathbf{(X&#39;\\hat{\\Omega}^{-1}X)^{-1}X&#39;\\hat{\\Omega}^{-1}y} \\] Properties of the Feasible group level Random Effects Estimator The infeasible group RE estimator is a linear estimator and is unbiased under A1-A3 for the unweighted equation A3 requires \\(E(\\epsilon_{gi}|\\mathbf{x}_i) = E(c_{g}|\\mathbf{x}_i)+ (u_{gi}|\\mathbf{x}_i)=0\\) so we generally assume \\(E(c_{g}|\\mathbf{x}_i)+ (u_{gi}|\\mathbf{x}_i)=0\\). The assumption \\(E(c_{g}|\\mathbf{x}_i)=0\\) is generally called random effects assumption The Feasible group level Random Effects is biased The Feasible group level Random Effects is consistent under A1-A3a, and A5a for the unweighted equation. A3a requires \\(E(\\mathbf{x}_i&#39;\\epsilon_{gi}) = E(\\mathbf{x}_i&#39;c_{g})+ (\\mathbf{x}_i&#39;u_{gi})=0\\) The Feasible group level Random Effects estimator is asymptotically more efficient than OLS if the errors follow the random effects specification If the errors do follow the random effects specification than the usual standard errors are consistent If there might be a more complex dependence structure or heteroskedasticity, then we need cluster robust standard errors. 6.1.6 Generalized Least Squares 6.1.7 Maximum Likelihood Premise: find values of the parameters that maximize the probability of observing the data In other words, we try to maximize the value of theta in the likelihood function \\[ L(\\theta)=\\prod_{i=1}^{n}f(y_i|\\theta) \\] \\(f(y|\\theta)\\) is the probability density of observing a single value of Y given some value of \\(\\theta\\) \\(f(y|\\theta)\\) can be specify as various type of distributions. You can review back section Distributions. For example If y is a dichotomous variable, then \\[ L(\\theta)=\\prod_{i=1}^{n}\\theta^{y_i}(1-\\theta)^{1-y_i} \\] \\(\\hat{\\theta}\\) is the Maximum Likelihood estimate if \\(L(\\hat{\\theta}) &gt; L(\\theta_0)\\) for all values of \\(\\theta_0\\) in the parameter space. 6.1.7.1 Motivation for MLE Suppose we know the conditional distribution of y given x: \\[ f_{Y|X}(y,x;\\theta) \\] where \\(\\theta\\) is the unknown parameter of distribution. Sometimes we are only concerned with the unconditional distribution \\(f_{Y}(y;\\theta)\\) Then given a sample of iid data, we can calculate the joint distribution of the entire sample, \\[ f_{Y_1,...,Y_n|X_1,...,X_n(y_1,...y_n,x_1,...,x_n;\\theta)}= \\prod_{i=1}^{n}f_{Y|X}(y_i,x_i;\\theta) \\] The joint distribution evaluated at the sample is the likelihood (probability) that we observed this particular sample (depends on \\(\\theta\\)) Idea for MLE: Given a sample, we choose our estimates of the parameters that gives the highest likelihood (probability) of observing our particular sample \\[ max_{\\theta} \\prod_{i=1}^{n}f_{Y|X}(y_i,x_i; \\theta) \\] Equivalently, \\[ max_{\\theta} \\prod_{i=1}^{n} ln(f_{Y|X}(y_i,x_i; \\theta)) \\] Solving for the Maximum Likelihood Estimator Solve First Order Condition \\[ \\frac{\\partial}{\\partial \\theta}\\sum_{i=1}^{n} ln(f_{Y|X}(y_i,x_i;\\hat{\\theta}_{MLE})) = 0 \\] where \\(\\hat{\\theta}_{MLE}\\) is defined. Evaluate Second Order Condition \\[ \\frac{\\partial^2}{\\partial \\theta^2} \\sum_{i=1}^{n} ln(f_{Y|X}(y_i,x_i;\\hat{\\theta}_{MLE})) &lt; 0 \\] where the above condition ensures we can solve for a maximum Examples: Unconditional Poisson Distribution: Number of products ordered on Amazon within an hour, number of website visits a day for a political campaign. Exponential Distribution: Length of time until an earthquake occurs, length of time a car battery lasts. \\[ f_{Y|X}(y,x;\\theta) = exp(-y/x\\theta)/x\\theta \\\\ f_{Y_1,..Y_n|X_1,...,X_n(y_1,...,y_n,x_1,...,x_n;\\theta)} = \\prod_{i=1}^{n}exp(-y_i/x_i \\theta)/x_i \\theta \\] 6.1.7.2 Assumption High Level Regulatory Assumptions is the sufficient condition used to show large sample properties Hence, for each MLE, we will need to either assume or verify if the regulatory assumptiosn holds. observations are independent and have the same density function. Under multivariate normal assumption, ML yields consistent estimates of the means and the covariance matrix for multivariate distribution with finite fourth moments (Little and Smith 1987) To find the MLE, we usually differentiate the log-likelihood function and set it equal to 0. \\[ \\frac{d}{d\\theta}l(\\theta) = 0 \\] This is the score equation Our confidence in the MLE is quantified by the pointedness of the log-likelihood \\[ I_O(\\theta)= \\frac{d^2}{d\\theta^2}l(\\theta) = 0 \\] called the observed information while \\[ I(\\theta)=E[I_O(\\theta;Y)] \\] is the expected information. (also known as Fisher Information). which we base our variance of the estimator. \\[ V(\\hat{\\Theta}) \\approx I(\\theta)^{-1} \\] Consistency of MLE Suppose that \\(y_i\\) and \\(x_i\\) are iid drawn from the true conditional pdf \\(f_{Y|X}(y_i,x_i;\\theta_0)\\). If the following regulatory assumptions hold, R1: If \\(\\theta \\neq \\theta_0\\) then \\(f_{Y|X}(y_i,x_i;\\theta) \\neq f_{Y|X}(y_i,x_i;\\theta_0)\\) R2: The set \\(\\Theta\\) that contains the true parameters \\(\\theta_0\\) is compact R3: The log-likelihood \\(ln(f_{Y|X}(y_i,x_i;\\theta_0))\\) is continuous at each \\(\\theta\\) with probability 1 R4: \\(E(sup_{\\theta \\in \\Theta}|ln(f_{Y|X}(y_i,x_i;\\theta_0))|)\\) then the MLE estimator is consistent, \\[ \\hat{\\theta}_{MLE} \\to^p \\theta_0 \\] Asymptotic Normality of MLE Suppose that \\(y_1\\) and \\(x_i\\) are iid drawn from the true conditional pdf \\(f_{Y|X}(y_i,x_i;\\theta)\\). If R1-R4 and the following hold R5: \\(\\theta_0\\) is in the interior of the set \\(\\Theta\\) R6: \\(f_{Y|X}(y_i,x_i;\\theta)\\) is twice continuously differentiable in \\(\\theta\\) and \\(f_{Y|X}(y_i,x_i;\\theta) &gt;0\\) for a neighborhood \\(N \\in \\Theta\\) around \\(\\theta_0\\) R7: \\(\\int sup_{\\theta \\in N}||\\partial f_{Y|X}(y_i,x_i;\\theta)\\partial\\theta||d(y,x) &lt;\\infty\\), \\(\\int sup_{\\theta \\in N} || \\partial^2 f_{Y|X}(y_i,x_i;\\theta)/\\partial \\theta \\partial \\theta&#39; || d(y,x) &lt; \\infty\\) and \\(E(sup_{\\theta \\in N} || \\partial^2ln(f_{Y|X}(y_i,x_i;\\theta)) / \\partial \\theta \\partial \\theta&#39; ||) &lt; \\infty\\) R8: The information matrix \\(I(\\theta_0) = Var(\\partial f_{Y|X}(y,x_i; \\theta_0)/\\partial \\theta)\\) exists and is non-singular then the MLE estimator is asymptotically normal, \\[ \\sqrt{n}(\\hat{\\theta}_{MLE} - \\theta_0) \\to^d N(0,I(\\theta_0)^{-1}) \\] 6.1.7.3 Properties (EJD, Agresti, and Finlay 1998) Consistent: estimates are approximately unbiased in large samples Asymptotically efficient: approximately smaller standard errors compared to other estimator Asymptotically normal: with repeated sampling, the estimates will have an approximately normal distribution. Invariance: MLE for \\(g(\\theta) = g(\\hat{\\theta})\\) \\[ \\hat{\\Theta} \\approx^d (\\theta,I(\\hat{\\theta)^{-1}})) \\] Explicit vs Implicit MLE If we solve the score equation to get an expression of MLE, then its called explicit If there is no closed form for MLE, and we need some algorithms to derive its expression, its called implicit Large Sample Property of MLE Implicit in these theorems is the assumption that we know what the conditional distribution, \\[ f_{Y|X}(y_i,x_i;\\theta_0) \\] but just do now know the exact parameter value. Any Distributional mis-specification will result in inconsistent parameter estimates. Quasi-MLE: Particular settings/ assumption that allow for certain types of distributional mis-specification (Ex: as long as the distribution is part of particular class or satisfies a particular assumption, then estimating with a wrong distribution will not lead to inconsistent parameter estimates). non-parametric/ Semi-parametric estimation: no or very little distributional assumption are made. (hard to implement, derive properties, and interpret) The asymptotic variance of the MLE achieves the Cramer-Rao Lower Bound The Cramer-Rao Lower Bound is a lower brand for the asymptotic variance of a consistent and asymptotically normally distributed estimator. If an estimator achieves the lower bound then it is the most efficient estimator. The maximum Likelihood estimator (assuming the distribution is correctly specified and R1-R8 hold) is the most efficient consistent and asymptotically normal estimator. * most efficient among ALL consistent estimators (not limited to unbiased or linear estimators). Note ML is better choice for binary, strictly positive, count, or inherent heteroskedasticity than linear model. ML will assume that we know the conditional distribution of the outcome, and derive an estimator using that information. Adds an assumption that we know the distribution (which is similar to A6 Normal Distribution in linear model) will produce a more efficient estimator. 6.1.7.4 Application Other applications of MLE Corner Solution Ex: hours worked, donations to charity Estimate with Tobit Non-negative count Ex: Numbers of arrest, Number of cigarettes smoked a day Estimate with Poisson regression Multinomial Choice Ex: Demand for cars, votes for primary election Estimate with mutinomial probit or logit Ordinal Choice Ex: Levels of Happiness, Levels of Income Ordered Probit Model for binary Response A binary variable will have a Bernoulli distribution: \\[ f_Y(y_i;p) = p^{y_i}(1-p)^{(1-y_i)} \\] where p is the probability of success. The conditional distribution is: \\[ f_{Y|X}(y_i,x_i;p(.)) = p(x_i)^{y_i} (1-p(x_i))^{(1-y_i)} \\] So choose \\(p(x_i)\\) to be a reasonable function of \\(x_i\\) and unknown parameters \\(\\theta\\) We can use latent variable model as probability functions \\[ y_i = 1\\{y_i^* &gt; 0 \\} \\\\ y_i^* = x_i \\beta-\\epsilon_i \\] \\(y_i^*\\) is a latent variable (unobserved) that is not well-defined in terms of units/magnitudes \\(\\epsilon_i\\) is a mean 0 unobserved random variable. We can rewrite the mdoel without the latent variable, \\[ y_i = 1\\{x_i beta &gt; \\epsilon_i \\} \\] Then the probability function, \\[ p(x_i) = P(y_i = 1|x_i) \\\\ = P(x_i \\beta &gt; \\epsilon_i | x_i) \\\\ = F_{\\epsilon|X}(x_i \\beta | x_i) \\] then we need to choose a conditional distribution for \\(\\epsilon_i\\). Hence, we can make additional strong independence assumption \\(\\epsilon_i\\) is independent of \\(x_i\\) Then the probability function is simply, \\[ p(x_i) = F_\\epsilon(x_i \\beta) \\] The probability function is also the conditional expectation function, \\[ E(y_i | x_i) = P(y_i = 1|x_i) = F_\\epsilon (x_i \\beta) \\] so we allow the conditional expectation function to be non-linear. Common distributional assumption Probit: Assume \\(\\epsilon_i\\) is standard normally distributed, then \\(F_\\epsilon(.) = \\Phi(.)\\) is the standard normal CDF. Logit: Assume \\(\\epsilon_i\\) is standard logistically distributed, then \\(F_\\epsilon(.) = \\Lambda(.)\\) is the standard normal CDF. Step to derive Choose a distribution (normal or logistic) and plug into the following log likelihood, \\[ ln(f_{Y|X} (y_i , x_i; \\beta)) = y_i ln(F_\\epsilon(x_i \\beta)) + (1-y_i)ln(1-F_\\epsilon(x_i \\beta)) \\] Solve the MLE by finding the Maximum of \\[ \\hat{\\beta}_{MLE} = argmax \\sum_{i=1}^{n}ln(f_{Y|X}(y_i,x_i; \\beta)) \\] Properties of the Probit and Logit Estimators Probit or Logit is consistent and asymptotically normal if [A2][] holds: \\(E(x_i&#39; x_i)\\) exists and is non-singular [A5][] (or A5a) holds: {y_i,x_i} are iid (or stationary and weakly dependent). Distributional assumptions on \\(\\epsilon_i\\) hold: Normal/Logistic and independent of \\(x_i\\) Under the same assumptions, Probit or Logit is also asymptotically efficient with asymptotic variance, \\[ I(\\beta_0)^{-1} = [E(\\frac{(f_\\epsilon(x_i \\beta_0))^2}{F_\\epsilon(x_i\\beta_0)(1-F_\\epsilon(x_i\\beta_0))}x_i&#39; x_i)]^{-1} \\] where \\(F_\\epsilon(x_i\\beta_0)\\) is the probability density function (derivative of the CDF) 6.1.7.4.1 Interpretation \\(\\beta\\) is the average response in the latent variable associated with a change in \\(x_i\\) Magnitudes do not have meaning Direction does have meaning The partial effect for a Non-linear binary response model \\[ E(y_i |x_i) = F_\\epsilon (x_i \\beta) \\\\ PE(x_{ij}) = \\frac{\\partial E(y_i |x_i)}{\\partial x_{ij}} = f_\\epsilon (x_i \\beta)\\beta_j \\] The partial effect is the coefficient parameter \\(\\beta_j\\) multiplied by a scaling factor \\(f_\\epsilon (x_i \\beta)\\) The scaling factor depends on \\(x_i\\) so the partial effect changes depending on what \\(x_i\\) is Single value for the partial effect Partial Effect at the Average (PEA) is the partial effect for an average individual \\[ f_{\\epsilon}(\\bar{x}\\hat{\\beta})\\hat{\\beta}_j \\] Average Partial Effect (APE) is the average of all partial effect for each individual. \\[ \\frac{1}{n}\\sum_{i=1}^{n}f_\\epsilon(x_i \\hat{\\beta})\\hat{\\beta}_j \\] In the linear model, APE = PEA. In a non-linear model (e.g., binary response), APE \\(\\neq\\) PEA References "],["quantile-regression.html", "6.2 Quantile Regression", " 6.2 Quantile Regression For academic review on quantile regression, check (Yu, Lu, and Stander 2003) Linear Regression is based on the conditional mean function \\(E(y|x)\\) In Quantile regression, we can view each points in the conditional distribution of y. Quantile regression estimates the conditional median or any other quantile of Y. In the case that were interested in the 50th percentile, quantile regression is median regression, also known as least-absolute-deviations (LAD) regression, minimizes \\(\\sum_{i}|e_i|\\) Properties of estimators \\(\\beta\\) Asymptotically normally distributed Advantages More robust to outliers compared to OLS In the case the dependent variable has a bimodal or multimodal (multiple humps with multiple modes) distribution, quantile regression can be extremely useful. Avoids parametric distribution assumption of the error process. In another word, no assumptions regarding the distribution of the error term. Better characterization of the data (not just its conditional mean) is invariant to monotonic transformations (such as log) while OLS is not. In another word, \\(E(g(y))=g(E(y))\\) Disadvantages The dependent variable needs to be continuous with no zeroes or too many repeated values. \\[ y_i = x_i&#39;\\beta_q + e_i \\] Let \\(e(x) = y -\\hat{y}(x)\\), then \\(L(e(x)) = L(y -\\hat{y}(x))\\) is the loss function of the error term. If \\(L(e) = |e|\\) (called absolute-error loss function) then \\(\\hat{\\beta}\\) can be estimated by minimizing \\(\\sum_{i}|y_i-x_i&#39;\\beta|\\) More specifically, the objective function is \\[ Q(\\beta_q)=\\sum_{i:y_i \\ge x_i&#39;\\beta}^{N} q|y_i - x_i&#39;\\beta_q| + \\sum_{i:y_i &lt; x_i&#39;\\beta}^{N} (1-q)|y_i-x_i&#39;\\beta_q \\] where \\(0&lt;q&lt;1\\) The sum penalizes \\(q|e_i|\\) for under-prediction and \\((1-q)|e_i|\\) for over-prediction We use simplex method to minimize this function (cannot use analytical solution since its non-differentiable). Standard errors can be estimated by bootstrap. The absolute-error loss function is symmetric. Interpretation For the jth regressor (\\(x_j\\)), the marginal effect is the coefficient for the qth quantile \\[ \\frac{\\partial Q_q(y|x)}{\\partial x_j} = \\beta_{qj} \\] At the quantile q of the dependent variable y, \\(\\beta_q\\) represents a one unit change in the independent variable \\(x_j\\) on the dependent variable y. In other words, at the qth percentile, a one unit change in x results in \\(\\beta_q\\) unit change in y. 6.2.1 Application # generate data with non-constant variance x &lt;- seq(0,100,length.out = 100) # independent variable sig &lt;- 0.1 + 0.05*x # non-constant variance b_0 &lt;- 3 # true intercept b_1 &lt;- 0.05 # true slope set.seed(1) # reproducibility e &lt;- rnorm(100,mean = 0, sd = sig) # normal random error with non-constant variance y &lt;- b_0 + b_1*x + e # dependent variable dat &lt;- data.frame(x,y) hist(y) library(ggplot2) ggplot(dat, aes(x,y)) + geom_point() ggplot(dat, aes(x,y)) + geom_point() + geom_smooth(method=&quot;lm&quot;) We follow (Koenker 1996) to estimate quantile regression library(quantreg) qr &lt;- rq(y ~ x, data=dat, tau = 0.5) # tau: quantile of interest. Here we have it at 50th percentile. summary(qr) ## ## Call: rq(formula = y ~ x, tau = 0.5, data = dat) ## ## tau: [1] 0.5 ## ## Coefficients: ## coefficients lower bd upper bd ## (Intercept) 3.02410 2.80975 3.29408 ## x 0.05351 0.03838 0.06690 adding the regression line ggplot(dat, aes(x,y)) + geom_point() + geom_abline(intercept=coef(qr)[1], slope=coef(qr)[2]) To have R estimate multiple quantile at once qs &lt;- 1:9/10 qr1 &lt;- rq(y ~ x, data=dat, tau = qs) #check for its coefficients coef(qr1) ## tau= 0.1 tau= 0.2 tau= 0.3 tau= 0.4 tau= 0.5 tau= 0.6 ## (Intercept) 2.95735740 2.93735462 3.19112214 3.08146314 3.02409828 3.16840820 ## x -0.01203696 0.01942669 0.02394535 0.04208019 0.05350556 0.06507385 ## tau= 0.7 tau= 0.8 tau= 0.9 ## (Intercept) 3.09507770 3.10539343 3.041681 ## x 0.07783556 0.08782548 0.111254 # plot ggplot(dat, aes(x,y)) + geom_point() + geom_quantile(quantiles = qs) To examine if the quantile regression is appropriate, we can see its plot compared to least squares regression plot(summary(qr1), parm=&quot;x&quot;) where red line is the least squares estimates, and its confidence interval. x-axis is the quantile y-axis is the value of the quantile regression coefficients at different quantile If the error term is normally distributed, the quantile regression line will fall inside the coefficient interval of least squares regression. # generate data with constant variance x &lt;- seq(0, 100, length.out = 100) # independent variable b_0 &lt;- 3 # true intercept b_1 &lt;- 0.05 # true slope set.seed(1) # reproducibility e &lt;- rnorm(100, mean = 0, sd = 1) # normal random error with constant variance y &lt;- b_0 + b_1 * x + e # dependent variable dat2 &lt;- data.frame(x, y) qr2 = rq(y ~ x, data = dat2, tau = qs) plot(summary(qr2), parm = &quot;x&quot;) References "],["non-linear-regression.html", "6.3 Non-linear Regression", " 6.3 Non-linear Regression 6.3.1 Non-linear Least Squares 6.3.2 Genelized Method of Moments 6.3.3 Minimum Distance 6.3.4 Spline Regression This chapter is based on CMU stat Definition: a k-th order spline is a piecewise polynomial function of degree k, that is continuous and has continuous derivatives of orders 1,, k -1, at its knot points Equivalently, a function f is a k-th order spline with knot points at \\(t_1 &lt; ...&lt; t_m\\) if f is a polynomial of degree k on each of the intervals \\((-\\infty, t_1], [t_1,t_2],...,[t_m, \\infty)\\) \\(f^{(j)}\\), the j-th derivative of f, is continuous at \\(t_1,...,t_m\\) for each j = 0,1,,k-1 A common case is when k = 3, called cubic splines. (piecewise cubic functions are continuous, and also continuous at its first and second derivatives) To parameterize the set of splines, we could use truncated power basis, defined as \\[ g_1(x) = 1 \\\\ g_2(x) = x \\\\ ... \\\\ g_{k+1}(x) = x^k \\\\ g_{k+1+j}(x) = (x-t_j)^k_+ \\] where j = 1,,m and \\(x_+\\) = max{x,0} However, now software typically use B-spline basis. 6.3.4.1 Regression Splines To estimate the regression function \\(r(X) = E(Y|X =x)\\), we can fit a k-th order spline with knots at some prespecified locations \\(t_1,...,t_m\\) Regression splines are functions of \\[ \\sum_{j=1}^{m+k+1} \\beta_jg_j \\] where \\(\\beta_1,..\\beta_{m+k+1}\\) are coefficients \\(g_1,...,g_{m+k+1}\\) are the truncated power basis functions for k-th order splines over the knots \\(t_1,...,t_m\\) To estimate the coefficients \\[ \\sum_{i=1}^{n} (y_i - \\sum_{j=1}^{m} \\beta_j g_j (x_i))^2 \\] then regression spline is \\[ \\hat{r}(x) = \\sum_{j=1}^{m+k+1} \\hat{\\beta}_j g_j (x) \\] If we define the basis matrix \\(G \\in R^{n \\times (m+k+1)}\\) by \\[ G_{ij} = g_j(x_i) \\] where \\(i = 1,..,n\\) , \\(j = 1,..,m+k+1\\) Then, \\[ \\sum_{i=1}^{n} (y_i - \\sum_{j=1}^{m} \\beta_j g_j (x_i))^2 = ||y - G \\beta||_2^2 \\] and the regression spline estimate at x is \\[ \\hat{r} (x) = g(x)^T \\hat{\\beta}= g(x)^T(G^TG)^{-1}G^Ty \\] 6.3.4.2 Natural splines A natural spline of order k, with knots at \\(t_1 &lt;...&lt; t_m\\), is a piecewise polynomial function f such that f is polynomial of degree k on each of \\([t_1,t_2],...,[t_{m-1},t_m]\\) f is a polynomial of degree \\((k-1)/2\\) on \\((-\\infty,t_1]\\) and \\([t_m,\\infty)\\) f is continuous and has continuous derivatives of orders 1,.,,, k -1 at its knots \\(t_1,..,t_m\\) Note natural splines are only defined for odd orders k. 6.3.4.3 Smoothing spliness These estimators use a regularized regression over the natural spline basis: placing knots at all points \\(x_1,...x_n\\) For the case of cubic splines, the coefficients are the minimization of \\[ ||y - G\\beta||^2_2 + \\lambda \\beta^T \\Omega \\beta \\] where \\(\\Omega \\in R^{n \\times n}\\) is the penalty matrix \\[ \\Omega_{ij} = \\int g&#39;&#39;_i(t) g&#39;&#39;_j(t) dt, \\] and i,j = 1,..,n and \\(\\lambda \\beta^T \\Omega \\beta\\) is the regularization term used to shrink the components of \\(\\hat{\\beta}\\) towards 0. \\(\\lambda &gt; 0\\) is the tuning parameter (or smoothing parameter). Higher value of \\(\\lambda\\), faster shrinkage (shrinking away basis functions) Note smoothing splines have similar fits as kernel regression. Smoothing splines kernel regression tuning parameter smoothing parameter \\(\\lambda\\) bandwidth h 6.3.4.4 Application library(tidyverse) library(caret) theme_set(theme_classic()) # Load the data data(&quot;Boston&quot;, package = &quot;MASS&quot;) # Split the data into training and test set set.seed(123) training.samples &lt;- Boston$medv %&gt;% createDataPartition(p = 0.8, list = FALSE) train.data &lt;- Boston[training.samples, ] test.data &lt;- Boston[-training.samples, ] knots &lt;- quantile(train.data$lstat, p = c(0.25, 0.5, 0.75)) # we use 3 knots at 25,50,and 75 quantile. library(splines) # Build the model knots &lt;- quantile(train.data$lstat, p = c(0.25, 0.5, 0.75)) model &lt;- lm (medv ~ bs(lstat, knots = knots), data = train.data) # Make predictions predictions &lt;- model %&gt;% predict(test.data) # Model performance data.frame( RMSE = RMSE(predictions, test.data$medv), R2 = R2(predictions, test.data$medv) ) ## RMSE R2 ## 1 5.317372 0.6786367 ggplot(train.data, aes(lstat, medv) ) + geom_point() + stat_smooth(method = lm, formula = y ~ splines::bs(x, df = 3)) attach(train.data) #fitting smoothing splines using smooth.spline(X,Y,df=...) fit1&lt;-smooth.spline(train.data$lstat,train.data$medv,df=3 ) # 3 degrees of freedom #Plotting both cubic and Smoothing Splines plot(train.data$lstat,train.data$medv,col=&quot;grey&quot;) lstat.grid = seq(from = range(lstat)[1], to = range(lstat)[2]) points(lstat.grid,predict(model,newdata = list(lstat=lstat.grid)),col=&quot;darkgreen&quot;,lwd=2,type=&quot;l&quot;) #adding cutpoints abline(v=c(10,20,30),lty=2,col=&quot;darkgreen&quot;) lines(fit1,col=&quot;red&quot;,lwd=2) legend(&quot;topright&quot;,c(&quot;Smoothing Spline with 3 df&quot;,&quot;Cubic Spline&quot;),col=c(&quot;red&quot;,&quot;darkgreen&quot;),lwd=2) 6.3.5 Generalized Additive Models To overcome Spline Regressions requirements for specifying the knots, we can use Generalized Additive Models or GAM. library(mgcv) # Build the model model &lt;- gam(medv ~ s(lstat), data = train.data) plot(model) # Make predictions # predictions &lt;- model %&gt;% predict(test.data) # Model performance data.frame( RMSE = RMSE(predictions, test.data$medv), R2 = R2(predictions, test.data$medv) ) ## RMSE R2 ## 1 5.317372 0.6786367 ggplot(train.data, aes(lstat, medv) ) + geom_point() + stat_smooth(method = gam, formula = y ~ s(x)) detach(train.data) "],["model-specification.html", "Chapter 7 Model Specification", " Chapter 7 Model Specification Test whether underlying assumptions hold true Nested Model (A1/A3) Non-Nested Model (A1/A3) Heteroskedasticity (A4) "],["nested-model.html", "7.1 Nested Model", " 7.1 Nested Model \\[ \\begin{align} y = \\beta_0 + x_1\\beta_1 + x_2\\beta-2 + x_3\\beta_3 + \\epsilon &amp;&amp; \\text{unrestricted model} \\\\ y = \\beta_0 + x_1\\beta_1 + \\epsilon &amp;&amp; \\text{restricted model} \\end{align} \\] Unrestricted model is always longer than the restricted model The restricted model is nested within the unrestricted model To determine which variables should be included or exclude, we could use the same Wald Test Adjusted \\(R^2\\) \\(R^2\\) will always increase with more variables included Adjusted \\(R^2\\) tries to correct by penalizing inclusion of unnecessary variables. \\[ {R}^2 = 1 - \\frac{SSR/n}{SST/n} \\\\ {R}^2_{adj}= 1- \\frac{SSR/(n-k)}{SST/(n-1)} = 1 - \\frac{(n-1)(1-R^2)}{(n-k)} \\] \\({R}^2_{adj}\\) increases if and only if the t-statistic on the additional variable is greater than 1 in absolute value. \\({R}^2_{adj}\\) is valid in models where there is no heteroskedasticity there fore it should not be used in determining which variables should be included in the model (the t or F-tests are more appropriate) 7.1.1 Chow test Should we run two different regressions for two groups? "],["non-nested-model.html", "7.2 Non-Nested Model", " 7.2 Non-Nested Model compare models with different non-nested specifications 7.2.1 Davidson-Mackinnon test 7.2.1.1 Independent Variable should the independent variables be logged? decide between non-nested alternatives \\[ \\begin{align} y = \\beta_0 + x_1\\beta_1 + x_2\\beta_2 + \\epsilon &amp;&amp; \\text{(level eq)} \\\\ y = \\beta_0 + ln(x_1)\\beta_1 + x_2\\beta_2 + \\epsilon &amp;&amp; \\text{(log eq)} \\end{align} \\] Obtain predict outcome when estimating the model in log equation \\(\\check{y}\\) and then estimate the following auxiliary equation, \\[ y = \\beta_0 + x_1\\beta_1 + x_2\\beta_2 + \\check{y}\\gamma + error \\] and evaluate the t-statistic for the null hypothesis \\(H_0: \\gamma = 0\\) Obtain predict outcome when estimating the model in the level equation \\(\\hat{y}\\), then estimate the following auxiliary equation, \\[ y = \\beta_0 + ln(x_1)\\beta_1 + x_2\\beta_2 + \\check{y}\\gamma + error \\] and evaluate the t-statistic for the null hypothesis \\(H_0: \\gamma = 0\\) If you reject the null in the (1) step but fail to reject the null in the second step, then the log equation is preferred. If fail to reject the null in the (1) step but reject the null in the (2) step then, level equation is preferred. If reject in both steps, then you have statistical evidence that neither model should be used and should re-evaluate the functional form of your model. If fail to reject in both steps, you do not have sufficient evidence to prefer one model over the other. You can compare the \\(R^2_{adj}\\) to choose between the two models. \\[ y = \\beta_0 + ln(x)\\beta_1 + \\epsilon \\\\ y = \\beta_0 + x(\\beta_1) + x^2\\beta_2 + \\epsilon \\] * Compare which better fits the data * Compare standard \\(R^2\\) is unfair because the second model is less parsimonious (more parameters to estimate) * The \\(R_{adj}^2\\) will penalize the second model for being less parsimonious + Only valid when there is no heteroskedasticity (A4 holds) * Should only compare after a Davidson-Mackinnon test 7.2.1.2 Dependent Variable \\[ \\begin{align} y = \\beta_0 + x_1\\beta_1 + \\epsilon &amp;&amp; \\text{level eq} \\\\ ln(y) = \\beta_0 + x_1\\beta_1 + \\epsilon &amp;&amp; \\text{log eq} \\\\ \\end{align} \\] In the level model, regardless of how big y is, x has a constant effect (i.e., one unit change in \\(x_1\\) results in a \\(\\beta_1\\) unit change in y) In the log model, the larger in y is, the effect of x is stronger (i.e., one unit change in \\(x_1\\) could increase y from 1 to \\(1+\\beta_1\\) or from 100 to 100+100x\\(\\beta_1\\)) Cannot compare \\(R^2\\) or \\(R^2_{adj}\\) because the outcomes are complement different, the scaling is different (SST is different) We need to un-transform the \\(ln(y)\\) back to the same scale as y and then compare, Estimate the model in the log equation to obtain the predicted outcome \\(\\hat{ln(y)}\\) Un-transform the predicted outcome \\[ \\hat{m} = exp(\\hat{ln(y)}) \\] 3. Estimate the following model (without an intercept) \\[ y = \\alpha\\hat{m} + error \\] and obtain predicted outcome \\(\\hat{y}\\) Then take the square of the correlation between \\(\\hat{y}\\) and y as a scaled version of the \\(R^2\\) from the log model that can now compare with the usual \\(R^2\\) in the level model. "],["heteroskedasticity-1.html", "7.3 Heteroskedasticity", " 7.3 Heteroskedasticity Using roust standard errors are always valid If there is significant evidence of heteroskedasticity implying A4 does not hold Gauss-Markov Theorem no longer holds, OLS is not BLUE. Should consider using a better linear unbiased estimator (Weighted Least Squares or Generalized Least Squares) 7.3.1 Breusch-Pagan test A4 implies \\[ E(\\epsilon_i^2|\\mathbf{x_i})=\\sigma^2 \\] \\[ \\epsilon_i^2 = \\gamma_0 + x_{i1}\\gamma_1 + ... + x_{ik -1}\\gamma_{k-1} + error \\] and determining whether or not \\(\\mathbf{x}_i\\) has any predictive value if \\(\\mathbf{x}_i\\) has predictive value, then the variance changes over the levels of \\(\\mathbf{x}_i\\) which is evidence of heteroskedasticity if \\(\\mathbf{x}_i\\) does not have predictive value, the variance is constant for all levels of \\(\\mathbf{x}_i\\) The Breusch-Pagan test for heteroskedasticity would compute the F-test of total significance for the following model \\[ e_i^2 = \\gamma_0 + x_{i1}\\gamma_1 + ... + x_{ik -1}\\gamma_{k-1} + error \\] A low p-value means we reject the null of homoskedasticity However, Breusch-Pagan test cannot detect heteroskedasticity in non-linear form 7.3.2 White test test heteroskedasticity would allow for a non-linear relationship by computing the F-test of total significance for the following model (assume there are three independent random variables) \\[ e_i^2=\\gamma_0 + x_i \\gamma_1 + x_{i2}\\gamma_2 + x_{i3}\\gamma_3 + x_{i1}^2\\gamma_4 + x_{i2}^2\\gamma_5 + x_{i3}^2\\gamma_6 + (x_{i1} \\times x_{i2})\\gamma_7 + (x_{i1} \\times x_{i3})\\gamma_8 + (x_{i2} \\times x_{i3})\\gamma_9 + error \\] A low p-value means we reject the null of homoskedasticity Equivalently, we can compute LM as \\(LM = nR^2_{e^2}\\) where the \\(R^2_{e^2}\\) come from the regression with the squared residual as the outcome The LM statistic has a \\(\\chi_k^2\\) distribution "],["endogeneity.html", "Chapter 8 Endogeneity", " Chapter 8 Endogeneity A3a requires \\(\\epsilon_i\\) to be uncorrelated with \\(\\mathbf{x}_i\\) Assume A1 , A2, A5 \\[ plim(\\hat{\\beta}_{OLS}) = \\beta + [E(\\mathbf{x_i&#39;x_i})]^{-1}E(\\mathbf{x_i&#39;}\\epsilon_i) \\] A3a is the weakest assumption needed for OLS to be consistent [A3] fails when \\(x_{ik}\\) is correlated with \\(\\epsilon_i\\) Omitted Variables Bias \\(\\epsilon_i\\) includes any other factors that may influence the dependent variable (linearly) Feedback Effect (Simultaneity) Demand and prices are simultaneously determined. Endogenous sample design (sample selection) we did not have iid sample Measurement Error Note Omitted Variable: an omitted variable is a variable, omitted from the model (but is in the \\(\\epsilon_i\\)) and unobserved has predictive power towards the outcome. Omitted Variable Bias: is the bias (and inconsistency when looking at large sample properties) of the OLS estimator when the omitted variable. The structural equation is used to emphasize that we are interested understanding a causal relationship \\[ y_{i1} = \\beta_0 + \\mathbf{z}_i1 \\beta_1 + y_{i2}\\beta_2 + \\epsilon_i \\] where \\(y_{it}\\) is the outcome variable (inherently correlated with \\(\\epsilon_i\\)) \\(y_{i2}\\) is the endogenous covariate (presumed to be correlated with \\(\\epsilon_i\\)) \\(\\beta_1\\) represents the causal effect of \\(y_{i2}\\) on \\(y_{i1}\\) \\(\\mathbf{z}_{i1}\\) is exogenous controls (uncorrelated with \\(\\epsilon_i\\)) (\\(E(z_{1i}&#39;\\epsilon_i) = 0\\)) OLS is an inconsistent estimator of the causal effect \\(\\beta_2\\) If there was no endogeneity \\(E(y_{i2}&#39;\\epsilon_i) = 0\\) the exogenous variation in \\(y_{i2}\\) is what identifies the causal effect If there is endogeneity Any wiggle in \\(y_{i2}\\) will shift simultaneously with \\(\\epsilon_i\\) \\[ plim(\\hat{\\beta}_{OLS}) = \\beta + [E(\\mathbf{x&#39;_ix_i})]^{-1}E(\\mathbf{x&#39;_i}\\epsilon_i) \\] where \\(\\beta\\) is the causal effect \\([E(\\mathbf{x&#39;_ix_i})]^{-1}E(\\mathbf{x&#39;_i}\\epsilon_i)\\) is the endogenous effect Hence \\(\\hat{\\beta}_{OLS}\\) can be either more positive and negative than the true causal effect. Motivation for Two Stage Least Squares (2SLS) \\[ y_{i1}=\\beta_0 + \\mathbf{z}_{i1}\\beta_1 + y_{i2}\\beta_2 + \\epsilon_i \\] We want to understand how movement in \\(y_{i2}\\) effects movement in \\(y_{i1}\\), but whenever we move \\(y_{i2}\\), \\(\\epsilon_i\\) also moves. Solution We need a way to move \\(y_{i2}\\) independently of \\(\\epsilon_i\\), then we can analyze the response in \\(y_{i1}\\) as a causal effect Find an instrumental variable(s) \\(z_{i2}\\) + Instrument Relevance: when \\(z_{i2}\\) moves then \\(y_{i2}\\) also moves + Instrument Exogeneity**: when \\(z_{i2}\\) moves then \\(\\epsilon_i\\) does not move. \\(z_{i2}\\) is the exogenous variation that identifies the causal effect \\(\\beta_2\\) Finding an Instrumental variable: Random Assignment: + Effect of class size on educational outcomes: instrument is initial random Relations Choice + Effect of Education on Fertility: instrument is parents educational level Eligibility + Trade-off between IRA and 401K retirement savings: instrument is 401k eligibility Example Return to College education is correlated with ability - endogenous Near 4year as an instrument + Instrument Relevance: when near moves then education also moves + Instrument Exogeneity: when near moves then \\(\\epsilon_i\\) does not move. Other potential instruments; near a 2-year college. Parents Education. Owning Library Card \\[ y_{i1}=\\beta_0 + \\mathbf{z}_{i1}\\beta_1 + y_{i2}\\beta_2 + \\epsilon_i \\] First Stage (Reduced Form) Equation: \\[ y_{i2} = \\pi_0 + \\mathbf{z_{i1}\\pi_1} + \\mathbf{z_{i2}\\pi_2} + v_i \\] where \\(\\pi_0 + \\mathbf{z_{i1}\\pi_1} + \\mathbf{z_{i2}\\pi_2}\\) is exogenous variation \\(v_i\\) is endogenous variation This is called a reduced form equation * Not interested in the causal interpretation of \\(\\pi_1\\) or \\(\\pi_2\\) * A linear projection of \\(z_{i1}\\) and \\(z_{i2}\\) on \\(y_{i2}\\) (simple correlations) * The projections \\(\\pi_1\\) and \\(\\pi_2\\) guarantee that \\(E(z_{i1}&#39;v_i)=0\\) and \\(E(z_{i2}&#39;v_i)=0\\) Instrumental variable \\(z_{i2}\\) Instrument Relevance: \\(\\pi_2 \\neq 0\\) Instrument Exogeneity: \\(E(\\mathbf{z_{i2}\\epsilon_i})=0\\) Moving only the exogenous part of \\(y_i2\\) is moving \\[ \\tilde{y}_{i2} = \\pi_0 + \\mathbf{z_{i1}\\pi_1 + z_{i2}\\pi_2} \\] two Stage Least Squares (2SLS) \\[ y_{i1} = \\beta_0 +\\mathbf{z_{i1}\\beta_1}+ y_{i2}\\beta_2 + \\epsilon_i \\\\ y_{i2} = \\pi_0 + \\mathbf{z_{i2}\\pi_2} + \\mathbf{v_i} \\] Equivalently, \\[ \\begin{equation} y_{i1} = \\beta_0 + \\mathbf{z_{i1}}\\beta_1 + \\tilde{y}_{i2}\\beta_2 + u_i \\tag{8.1} \\end{equation} \\] where \\(\\tilde{y}_{i2} =\\pi_0 + \\mathbf{z_{i2}\\pi_2}\\) \\(u_i = v_i \\beta_2+ \\epsilon_i\\) The (8.1) holds for A1, A5 A2 holds if the instrument is relevant \\(\\pi_2 \\neq 0\\) + \\(y_{i1} = \\beta_0 + \\mathbf{z_{i1}\\beta_1 + (\\pi_0 + z_{i1}\\pi_1 + z_{i2}\\pi_2)}\\beta_2 + u_i\\) A3a holds if the instrument is exogenous \\(E(\\mathbf{z}_{i2}\\epsilon_i)=0\\) \\[ \\begin{align} E(\\tilde{y}_{i2}&#39;u_i) &amp;= E((\\pi_0 + \\mathbf{z_{i1}\\pi_1+z_{i2}})(v_i\\beta_2 + \\epsilon_i)) \\\\ &amp;= E((\\pi_0 + \\mathbf{z_{i1}\\pi_1+z_{i2}})( \\epsilon_i)) \\\\ &amp;= E(\\epsilon_i)\\pi_0 + E(\\epsilon_iz_{i1})\\pi_1 + E(\\epsilon_iz_{i2}) \\\\ &amp;=0 \\end{align} \\] Hence, (8.1) is consistent The 2SLS Estimator 1. Estimate the first stage using OLS \\[ y_{i2} = \\pi_0 + \\mathbf{z_{i2}\\pi_2} + \\mathbf{v_i} \\] and obtained estimated value \\(\\hat{y}_{i2}\\) Estimate the altered equation using OLS \\[ y_{i1} = \\beta_0 +\\mathbf{z_{i1}\\beta_1}+ \\hat{y}_{i2}\\beta_2 + \\epsilon_i \\\\ \\] Properties of the 2SLS Estimator Under A1, A2, A3a (for \\(z_{i1}\\)), A5 and if the instrument satisfies the following two conditions, + Instrument Relevance: \\(\\pi_2 \\neq 0\\) + Instrument Exogeneity: \\(E(\\mathbf{z}_{i2}&#39;\\epsilon_i) = 0\\) then the 2SLS estimator is consistent Can handle more than one endogenous variable and more than one instrumental variable \\[ y_{i1} = \\beta_0 + z_{i1}\\beta_1 + y_{i2}\\beta_2 + y_{i3}\\beta_3 + \\epsilon_i \\\\ y_{i2} = \\pi_0 + z_{i1}\\pi_1 + z_{i2}\\pi_2 + z_{i3}\\pi_3 + z_{i4}\\pi_4 + v_{i2} \\\\ y_{i3} = \\gamma_0 + z_{i1}\\gamma_1 + z_{i2}\\gamma_2 + z_{i3}\\gamma_3 + z_{i4}\\gamma_4 + v_{i3} \\] + **IV estimator**: one endogenous variable with a single instrument + **2SLS estimator**: one endogenous variable with multiple instruments + **GMM estimator**: multiple endogenous variables with multiple instruments Standard errors produced in the second step are not correct + Because we do not know \\(\\tilde{y}\\) perfectly and need to estimate it in the firs step, we are introducing additional variation + We did not have this problem with FGLS because the first stage was orthogonal to the second stage. This is generally not true for most multi-step procedure. + If A4 does not hold, need to report robust standard errors. 2SLS is less efficient than OLS and will always have larger standard errors. + First, \\(Var(u_i) = Var(v_i\\beta_2 + \\epsilon_i) &gt; Var(\\epsilon_i)\\) + Second, \\(\\hat{y}_{i2}\\) is generally highly collinear with \\(\\mathbf{z}_{i1}\\) The number of instruments need to be at least as many or more the number of endogenous variables. Note 2SLS can be combined with FGLS to make the estimator more efficient: You have the same first-stage, and in the second-stage, instead of using OLS, you can use FLGS with the weight matrix \\(\\hat{w}\\) Generalized Method of Moments can be more efficient than 2SLS. In the second-stage of 2SLS, you can also use MLE, but then you are making assumption on the distribution of the outcome variable, the endogenous variable, and their relationship (joint distribution). "],["testing-assumption.html", "8.1 Testing Assumption", " 8.1 Testing Assumption Test of Endogeneity: Is \\(y_{i2}\\) truly endogenous (i.e., can we just use OLS instead of 2SLS)? Testing Instruments assumptions + Exogeneity: Cannot always test (and when you can it might not be informative) + Relevancy: need to avoid weak instruments 8.1.1 Test of Endogeneity 2SLS is generally so inefficient that we may prefer OLS if there is not much endogeneity Biased but inefficient vs efficient but biased Want a sense of how endogenous \\(y_{i2}\\) is + if very endgeneous - should use 2SLS + if not very endogenous - perhaps prefer OLS Invalid Test of Endogeneity * \\(y_{i2}\\) is endogenous if it is correlated with \\(\\epsilon_i\\), \\[ \\epsilon_i = \\gamma_0 + y_{i2}\\gamma_1 + error_i \\] where \\(\\gamma_1 \\neq 0\\) implies that there is endogeneity \\(\\epsilon_i\\) is not observed, but using the residuals \\[ e_i = \\gamma_0 + y_{i2}\\gamma_1 + error_i \\] is NOT a valid test of endogeneity + The OLS residual, e is mechanically uncorrelated with \\(y_{i2}\\) (by FOC for OLS) + In every situation, \\(\\gamma_1\\) will be essentially 0 and you will never be able to reject the null of no endogeneity Valid test of endogeneity If \\(y_{i2}\\) is not endogenous then \\(\\epsilon_i\\) and v are uncorrelated \\[ y_{i1} = \\beta_0 + \\mathbf{z}_{i1}\\beta_1 + y_{i2}\\beta_2 + \\epsilon_i \\\\ y_{i2} = \\pi_0 + \\mathbf{z}_{i1}\\pi_1 + z_{i2}\\pi_2 + v_i \\] variable Addition test: include the first stage residuals as an additional variable, \\[ y_{i1} = \\beta_0 + \\mathbf{z}_{i1}\\beta_1 + y_{i2}\\beta_2 + \\hat{v}_i \\theta + error_i \\] Then the usual t-test of significance is a valid test to evaluate the following hypothesis. note this test requires your instrument to be valid instrument. \\[ \\begin{split} H_0: \\theta = 0 &amp;&amp; \\text{ (not endogenous)} \\\\ H_1: \\theta \\neq 0 &amp;&amp; \\text{ (endogenous)} \\end{split} \\] 8.1.2 Testing Instruments assumptions The instrumental variable must satisfy Exogeneity Relevancy 8.1.2.1 Exogeneity Why exogeneity matter? \\[ E(\\mathbf{z}_{i2}&#39;\\epsilon_i) = 0 \\] If A3a fails - 2SLS is also inconsistent If instrument is not exogenous, then we need to find a new one. Similar to Test of Endogeneity, when there is a single instrument \\[ e_i = \\gamma_0 + \\mathbf{z}_{i2}\\gamma_1 + error_i \\\\ H_0: \\gamma_1 = 0 \\] is NOT a valid test of endogeneity * the OLS residual, e is mechanically uncorrelated with \\(z_{i2}\\): \\(\\hat{\\gamma}_1\\) will be essentially 0 and you will never be able to determine if the instrument is endogenous. Solution Testing Instrumental Exegeneity in an Over-identified Model * When there is more than one exogenous instrument (per endogenous variable), we can test for instrument exogeneity. + When we have multiple instruments, the model is said to be over-identiifed. + Could estimate the same model several ways (i.e., can identify/ estimate \\(\\beta_1\\) more than one way) * Idea behind the test: if the controls and instruments are truly exogenous then OLS estimation of the following regression, \\[ \\epsilon_i = \\gamma_0 + \\mathbf{z}_{i1}\\gamma_1 + \\mathbf{z}_{i2}\\gamma_2 + error_i \\] should have a very low \\(R^2\\) * if the model is just identified (one instrument per endogenous variable) then the \\(R^2 = 0\\) Steps: Estimate the structural equation by 2SLS (using all available instruments) and obtain the residuals e Regress e on all controls and instruments and obtain the \\(R^2\\) Under the null hypothesis (all IVs are uncorrelated), \\(nR^2 \\sim \\chi^2(q)\\), where q is the number of instrumental variables minus the number of endogenous variables if the model is just identified (one instrument per endogenous variable) then q = 0, and the distribution under the null collapses. low p-value means you reject the null of exogenous instruments. Hence you would like to have high p-value in this test. Pitfalls for the Overid test the overid test is essentially compiling the following information. + Conditional on first instrument being exogenous is the other instrument exogenous? + Conditional on the other instrument being exogenous, is the first instrument exogenous? If all instruments are endogenous than neither test will be valid really only useful if one instrument is thought to be truly exogenous (randomly assigned). even f you do reject the null, the test does not tell you which instrument is exogenous and which is endogenous. Result Implication reject the null you can be pretty sure there is an endogenous instrument, but dont know which one. fail to reject could be either (1) they are both exogenous, (2) they are both endogenous. 8.1.2.2 Relevancy Why Relevance matter? \\[ \\pi_2 \\neq 0 \\] * used to show A2 holds + If \\(\\pi_2 = 0\\) (instrument is not relevant) then A2 fails - perfect multicollinearity + If \\(\\pi_2\\) is close to 0 (weak instrument) then there is near perfect multicollinearity - 2SLS is highly inefficient (Large standard errors). * A weak instrument will exacerbate any inconsistency due to an instrument being (even slightly) endogenous. + In the simple case with no controls and a single endogenous variable and single instrumental variable, \\[ plim(\\hat{\\beta}_{2_{2SLS}}) = \\beta_2 + \\frac{E(z_{i2}\\epsilon_i)}{E(z_{i2}y_{i2})} \\] Testing Weak Instruments can use t-test (or F-test for over-identified models) in the first stage to determine if there is a weak instrument problem. (Stock and Yogo 2005): a statistical rejection of the null hypothesis in the first stage at the 5% (or even 1%) level is not enough to insure the instrument is not weak + Rule of Thumb: need a F-stat of at least 10 (or a t-stat of at least 3.2) to reject the null hypothesis that the instrument is weak. Summary of the 2SLS Estimator \\[ y_{i1}=\\beta_0 + \\mathbf{z}_{i1}\\beta_1 + y_{i2}\\beta_2 + \\epsilon_i \\\\ y_{i2} = \\pi_0 + \\mathbf{z_{i1}\\pi_1} + \\mathbf{z_{i2}\\pi_2} + v_i \\] when A3a does not hold \\[ E(y_{i2}&#39;\\epsilon_i) \\neq 0 \\] + Then the OLS estimator is no longer unbiased or consistent. * If we have valid instruments \\(\\mathbf{z}_{i2}\\) + Exogeneity: \\(E(\\mathbf{z}_{i2}&#39;\\epsilon_i) = 0\\) + Relevancy: \\(\\pi_2 \\neq 0\\) Then the 2SLS estimator is consistent under A1, A2, A5a, and the above two conditions. + If A4 also holds, then the usual standard errors are valid. + If A4 does not hold then use the robust standard errors. \\[ y_{i1}=\\beta_0 + \\mathbf{z}_{i1}\\beta_1 + y_{i2}\\beta_2 + \\epsilon_i \\\\ y_{i2} = \\pi_0 + \\mathbf{z_{i1}\\pi_1} + \\mathbf{z_{i2}\\pi_2} + v_i \\] * When A3a does hold \\[ E(y_{i2}&#39;\\epsilon_i) = 0 \\] and we have valid instruments, then both the OLS and 2SLS estimators are consistent. + The OLS estimator is always more efficient + can use the variable addition test to determine if 2SLS is need (A3a does hold) or if OLS is valid (A3a does not hold) Sometimes we can test the assumption for instrument to be valid: + Exogeneity: Only table when there are more instruments than endogenous variables. + Relevancy: Always testable, need the F-stat to be greater than 10 to rule out a weak instrument References "],["omitted-variables-bias.html", "8.2 Omitted Variables Bias", " 8.2 Omitted Variables Bias "],["feedback-effect-simultaneity.html", "8.3 Feedback Effect (Simultaneity)", " 8.3 Feedback Effect (Simultaneity) "],["endogenous-sample-design-sample-selection.html", "8.4 Endogenous sample design (sample selection)", " 8.4 Endogenous sample design (sample selection) "],["measurement-error.html", "8.5 Measurement Error", " 8.5 Measurement Error "],["proxy-variables.html", "8.6 Proxy Variables", " 8.6 Proxy Variables Can be in place of the omitted variable, * will not be able to estimate the effect of the omitted variable * will be able to reduce some endogeneity caused bye the omitted variable Criteria for a proxy variable: The proxy is correlated with the omitted variable. Having the omitted variable in the regression will solve the problem of endogeneity 3.The variation of the omitted variable unexplained by the proxy is uncorrelated with all independent variables, including the proxy. IQ test can be a proxy for ability in the regression between wage explained education. For the third requirement \\[ ability = \\gamma_0 + \\gamma_1 IQ + \\epsilon \\] where \\(\\epsilon\\) is uncorrelated with education and IQ test. "],["data.html", "Chapter 9 Data", " Chapter 9 Data There are multiple ways to categorize data. For example, Qualitative vs. Quantitative: Qualitative Quantitative in-depth interviews, documents, focus groups, case study, ethnography. open-ended questions. observations in words experiments, observation in words, survey with closed-end questions, structured interviews language, descriptive quantities, numbers Text-based Numbers-based Subjective Objectivity "],["cross-sectional.html", "9.1 Cross-Sectional", " 9.1 Cross-Sectional "],["time-series.html", "9.2 Time Series", " 9.2 Time Series \\[ y_t = \\beta_0 + x_{t1}\\beta_1 + x_{t2}\\beta_2 + ... + x_{t(k-1)}\\beta_{k-1} + \\epsilon_t \\] Examples Static Model \\(y_t=\\beta_0 + x_1\\beta_1 + x_2\\beta_2 - x_3\\beta_3 - \\epsilon_t\\) Finite Distributed Lag model \\(y_t=\\beta_0 + pe_t\\delta_0 + pe_{t-1}\\delta_1 +pe_{t-2}\\delta_2 + \\epsilon_t\\) Long Run Propensity (LRP) is \\(LRP = \\delta_0 + \\delta_1 + \\delta_2\\) Dynamic Model \\(GDP_t = \\beta_0 + \\beta_1GDP_{t-1} - \\epsilon_t\\) Finite Sample Properties for Time Series: A1-A3: OLS is unbiased A1-A4: usual standard errors are consistent and Gauss-Markov Theorem holds (OLS is BLUE) A1-A6, A6: Finite Sample Wald Test (t-test and F-test) are valid A3 might not hold under time series setting Spurious Time Trend - solvable Strict vs Contemporaneous Exogeneity - not solvable In time series data, there are many processes: Autoregressive model of order p: AR(p) Moving average model of order q: MA(q) Autoregressive model of order p and moving average model of order q: ARMA(p,q) Autoregressive conditional heteroskedasticity model of order p: ARCH(p) Generalized Autoregressive conditional heteroskedasticity of orders p and q; GARCH(p.q) 9.2.1 Deterministic Time trend Both the dependent and independent variables are trending over time Spurious Time Series Regression \\[ y_t = \\alpha_0 + t\\alpha_1 + v_t \\] and x takes the form \\[ x_t = \\lambda_0 + t\\lambda_1 + u_t \\] \\(\\alpha_1 \\neq 0\\) and \\(\\lambda_1 \\neq 0\\) \\(v_t\\) and \\(u_t\\) are independent there is no relationship between \\(y_t\\) and \\(x_t\\) If we estimate the regression, \\[ y_t = \\beta_0 + x_t\\beta_1 + \\epsilon_t \\] so the true \\(\\beta_1=0\\) Inconsistent: \\(plim(\\hat{\\beta}_1)=\\frac{\\alpha_1}{\\lambda_1}\\) Invalid Inference: \\(|t| \\to^d \\infty\\) for \\(H_0: \\beta_1=0\\), will always reject the null as \\(n \\to \\infty\\) Uninformative \\(R^2\\): \\(plim(R^2) = 1\\) will be able to perfectly predict as \\(n \\to \\infty\\) We can rewrite the equation as \\[ y_t=\\beta_0 + \\beta_1x_t+\\epsilon_t \\\\ \\epsilon_t = \\alpha_1t + v_t \\] where \\(\\beta_0 = \\alpha_0\\) and \\(\\beta_1=0\\). Since \\(x_t\\) is a deterministic function of time, \\(\\epsilon_t\\) is correlated with \\(x_t\\) and we have the usual omitted variable bias. Even when \\(y_t\\) and \\(x_t\\) are related (\\(\\beta_1 \\neq 0\\)) but they are both trending over time, we still get spurious results with the simple regression on \\(y_t\\) on \\(x_t\\) Solutions to Spurious Trend Include time trend t as an additional control consistent parameter estimates and valid inference Detrend both dependent and independent variables and then regress the detrended outcome on detrended independent variables (i.e., regress residuals \\(\\hat{u}_t\\) on residuals \\(\\hat{v}_t\\)) Detrending is the same as partialling out in the Frisch-Waugh-Lovell Theorem Could allow for non-linear time trends by including t \\(t^2\\), and exp(t) Allow for seasonality by including indicators for relevant seasons (quarters, months, weeks). A3 does not hold under: Feedback Effect \\(\\epsilon_t\\) influences next periods independent variables Dynamic Specification include last time period outcome as an explanatory variable Dynamically Complete For finite distrusted lag model, the number of lags needs to be absolutely correct. 9.2.2 Feedback Effect \\[ y_t = \\beta_0 + x_t\\beta_1 + \\epsilon_t \\] A3 \\[ E(\\epsilon_t|\\mathbf{X})= E(\\epsilon_t| x_1,x_2, ...,x_t,x_{t+1},...,x_T) \\] will not equal 0, because \\(y_t\\) will likely influence \\(x_{t+1},..,x_T\\) A3 is violated because we require the error to be uncorrelated with all time observation of the independent regressors (strict exogeneity) 9.2.3 Dynamic Specification \\[ y_t = \\beta_0 + y_{t-1}\\beta_1 + \\epsilon_t \\] \\[ E(\\epsilon_t|\\mathbf{X})= E(\\epsilon_t| y_1,y_2, ...,y_t,y_{t+1},...,y_T) \\] will not equal 0, because \\(y_t\\) and \\(\\epsilon_t\\) are inherently correlated A3 is violated because we require the error to be uncorrelated with all time observation of the independent regressors (strict exogeneity) Dynamic Specification is not allowed under A3 9.2.4 Dynamically Complete \\[ y_t = \\beta_0 + x_t\\delta_0 + x_{t-1}\\delta_1 + \\epsilon_t \\] \\[ E(\\epsilon_t|\\mathbf{X})= E(\\epsilon_t| x_1,x_2, ...,x_t,x_{t+1},...,x_T) \\] will not equal 0, because if we did not include enough lags, \\(x_{t-2}\\) and \\(\\epsilon_t\\) are correlated A3 is violated because we require the error to be uncorrelated with all time observation of the independent regressors (strict exogeneity) Can be corrected by including more lags (but when stop? ) Without A3 OLS is biased Gauss-Markov Theorem Finite Sample Properties are invalid then, we can Focus on Large Sample Properties Can use A3a instead of A3 A3a in time series become \\[ A3a: E(\\mathbf{x}_t&#39;\\epsilon_t)= 0 \\] only the regressors in this time period need to be independent from the error in this time period (COntemporaneous Exogeneity) \\(\\epsilon_t\\) can be correlated with \\(...,x_{t-2},x_{t-1},x_{t+1}, x_{t+2},...\\) can have a dynamic specification \\(y_t = \\beta_0 + y_{t-1}\\beta_1 + \\epsilon_t\\) Deriving Large Sample Properties for Time Series Assumptions A1, A2, A3a Weak Law and Central Limit Theorem depend on A5 \\(x_t\\) and \\(\\epsilon_t\\) are dependent over t without Weak Law or Central Limit Theorem depend on A5, we cannot have Large Sample Properties for OLS Instead of A5, we consider A5a Derivation of the Asymptotic Variance depends on A4 time series setting introduces Serial Correlation: \\(Cov(\\epsilon_t, \\epsilon_s) \\neq 0\\) under A1, A2, A3a, and A5a, OLS estimator is consistent, and asymptotically normal 9.2.5 Highly Persistent Data If \\(y_t, \\mathbf{x}_t\\) are not weakly dependent stationary process * \\(y_t\\) and \\(y_{t-h}\\) are not almost independent for large h * A5a does not hold and OLS is not consistent and does not have a limiting distribution. * Example + Random Walk \\(y_t = y_{t-1} + u_t\\) + Random Walk with a drift: \\(y_t = \\alpha+ y_{t-1} + u_t\\) Solution First difference is a stationary process \\[ y_t - y_{t-1} = u_t \\] If \\(u_t\\) is a weakly dependent process (also called integrated of order 0) then \\(y_t\\) is said to be difference-stationary process (integrated of order 1) For regression, if \\(\\{y_t, \\mathbf{x}_t \\}\\) are random walks (integrated at order 1), can consistently estimate the first difference equation \\[ \\begin{align} y_t - y_{t-1} &amp;= (\\mathbf{x}_t - \\mathbf{x}_{t-1}\\beta + \\epsilon_t - \\epsilon_{t-1}) \\\\ \\Delta y_t &amp;= \\Delta \\mathbf{x}\\beta + \\Delta u_t \\end{align} \\] Unit Root Test \\[ y_t = \\alpha + \\alpha y_{t-1} + u_t \\] tests if \\(\\rho=1\\) (integrated of order 1) * Under the null \\(H_0: \\rho = 1\\), OLS is not consistent or asymptotically normal. * Under the alternative \\(H_a: \\rho &lt; 1\\), OLS is consistent and asymptotically normal. * usual t-test is not valid, will need to use the transformed equation to produce a valid test. Dickey-Fuller Test \\[ \\Delta y_t= \\alpha + \\theta y_{t-1} + v_t \\] where \\(\\theta = \\rho -1\\) * \\(H_0: \\theta = 0\\) and \\(H_a: \\theta &lt; 0\\) * Under the null, \\(\\Delta y_t\\) is weakly dependent but \\(y_{t-1}\\) is not. * Dickey and Fuller derived the non-normal asymptotic distribution. If you reject the null then \\(y_t\\) is not a random walk. Concerns with the standard Dickey Fuller Test 1. Only considers a fairly simplistic dynamic relationship \\[ \\Delta y_t = \\alpha + \\theta y_{t-1} + \\gamma_1 \\Delta_{t-1} + ..+ \\gamma_p \\Delta_{t-p} +v_t \\] * with one additional lag, under the null \\(\\Delta_{y_t}\\) is an AR(1) process and under the alternative \\(y_t\\) is an AR(2) process. * Solution: include lags of \\(\\Delta_{y_t}\\) as controls. Does not allow for time trend \\[ \\Delta y_t = \\alpha + \\theta y_{t-1} + \\delta t + v_t \\] allows \\(y_t\\) to have a quadratic relationship with t Solution: include time trend (changes the critical values). Adjusted Dickey-Fuller Test \\[ \\Delta y_t = \\alpha + \\theta y_{t-1} + \\delta t + \\gamma_1 \\Delta y_{t-1} + ... + \\gamma_p \\Delta y_{t-p} + v_t \\] where \\(\\theta = 1 - \\rho\\) * \\(H_0: \\theta_1 = 0\\) and \\(H_a: \\theta_1 &lt; 0\\) * Under the null, \\(\\Delta y_t\\) is weakly dependent but \\(y_{t-1}\\) is not * Critical values are different with the time trend, if you reject the null then \\(y_t\\) is not a random walk. 9.2.5.0.1 Newey West Standard Errors If A4 does not hold, we can use Newey West Standard Errors (HAC - Heteroskedasticity Autocorrelation Consistent) \\[ \\hat{B} = T^{-1} \\sum_{t=1}^{T} e_t^2 \\mathbf{x&#39;_tx_t} + \\sum_{h=1}^{g}(1-\\frac{h}{g+1})T^{-1}\\sum_{t=h+1}^{T} e_t e_{t-h}(\\mathbf{x_t&#39;x_{t-h}+ x_{t-h}&#39;x_t}) \\] estimates the covariances up to a distance g part downweights to insure \\(\\hat{B}\\) is PSD How to choose g: For yearly data: g = 1 or 2 is likely to account for most of the correlation For quarterly or monthly data: g should be larger (g = 4 or 8 for quarterly and g = 12 or 14 for monthly) can also take integer part of \\(4(T/100)^{2/9}\\) or integer part of \\(T^{1/4}\\) Testing for Serial Correlation Run OLS regression of \\(y_t\\) on \\(\\mathbf{x_t}\\) and obtain residuals \\(e_t\\) Run OLS regression of \\(e_t\\) on \\(\\mathbf{x}_t, e_{t-1}\\) and test whether coefficient on \\(e_{t-1}\\) is significant. Reject the null of no serial correlation if the coefficient is significant at the 5% level. Test using heteroskedastic robust standard errors can include \\(e_{t-2},e_{t-3},..\\) in step 2 to test for higher order serial correlation (t-test would now be an F-test of joint significance) "],["repeated-cross-sections.html", "9.3 Repeated Cross Sections", " 9.3 Repeated Cross Sections For each time point (day, month, year, etc.), a set of data is sampled. This set of data can be different among different time points. For example, you can sample different groups of students each time you survey. Allowing structural change in pooled cross section \\[ y_i = \\mathbf{x}_i \\beta + \\delta_1 y_1 + ... + \\delta_T y_T + \\epsilon_i \\] Dummy variables for all but one time period * allows different intercept for each time period * allows outcome to change on average for each time period Allowing for structural change in pooled cross section \\[ y_i = \\mathbf{x}_i \\beta + \\mathbf{x}_i y_1 \\gamma_1 + ... + \\mathbf{x}_i y_T \\gamma_T + \\delta_1 y_1 + ...+ \\delta_T y_T + \\epsilon_i \\] Interact \\(x_i\\) with time period dummy variables allows different slopes for each time period allows effects to change based on time period (structural break) Interacting all time period dummies with \\(x_i\\) can produce many variables - use hypothesis testing to determine which structural breaks are needed. 9.3.1 Pooled Cross Section \\[ y_i=\\mathbf{x_i\\beta +x_i \\times y1\\gamma_1 + ...+ x_i \\times yT\\gamma_T + \\delta_1y_1+...+ \\delta_Ty_T + \\epsilon_i} \\] Interact \\(x_i\\) with time period dummy variables allows different slopes for each time period allows effect to change based on time period (structural break) interacting all time period dummies with \\(x_i\\) can produce many variables - use hypothesis testing to determine which structural breaks are needed. "],["panel-data.html", "9.4 Panel Data", " 9.4 Panel Data Follows an individual over T time periods. Panel data structure is like having n samples of time series data Characteristics Information both across individuals and over time (cross-sectional and time-series) N individuals and T time periods Data can be either Balanced: all individuals are observed in all time periods Unbalanced: all individuals are not observed in all time periods. Assume correlation (clustering) over time for a given individual, with independence over individuals. Types Short panel: many individuals and few time periods. Long panel: many time periods and few individuals Both: many time periods and many individuals Time Trends and Time Effects Nonlinear Seasonality Discontinuous shocks Regressors Time-invariant regressors \\(x_{it}=x_i\\) for all t (e.g., gender, race, education) have zero within variation Individual-invariant regressors \\(x_{it}=x_{t}\\) for all i (e.g., time trend, economy trends) have zero between variation Variation for the dependent variable and regressors Overall variation: variation over time and individuals. Between variation: variation between individuals Within variation: variation within individuals (over time). Estimate Formula Individual mean \\(\\bar{x_i}= \\frac{1}{T} \\sum_{t}x_{it}\\) Overall mean \\(\\bar{x}=\\frac{1}{NT} \\sum_{i} \\sum_t x_{it}\\) Overall Variance \\(s_O^2 = \\frac{1}{NT-1} \\sum_i \\sum_t (x_{it} - \\bar{x})^2\\) Between variance \\(s_B^2 = \\frac{1}{N-1} \\sum_i (\\bar{x_i} -\\bar{x})^2\\) Within variance \\(s_W^2= \\frac{1}{NT-1} \\sum_i \\sum_t (x_{it} - \\bar{x_i})^2 = \\frac{1}{NT-1} \\sum_i \\sum_t (x_{it} - \\bar{x_i} +\\bar{x})^2\\) Note: \\(s_O^2 \\approx s_B^2 + s_W^2\\) Since we have n observation for each time period t, we can control for each time effect separately by including time dummies (time effects) \\[ y_{it}=\\mathbf{x_{it}\\beta} + d_1\\delta_1+...+d_{T-1}\\delta_{T-1} + \\epsilon_{it} \\] Note: we cannot use these many time dummies in time series data because in time series data, our n is 1. Hence, there is no variation, and sometimes not enough data compared to variables to estimate coefficients. Unobserved Effects Model Similar to group clustering, assume that there is a random effect that captures differences across individuals but is constant in time. \\[ y_it=\\mathbf{x_{it}\\beta} + d_1\\delta_1+...+d_{T-1}\\delta_{T-1} + c_i + u_{it} \\] where \\(c_i + u_{it} = \\epsilon_{it}\\) \\(c_i\\) unobserved individual heterogeneity (effect) \\(u_{it}\\) idiosyncratic shock \\(\\epsilon_{it}\\) unobserved error term. 9.4.1 Pooled OLS Esimator If \\(c_i\\) is uncorrelated with \\(x_{it}\\) \\[ E(\\mathbf{x_{it}&#39;}(c_i+u_{it})) = 0 \\] then A3a still holds. And we have Pooled OLS consistent. If A4 does not hold, OLS is still consistent, but not efficient, and we need cluster robust SE. Sufficient for A3a to hold, we need Exogeneity for \\(u_{it}\\) A3a (contemporaneous exogeneity): \\(E(\\mathbf{x_{it}&#39;}u_{it})=0\\) time varying error Random Effect Assumption (time constant error): \\(E(\\mathbf{x_{it}&#39;}c_{i})=0\\) Pooled OLS will give you consistent coefficient estimates under A1, A2, A3a (for both \\(u_{it}\\) and RE assumption), and A5 (randomly sampling across i). 9.4.2 Individual-specific effects model If we believe that there is unobserved heterogeneity across individual (e.g., unobserved ability of an individual affects \\(y\\)), If the individual-specific effects are correlated with the regressors, then we have the Fixed Effects Estimator. and if they are not correlated we have the Random Effects Estimator. 9.4.2.1 Random Effects Estimator Random Effects estimator is the Feasible GLS estimator that assumes \\(u_{it}\\) is serially uncorrelated and homoskedastic Under A1, A2, A3a (for both \\(u_{it}\\) and RE assumption) and A5 (randomly sampling across i), RE estimator is consistent. If A4 holds for \\(u_{it}\\), RE is the most efficient estimator If A4 fails to hold (may be heteroskedasticity across i, and serial correlation over t), then RE is not the most efficient, but still more efficient than pooled OLS. 9.4.2.2 Fixed Effects Estimator also known as Within Estimator uses within variation (over time) If the RE assumption is not hold (\\(E(\\mathbf{x_{it}&#39;}c_i) \\neq 0\\)), then A3a does not hold (\\(E(\\mathbf{x_{it}&#39;}\\epsilon_i) \\neq 0\\)). Hence, the OLS and RE are inconsistent/biased (because of omitted variable bias) To deal with violation in \\(c_i\\), we have \\[ y_{it}= \\mathbf{x_{it}\\beta} + c_i + u_{it} \\] \\[ \\bar{y_i}=\\bar{\\mathbf{x_i}} \\beta + c_i + \\bar{u_i} \\] where the second equation is the time averaged equation using within transformation, we have \\[ y_{it} - \\bar{y_i} = \\mathbf{(x_{it} - \\bar{x_i}\\beta)} + u_{it} - \\bar{u_i} \\] because \\(c_i\\) is time constant. The Fixed Effects estimator uses POLS on the transformed equation \\[ y_{it} - \\bar{y_i} = \\mathbf{(x_{it} - \\bar{x_i}\\beta)} + d_1\\delta_1 + ... + d_{T-2}\\delta_{T-2} + u_{it} - \\bar{u_i} \\] we need A3 (strict exogeneity) (\\(E((\\mathbf{x_{it}-\\bar{x_i}})&#39;(u_{it}-\\bar{u_i})=0\\)) to have FE consistent. Variables that are time constant will be absorbed into \\(c_i\\). Hence we cannot make inference on time constant independent variables. If you are interested in the effects of time-invariant variables, you could consider the OLS or between estimator Its recommended that you should still use cluster robust standard errors. Equivalent to the within transformation, we can have the fixed effect estimator be the same with the dummy regression \\[ y_{it} = x_{it}\\beta + d_1\\delta_1 + ... + d_{T-2}\\delta_{T-2} + c_1\\gamma_1 + ... + c_{n-1}\\gamma_{n-1} + u_{it} \\] where \\[\\begin{equation} c_i = \\begin{cases} 1 &amp;\\text{if observation is i} \\\\ 0 &amp;\\text{otherwise} \\\\ \\end{cases} \\end{equation}\\] The standard error is incorrectly calculated. the FE within transformation is controlling for any difference across individual which is allowed to correlated with observables. 9.4.3 Tests for Assumptions 9.4.3.1 Cross-sectional dependence/contemporaneous correlation Null hypothesis: residuals across entities are not correlated. usually seen in macro panels with long time series (large N and T), not seen in micro panels (small T and large N) 9.4.3.2 Serial Correlation Null hypothesis: there is no serial correlation usually seen in macro panels with long time series (large N and T), not seen in micro panels (small T and large N) 9.4.3.3 Unit roots/stationarity Dickey-Fuller test for stochastic trends. Null hypothesis: the series is non-stationary (unit root) You would want your test to be less than the critical value (p&lt;.5) so that there is evidence there is not unit roots. 9.4.3.4 Heteroskedasticity Breusch-Pagan test Null hypothesis: the data is homoskedastic If there is evidence for heteroskedasticity, robust covariance matrix is advised. To control for heteroskedasticity: Robust covariance matrix estimation (Sandwich estimator) white1 - for general heteroskedasticity but no serial correlation (check serial correlation first). Recommended for random effects. white2 - is white1 restricted to a common variance within groups. Recommended for random effects. arellano - both heteroskedasticity and serial correlation. Recommended for fixed effects 9.4.4 Model Selection 9.4.4.1 POLS vs. RE The continuum between RE (used FGLS which more assumption ) and POLS check back on the section of FGLS Breusch-Pagan LM test Test for the random effect model based on the OLS residual Null hypothesis: variances across entities is zero. In another word, no panel effect. If the test is significant, RE is preferable compared to POLS 9.4.4.2 FE vs. RE RE does not require strict exogeneity for consistency (feedback effect between residual and covariates) Hypothesis If true \\(H_0: Cov(c_i,\\mathbf{x_{it}})=0\\) \\(\\hat{\\beta}_{RE}\\) is consistent and efficient, while \\(\\hat{\\beta}_{FE}\\) is consistent \\(H_0: Cov(c_i,\\mathbf{x_{it}}) \\neq 0\\) \\(\\hat{\\beta}_{RE}\\) is inconsistent, while \\(\\hat{\\beta}_{FE}\\) is consistent Hausman Test For the Hausman test to run, you need to assume that strict exogeneity hold A4 to hold for \\(u_{it}\\) Then, Hausman test statistic: \\(H=(\\hat{\\beta}_{RE}-\\hat{\\beta}_{FE})&#39;(V(\\hat{\\beta}_{RE})- V(\\hat{\\beta}_{FE}))(\\hat{\\beta}_{RE}-\\hat{\\beta}_{FE}) \\sim \\chi_{n(X)}^2\\) where \\(n(X)\\) is the number of parameters for the time-varying regressors. A low p-value means that we would reject the null hypothesis and prefer FE A high p-value means that we would not reject the null hypothesis and consider RE estimator. Violation Estimator Basic Estimator Instrumental variable Estimator Variable Coefficients estimator Generalized Method of Moments estimator General FGLS estimator Means groups estimator CCEMG Estimator for limited dependent variables 9.4.5 Summary All three estimators (POLS, RE, FE) require A1, A2, A5 (for individuals) to be consistent. Additionally, POLS is consistent under A3a(for \\(u_{it}\\)): \\(E(\\mathbf{x}_{it}&#39;u_{it})=0\\), and RE Assumption \\(E(\\mathbf{x}_{it}&#39;c_{i})=0\\) If A4 does not hold, use cluster robust SE but POLS is not efficient RE is consistent under A3a(for \\(u_{it}\\)): \\(E(\\mathbf{x}_{it}&#39;u_{it})=0\\), and RE Assumption \\(E(\\mathbf{x}_{it}&#39;c_{i})=0\\) If A4 (for \\(u_{it}\\)) holds then usual SE are valid and RE is most efficient If A4 (for \\(u_{it}\\)) does not hold, use cluster robust SE ,and RE is no longer most efficient (but still more efficient than POLS) FE is consistent under A3 \\(E((\\mathbf{x}_{it}-\\bar{\\mathbf{x}}_{it})&#39;(u_{it} -\\bar{u}_{it}))=0\\) Cannot estimate effects of time constant variables A4 generally does not hold for \\(u_{it} -\\bar{u}_{it}\\) so cluster robust SE are needed Note: A5 for individual (not for time dimension) implies that you have A5a for the entire data set. Estimator / True Model POLS RE FE POLS Consistent Consistent Inconsistent FE Consistent Consistent Consistent RE Consistent Consistent Inconsistent Based on table provided by Ani Katchova 9.4.6 Application #install.packages(&quot;plm&quot;) library(&quot;plm&quot;) library(foreign) Panel &lt;- read.dta(&quot;http://dss.princeton.edu/training/Panel101.dta&quot;) attach(Panel) Y &lt;- cbind(y) X &lt;- cbind(x1, x2, x3) # Set data as panel data pdata &lt;- pdata.frame(Panel, index=c(&quot;country&quot;,&quot;year&quot;)) # Pooled OLS estimator pooling &lt;- plm(Y ~ X, data=pdata, model= &quot;pooling&quot;) summary(pooling) # Between estimator between &lt;- plm(Y ~ X, data=pdata, model= &quot;between&quot;) summary(between) # First differences estimator firstdiff &lt;- plm(Y ~ X, data=pdata, model= &quot;fd&quot;) summary(firstdiff) # Fixed effects or within estimator fixed &lt;- plm(Y ~ X, data=pdata, model= &quot;within&quot;) summary(fixed) # Random effects estimator random &lt;- plm(Y ~ X, data=pdata, model= &quot;random&quot;) summary(random) # LM test for random effects versus OLS # Accept Null, then OLS, Reject Null then RE plmtest(pooling,effect = &quot;individual&quot;, type = c(&quot;bp&quot;)) # other type: &quot;honda&quot;, &quot;kw&quot;,&quot; &quot;ghm&quot;; other effect : &quot;time&quot; &quot;twoways&quot; # B-P/LM and Pesaran CD (cross-sectional dependence) test pcdtest(fixed, test = c(&quot;lm&quot;)) # Breusch and Pagan&#39;s original LM statistic pcdtest(fixed, test = c(&quot;cd&quot;)) # Pesaran&#39;s CD statistic # Serial Correlation pbgtest(fixed) # stationary library(&quot;tseries&quot;) adf.test(pdata$y, k = 2) # LM test for fixed effects versus OLS pFtest(fixed, pooling) # Hausman test for fixed versus random effects model phtest(random, fixed) # Breusch-Pagan heteroskedasticity library(lmtest) bptest(y ~ x1 + factor(country), data = pdata) # If there is presence of heteroskedasticity ## For RE model coeftest(random) #orginal coef coeftest(random, vcovHC) # Heteroskedasticity consistent coefficients t(sapply(c(&quot;HC0&quot;, &quot;HC1&quot;, &quot;HC2&quot;, &quot;HC3&quot;, &quot;HC4&quot;), function(x) sqrt(diag(vcovHC(random, type = x))))) #show HC SE of the coef # HC0 - heteroskedasticity consistent. The default. # HC1,HC2, HC3  Recommended for small samples. HC3 gives less weight to influential observations. # HC4 - small samples with influential observations # HAC - heteroskedasticity and autocorrelation consistent ## For FE model coeftest(fixed) # Original coefficients coeftest(fixed, vcovHC) # Heteroskedasticity consistent coefficients coeftest(fixed, vcovHC(fixed, method = &quot;arellano&quot;)) # Heteroskedasticity consistent coefficients (Arellano) t(sapply(c(&quot;HC0&quot;, &quot;HC1&quot;, &quot;HC2&quot;, &quot;HC3&quot;, &quot;HC4&quot;), function(x) sqrt(diag(vcovHC(fixed, type = x))))) #show HC SE of the coef Advanced Other methods to estimate the random model: \"swar\": default (Swamy and Arora 1972) \"walhus\": (Wallace and Hussain 1969) \"amemiya\": (Fuller and Battese 1974) \"nerlove\"\" (Nerlove 1971) Other effects: Individual effects: default Time effects: \"time\" Individual and time effects: \"twoways\" Note: no random two-ways effect model for random.method = nerlove amemiya &lt;- plm(Y ~ X, data=pdata, model= &quot;random&quot;,random.method = &quot;amemiya&quot;,effect = &quot;twoways&quot;) To call the estimation of the variance of the error components ercomp(Y~X, data=pdata, method = &quot;amemiya&quot;, effect = &quot;twoways&quot;) Check for the unbalancedness. Closer to 1 indicates balanced data (Ahrens and Pincus 1981) punbalancedness(random) Instrumental variable \"bvk\": default (Balestra and Varadharajan-Krishnakumar 1987) \"baltagi\": (Baltagi 1981) \"am\" (Amemiya and MaCurdy 1986) \"bms\": (Breusch, Mizon, and Schmidt 1989) instr &lt;- plm(Y ~ X | X_ins, data = pdata, random.method = &quot;ht&quot;, model = &quot;random&quot;, inst.method = &quot;baltagi&quot;) 9.4.7 Other Estimators 9.4.7.1 Variable Coeffiicents Model fixed_pvcm &lt;- pvcm(Y~X, data=pdata, model=&quot;within&quot;) random_pvcm &lt;- pvcm(Y~X, data=pdata, model=&quot;random&quot;) More details can be found here 9.4.7.2 Generalized Method of Moments Estimator Typically use in dynamic models. Example is from plm package z2 &lt;- pgmm(log(emp) ~ lag(log(emp), 1)+ lag(log(wage), 0:1) + lag(log(capital), 0:1) | lag(log(emp), 2:99) + lag(log(wage), 2:99) + lag(log(capital), 2:99), data = EmplUK, effect = &quot;twoways&quot;, model = &quot;onestep&quot;, transformation = &quot;ld&quot;) summary(z2, robust = TRUE) 9.4.7.3 General Feasible Generalized Least Squares Models Assume there is no cross-sectional correlation Robust against intragroup heteroskedasticity and serial correlation. Suited when n is much larger than T (long panel) However, inefficient under groupwise heteorskedasticity. # Random Effects zz &lt;- pggls(log(emp)~log(wage)+log(capital), data=EmplUK, model=&quot;pooling&quot;) # Fixed zz &lt;- pggls(log(emp)~log(wage)+log(capital), data=EmplUK, model=&quot;within&quot;) References "],["hypothesis-testing.html", "Chapter 10 Hypothesis Testing", " Chapter 10 Hypothesis Testing Note Always written in terms of the population parameter (\\(\\beta\\)) not the estimator/estimate (\\(\\hat{\\beta}\\)) Assuming the null hypothesis is true, what is the (asymptotic) distribution of the estimator Two-sided \\[ H_0: \\beta_j = 0 \\\\ H_1: \\beta_j \\neq 0 \\] then under the null, the OLS estimator has the following distribution \\[ A1-A3a, A5: \\sqrt{n} \\hat{\\beta_j} \\sim N(0,Avar(\\sqrt{n}\\hat{\\beta}_j)) \\] For the one-sided test, the null is a set of values, so now you choose the worst case single value that is hardest to prove and derive the distribution under the null One-sided \\[ H_0: \\beta_j\\ge 0 \\\\ H_1: \\beta_j &lt; 0 \\] then the hardest null value to prove is \\(H_0: \\beta_j=0\\). Then under this specific null, the OLS estimator has the following asymptotic distribution \\[ A1-A3a, A5: \\sqrt{n}\\hat{\\beta_j} \\sim N(0,Avar(\\sqrt{n}\\hat{\\beta}_j)) \\] "],["types-of-hypothesis-testing.html", "10.1 Types of hypothesis testing", " 10.1 Types of hypothesis testing \\(H_0 : \\theta = \\theta_0\\) \\(H_1 : \\theta \\neq \\theta_0\\) How far away / extreme \\(\\theta\\) can be if our null hypothesis is true Assume that our likelihood function for q is \\(L(q) = q^{30}(1-q)^{70}\\) Likelihood function q = seq(0,1,length=100) L= function(q){q^30 * (1-q)^70} plot(q,L(q),ylab=&quot;L(q)&quot;,xlab=&quot;q&quot;,type=&quot;l&quot;) Log-Likelihood function q = seq(0,1,length=100) l= function(q){30*log(q) + 70 * log(1-q)} plot(q,l(q)-l(0.3),ylab=&quot;l(q) - l(qhat)&quot;,xlab=&quot;q&quot;,type=&quot;l&quot;) abline(v=0.2) (Fox 1991) References "],["wald-test.html", "10.2 Wald test", " 10.2 Wald test \\[ t_W=\\frac{(\\hat{\\theta}-\\theta_0)^2}{I(\\theta_0)^{-1}} \\sim \\chi^2_{(v)} \\] where v is the degree of freedom. Equivalently, \\[ s_W= \\frac{\\hat{\\theta}-\\theta_0}{\\sqrt{I(\\hat{\\theta})^{-1}}} \\sim Z \\] How far away in the distribution your sample estimate is from the hypothesized population parameter. For a null value, what is the probability you would obtained a realization more extreme or worse than the estimate you actually obtained. Significance Level (\\(\\alpha\\)) and Confidence Level (\\(1-\\alpha\\)) The significance level is the benchmark in which the probability is so low that we would have to reject the null The confidence level is the probability that sets the bounds on how far away the realization of the estimator would have to be to reject the null. Test Statistics Standardized (transform) the estimator and null value to a test statistic that always has the same distribution Test Statistic for the OLS estimator for a single hypothesis \\[ T = \\frac{\\sqrt{n}(\\hat{\\beta}_j-\\beta_{j0})}{\\sqrt{n}SE(\\hat{\\beta_j})} \\sim^a N(0,1) \\] Equivalently, \\[ T = \\frac{(\\hat{\\beta}_j-\\beta_{j0})}{SE(\\hat{\\beta_j})} \\sim^a N(0,1) \\] the test statistic is another random variable that is a function of the data and null hypothesis. T denotes the random variable test statistic t denotes the single realization of the test statistic Evaluating Test Statistic: determine whether or not we reject or fail to reject the null hypothesis at a given significance / confidence level Three equivalent ways Critical Value P-value Confidence Interval Critical Value For a given significance level, will determine the critical value (c) * One-sided: \\(H_0: \\beta_j \\ge \\beta_{j0}\\) \\[ P(T&lt;c|H_0)=\\alpha \\] Reject the null if \\(t&lt;c\\) One-sided: \\(H_0: \\beta_j \\le \\beta_{j0}\\) \\[ P(T&gt;c|H_0)=\\alpha \\] Reject the null if \\(t&gt;c\\) TWo-sided: \\(H_0: \\beta_j \\neq \\beta_{j0}\\) \\[ P(|T|&gt;c|H_0)=\\alpha \\] Reject the null if \\(|t|&gt;c\\) p-value Calculate the probability that the test statistic was worse than the realization you have One-sided: \\(H_0: \\beta_j \\ge \\beta_{j0}\\) \\[ \\text{p-value} = P(T&lt;t|H_0) \\] One-sided: \\(H_0: \\beta_j \\le \\beta_{j0}\\) \\[ \\text{p-value} = P(T&gt;t|H_0) \\] Two-sided: \\(H_0: \\beta_j \\neq \\beta_{j0}\\) \\[ \\text{p-value} = P(|T|&lt;t|H_0) \\] reject the null if p-value \\(&lt; \\alpha\\) Confidence Interval Using the critical value associated with a null hypothesis and significance level, create an interval \\[ CI(\\hat{\\beta}_j)_{\\alpha} = [\\hat{\\beta}_j-(c \\times SE(\\hat{\\beta}_j)),\\hat{\\beta}_j+(c \\times SE(\\hat{\\beta}_j))] \\] If the null set lies outside the interval then we reject the null. We are not testing whether the true population value is close to the estimate, we are testing that given a field true population value of the parameter, how like it is that we observed this estimate. Can be interpreted as we believe with \\((1-\\alpha)\\times 100 \\%\\) probability that the confidence interval captures the true parameter value. With stronger assumption (A1-A6), we could consider Finite Sample Properties \\[ T = \\frac{\\hat{\\beta}_j-\\beta_{j0}}{SE(\\hat{\\beta}_j)} \\sim T(n-k) \\] This above distributional derivation is strongly dependent on A4 and A5 T has a student t-distribution because the numerator is normal and the denominator is \\(\\chi^2\\). Critical value and p-values will be calculated from the student t-distribution rather than the standard normal distribution. \\(n \\to \\infty\\), \\(T(n-k)\\) is asymptotically standard normal. Rule of thumb if \\(n-k&gt;120\\): the critical values and p-values from the t-distribution are (almost) the same as the critical values and p-values from the standard normal distribution. if \\(n-k&lt;120\\) if (A1-A6) hold then the t-test is an exact finite distribution test if (A1-A3a, A5) hold, because the t-distribution is asymptotically normal, computing the critical values from a t-distribution is still a valid asymptotic test (i.e., not quite the right critical values and p0values, the difference goes away as \\(n \\to \\infty\\)) 10.2.1 Multiple Hypothesis test multiple parameters as the same time \\(H_0: \\beta_1 = 0\\ \\&amp; \\ \\beta_2 = 0\\) \\(H_0: \\beta_1 = 1\\ \\&amp; \\ \\beta_2 = 0\\) perform a series of simply hypothesis does not answer the question (joint distribution vs. two marginal distributions). The test statistic is based on a restriction written in matrix form. \\[ y=\\beta_0+x_1\\beta_1 + x_2\\beta_2 + x_3\\beta_3 + \\epsilon \\] Null hypothesis is \\(H_0: \\beta_1 = 0 \\text{ &amp; } \\beta_2=0\\) can be rewritten as \\(H_0: \\mathbf{R}\\beta -\\mathbf{q}=0\\) where \\(\\mathbf{R}\\) is a m x k matrix where m is the number of restrictions and k is the number of parameters. \\(\\mathbf{q}\\) is a k x 1 vector \\(\\mathbf{R}\\) picks up the relevant parameters while \\(\\mathbf{q}\\) is a the null value of the parameter \\[ \\mathbf{R}= \\left( \\begin{array}{c} 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ \\end{array} \\right), \\mathbf{q} = \\left( \\begin{array}{c} 0 \\\\ 0 \\\\ \\end{array} \\right) \\] Test Statistic for OLS estimator for a multiple hypothesis \\[ F = \\frac{(\\mathbf{R\\hat{\\beta}-q})\\hat{\\Sigma}^{-1}(\\mathbf{R\\hat{\\beta}-q})}{m} \\sim^a F(m,n-k) \\] \\(\\hat{\\Sigma}^{-1}\\) is the estimator for the asymptotic variance-covariance matrix if A4 holds, both the homoskedastic and heteroskedastic versions produce valid estimator If A4 does not hold, only the heteroskedastic version produces valid estimators. When m = 1, there is only a single restriction, then the F-statistic is the t-statistic squared. F distribution is strictly positive, check F-Distribution for more details. 10.2.2 Linear Combination Testing multiple parameters as the same time \\[ H_0: \\beta_1 -\\beta_2 = 0 \\\\ H_0: \\beta_1 - \\beta_2 &gt; 0 \\\\ H_0: \\beta_1 - 2*\\beta_2 =0 \\] Each is a single restriction on a function of the parameters. Null hypothesis: \\[ H_0: \\beta_1 -\\beta_2 = 0 \\] can be rewritten as \\[ H_0: \\mathbf{R}\\beta -\\mathbf{q}=0 \\] where \\(\\mathbf{R}\\)=(0 1 -1 0 0) and \\(\\mathbf{q}=0\\) 10.2.3 Application library(&quot;car&quot;) # Multiple hypothesis mod.davis &lt;- lm(weight ~ repwt, data=Davis) linearHypothesis(mod.davis, c(&quot;(Intercept) = 0&quot;, &quot;repwt = 1&quot;),white.adjust = TRUE) ## Linear hypothesis test ## ## Hypothesis: ## (Intercept) = 0 ## repwt = 1 ## ## Model 1: restricted model ## Model 2: weight ~ repwt ## ## Note: Coefficient covariance matrix supplied. ## ## Res.Df Df F Pr(&gt;F) ## 1 183 ## 2 181 2 3.3896 0.03588 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Linear Combination mod.duncan &lt;- lm(prestige ~ income + education, data=Duncan) linearHypothesis(mod.duncan, &quot;1*income - 1*education = 0&quot;) ## Linear hypothesis test ## ## Hypothesis: ## income - education = 0 ## ## Model 1: restricted model ## Model 2: prestige ~ income + education ## ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 43 7518.9 ## 2 42 7506.7 1 12.195 0.0682 0.7952 "],["the-likelihood-ratio-test.html", "10.3 The likelihood ratio test", " 10.3 The likelihood ratio test \\[ t_{LR} = 2[l(\\hat{\\theta})-l(\\theta_0)] \\sim \\chi^2_v \\] where v is the degree of freedom. Compare the height of the log-likelihood of the sample estimate in relation to the height of log-likelihood of the hypothesized population parameter "],["lagrange-multiplier-score.html", "10.4 Lagrange Multiplier (Score)", " 10.4 Lagrange Multiplier (Score) \\[ t_S= \\frac{S(\\theta_0)^2}{I(\\theta_0)} \\sim \\chi^2_v \\] where v is the degree of freedom. Compare the slope of the log-likelihood of the sample estimate in relation to the slope of the log-likelihood of the hypothesized population parameter "],["imputation-missing-data.html", "Chapter 11 Imputation (Missing Data)", " Chapter 11 Imputation (Missing Data) Imputation is usually seen as the illegitimate child of statistical analysis. Several reasons that contribute to this negative views could be: Peopled hardly do imputation correctly Imputation can only be applied to a small range of problems correctly If you have missing data on y (dependent variable), you probability would not be able to do any imputation appropriately. However, if you have certain type of missing data (e.g., non-random missing data) in the xs variable (independent variables), then you can still salvage your collected data points with imputation. We also need to talk why you would want to do imputation in the first place. If your purpose is inference/ explanation (valid statistical inference not optimal point prediction), then imputation would not offer much help (Rubin 1996). However, if your purpose is prediction, you would want your standard error to be reduced by including information (non-missing data) on other variables of a data point. Then imputation could be the tool that youre looking for. For most software packages, it will use listwise deletion or casewise deletion to have complete case analysis (analysis with only observations with all information). Not until recently that statistician can propose some methods that are a bit better than listwise deletion which are maximum likelihood and multiple imputation. Judging the quality of missing data procedures by their ability to recreate the individual missing values (according to hit rate, mean square error, etc) does not lead to choosing procedures that result in valid inference, (Rubin 1996) References "],["assumptions.html", "11.1 Assumptions", " 11.1 Assumptions 11.1.1 Missing Completely at Random (MCAR) Missing Completely at Random, MCAR, means there is no relationship between the missingness of the data and any values, observed or missing. Those missing data points are a random subset of the data. There is nothing systematic going on that makes some data more likely to be missing than others. The probability of missing data on a variable is unrelated to the value of it or to the values of any other variables in the data set. Note: the missingness on Y can be correlated with the missingness on X We can compare the value of other variables for the observations with missing data, and observations without missing data. If we reject the t-test for mean difference, we can say there is evidence that the data are not MCAR. But we cannot say that our data are MCAR if we fail to reject the t-test. the propensity for a data point to be missing is completely random. Theres no relationship between whether a data point is missing and any values in the data set, missing or observed. The missing data are just a random subset of the data. 11.1.2 Missing at Random (MAR) Missing at Random, MAR, means there is a systematic relationship between the propensity of missing values and the observed data, but not the missing data. Whether an observation is missing has nothing to do with the missing values, but it does have to do with the values of an individuals observed variables. So, for example, if men are more likely to tell you their weight than women, weight is MAR. MAR is weaker than MCAR \\[ P(Y_{missing}|Y,X)= P(Y_{missing}|X) \\] The probability of Y missing given Y and X equal to the probability of of Y missing given X. However, it is impossible to provide evidence to the MAR condition. the propensity for a data point to be missing is not related to the missing data, but it is related to some of the observed data. In another word, there is a systematic relationship between the propensity of missing values and the observed data, but not the missing data. For example, if men are more likely to tell you their weight than women, weight is MAR MAR requires that the cause of the missing data is unrelated to the missing values but may be related to the observed values of other variables. MAR means that the missing values are related to observed values on other variables. As an example of CD missing data, missing income data may be unrelated to the actual income values but are related to education. Perhaps people with more education are less likely to reveal their income than those with less education 11.1.3 Ignorable The missing data mechanism is ignorable when The data are MAR the parameters in the function of the missing data process are unrelated to the parameters (of interest) that need to be estimated. In this case, you actually dont need to model the missing data mechanisms unless you would like to improve on your accuracy, in which case you still need to be very rigorous about your approach to improve efficiency in your parameters. 11.1.4 Nonignorable Missing Not at Random, MNAR, means there is a relationship between the propensity of a value to be missing and its values. Example: people with the lowest education are missing on education or the sickest people are most likely to drop out of the study. MNAR is called Nonignorable because the missing data mechanism itself has to be modeled as you deal with the missing data. You have to include some model for why the data are missing and what the likely values are. Hence, in the case of nonignorable, the data are not MAR. Then, your parameters of interest will be biased if you do not model the missing data mechanism. One of the most widely used approach for nonignorable missing data is (Heckman 1976) Another name: Missing Not at Random (MNAR): there is a relationship between the propensity of a value to be missing and its values For example, people with low education will be less likely to report it. We need to model why the data are missing and what the likely values are. the missing data mechanism is related to the missing values It commonly occurs when people do not want to reveal something very personal or unpopular about themselves Complete case analysis can give highly biased results for NI missing data. If proportionally more low and moderate income individuals are left in the sample because high income people are missing, an estimate of the mean income will be lower than the actual population mean. References "],["solutions-to-missing-data.html", "11.2 Solutions to Missing data", " 11.2 Solutions to Missing data 11.2.1 Listwise Deletion Advantages: Can be applied to any statistical test (SEM, multi-level regression, etc.) In the case of MCAR, both the parameters estimates and its standard errors are unbiased. In the case of MAR among independent variables (not depend on the values of dependent variables), then listwise deletion parameter estimates can still be unbiased. (Little 1992) For example, you have a model \\(y=\\beta_{0}+\\beta_1X_1 + \\beta_2X_2 +\\epsilon\\) if the probability of missing data on X1 is independent of Y, but dependent on the value of X1 and X2, then the model estimates are still unbiased. The missing data mechanism the depends on the values of the independent variables are the same as stratified sampling. And stratified sampling does not bias your estimates In the case of logistic regression, if the probability of missing data on any variable depends on the value of the dependent variable, but independent of the value of the independent variables, then the listwise deletion will yield biased intercept estimate, but consistent estimates of the slope and their standard errors (Vach 1994). However, logistic regression will still fail if the probability of missing data is dependent on both the value of the dependent and independent variables. Under regression analysis, listwise deletion is more robust than maximum likelihood and multiple imputation when MAR assumption is violated. Disadvantages: It will yield a larger standard errors than other more sophisticated methods discussed later. If the data are not MCAR, but MAR, then your listwise deletion can yield biased estimates. In other cases than regression analysis, other sophisticated methods can yield better estimates compared to listwise deletion. 11.2.2 Pairwise Deletion This method could only be used in the case of linear models such as linear regression, factor analysis, or SEM. The premise of this method based on that the coefficient estimates are calculated based on the means, standard deviations, and correlation matrix. Compared to listwise deletion, we still utilized as many correlation between variables as possible to compute the correlation matrix. Advantages: If the true missing data mechanism is MCAR, pair wise deletion will yield consistent estimates, and unbiased in large samples Compared to listwise deletion: (Glasser 1964) If the correlation among variables are low, pairwise deletion is more efficient estimates than listwise If the correlations among variables are high, listwise deletion is more efficient than pairwise. Disadvantages: If the data mechanism is MAR, pairwise deletion will yield biased estimates. In small sample, sometimes covariance matrix might not be positive definite, which means coefficients estimates cannot be calculated. Note: You need to read carefully on how your software specify the sample size because it will alter the standard errors. 11.2.3 Dummy Variable Adjustment Also known as Missing Indicator Method or Proxy Variable Add another variable in the database to indicate whether a value is missing. Create 2 variables \\[\\begin{equation} D= \\begin{cases} 1 &amp; \\text{data on X are missing} \\\\ 0 &amp; \\text{otherwise}\\\\ \\end{cases} \\end{equation}\\] \\[\\begin{equation} X^* = \\begin{cases} X &amp; \\text{data are available} \\\\ c &amp; \\text{data are missing}\\\\ \\end{cases} \\end{equation}\\] Note: A typical choice for c is usually the mean of X Interpretation: Coefficient of D is the the difference in the expected value of Y between the group with data and the group without data on X. Coefficient of X* is the effect of the group with data on Y Disadvantages: This method yields bias estimates of the coefficient even in the case of MCAR (Jones 1996) 11.2.4 Imputation 11.2.4.1 Maximum Likelihood When missing data are MAR and monotonic (such as in the case of panel studies), ML can be adequately in estimating coefficients. Monotonic means that if you are missing data on X1, then that observation also has missing data on all other variables that come after it. ML can generally handle linear models, log-linear model, but beyond that, ML still lacks both theory and software to implement. 11.2.4.1.1 Expectation-Maximization Algorithm (EM Algorithm) An iterative process: Other variables are used to impute a value (Expectation). Check whether the value is most likely (Maximization). If not, it re-imputes a more likely value. You start your regression with your estimates based on either listwise deletion or pairwise deletion. After regressing missing variables on available variables, you obtain a regression model. Plug the missing data back into the original model, with modified variances and covariances For example, if you have missing data on \\(X_{ij}\\) you would regress it on available data of \\(X_{i(j)}\\), then plug the expected value of \\(X_{ij}\\) back with its \\(X_{ij}^2\\) turn into \\(X_{ij}^2 + s_{j(j)}^2\\) where \\(s_{j(j)}^2\\) stands for the residual variance from regressing \\(X_{ij}\\) on \\(X_{i(j)}\\) With the new estimated model, you rerun the process until the estimates converge. Advantages: easy to use preserves the relationship with other variables Disadvantages: Standard errors of the coefficients are incorrect (biased usually downward) Models with overidentification, the estimates will not be efficient. 11.2.4.1.2 Direct ML (raw maximum likelihood) Advantages efficient estimates and correct standard errors. Disadvantages: Hard to implements 11.2.4.2 Multiple Imputation MI is designed to use the Bayesian model-based approach to create procedures, and the frequentist (randomization-based approach) to evaluate procedures. (Rubin 1996) MI estimates have the same properties as ML when the data is MAR Consistent Asymptotically efficient Asymptotically normal MI can be applied to any type of model, unlike Maximum Likelihood that is only limited to a small set of models. A drawback of MI is that it will produce slightly different estimates every time you run it. To avoid such problem, you can set seed when doing your analysis to ensure its reproducibility. 11.2.4.2.1 Single Random Imputaiton Random draws form the residual distribution of each imputed variable and add those random numbers to the imputed values. For example, if we have missing data on X, and its MCAR, then regress X on Y (Listwise Deletion method) to get its residual distribution. For every missing value on X, we substitute with \\(\\tilde{x_i}=\\hat{x_i} + \\rho u_i\\) where \\(u_i\\) is a random draw from a standard normal distribution \\(x_i\\) is the predicted value from the regression of X and Y \\(\\rho\\) is the standard deviation of the residual distribution of X regressed on Y. However, the model you run with the imputed data still thinks that your data are collected, not imputed, which leads your standard error estimates to be too low and test statistics too high. To address this problem, we need to repeat the imputation process which leads us to repeated imputation or multiple random imputation. 11.2.4.2.2 Repeated Imputation Repeated imputations are draws from the posterior predictive distribution of the missing values under a specific model , a particular Bayesian model for both the data and the missing mechanism.(Rubin 1996) Repeated imputation, also known as, multiple random imputation, allows us to have multiple completed data sets. The variability across imputations will adjust the standard errors upward. The estimate of the standard error of \\(\\bar{r}\\) (mean correlation estimates between X and Y) is \\[ SE(\\bar{r})=\\sqrt{\\frac{1}{M}\\sum_{k}s_k^2+ (1+\\frac{1}{M})(\\frac{1}{M-1})\\sum_{k}(r_k-\\bar{r})^2} \\] where M is the number of replications, \\(r_k\\) is the the correlation in replication k, \\(s_k\\) is the estimated standard error in replication k. However, this method still considers the parameter in predicting \\(\\tilde{x}\\) is still fixed, which means we assume that we are using the true parameters to predict \\(\\tilde{x}\\). To overcome this challenge, we need to introduce variability into our model for \\(\\tilde{x}\\) by treating the parameters as a random variables and use Bayesian posterior distribution of the parameters to predict the parameters. However, if your sample is large and the proportion of missing data is small, the extra Bayesian step might not be necessary. If your sample is small or the proportion of missing data is large, the extra Bayesian step is necessary. Two algorithms to get random draws of the regression parameters from its posterior distribution: Data Augmentation Sampling importance/resampling (SIR) Authors have argued for SIR superiority due to its computer time (King et al. 2001) 11.2.4.2.2.1 Data Augmentation Steps for data augmentation: Choose starting values for the parameters (e.g., for multivariate normal, choose means and covariance matrix). These values can come from previous values, expert knowledge, or from listwise deletion or pairwise deletion or EM estimation. Based on the current values of means and covariances calculate the coefficients estimates for the equation that variable with missing data is regressed on all other variables (or variables that you think will help predict the missing values, could also be variables that are not in the final estimation model) Use the estimates in step (2) to predict values for missing values. For each predicted value, add a random error from the residual normal distribution for that variable. From the complete data set, recalculate the means and covariance matrix. And take a random draw from the posterior distribution of the means and covariances with Jeffreys prior. Using the random draw from step (4), repeat step (2) to (4) until the means and covariances stabilize (converged). The iterative process allows us to get random draws from the joint posterior distribution of both data nd parameters, given the observed data. Rules of thumb regarding convergence: The higher the proportion of missing, the more iterations the rate of convergence for EM algorithm should be the minimum threshold for DA. You can also check if your distribution has been converged by diagnostic statistics Can check Bayesian Diagnostics for some introduction. Types of chains Parallel: Run a separate chain of iterations for each of data set. Different starting values are encouraged. For example, one could use bootstrap to generate different data set with replacement, and for each data set, calculate the starting values by EM estimates. Pro: Run faster, and less likely to have dependence in the resulting data sets. Con: Sometimes it will not converge Sequential one long chain of data augmentation cycles. After burn-in and thinning, you will have to data sets Pro: Converged to the true posterior distribution is more likely. Con: The resulting data sets are likely to be dependent. Remedies can be thinning and burn-in. Note on Non-normal or categorical data The normal-based methods still work well, but you will need to do some transformation. For example, If the data is skewed, then log-transform, then impute, the exponentiate to have the missing data back to its original metric. If the data is proportion, logit-transform, impute, then de-transform the missing data. If you want to impute non-linear relationship, such as interaction between 2 variables and 1 variable is categorical. You can do separate imputation for different levels of that variable separately, then combined for the final analysis. If all variables that have missing data are categorical, then unrestricted multinomial model or log-linear model is recommended. If a single categorical variable, logistic (logit) regression would be sufficient. 11.2.5 Heckmans Sample Selection Model 11.2.5.1 Mean, Mode, Median Imputation Bad: Mean imputation does not preserve the relationships among variables Mean imputation leads to An Underestimate of Standard Errors  youre making Type I errors without realizing it. Biased estimates of variances and covariances (Haitovsky 1968) . 11.2.5.2 Hot Deck Imputation A randomly chosen value from an individual in the sample who has similar values on other variables. In other words, find all the sample subjects who are similar on other variables, then randomly choose one of their values on the missing variable. Good: Constrained to only possible values. Since the value is picked at random, it adds some variability, which might come in handy when calculating standard errors. 11.2.5.3 Cold Deck Imputation Contrary to Hot Deck, Cold Deck choose value systematically from an observation that has similar values on other variables, which remove the random variation that we want. Bayesian ridge regression implementation 11.2.5.4 Regression Imputation Also known as conditional mean imputation Missing value is based (regress) on other variables. Good: Maintain the relationship with other variables If the data are MCAR, least-squares coefficients estimates will be consistent, and approximately unbiased in large samples (Gourieroux and Monfort 1981) Can have improvement on efficiency by using weighted least squares (Beale and Little 1975) or generalized least squares (Gourieroux and Monfort 1981). Bad: No variability left. treated data as if they were collected. Underestimate the standard errors and overestimate test statistics 11.2.5.5 Stochatistc Imputation Regression imputation + random residual = Stochastic Imputation Most multiple imputation is based off of some form of stochastic regression imputation. Good: Has all the advantage of Regression Imputation and also has the random components Note Multiple Imputation usually based on some form of stochastic regression imputation. 11.2.5.6 Interpolation and Extrapolation An estimated value from other observations from the same individual. It usually only works in longitudinal data. 11.2.5.7 K-nearest neighbour (KNN) imputation The above methods are model-based imputation (regression). This is an example of neighbor-based imputation (K-nearest neighbor). For every observation that needs to be imputed, the algorithm identifies k closest observations based on some types distance (e.g., Euclidean) and computes the weighted average (weighted based on distance) of these k obs. For a discrete variable, it uses the most frequent value among the k nearest neighbors. * Distance metrics: Hamming distance. For a continuous variable, it uses the mean or mode. Distance metrics: Euclidean Mahalanobis Manhattan 11.2.5.8 Bayesian Ridge regression implementation 11.2.5.9 E-M Algorithm the E-M Algorithm, which stands for Expectation-Maximization. It is an iterative procedure in which it uses other variables to impute a value (Expectation), then checks whether that is the value most likely (Maximization). If not, it re-imputes a more likely value. This goes on until it reaches the most likely value. EM imputations are better than mean imputations because they preserve the relationship with other variables, which is vital if you go on to use something like Factor Analysis or Linear Regression. EM Imputations still underestimate standard error, however. Once again, this approach is only reasonable if the standard error of individual items is not vital, like in Factor Analysis, which doesnt have p-values. References "],["criteria-for-choosing-an-effective-approach.html", "11.3 Criteria for Choosing an Effective Approach", " 11.3 Criteria for Choosing an Effective Approach Criteria for an ideal technique in treating missing data: Unbiased parameter estimates Adequate power Accurate standard errors (p-values, confidence intervals) The Multiple Imputation and Full Information Maximum Likelihood are the the most ideal candidate. Single imputation will generally lead to underestimation of standard errors. "],["another-perspective.html", "11.4 Another Perspective", " 11.4 Another Perspective Model bias can arisen from various factors including: Imputation method Missing data mechanism (MCAR vs. MAR) Proportion of the missing data Information available in the data set Since the imputed observations are themselves estimates, their values have corresponding random error. But when you put in that estimate as a data point, your software doesnt know that. So it overlooks the extra source of error, resulting in too-small standard errors and too-small p-values. So multiple imputation comes up with multiple estimates. Because multiple imputation have a random component, the multiple estimates are slightly different. This re-introduces some variation that your software can incorporate in order to give your model accurate estimates of standard error. Multiple imputation was a huge breakthrough in statistics about 20 years ago. It solves a lot of problems with missing data (though, unfortunately not all) and if done well, leads to unbiased parameter estimates and accurate standard errors. If your rate of missing data is very, very small (2-3%) it doesnt matter what technique you use. Remember that there are three goals of multiple imputation, or any missing data technique: Unbiased parameter estimates in the final analysis (regression coefficients, group means, odds ratios, etc.) accurate standard errors of those parameter estimates, and therefore, accurate p-values in the analysis adequate power to find meaningful parameter values significant. Hence, Dont round off imputations for dummy variables. Many common imputation techniques, like MCMC, require normally distributed variables. Suggestions for imputing categorical variables were to dummy code them, impute them, then round off imputed values to 0 or 1. Recent research, however, has found that rounding off imputed values actually leads to biased parameter estimates in the analysis model. You actually get better results by leaving the imputed values at impossible values, even though its counter-intuitive. Dont transform skewed variables. Likewise, when you transform a variable to meet normality assumptions before imputing, you not only are changing the distribution of that variable but the relationship between that variable and the others you use to impute. Doing so can lead to imputing outliers, creating more bias than just imputing the skewed variable. Use more imputations. The advice for years has been that 5-10 imputations are adequate. And while this is true for unbiasedness, you can get inconsistent results if you run the multiple imputation more than once. (Bodner 2008) recommends having as many imputations as the percentage of missing data. Since running more imputations isnt any more work for the data analyst, theres no reason not to. Create multiplicative terms before imputing. When the analysis model contains a multiplicative term, like an interaction term or a quadratic, create the multiplicative terms first, then impute. Imputing first, and then creating the multiplicative terms actually biases the regression parameters of the multiplicative term (Hippel 2009) References "],["diagnosing-the-mechanism.html", "11.5 Diagnosing the Mechanism", " 11.5 Diagnosing the Mechanism 11.5.1 MAR vs. MNAR The only true way to distinguish between MNAR and MAR is to measure some of that missing data. Its a common practice among professional surveyors to, for example, follow-up on a paper survey with phone calls to a group of the non-respondents and ask a few key survey items. This allows you to compare respondents to non-respondents. If their responses on those key items differ by very much, thats good evidence that the data are MNAR. However in most missing data situations, we cant get a hold of the missing data. So while we cant test it directly, we can examine patterns in the data get an idea of whats the most likely mechanism. The first thing in diagnosing randomness of the missing data is to use your substantive scientific knowledge of the data and your field. The more sensitive the issue, the less likely people are to tell you. Theyre not going to tell you as much about their cocaine usage as they are about their phone usage. Likewise, many fields have common research situations in which non-ignorable data is common. Educate yourself in your fields literature. 11.5.2 MCAR vs. MAR There is a very useful test for MCAR, Littles test. A second technique is to create dummy variables for whether a variable is missing. 1 = missing 0 = observed You can then run t-tests and chi-square tests between this variable and other variables in the data set to see if the missingness on this variable is related to the values of other variables. For example, if women really are less likely to tell you their weight than men, a chi-square test will tell you that the percentage of missing data on the weight variable is higher for women than men. "],["application-6.html", "11.6 Application", " 11.6 Application How many imputation: usually 5. (unless you have extremely high portion of missing, in which case you probably need to check your data again) According to Rubin, the relative efficiency of an estimate based on m imputations to infinity imputation is approximately \\[ (1+\\frac{\\lambda}{m})^{-1} \\] where \\(\\lambda\\) is the rate of missing data Example: 50% of missing data means an estimate based on 5 imputation has standard deviation that is only 5% wider compared to an estimate based on infinity imputation (\\(\\sqrt{1+0.5/5}=1.049\\)) library(missForest) #load data data &lt;- iris #Generate 10% missing values at Random set.seed(1) iris.mis &lt;- prodNA(iris, noNA = 0.1) #remove categorical variables iris.mis.cat &lt;- iris.mis iris.mis &lt;- subset(iris.mis, select = -c(Species)) 11.6.1 Imputation with mean / median / mode # whole data set e1071::impute(iris.mis, what = &quot;mean&quot;) # replace with mean e1071::impute(iris.mis, what = &quot;median&quot;) # replace with median # by variables Hmisc::impute(iris.mis$Sepal.Length, mean) # mean Hmisc::impute(iris.mis$Sepal.Length, median) # median Hmisc::impute(iris.mis$Sepal.Length, 0) # replace specific number check accurary library(DMwR) actuals &lt;- iris$Sepal.Width[is.na(iris.mis$Sepal.Width)] predicteds &lt;- rep(mean(iris$Sepal.Width, na.rm=T), length(actuals)) regr.eval(actuals, predicteds) ## mae mse rmse mape ## 0.2870303 0.1301598 0.3607767 0.1021485 11.6.2 KNN library(DMwR) # iris.mis[,!names(iris.mis) %in% c(&quot;Sepal.Length&quot;)] # data should be this line. But since knn cant work with 3 or less variables, we need to use at least 4 variables. # knn is not appropriate for categorical variables knnOutput &lt;- knnImputation(data = iris.mis.cat, #k = 10, meth = &quot;median&quot; # could use &quot;median&quot; or &quot;weighAvg&quot; ) # should exclude the dependent variable: Sepal.Length anyNA(knnOutput) ## [1] FALSE library(DMwR) actuals &lt;- iris$Sepal.Width[is.na(iris.mis$Sepal.Width)] predicteds &lt;- knnOutput[is.na(iris.mis$Sepal.Width), &quot;Sepal.Width&quot;] regr.eval(actuals, predicteds) ## mae mse rmse mape ## 0.2318182 0.1038636 0.3222788 0.0823571 Compared to mape (mean absolute percentage error) of mean imputation, we see almost always see improvements. 11.6.3 rpart For categorical (factor) variables, rpart can handle library(rpart) class_mod &lt;- rpart(Species ~ . - Sepal.Length, data=iris.mis.cat[!is.na(iris.mis.cat$Species), ], method=&quot;class&quot;, na.action=na.omit) # since Species is a factor, and exclude dependent variable &quot;Sepal.Length&quot; anova_mod &lt;- rpart(Sepal.Width ~ . - Sepal.Length, data=iris.mis[!is.na(iris.mis$Sepal.Width), ], method=&quot;anova&quot;, na.action=na.omit) # since Sepal.Width is numeric. species_pred &lt;- predict(class_mod, iris.mis.cat[is.na(iris.mis.cat$Species), ]) width_pred &lt;- predict(anova_mod, iris.mis[is.na(iris.mis$Sepal.Width), ]) 11.6.4 MICE (Multivariate Imputation via Chained Equations) Assumption: data are MAR It imputes data per variable by specifying an imputation model for each variable Example We have \\(X_1, X_2,..,X_k\\). If \\(X_1\\) has missing data, then it is regressed on the rest of the variables. Same procedure applies if \\(X_2\\) has missing data. Then, predicted values are used in place of missing values. By default, Continuous variables use linear regression. Categorical Variables use logistic regression. Methods in MICE: PMM (Predictive Mean Matching)  For numeric variables logreg(Logistic Regression)  For Binary Variables( with 2 levels) polyreg(Bayesian polytomous regression)  For Factor Variables (&gt;= 2 levels) Proportional odds model (ordered, &gt;= 2 levels) # load package library(mice) library(VIM) # check missing values md.pattern(iris.mis) ## Sepal.Width Sepal.Length Petal.Length Petal.Width ## 100 1 1 1 1 0 ## 15 1 1 1 0 1 ## 8 1 1 0 1 1 ## 2 1 1 0 0 2 ## 11 1 0 1 1 1 ## 1 1 0 1 0 2 ## 1 1 0 0 1 2 ## 1 1 0 0 0 3 ## 7 0 1 1 1 1 ## 3 0 1 0 1 2 ## 1 0 0 1 1 2 ## 11 15 15 19 60 #plot the missing values aggr(iris.mis, col=mdc(1:2), numbers=TRUE, sortVars=TRUE, labels=names(iris.mis), cex.axis=.7, gap=3, ylab=c(&quot;Proportion of missingness&quot;,&quot;Missingness Pattern&quot;)) ## ## Variables sorted by number of missings: ## Variable Count ## Petal.Width 0.12666667 ## Sepal.Length 0.10000000 ## Petal.Length 0.10000000 ## Sepal.Width 0.07333333 mice_plot &lt;- aggr(iris.mis, col=c(&#39;navyblue&#39;,&#39;yellow&#39;), numbers=TRUE, sortVars=TRUE, labels=names(iris.mis), cex.axis=.7, gap=3, ylab=c(&quot;Missing data&quot;,&quot;Pattern&quot;)) ## ## Variables sorted by number of missings: ## Variable Count ## Petal.Width 0.12666667 ## Sepal.Length 0.10000000 ## Petal.Length 0.10000000 ## Sepal.Width 0.07333333 Impute Data imputed_Data &lt;- mice( iris.mis, m = 5, # number of imputed datasets maxit = 50, # number of iterations taken to impute missing values method = &#39;pmm&#39;, # method used in imputation. Here, we used predictive mean matching # other methods can be # &quot;pmm&quot;: Predictive mean matching # &quot;midastouch&quot; : weighted predictive mean matching # &quot;sample&quot;: Random sample from observed values # &quot;cart&quot;: classification and regression trees # &quot;rf&quot;: random forest imputations. # &quot;2lonly.pmm&quot;: Level-2 class predictive mean matching # Other methods based on whether variables are (1) numeric, (2) binary, (3) ordered, (4), unordered seed = 500 ) summary(imputed_Data) ## Class: mids ## Number of multiple imputations: 5 ## Imputation methods: ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &quot;pmm&quot; &quot;pmm&quot; &quot;pmm&quot; &quot;pmm&quot; ## PredictorMatrix: ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 0 1 1 1 ## Sepal.Width 1 0 1 1 ## Petal.Length 1 1 0 1 ## Petal.Width 1 1 1 0 #make a density plot densityplot(imputed_Data) #the red (imputed values) should be similar to the blue (observed) Check imputed dataset # 1st dataset completeData &lt;- complete(imputed_Data,1) # 2nd dataset complete(imputed_Data,2) Regression model using imputed datasets # regression model fit &lt;- with(data = imputed_Data, exp = lm(Sepal.Width ~ Sepal.Length + Petal.Width)) #combine results of all 5 models combine &lt;- pool(fit) summary(combine) ## term estimate std.error statistic df p.value ## 1 (Intercept) 1.8963130 0.32453912 5.843095 131.0856 3.838556e-08 ## 2 Sepal.Length 0.2974293 0.06679204 4.453066 130.2103 1.802241e-05 ## 3 Petal.Width -0.4811603 0.07376809 -6.522608 108.8253 2.243032e-09 11.6.5 Amelia Use bootstrap based EMB algorithm (faster and robust to impute many variables including cross sectional, time series data etc) Use parallel imputation feature using multicore CPUs. Assumptions All variables follow Multivariate Normal Distribution (MVN). Hence, this package works best when data is MVN, or transformation to normality. Missing data is Missing at Random (MAR) Steps: m bootstrap samples and applies EMB algorithm to each sample. Then we have m different estimates of mean and variances. the first set of estimates are used to impute first set of missing values using regression, then second set of estimates are used for second set and so on. However, Amelia is different from MICE MICE imputes data on variable by variable basis whereas MVN uses a joint modeling approach based on multivariate normal distribution. MICE can handle different types of variables while the variables in MVN need to be normally distributed or transformed to approximate normality. MICE can manage imputation of variables defined on a subset of data whereas MVN cannot. library(Amelia) data(&quot;iris&quot;) #seed 10% missing values iris.mis &lt;- prodNA(iris, noNA = 0.1) # idvars  keep all ID variables and other variables which you dont want to impute # noms  keep nominal variables here #specify columns and run amelia amelia_fit &lt;- amelia(iris.mis, m=5, parallel = &quot;multicore&quot;, noms = &quot;Species&quot;) ## -- Imputation 1 -- ## ## 1 2 3 4 5 6 7 8 ## ## -- Imputation 2 -- ## ## 1 2 3 4 5 6 7 8 ## ## -- Imputation 3 -- ## ## 1 2 3 4 5 ## ## -- Imputation 4 -- ## ## 1 2 3 4 5 6 7 ## ## -- Imputation 5 -- ## ## 1 2 3 4 5 6 7 # access imputed outputs # amelia_fit$imputations[[1]] 11.6.6 missForest an implementation of random forest algorithm (a non parametric imputation method applicable to various variable types). Hence, no assumption about function form of f. Instead, it tries to estimate f such that it can be as close to the data points as possible. builds a random forest model for each variable. Then it uses the model to predict missing values in the variable with the help of observed values. It yields out of bag imputation error estimate. Moreover, it provides high level of control on imputation process. Since bagging works well on categorical variable too, we dont need to remove them here. library(missForest) #impute missing values, using all parameters as default values iris.imp &lt;- missForest(iris.mis) ## missForest iteration 1 in progress...done! ## missForest iteration 2 in progress...done! ## missForest iteration 3 in progress...done! ## missForest iteration 4 in progress...done! # check imputed values # iris.imp$ximp # check imputation error # NRMSE is normalized mean squared error. It is used to represent error derived from imputing continuous values. # PFC (proportion of falsely classified) is used to represent error derived from imputing categorical values. iris.imp$OOBerror ## NRMSE PFC ## 0.13631893 0.04477612 #comparing actual data accuracy iris.err &lt;- mixError(iris.imp$ximp, iris.mis, iris) iris.err ## NRMSE PFC ## 0.1501524 0.0625000 This means categorical variables are imputed with 5% error and continuous variables are imputed with 14% error. This can be improved by tuning the values of mtry and ntree parameter. mtry refers to the number of variables being randomly sampled at each split. ntree refers to number of trees to grow in the forest. 11.6.7 Hmisc impute() function imputes missing value using user defined statistical method (mean, max, mean). Its default is median. aregImpute() allows mean imputation using additive regression, bootstrapping, and predictive mean matching. In bootstrapping, different bootstrap resamples are used for each of multiple imputations. Then, a flexible additive model (non parametric regression method) is fitted on samples taken with replacements from original data and missing values (acts as dependent variable) are predicted using non-missing values (independent variable). it uses predictive mean matching (default) to impute missing values. Predictive mean matching works well for continuous and categorical (binary &amp; multi-level) without the need for computing residuals and maximum likelihood fit. Note For predicting categorical variables, Fishers optimum scoring method is used. Hmisc automatically recognizes the variables types and uses bootstrap sample and predictive mean matching to impute missing values. missForest can outperform Hmisc if the observed variables have sufficient information. Assumption linearity in the variables being predicted. library(Hmisc) # impute with mean value iris.mis$imputed_age &lt;- with(iris.mis, impute(Sepal.Length, mean)) # impute with random value iris.mis$imputed_age2 &lt;- with(iris.mis, impute(Sepal.Length, &#39;random&#39;)) # could also use min, max, median to impute missing value # using argImpute impute_arg &lt;- aregImpute(~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width + Species, data = iris.mis, n.impute = 5) # argImpute() automatically identifies the variable type and treats them accordingly. ## Iteration 1 Iteration 2 Iteration 3 Iteration 4 Iteration 5 Iteration 6 Iteration 7 Iteration 8 impute_arg # R-squares are for predicted missing values. ## ## Multiple Imputation using Bootstrap and PMM ## ## aregImpute(formula = ~Sepal.Length + Sepal.Width + Petal.Length + ## Petal.Width + Species, data = iris.mis, n.impute = 5) ## ## n: 150 p: 5 Imputations: 5 nk: 3 ## ## Number of NAs: ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 11 11 13 24 16 ## ## type d.f. ## Sepal.Length s 2 ## Sepal.Width s 2 ## Petal.Length s 2 ## Petal.Width s 2 ## Species c 2 ## ## Transformation of Target Variables Forced to be Linear ## ## R-squares for Predicting Non-Missing Values for Each Variable ## Using Last Imputations of Predictors ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 0.907 0.660 0.978 0.963 0.993 # check imputed variable Sepal.Length impute_arg$imputed$Sepal.Length ## [,1] [,2] [,3] [,4] [,5] ## 19 5.2 5.2 5.2 5.8 5.7 ## 21 5.1 5.0 5.1 5.7 5.4 ## 31 4.8 5.0 5.2 5.0 4.8 ## 35 4.6 4.9 4.9 4.9 4.8 ## 49 5.0 5.1 5.1 5.1 5.1 ## 62 6.2 5.7 6.0 6.4 5.6 ## 65 5.5 5.5 5.2 5.8 5.5 ## 67 6.5 5.8 5.8 6.3 6.5 ## 82 5.2 5.1 5.7 5.8 5.5 ## 113 6.4 6.5 7.4 7.2 6.3 ## 122 6.2 5.8 5.5 5.8 6.7 11.6.8 mi allows graphical diagnostics of imputation models and convergence of imputation process. uses Bayesian version of regression models to handle issue of separation. automatically detects irregularities in data (e.g., high collinearity among variables). adds noise to imputation process to solve the problem of additive constraints. library(mi) # default values of parameters # 1. rand.imp.method as bootstrap # 2. n.imp (number of multiple imputations) as 3 # 3. n.iter ( number of iterations) as 30 mi_data &lt;- mi(iris.mis, seed = 335) summary(mi_data) ## $Sepal.Length ## $Sepal.Length$is_missing ## missing ## FALSE TRUE ## 139 11 ## ## $Sepal.Length$imputed ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.45626 -0.08236 -0.01041 -0.01500 0.06622 0.29015 ## ## $Sepal.Length$observed ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.92742 -0.45370 -0.03919 0.00000 0.31610 1.20432 ## ## ## $Sepal.Width ## $Sepal.Width$is_missing ## missing ## FALSE TRUE ## 139 11 ## ## $Sepal.Width$imputed ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.83720 -0.22331 0.06717 0.13373 0.45523 1.30866 ## ## $Sepal.Width$observed ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.2494 -0.3068 -0.0712 0.0000 0.2823 1.3427 ## ## ## $Petal.Length ## $Petal.Length$is_missing ## missing ## FALSE TRUE ## 137 13 ## ## $Petal.Length$imputed ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.84561 -0.59824 -0.03837 -0.12928 0.23146 0.60995 ## ## $Petal.Length$observed ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.7892 -0.6196 0.1721 0.0000 0.3701 0.8790 ## ## ## $Petal.Width ## $Petal.Width$is_missing ## missing ## FALSE TRUE ## 126 24 ## ## $Petal.Width$imputed ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.98946 -0.57303 0.02056 -0.05426 0.39918 0.77380 ## ## $Petal.Width$observed ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.72386 -0.59081 0.07444 0.00000 0.40707 0.87275 ## ## ## $Species ## $Species$crosstab ## ## observed imputed ## setosa 184 19 ## versicolor 188 14 ## virginica 164 31 ## ## ## $imputed_age ## $imputed_age$is_missing ## [1] &quot;all values observed&quot; ## ## $imputed_age$observed ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.9637 -0.4714 0.0000 0.0000 0.3285 1.2514 ## ## ## $imputed_age2 ## $imputed_age2$is_missing ## [1] &quot;all values observed&quot; ## ## $imputed_age2$observed ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.92932 -0.45738 -0.04444 0.00000 0.35375 1.19439 "],["analysis-of-variance-anova.html", "Chapter 12 Analysis of Variance (ANOVA)", " Chapter 12 Analysis of Variance (ANOVA) ANOVA is using the same underlying mechanism as linear regression. However, the angle that ANOVA chooses to look at is slightly different from the traditional linear regression. It can be more useful in the case with qualitative variables and designed experiments. Experimental Design Factor: explanatory or predictor variable to be studied in an investigation Treatment (or Factor Level): value of a factor applied to the experimental unit Experimental Unit: person, animal, piece of material, etc. that is subjected to treatment(s) and provides a response Single Factor Experiment: one explanatory variable considered Multifactor Experiment: more than one explanatory variable Classification Factor: A factor that is not under the control of the experimenter (observational data) Experimental Factor: assigned by the experimenter Basics of experimental design: Choices that a statistician has to make: set of treatments set of experimental units treatment assignment (selection bias) measurement (measurement bias, blind experiments) Advancements in experimental design: Factorial Experiments: consider multiple factors at the same time (interaction) Replication: repetition of experiment assess mean squared error control over precision of experiment (power) Randomization Before R.A. Fisher (1900s), treatments were assigned systematically or subjectively randomization: assign treatments to experimental units at random, which averages out systematic effects that cannot be control by the investigator Local control: Blocking or Stratification Reduce experimental errors and increase power by placing restrictions on the randomization of treatments to experimental units. Randomization may also eliminate correlations due to time and space. "],["completely-randomized-design-crd.html", "12.1 Completely Randomized Design (CRD)", " 12.1 Completely Randomized Design (CRD) Treatment factor A with \\(a\\ge2\\) treatments levels. Experimental units are randomly assinged to each treatment. The number of experiemntal units in each group can be equal (balanced): n unequal (unbalanced): \\(n_i\\) for the i-th group (i = 1,,a). The total sample size is \\(N=\\sum_{i=1}^{a}n_i\\) Possible assignments of units to treatments are \\(k=\\frac{N!}{n_1!n_2!...n_a!}\\) Each has probability 1/k of being selected. Each experimental unit is measured with a response \\(Y_{ij}\\), in which j denotes unit and i denotes treatment. Treatment 1 2  a \\(Y_{11}\\) \\(Y_{21}\\)  \\(Y_{a1}\\) \\(Y_{12}\\)        Sample Mean \\(\\bar{Y_{1.}}\\) \\(\\bar{Y_{2.}}\\)  \\(\\bar{Y_{a.}}\\) Sample SD \\(s_1\\) \\(s_2\\)  \\(s_a\\) where \\(\\bar{Y_{i.}}=\\frac{1}{n_i}\\sum_{j=1}^{n_i}Y_{ij}\\) \\(s_i^2=\\frac{1}{n_i-1}\\sum_{j=1}^{n_i}(Y_{ij}-\\bar{Y_i})^2\\) And the grand mean is \\(\\bar{Y_{..}}=\\frac{1}{N}\\sum_{i}\\sum_{j}Y_{ij}\\) 12.1.1 Single Factor (One-Way) ANOVA Partitioning the Variance The total variability of the \\(Y_{ij}\\) observation can be measured as the deviation of \\(Y_{ij}\\) around the overall mean \\(\\bar{Y_{..}}\\): \\(Y_{ij} - \\bar{Y_{..}}\\) This can be rewritten as: \\[ \\begin{split} Y_{ij} - \\bar{Y_{..}}&amp;=Y_{ij} - \\bar{Y_{..}} + \\bar{Y_{i.}} - \\bar{Y_{i.}} \\\\ &amp;= (\\bar{Y_{i.}}-\\bar{Y_{..}})+(Y_{ij}-\\bar{Y_{i.}}) \\end{split} \\] where the first term is the between treatment differences (i.e., the deviation of the treatment mean from the overall mean) the second term is within treatment differences (i.e., the deviation of the observation around its treatment mean) \\[ \\begin{split} \\sum_{i}\\sum_{j}(Y_{ij} - \\bar{Y_{..}})^2 &amp;= \\sum_{i}n_i(\\bar{Y_{i.}}-\\bar{Y_{..}})^2+\\sum_{i}\\sum_{j}(Y_{ij}-\\bar{Y_{i.}})^2 \\\\ SSTO &amp;= SSTR + SSE \\\\ total~SS &amp;= treatment~SS + error~SS \\\\ (N-1)~d.f. &amp;= (a-1)~d.f. + (N - a) ~ d.f. \\end{split} \\] we lose a d.f. for the total corrected SSTO because of the estimation of the mean (\\(\\sum_{i}\\sum_{j}(Y_{ij} - \\bar{Y_{..}})=0\\)) And, for the SSTR \\(\\sum_{i}n_i(\\bar{Y_{i.}}-\\bar{Y_{..}})=0\\) Accordingly, \\(MSTR= \\frac{SST}{a-1}\\) and \\(MSR=\\frac{SSE}{N-a}\\) ANOVA Table Source of Variation SS df MS Between Treatments \\(\\sum_{i}n_i(\\bar{Y_{i.}}-\\bar{Y_{..}})^2\\) a-1 SSTR/(a-1) Error (within treatments) \\(\\sum_{i}\\sum_{j}(Y_{ij}-\\bar{Y_{i.}})^2\\) N-a SSE/(N-a) Total (corrected) \\(\\sum_{i}n_i(\\bar{Y_{i.}}-\\bar{Y_{..}})^2\\) N-1 Linear Model Explanation of ANOVA 12.1.1.1 Cell means model \\(Y_{ij}=\\mu_i+\\epsilon_{ij}\\) where * \\(Y_{ij}\\) response variable in j-th subject for the i-th treatment * \\(\\mu_i\\): parameters (fixed) representing the unknown population mean for the i-th treatment * \\(\\epsilon_{ij}\\) independent \\(N(0,\\sigma^2)\\) errors \\(E(Y_{ij})=\\mu_i\\) \\(var(Y_{ij})=var(\\epsilon_{ij})=\\sigma^2\\) All observations have the same variance Example: a = 3 (3 treatments) \\(n_1=n_2=n_3=2\\) \\[ \\begin{split} \\left(\\begin{array}{c} Y_{11}\\\\ Y_{12}\\\\ Y_{21}\\\\ Y_{22}\\\\ Y_{31}\\\\ Y_{32}\\\\ \\end{array}\\right) &amp;= \\left(\\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\end{array}\\right) \\left(\\begin{array}{c} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\\\ \\end{array}\\right) + \\left(\\begin{array}{c} \\epsilon_{11} \\\\ \\epsilon_{12} \\\\ \\epsilon_{21} \\\\ \\epsilon_{22} \\\\ \\epsilon_{31} \\\\ \\epsilon_{32} \\\\ \\end{array}\\right)\\\\ \\mathbf{y} &amp;= \\mathbf{X\\beta} +\\mathbf{\\epsilon} \\end{split} \\] \\(X_{k,ij}=1\\) if the k-th treatment is used \\(X_{k,ij}=0\\) Otherwise Note: no intercept term. \\[\\begin{equation} \\begin{split} \\mathbf{b}= \\left[\\begin{array}{c} \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\\\ \\end{array}\\right] &amp;= (\\mathbf{x}&#39;\\mathbf{x})^{-1}\\mathbf{x}&#39;\\mathbf{y} \\\\ &amp; = \\left[\\begin{array}{ccc} n_1 &amp; 0 &amp; 0\\\\ 0 &amp; n_2 &amp; 0\\\\ 0 &amp; 0 &amp; n_3 \\\\ \\end{array}\\right]^{-1} \\left[\\begin{array}{c} Y_1\\\\ Y_2\\\\ Y_3\\\\ \\end{array}\\right] \\\\ &amp; = \\left[\\begin{array}{c} \\bar{Y_1}\\\\ \\bar{Y_2}\\\\ \\bar{Y_3}\\\\ \\end{array}\\right] \\end{split} \\tag{12.1} \\end{equation}\\] is the BLUE (best linear unbiased estimator) for \\(\\beta=[\\mu_1 \\mu_2\\mu_3]&#39;\\) \\[E(\\mathbf{b})=\\beta\\] \\[ var(\\mathbf{b})=\\sigma^2(\\mathbf{X&#39;X})^{-1}=\\sigma^2 \\left[\\begin{array}{ccc} 1/n_1 &amp; 0 &amp; 0\\\\ 0 &amp; 1/n_2 &amp; 0\\\\ 0 &amp; 0 &amp; 1/n_3\\\\ \\end{array}\\right] \\] \\(var(b_i)=var(\\hat{\\mu_i})=\\sigma^2/n_i\\) where \\(\\mathbf{b} \\sim N(\\beta,\\sigma^2(\\mathbf{X&#39;X})^{-1})\\) \\[ \\begin{split} MSE &amp;= \\frac{1}{N-a} \\sum_{i}\\sum_{j}(Y_{ij}-\\bar{Y_{i.}})^2 \\\\ &amp;= \\frac{1}{N-a} \\sum_{i}[(n_i-1)\\frac{\\sum_{i}(Y_{ij}-\\bar{Y_{i.}})^2}{n_i-1}] \\\\ &amp;= \\frac{1}{N-a} \\sum_{i}(n_i-1)s_1^2 \\end{split} \\] We have \\(E(s_i^2)=\\sigma^2\\) \\(E(MSE)=\\frac{1}{N-a}\\sum_{i}(n_i-1)\\sigma^2=\\sigma^2\\) Hence, MSE is an unbiased estimator of \\(\\sigma^2\\), regardless of whether the treatment means are equal or not. \\(E(MSTR)=\\sigma^2+\\frac{\\sum_{i}n_i(\\mu_i-\\mu_.)^2}{a-1}\\) where \\(\\mu_.=\\frac{\\sum_{i=1}^{a}n_i\\mu_i}{\\sum_{i=1}^{a}n_i}\\) If all treatment means are equals (=\\(\\mu_.\\)), \\(E(MSTR)=\\sigma^2\\). Then we can use an F-test for teh equality of all treatment means: \\[H_0:\\mu_1=\\mu_2=..=\\mu_a\\] \\[H_a: not~al l~ \\mu_i ~ are ~ equal \\] \\(F=\\frac{MSTR}{MSE}\\) where large values of F support \\(H_a\\) (since MSTR will tend to exceed MSE when \\(H_a\\) holds) and F near 1 support \\(H_0\\) (upper tail test) Equivalently, when \\(H_0\\) is true, \\(F \\sim f_{(a-1,N-a)}\\) If \\(F \\leq f_{(a-1,N-a;1-\\alpha)}\\), we cannot reject \\(H_0\\) If \\(F \\geq f_{(a-1,N-a;1-\\alpha)}\\), we reject \\(H_0\\) Note: If a = 2 (2 treatments), F-test = two sample t-test 12.1.1.2 Treatment Effects (Factor Effects) Besides Cell means model, we have another way to formalize one-way ANOVA: \\[Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\] where \\(Y_{ij}\\) is the j-th response for the i-th treatment \\(\\tau_i\\) i-th treatment effect \\(\\mu\\) constant component, common to all observations \\(\\epsilon_{ij}\\) independent random errors ~ \\(N(0,\\sigma^2)\\) For example, a = 3, \\(n_1=n_2=n_3=2\\) \\[\\begin{equation} \\begin{split} \\left(\\begin{array}{c} Y_{11}\\\\ Y_{12}\\\\ Y_{21}\\\\ Y_{22}\\\\ Y_{31}\\\\ Y_{32}\\\\ \\end{array}\\right) &amp;= \\left(\\begin{array}{cccc} 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{array}\\right) \\left(\\begin{array}{c} \\mu \\\\ \\tau_1 \\\\ \\tau_2 \\\\ \\tau_3\\\\ \\end{array}\\right) + \\left(\\begin{array}{c} \\epsilon_{11} \\\\ \\epsilon_{12} \\\\ \\epsilon_{21} \\\\ \\epsilon_{22} \\\\ \\epsilon_{31} \\\\ \\epsilon_{32} \\\\ \\end{array}\\right)\\\\ \\mathbf{y} &amp;= \\mathbf{X\\beta} +\\mathbf{\\epsilon} \\end{split} \\tag{12.2} \\end{equation}\\] However, \\[ \\mathbf{X&#39;X} = \\left(\\begin{array}{cccc} \\sum_{i}n_i &amp; n_1 &amp; n_2 &amp; n_3 \\\\ n_1 &amp; n_1 &amp; 0 &amp; 0 \\\\ n_2 &amp; 0 &amp; n_2 &amp; 0 \\\\ n_3 &amp; 0 &amp; 0 &amp; n_3 \\\\ \\end{array}\\right) \\] is singular thus does not exist, \\(\\mathbf{b}\\) is insolvable (infinite solutions) Hence, we have to impose restrictions on the parameters to a model matrix \\(\\mathbf{X}\\) of full rank. Whatever restriction we use, we still have: \\(E(Y_{ij})=\\mu + \\tau_i = \\mu_i = mean ~ response ~ for ~ i-th ~ treatment\\) 12.1.1.2.1 Restriction on sum of tau \\(\\sum_{i=1}^{a}\\tau_i=0\\) implies \\[ \\mu= \\mu +\\frac{1}{a}\\sum_{i=1}^{a}(\\mu+\\tau_i) \\] is the average of the treatment mean (grand mean) (overall mean) \\[ \\begin{split} \\tau_i &amp;=(\\mu+\\tau_i) -\\mu = \\mu_i-\\mu \\\\ &amp;= (treatment ~ mean) -(grand~mean) \\\\ &amp;= treatment ~ effect \\end{split} \\] \\[ \\tau_a=-\\tau_1-\\tau_2-...-\\tau_{a-1} \\] Hence, the mean for the a-th treatment is \\[ \\mu_a=\\mu+\\tau_a=\\mu-\\tau_1-\\tau_2-...-\\tau_{a-1} \\] Hence, the model need only a parameters: \\[ \\mu,\\tau_1,\\tau_2,..,\\tau_{a-1} \\] Equation (12.2) becomes \\[\\begin{equation} \\begin{split} \\left(\\begin{array}{c} Y_{11}\\\\ Y_{12}\\\\ Y_{21}\\\\ Y_{22}\\\\ Y_{31}\\\\ Y_{32}\\\\ \\end{array}\\right) &amp;= \\left(\\begin{array}{cccc} 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; -1 &amp; -1 \\\\ 1 &amp; -1 &amp; -1 \\\\ \\end{array}\\right) \\left(\\begin{array}{c} \\mu \\\\ \\tau_1 \\\\ \\tau_2 \\\\ \\end{array}\\right) + \\left(\\begin{array}{c} \\epsilon_{11} \\\\ \\epsilon_{12} \\\\ \\epsilon_{21} \\\\ \\epsilon_{22} \\\\ \\epsilon_{31} \\\\ \\epsilon_{32} \\\\ \\end{array}\\right)\\\\ \\mathbf{y} &amp;= \\mathbf{X\\beta} +\\mathbf{\\epsilon} \\end{split} \\end{equation}\\] where \\(\\beta\\equiv[\\mu,\\tau_1,\\tau_2]&#39;\\) Equation (12.1) with \\(\\sum_{i}\\tau_i=0\\) becomes \\[ \\begin{equation} \\begin{split} \\mathbf{b}= \\left[\\begin{array}{c} \\hat{\\mu} \\\\ \\hat{\\tau_1} \\\\ \\hat{\\tau_2} \\\\ \\end{array}\\right] &amp;= (\\mathbf{x}&#39;\\mathbf{x})^{-1}\\mathbf{x}&#39;\\mathbf{y} \\\\ &amp; = \\left[\\begin{array}{ccc} \\sum_{i}n_i &amp; n_1-n_3 &amp; n_2-n_3\\\\ n_1-n_3 &amp; n_1+n_3 &amp; n_3\\\\ n_2-n_3 &amp; n_3 &amp; n_2-n_3 \\\\ \\end{array}\\right]^{-1} \\left[\\begin{array}{c} Y_{..}\\\\ Y_{1.}-Y_{3.}\\\\ Y_{2.}-Y_{3.}\\\\ \\end{array}\\right] \\\\ &amp; = \\left[\\begin{array}{c} \\frac{1}{3}\\sum_{i=1}^{3}\\bar{Y_{i.}}\\\\ \\bar{Y_{1.}}-\\frac{1}{3}\\sum_{i=1}^{3}\\bar{Y_{i.}}\\\\ \\bar{Y_{2.}}-\\frac{1}{3}\\sum_{i=1}^{3}\\bar{Y_{i.}}\\\\ \\end{array}\\right]\\\\ &amp; = \\left[\\begin{array}{c} \\hat{\\mu}\\\\ \\hat{\\tau_1}\\\\ \\hat{\\tau_2}\\\\ \\end{array}\\right] \\end{split} \\end{equation} \\] and \\(\\hat{\\tau_3}=-\\hat{\\tau_1}-\\hat{\\tau_2}=\\bar{Y_3}-\\frac{1}{3} \\sum_{i}\\bar{Y_{i.}}\\) 12.1.1.2.2 Restriction on first tau In R, lm() uses the restriction \\(\\tau_1=0\\) For the previous example, for \\(n_1=n_2=n_3=2\\), and \\(\\tau_1=0\\). Then the treatment means can be written as: \\[ \\mu_1= \\mu + \\tau_1 = \\mu + 0 = \\mu \\\\ \\mu_2= \\mu + \\tau_2 \\\\ \\mu_3 = \\mu + \\tau_3 \\] Hence, \\(\\mu\\) is the mean response for the first treatment In the matrix form, \\[ \\begin{equation} \\begin{split} \\left(\\begin{array}{c} Y_{11}\\\\ Y_{12}\\\\ Y_{21}\\\\ Y_{22}\\\\ Y_{31}\\\\ Y_{32}\\\\ \\end{array}\\right) &amp;= \\left(\\begin{array}{cccc} 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 1 \\\\ \\end{array}\\right) \\left(\\begin{array}{c} \\mu \\\\ \\tau_2 \\\\ \\tau_3 \\\\ \\end{array}\\right) + \\left(\\begin{array}{c} \\epsilon_{11} \\\\ \\epsilon_{12} \\\\ \\epsilon_{21} \\\\ \\epsilon_{22} \\\\ \\epsilon_{31} \\\\ \\epsilon_{32} \\\\ \\end{array}\\right)\\\\ \\mathbf{y} &amp;= \\mathbf{X\\beta} +\\mathbf{\\epsilon} \\end{split} \\end{equation} \\] \\(\\beta = [\\mu,\\tau_2,\\tau_3]&#39;\\) \\[ \\begin{equation} \\begin{split} \\mathbf{b}= \\left[\\begin{array}{c} \\hat{\\mu} \\\\ \\hat{\\tau_2} \\\\ \\hat{\\tau_3} \\\\ \\end{array}\\right] &amp;= (\\mathbf{x}&#39;\\mathbf{x})^{-1}\\mathbf{x}&#39;\\mathbf{y} \\\\ &amp; = \\left[\\begin{array}{ccc} \\sum_{i}n_i &amp; n_2 &amp; n_3\\\\ n_2 &amp; n_2 &amp; 0\\\\ n_3 &amp; 0 &amp; n_3 \\\\ \\end{array}\\right]^{-1} \\left[\\begin{array}{c} Y_{..}\\\\ Y_{2.}\\\\ Y_{3.}\\\\ \\end{array}\\right] \\\\ &amp; = \\left[ \\begin{array}{c} \\bar{Y_{1.}} \\\\ \\bar{Y_{2.}} - \\bar{Y_{1.}} \\\\ \\bar{Y_{3.}} - \\bar{Y_{1.}}\\\\ \\end{array}\\right] \\end{split} \\end{equation} \\] \\[ E(\\mathbf{b})= \\beta = \\left[\\begin{array}{c} {\\mu}\\\\ {\\tau_2}\\\\ {\\tau_3}\\\\ \\end{array}\\right] = \\left[\\begin{array}{c} \\mu_1\\\\ \\mu_2-\\mu_1\\\\ \\mu_3-\\mu_1\\\\ \\end{array}\\right] \\] \\[ var(\\mathbf{b}) = \\sigma^2(\\mathbf{X&#39;X})^{-1} \\\\ var(\\hat{\\mu}) = var(\\bar{Y_{1.}})=\\sigma^2/n_1 \\\\ var(\\hat{\\tau_2}) = var(\\bar{Y_{2.}}-\\bar{Y_{1.}}) = \\sigma^2/n_2 + \\sigma^2/n_1 \\\\ var(\\hat{\\tau_3}) = var(\\bar{Y_{3.}}-\\bar{Y_{1.}}) = \\sigma^2/n_3 + \\sigma^2/n_1 \\] Note For all three parameterization, the ANOVA table is the same Model 1: \\(Y_{ij} = \\mu_i + \\epsilon_{ij}\\) Model 2: \\(Y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\\) where \\(\\sum_{i} \\tau_i=0\\) Model 3: \\(Y_{ij}= \\mu + \\tau_i + \\epsilon_{ij}\\) where \\(\\tau_1=0\\) All models have the same calculation for \\(\\hat{Y}\\) as \\[ \\mathbf{\\hat{Y} = X(X&#39;X)^{-1}X&#39;Y=PY = Xb} \\] ANOVA Table Source of Variation SS df MS F Between Treatments \\(\\sum_{i}n_i(\\bar{Y_{i.}}-\\bar{Y_{..}})^2 = \\mathbf{Y&#39;(P-P_1)Y}\\) a-1 SSTR/(a-1) MSTR/MSE Error (within treatments) \\(\\sum_{i}\\sum_{j}(Y_{ij}-\\bar{Y_{i.}})^2=\\mathbf{e&#39;e}\\) N-a SSE/(N-a) Total (corrected) \\(\\sum_{i}n_i(\\bar{Y_{i.}}-\\bar{Y_{..}})^2=\\mathbf{Y&#39;Y - Y&#39;P_1Y}\\) N-1 where \\(\\mathbf{P_1} = \\frac{1}{n}\\mathbf{J}\\) The F-statistic here has (a-1,N-a) degrees of freedom, which gives the same value for all three parameterization, but the hypothesis test is written a bit different: \\[ H_0 : \\mu_1 = \\mu_2 = ... = \\mu_a \\\\ H_0 : \\mu + \\tau_1 = \\mu + tau_2 = ... = \\mu + \\tau_a \\\\ H_0 : \\tau_1 = \\tau_2 = ...= \\tau_a \\] The F-test here serves as a preliminary analysis, to see if there is any difference at different factors. For more in-depth analysis, we consider different testing of treatment effects. 12.1.1.3 Testing of Treatment Effects A Single Treatment Mean \\(\\mu_i\\) A Differences Between Treatment Means A contrast among treatment means A linear combination of treatment means 12.1.1.3.1 Single Treatment Mean We have \\(\\hat{\\mu_i}=\\bar{Y_{i.}}\\) where \\(E(\\bar{Y_{i.}})=\\mu_i\\) \\(var(\\bar{Y_{i}})=\\sigma^2/n_i\\) estimated by \\(s^2(\\bar{Y_{i.}})=MSE / n_i\\) Since \\(\\frac{\\bar{Y_{i.}}-\\mu_i}{s(\\bar{Y_{i.}})} \\sim t_{N-a}\\) and the confidence interval for \\(\\mu_i\\) is \\(\\bar{Y_{i.}} \\pm t_{1-\\alpha/2;N-a}s(\\bar{Y_{i.}})\\), then we can do a t-test for the means difference with some constant c \\[ H_0: \\mu_i = c \\\\ H_1: \\mu_i \\neq c \\] where \\[ T =\\frac{\\bar{Y_{i.}}-c}{s(\\bar{Y_{i.}})} \\] follows \\(t_{N-a}\\) when \\(H_0\\) is true. If \\(|T| &gt; t_{1-\\alpha/2;N-a}\\), we can reject \\(H_0\\) 12.1.1.3.2 Differences Between Treatment Means Let \\(D=\\mu_i - \\mu_i&#39;\\), also known as pairwise comparison \\(D\\) can be estimated by \\(\\hat{D}=\\bar{Y_{i}}-\\bar{Y_{i}}&#39;\\) is unbiased (\\(E(\\hat{D})=\\mu_i-\\mu_i&#39;\\)) Since \\(\\bar{Y_{i}}\\) and \\(\\bar{Y_{i}}&#39;\\) are independent, then \\[ var(\\hat{D})=var(\\bar{Y_{i}}) + var(\\bar{Y_{i&#39;}}) = \\sigma^2(1/n_i + 1/n_i&#39;) \\] can be estimated with \\[ s^2(\\hat{D}) = MSE(1/n_i + 1/n_i&#39;) \\] slide 45 12.1.2 Two Factor Fixed Effect ANOVA 12.1.2.1 Balanced 12.1.2.2 Unbalanced "],["deep-learning.html", "Chapter 13 Deep Learning ", " Chapter 13 Deep Learning "],["overview.html", "13.1 Overview", " 13.1 Overview This section is based on (???) What is deep learning? Input to output via layers of representation What are layers? A layer is a geometric transformation function on the data that goes through it Weights determine the data transformation behavior of a layer Learning Representation Transforming input data into useful representation The deep in deep learning multiple layers Other possibly more appropriate names for the field: Layered representations learning Hierarchical representations learning Chained geometric transformation learning New problem domains for R: Computer vision Computer speech recognition Reinforcement learning applications How do we train deep learning models? Basic of machine learning algorithms Machine learning vs. statistical modeling MNIST example Model definition in R Layers of representation The training loop Machine learning algorithms Learning model parameters via exposure to many example data points Statistics: Often focused on inferring the process by which data is generated Machine Learning: Principally focused on predicting future data Deep learning frontiers Computer vision Natural language processing Time series Biomedical "],["tensor-flow-and-r.html", "13.2 Tensor Flow and R", " 13.2 Tensor Flow and R TensorFlow APIs Keras API Estimator API Core API 13.2.1 R packages TensorFlow APIs * keras: Interface for neural networks, with a focus on enabling fast experimentation. * tfestimators: Implementations of common model types such as regressors and classifiers. * tensorflow: Low level interface to the TensorFlow computational graph. tfdatasets: Scalable input pipelines for TensorFlow models. Supporting Tools * tfruns : Track , visualize, and manage TensorFlow training runs and experiments * tfdeploy : Tools designed to make exporting and serving TensorFlow models straightforward. * cloudml : R interface to Google Cloud Machine Learning Engine. R interface to Keras Step by step example Keras layers Compiling models Losses, Optimizers, and Metrics More examples 13.2.2 Keras layers 65 layers available Dense layers: classic fully connected neural network layers Convolutional layers: \"Filters for learning local patterns in data Recurrent layers: Layers that maintain state based on on previously seen data Embedding layers: Vectorization of text that reflects semantic relationships between words "],["compiling-models.html", "13.3 Compiling models", " 13.3 Compiling models Model compilation prepares the model for training by: 1. Converting the layers into a TensorFlow graph 2. Applying the specified loss function and optimizer 3. Arranging for the collection of metrics during training "],["losses-optimizers-and-metrics.html", "13.4 Losses, Optimizers, and Metrics", " 13.4 Losses, Optimizers, and Metrics All available at Keras for R cheatsheet Example of Image classificaiton "],["nyu.html", "13.5 NYU", " 13.5 NYU Materials in this chapter is based on NYU Deep Learning Deep in Deep Learning means multi-layers. A deep network has several layers and uses them to build a hierarchy of features of increasing complexity Supervised Learning (= Function Optimization): With inputs and outputs, you train the machine to tweak its parameter to have the correct outputs from inputs. In traditional machine learning, you have to engineer feature extractor, but in deep learning you can multi-layers feature extractor can be trained. And all layers are non-linear, because if they are linear, then all layers would be collapse into one layer. Natural Data is compositional. Hence, it is efficiently representable hierarchically. Revolutions in Deep Learning: Speech Recognition: 2010 Image Recognition: 2013 Natural Language Processing (NLP): 2015 Deep Learning = Learning Representations/ Features Representation means you turn raw data into something useful Image Recognition: Pixel, edge, texton, motif, part, object Text Character, group, word group, clause, sentence, story Speech Sample, spectral band, sound, phone, phoneme, word. Difference between Deep Learning Support-Vector Machines (SVM) SVM is a 2-layer neural net, where layer 1 = templates layer 2 = linear classifier. where Deep Learning is A deep network has several layers and uses them to build a hierarchy of features of increasing complexity Deep Machines are more efficient than SVM in representing certain classes of functions. The Manifold Hypothesis states that real-world high-dimensional data lie on low-dimensional manifolds embedded within the high-dimensional space Difference between Deep Learning and PCA (Principal Components Analysis) is that Deep learning uses non-linear function, while PCA uses linear function. "],["causality.html", "Chapter 14 Causality", " Chapter 14 Causality After all of the mumbo jumbo that we have learned so far, I want to now talk about the concept of causality. We usually say that correlation is not causation. Then, what is causation? One of my favorite books has explained this concept beautifully (Mackenzie and Pearl 2018). And I am just going to quickly summarize the gist of it from my understanding. I hope that it can give you an initial grasp on the concept so that later you can continue to read up and develop a deeper understanding. Its important to have a deep understanding regarding the method research. However, one needs to be aware of its limitation and compliment with conceptual understanding. The aspect of concepts is typically referred in statistics when as expert knowledge. As mentioned in various sections throughout the book, we see that we need to ask experts for number as our baseline or visit literature to gain insight from past research. Here, we dive in a more conceptual side statistical analysis as a whole, regardless of particular approach. References "],["report.html", "Chapter 15 Report", " Chapter 15 Report This chapter is based on the jtools package. More information can be found here. "],["one-summary-table.html", "15.1 One summary table", " 15.1 One summary table library(jtools) data(movies) fit &lt;- lm(metascore ~ budget + us_gross + year, data = movies) summ(fit) Observations 831 (10 missing obs. deleted) Dependent variable metascore Type OLS linear regression F(3,827) 26.23 R² 0.09 Adj. R² 0.08 Est. S.E. t val. p (Intercept) 52.06 139.67 0.37 0.71 budget -0.00 0.00 -5.89 0.00 us_gross 0.00 0.00 7.61 0.00 year 0.01 0.07 0.08 0.94 Standard errors: OLS summ(fit, scale = TRUE, vifs = TRUE, part.corr = TRUE, confint = TRUE, pvals = FALSE) #notice that scale here is TRUE Observations 831 (10 missing obs. deleted) Dependent variable metascore Type OLS linear regression F(3,827) 26.23 R² 0.09 Adj. R² 0.08 Est. 2.5% 97.5% t val. VIF partial.r part.r (Intercept) 63.01 61.91 64.11 112.23 NA NA NA budget -3.78 -5.05 -2.52 -5.89 1.31 -0.20 -0.20 us_gross 5.28 3.92 6.64 7.61 1.52 0.26 0.25 year 0.05 -1.18 1.28 0.08 1.24 0.00 0.00 Standard errors: OLS; Continuous predictors are mean-centered and scaled by 1 s.d. #obtain clsuter-robust SE data(&quot;PetersenCL&quot;, package = &quot;sandwich&quot;) fit2 &lt;- lm(y ~ x, data = PetersenCL) summ(fit2, robust = &quot;HC3&quot;, cluster = &quot;firm&quot;) Observations 5000 Dependent variable y Type OLS linear regression F(1,4998) 1310.74 R² 0.21 Adj. R² 0.21 Est. S.E. t val. p (Intercept) 0.03 0.07 0.44 0.66 x 1.03 0.05 20.36 0.00 Standard errors: Cluster-robust, type = HC3 "],["model-comparison.html", "15.2 Model Comparison", " 15.2 Model Comparison fit &lt;- lm(metascore ~ log(budget), data = movies) fit_b &lt;- lm(metascore ~ log(budget) + log(us_gross), data = movies) fit_c &lt;- lm(metascore ~ log(budget) + log(us_gross) + runtime, data = movies) coef_names &lt;- c(&quot;Budget&quot; = &quot;log(budget)&quot;, &quot;US Gross&quot; = &quot;log(us_gross)&quot;, &quot;Runtime (Hours)&quot; = &quot;runtime&quot;, &quot;Constant&quot; = &quot;(Intercept)&quot;) export_summs(fit, fit_b, fit_c, robust = &quot;HC3&quot;, coefs = coef_names) Table 15.1: Model 1Model 2Model 3 Budget-2.43 ***-5.16 ***-6.70 *** (0.44)&nbsp;&nbsp;&nbsp;(0.62)&nbsp;&nbsp;&nbsp;(0.67)&nbsp;&nbsp;&nbsp; US Gross&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.96 ***3.85 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(0.51)&nbsp;&nbsp;&nbsp;(0.48)&nbsp;&nbsp;&nbsp; Runtime (Hours)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;14.29 *** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1.63)&nbsp;&nbsp;&nbsp; Constant105.29 ***81.84 ***83.35 *** (7.65)&nbsp;&nbsp;&nbsp;(8.66)&nbsp;&nbsp;&nbsp;(8.82)&nbsp;&nbsp;&nbsp; N831&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;831&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;831&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R20.03&nbsp;&nbsp;&nbsp;&nbsp;0.09&nbsp;&nbsp;&nbsp;&nbsp;0.17&nbsp;&nbsp;&nbsp;&nbsp; Standard errors are heteroskedasticity robust. *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. "],["changes-in-an-estimate.html", "15.3 Changes in an estimate", " 15.3 Changes in an estimate coef_names &lt;- coef_names[1:3] # Dropping intercept for plots plot_summs(fit, fit_b, fit_c, robust = &quot;HC3&quot;, coefs = coef_names) plot_summs(fit_c, robust = &quot;HC3&quot;, coefs = coef_names, plot.distributions = TRUE) "],["research-methods.html", "Chapter 16 Research Methods ", " Chapter 16 Research Methods "],["but-for-world.html", "16.1 But-for World", " 16.1 But-for World (Mahajan, Sharma, and Buzzell 1993) (Landsman and Stremersch 2020) References "],["appendix.html", "A Appendix", " A Appendix # to see non-scientific notation a result format(12e-17, scientific = FALSE) ## [1] &quot;0.00000000000000012&quot; "],["short-cut.html", "A.1 Short-cut", " A.1 Short-cut These are shortcuts that you probably you remember when working with R. Even though it might take a bit of time to learn and use them as your second nature, but they will save you a lot of time. Just like learning another language, the more you speak and practice it, the more comfortable you are speaking it. function short-cut navigate folders in console \" \" + tab pull up short-cut cheat sheet ctrl + shift + k go to file/function (everything in your project) ctrl + . search everything cmd + shift + f navigate between tabs Crtl + shift + . type function faster snip + shift + tab type faster use tab for fuzzy match cmd + up ctrl + . Sometimes you cant stage a folder because its too large. In such case, use Terminal pane in Rstudio then type git add -A to stage all changes then commit and push like usual. "],["function-short-cut.html", "A.2 Function short-cut", " A.2 Function short-cut apply one function to your data to create a new variable: mutate(mod=map(data,function)) instead of using i in 1:length(object): for (i in seq_along(object)) apply multiple function: map_dbl apply multiple function to multiple variables:map2 autoplot(data) plot times series data mod_tidy = linear(reg) %&gt;% set_engine('lm') %&gt;% fit(price ~ ., data=data) fit lm model. It could also fit other models (stan, spark, glmnet, keras) Sometimes, data-masking will not be able to recognize whether youre calling from environment or data variables. To bypass this, we use .data$variable or .env$variable. For example data %&gt;% mutate(x=.env$variable/.data$variable Problems with data-masking: + Unexpected masking by data-var: Use .data and .env to disambiguate + Data-var cant get through: + Tunnel data-var with {{}} + Subset .data with [[]] Passing Data-variables through arguments library(&quot;dplyr&quot;) mean_by &lt;- function(data,by,var){ data %&gt;% group_by({{{by}}}) %&gt;% summarise(&quot;{{var}}&quot;:=mean({{var}})) # new name for each var will be created by tunnel data-var inside strings } mean_by &lt;- function(data,by,var){ data %&gt;% group_by({{{by}}}) %&gt;% summarise(&quot;{var}&quot;:=mean({{var}})) # use single {} to glue the string, but hard to reuse code in functions } Trouble with selection: library(&quot;purrr&quot;) name &lt;- c(&quot;mass&quot;,&quot;height&quot;) starwars %&gt;% select(name) # Data-var. Here you are referring to variable named &quot;name&quot; starwars %&gt;% select(all_of((name))) # use all_of() to disambiguate when averages &lt;- function(data,vars){ # take character vectors with all_of() data %&gt;% select(all_of(vars)) %&gt;% map_dbl(mean,na.rm=TRUE) } x = c(&quot;Sepal.Length&quot;,&quot;Petal.Length&quot;) iris %&gt;% averages(x) # Another way averages &lt;- function(data,vars){ # Tunnel selectiosn with {{}} data %&gt;% select({{vars}}) %&gt;% map_dbl(mean,na.rm=TRUE) } x = c(&quot;Sepal.Length&quot;,&quot;Petal.Length&quot;) iris %&gt;% averages(x) "],["citation.html", "A.3 Citation", " A.3 Citation include a citation by [@Farjam_2015] cite packages used in this session package=ls(sessionInfo()$loadedOnly) for (i in package){print(toBibtex(citation(i)))} package=ls(sessionInfo()$loadedOnly) for (i in package){ print(toBibtex(citation(i))) } "],["bookdown-cheat-sheet.html", "B Bookdown cheat sheet ", " B Bookdown cheat sheet "],["math-expresssion-syntax.html", "B.1 Math Expresssion/ Syntax", " B.1 Math Expresssion/ Syntax Full list Aligning equations \\begin{align*} a &amp; = b \\\\ X &amp;\\sim {\\sf Norm}(10, 3) \\\\ 5 &amp; \\le 10 \\end{align*} \\[\\begin{align*} a &amp; = b \\\\ X &amp;\\sim {\\sf Norm}(10, 3) \\\\ 5 &amp; \\le 10 \\end{align*}\\] Syntax Notation Math $\\pm$ \\(\\pm\\) $\\ge$ \\(\\ge\\) $\\le$ \\(\\le\\) $\\neq$ \\(\\neq\\) $\\equiv$ \\(\\equiv\\) $^\\circ$ \\(^\\circ\\) $\\times$ \\(\\times\\) $\\cdot$ \\(\\cdot\\) $\\leq$ \\(\\leq\\) $\\geq$ \\(\\geq\\) \\propto \\(\\propto\\) $\\subset$ \\(\\subset\\) $\\subseteq$ \\(\\subseteq\\) $\\leftarrow$ \\(\\leftarrow\\) $\\rightarrow$ \\(\\rightarrow\\) $\\Leftarrow$ \\(\\Leftarrow\\) $\\Rightarrow$ \\(\\Rightarrow\\) $\\approx$ \\(\\approx\\) $\\mathbb{R}$ \\(\\mathbb{R}\\) $\\sum_{n=1}^{10} n^2$ \\(\\sum_{n=1}^{10} n^2\\) $$\\sum_{n=1}^{10} n^2$$ \\[\\sum_{n=1}^{10} n^2\\] $x^{n}$ \\(x^{n}\\) $x_{n}$ \\(x_{n}\\) $\\overline{x}$ \\(\\overline{x}\\) $\\hat{x}$ \\(\\hat{x}\\) $\\tilde{x}$ \\(\\tilde{x}\\) \\check{} \\(\\check{}\\) \\underset{\\gamma}{\\operatorname{argmin}} \\(\\underset{\\gamma}{\\operatorname{argmin}}\\) $\\frac{a}{b}$ \\(\\frac{a}{b}\\) $\\frac{a}{b}$ \\(\\frac{a}{b}\\) $\\displaystyle \\frac{a}{b}$ \\(\\displaystyle \\frac{a}{b}\\) $\\binom{n}{k}$ \\(\\binom{n}{k}\\) $x_{1} + x_{2} + \\cdots + x_{n}$ \\(x_{1} + x_{2} + \\cdots + x_{n}\\) $x_{1}, x_{2}, \\dots, x_{n}$ \\(x_{1}, x_{2}, \\dots, x_{n}\\) \\mathbf{x} = \\langle x_{1}, x_{2}, \\dots, x_{n}\\rangle$ \\(\\mathbf{x} = \\langle x_{1}, x_{2}, \\dots, x_{n}\\rangle\\) $x \\in A$ \\(x \\in A\\) $|A|$ \\(|A|\\) $x \\in A$ \\(x \\in A\\) $x \\subset B$ \\(x \\subset B\\) $x \\subseteq B$ \\(x \\subseteq B\\) $A \\cup B$ \\(A \\cup B\\) $A \\cap B$ \\(A \\cap B\\) $X \\sim {\\sf Binom}(n, \\pi)$ \\(X \\sim {\\sf Binom}(n, \\pi)\\) $\\mathrm{P}(X \\le x) = {\\tt pbinom}(x, n, \\pi)$ \\(\\mathrm{P}(X \\le x) = {\\tt pbinom}(x, n, \\pi)\\) $P(A \\mid B)$ \\(P(A \\mid B)\\) $\\mathrm{P}(A \\mid B)$ \\(\\mathrm{P}(A \\mid B)\\) $\\{1, 2, 3\\}$ \\(\\{1, 2, 3\\}\\) $\\sin(x)$ \\(\\sin(x)\\) $\\log(x)$ \\(\\log(x)\\) $\\int_{a}^{b}$ \\(\\int_{a}^{b}\\) $\\left(\\int_{a}^{b} f(x) \\; dx\\right)$ \\(\\left(\\int_{a}^{b} f(x) \\; dx\\right)\\) $\\left[\\int_{\\-infty}^{\\infty} f(x) \\; dx\\right]$ \\(\\left[\\int_{-\\infty}^{\\infty} f(x) \\; dx\\right]\\) $\\left. F(x) \\right|_{a}^{b}$ \\(\\left. F(x) \\right|_{a}^{b}\\) $\\sum_{x = a}^{b} f(x)$ \\(\\sum_{x = a}^{b} f(x)\\) $\\prod_{x = a}^{b} f(x)$ \\(\\prod_{x = a}^{b} f(x)\\) $\\lim_{x \\to \\infty} f(x)$ \\(\\lim_{x \\to \\infty} f(x)\\) $\\displaystyle \\lim_{x \\to \\infty} f(x)$ \\(\\displaystyle \\lim_{x \\to \\infty} f(x)\\) Greek Letters $\\alpha A$ \\(\\alpha A\\) $\\beta B$ \\(\\beta B\\) $\\gamma \\Gamma$ \\(\\gamma \\Gamma\\) $\\delta \\Delta$ \\(\\delta \\Delta\\) $\\epsilon \\varepsilon E$ \\(\\epsilon \\varepsilon E\\) $\\zeta Z \\sigma \\,\\!$ \\(\\zeta Z \\sigma \\,\\!\\) $\\eta H$ \\(\\eta H\\) $\\theta \\vartheta \\Theta$ \\(\\theta \\vartheta \\Theta\\) $\\iota I$ \\(\\iota I\\) $\\kappa K$ \\(\\kappa K\\) $\\lambda \\Lambda$ \\(\\lambda \\Lambda\\) $\\mu M$ \\(\\mu M\\) $\\nu N$ \\(\\nu N\\) $\\xi\\Xi$ \\(\\xi\\Xi\\) $o O$ \\(o O\\) $\\pi \\Pi$ \\(\\pi \\Pi\\) $\\rho\\varrho P$ \\(\\rho\\varrho P\\) $\\sigma \\Sigma$ \\(\\sigma \\Sigma\\) $\\tau T$ \\(\\tau T\\) $\\upsilon \\Upsilon$ \\(\\upsilon \\Upsilon\\) $\\phi \\varphi \\Phi$ \\(\\phi \\varphi \\Phi\\) $\\chi X$ \\(\\chi X\\) $\\psi \\Psi$ \\(\\psi \\Psi\\) $\\omega \\Omega$ \\(\\omega \\Omega\\) Limit P(\\lim_{n\\to \\infty}\\bar{X}_n =\\mu) =1 \\[ P(\\lim_{n\\to \\infty}\\bar{X}_n =\\mu) =1 \\] Matrices $$\\begin{array} {rrr} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{array} $$ \\[\\begin{array} {rrr} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{array} \\] $$\\mathbf{X} = \\left[\\begin{array} {rrr} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{array}\\right] $$ \\[\\mathbf{X} = \\left[\\begin{array} {rrr} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{array}\\right] \\] Aligning Equations Aligning Equations with Comments \\begin{align} 3+x &amp;=4 &amp;&amp; \\text{(Solve for} x \\text{.)}\\\\ x &amp;=4-3 &amp;&amp; \\text{(Subtract 3 from both sides.)}\\\\ x &amp;=1 &amp;&amp; \\text{(Yielding the solution.)} \\end{align} \\[\\begin{align} 3+x &amp;=4 &amp;&amp; \\text{(Solve for} x \\text{.)}\\\\ x &amp;=4-3 &amp;&amp; \\text{(Subtract 3 from both sides.)}\\\\ x &amp;=1 &amp;&amp; \\text{(Yielding the solution.)} \\end{align}\\] B.1.1 Statistics Notation $$ f(y|N,p) = \\frac{N!}{y!(N-y)!}\\cdot p^y \\cdot (1-p)^{N-y} = {{N}\\choose{y}} \\cdot p^y \\cdot (1-p)^{N-y} $$ \\[ f(y|N,p) = \\frac{N!}{y!(N-y)!}\\cdot p^y \\cdot (1-p)^{N-y} = {{N}\\choose{y}} \\cdot p^y \\cdot (1-p)^{N-y} \\] \\begin{cases} \\frac{1}{b-a}&amp;\\text{for $x\\in[a,b]$}\\\\ 0&amp;\\text{otherwise}\\\\ \\end{cases} \\[\\begin{cases} \\frac{1}{b-a}&amp;\\text{for $x\\in[a,b]$}\\\\ 0&amp;\\text{otherwise}\\\\ \\end{cases}\\] "],["table.html", "B.2 Table", " B.2 Table +---------------+---------------+--------------------+ | Fruit | Price | Advantages | +===============+===============+====================+ | *Bananas* | $1.34 | - built-in wrapper | | | | - bright color | +---------------+---------------+--------------------+ | Oranges | $2.10 | - cures scurvy | | | | - **tasty** | +---------------+---------------+--------------------+ Fruit Price Advantages Bananas $1.34 built-in wrapper bright color Oranges $2.10 cures scurvy tasty (\\mathbf{x}^T\\mathbf{x})^{-1}\\mathbf{x}^T\\mathbf{y} \\((\\mathbf{x}^T\\mathbf{x})^{-1}\\mathbf{x}^T\\mathbf{y}\\) "],["references.html", "References", " References "]]
