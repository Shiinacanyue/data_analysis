<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>11.2 Solutions to Missing data | A Guide on Data Analysis</title>
  <meta name="description" content="This is a guide on how to conduct data analysis in the field of data science, statistics, or machine learning." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="11.2 Solutions to Missing data | A Guide on Data Analysis" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://bookdown.org/mike/data_analysis/" />
  <meta property="og:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg" />
  <meta property="og:description" content="This is a guide on how to conduct data analysis in the field of data science, statistics, or machine learning." />
  <meta name="github-repo" content="mikenguyen13/data_analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="11.2 Solutions to Missing data | A Guide on Data Analysis" />
  
  <meta name="twitter:description" content="This is a guide on how to conduct data analysis in the field of data science, statistics, or machine learning." />
  <meta name="twitter:image" content="https://bookdown.org/mike/data_analysis//images/cover.jpg" />

<meta name="author" content="Mike Nguyen" />


<meta name="date" content="2022-09-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  <link rel="apple-touch-icon-precomposed" sizes="120x120" href="logo.png" />
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="assumptions-1.html"/>
<link rel="next" href="criteria-for-choosing-an-effective-approach.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/proj4js-2.3.15/proj4.js"></script>
<link href="libs/highcharts-9.3.1/css/motion.css" rel="stylesheet" />
<script src="libs/highcharts-9.3.1/highcharts.js"></script>
<script src="libs/highcharts-9.3.1/highcharts-3d.js"></script>
<script src="libs/highcharts-9.3.1/highcharts-more.js"></script>
<script src="libs/highcharts-9.3.1/modules/stock.js"></script>
<script src="libs/highcharts-9.3.1/modules/map.js"></script>
<script src="libs/highcharts-9.3.1/modules/data.js"></script>
<script src="libs/highcharts-9.3.1/modules/exporting.js"></script>
<script src="libs/highcharts-9.3.1/modules/offline-exporting.js"></script>
<script src="libs/highcharts-9.3.1/modules/drilldown.js"></script>
<script src="libs/highcharts-9.3.1/modules/item-series.js"></script>
<script src="libs/highcharts-9.3.1/modules/overlapping-datalabels.js"></script>
<script src="libs/highcharts-9.3.1/modules/annotations.js"></script>
<script src="libs/highcharts-9.3.1/modules/export-data.js"></script>
<script src="libs/highcharts-9.3.1/modules/funnel.js"></script>
<script src="libs/highcharts-9.3.1/modules/heatmap.js"></script>
<script src="libs/highcharts-9.3.1/modules/treemap.js"></script>
<script src="libs/highcharts-9.3.1/modules/sankey.js"></script>
<script src="libs/highcharts-9.3.1/modules/dependency-wheel.js"></script>
<script src="libs/highcharts-9.3.1/modules/organization.js"></script>
<script src="libs/highcharts-9.3.1/modules/solid-gauge.js"></script>
<script src="libs/highcharts-9.3.1/modules/streamgraph.js"></script>
<script src="libs/highcharts-9.3.1/modules/sunburst.js"></script>
<script src="libs/highcharts-9.3.1/modules/vector.js"></script>
<script src="libs/highcharts-9.3.1/modules/wordcloud.js"></script>
<script src="libs/highcharts-9.3.1/modules/xrange.js"></script>
<script src="libs/highcharts-9.3.1/modules/tilemap.js"></script>
<script src="libs/highcharts-9.3.1/modules/venn.js"></script>
<script src="libs/highcharts-9.3.1/modules/gantt.js"></script>
<script src="libs/highcharts-9.3.1/modules/timeline.js"></script>
<script src="libs/highcharts-9.3.1/modules/parallel-coordinates.js"></script>
<script src="libs/highcharts-9.3.1/modules/bullet.js"></script>
<script src="libs/highcharts-9.3.1/modules/coloraxis.js"></script>
<script src="libs/highcharts-9.3.1/modules/dumbbell.js"></script>
<script src="libs/highcharts-9.3.1/modules/lollipop.js"></script>
<script src="libs/highcharts-9.3.1/modules/series-label.js"></script>
<script src="libs/highcharts-9.3.1/plugins/motion.js"></script>
<script src="libs/highcharts-9.3.1/custom/reset.js"></script>
<script src="libs/highcharts-9.3.1/modules/boost.js"></script>
<script src="libs/highchart-binding-0.9.4/highchart.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=GA_MEASUREMENT_ID"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){window.dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-DMNX2X65HQ');
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Guide on Data Analysis</a></li>

<li class="divider"></li>
<li><a href="index.html#preface" id="toc-preface">Preface</a></li>
<li><a href="introduction.html#introduction" id="toc-introduction"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="prerequisites.html#prerequisites" id="toc-prerequisites"><span class="toc-section-number">2</span> Prerequisites</a>
<ul>
<li><a href="matrix-theory.html#matrix-theory" id="toc-matrix-theory"><span class="toc-section-number">2.1</span> Matrix Theory</a>
<ul>
<li><a href="matrix-theory.html#rank" id="toc-rank"><span class="toc-section-number">2.1.1</span> Rank</a></li>
<li><a href="matrix-theory.html#inverse" id="toc-inverse"><span class="toc-section-number">2.1.2</span> Inverse</a></li>
<li><a href="matrix-theory.html#definiteness" id="toc-definiteness"><span class="toc-section-number">2.1.3</span> Definiteness</a></li>
<li><a href="matrix-theory.html#matrix-calculus" id="toc-matrix-calculus"><span class="toc-section-number">2.1.4</span> Matrix Calculus</a></li>
<li><a href="matrix-theory.html#optimization" id="toc-optimization"><span class="toc-section-number">2.1.5</span> Optimization</a></li>
</ul></li>
<li><a href="probability-theory.html#probability-theory" id="toc-probability-theory"><span class="toc-section-number">2.2</span> Probability Theory</a>
<ul>
<li><a href="probability-theory.html#axiom-and-theorems-of-probability" id="toc-axiom-and-theorems-of-probability"><span class="toc-section-number">2.2.1</span> Axiom and Theorems of Probability</a></li>
<li><a href="probability-theory.html#central-limit-theorem" id="toc-central-limit-theorem"><span class="toc-section-number">2.2.2</span> Central Limit Theorem</a></li>
<li><a href="probability-theory.html#random-variable" id="toc-random-variable"><span class="toc-section-number">2.2.3</span> Random variable</a></li>
<li><a href="probability-theory.html#moment-generating-function" id="toc-moment-generating-function"><span class="toc-section-number">2.2.4</span> Moment generating function</a></li>
<li><a href="probability-theory.html#moment" id="toc-moment"><span class="toc-section-number">2.2.5</span> Moment</a></li>
<li><a href="probability-theory.html#distributions" id="toc-distributions"><span class="toc-section-number">2.2.6</span> Distributions</a></li>
</ul></li>
<li><a href="general-math.html#general-math" id="toc-general-math"><span class="toc-section-number">2.3</span> General Math</a>
<ul>
<li><a href="general-math.html#law-of-large-numbers" id="toc-law-of-large-numbers"><span class="toc-section-number">2.3.1</span> Law of large numbers</a></li>
<li><a href="general-math.html#law-of-iterated-expectation" id="toc-law-of-iterated-expectation"><span class="toc-section-number">2.3.2</span> Law of Iterated Expectation</a></li>
<li><a href="general-math.html#convergence" id="toc-convergence"><span class="toc-section-number">2.3.3</span> Convergence</a></li>
<li><a href="general-math.html#sufficient-statistics" id="toc-sufficient-statistics"><span class="toc-section-number">2.3.4</span> Sufficient Statistics</a></li>
<li><a href="general-math.html#parameter-transformations" id="toc-parameter-transformations"><span class="toc-section-number">2.3.5</span> Parameter transformations</a></li>
</ul></li>
<li><a href="data-importexport.html#data-importexport" id="toc-data-importexport"><span class="toc-section-number">2.4</span> Data Import/Export</a>
<ul>
<li><a href="data-importexport.html#medium-size" id="toc-medium-size"><span class="toc-section-number">2.4.1</span> Medium size</a></li>
<li><a href="data-importexport.html#large-size" id="toc-large-size"><span class="toc-section-number">2.4.2</span> Large size</a></li>
</ul></li>
<li><a href="data-manipulation.html#data-manipulation" id="toc-data-manipulation"><span class="toc-section-number">2.5</span> Data Manipulation</a></li>
</ul></li>
<li><a href="#part-i.-basic" id="toc-part-i.-basic">(PART*) I. BASIC</a></li>
<li><a href="descriptive-stat.html#descriptive-stat" id="toc-descriptive-stat"><span class="toc-section-number">3</span> Descriptive Statistics</a>
<ul>
<li><a href="numerical-measures.html#numerical-measures" id="toc-numerical-measures"><span class="toc-section-number">3.1</span> Numerical Measures</a></li>
<li><a href="graphical-measures.html#graphical-measures" id="toc-graphical-measures"><span class="toc-section-number">3.2</span> Graphical Measures</a>
<ul>
<li><a href="graphical-measures.html#shape" id="toc-shape"><span class="toc-section-number">3.2.1</span> Shape</a></li>
<li><a href="graphical-measures.html#scatterplot" id="toc-scatterplot"><span class="toc-section-number">3.2.2</span> Scatterplot</a></li>
</ul></li>
<li><a href="normality-assessment.html#normality-assessment" id="toc-normality-assessment"><span class="toc-section-number">3.3</span> Normality Assessment</a>
<ul>
<li><a href="normality-assessment.html#graphical-assessment" id="toc-graphical-assessment"><span class="toc-section-number">3.3.1</span> Graphical Assessment</a></li>
<li><a href="normality-assessment.html#summary-statistics" id="toc-summary-statistics"><span class="toc-section-number">3.3.2</span> Summary Statistics</a></li>
</ul></li>
<li><a href="bivariate-statistics.html#bivariate-statistics" id="toc-bivariate-statistics"><span class="toc-section-number">3.4</span> Bivariate Statistics</a></li>
<li><a href="two-continuous.html#two-continuous" id="toc-two-continuous"><span class="toc-section-number">3.5</span> Two Continuous</a>
<ul>
<li><a href="two-continuous.html#pearson-correlation" id="toc-pearson-correlation"><span class="toc-section-number">3.5.1</span> Pearson Correlation</a></li>
<li><a href="two-continuous.html#spearman-correlation" id="toc-spearman-correlation"><span class="toc-section-number">3.5.2</span> Spearman Correlation</a></li>
</ul></li>
<li><a href="categorical-and-continuous.html#categorical-and-continuous" id="toc-categorical-and-continuous"><span class="toc-section-number">3.6</span> Categorical and Continuous</a>
<ul>
<li><a href="categorical-and-continuous.html#point-biserial-correlation" id="toc-point-biserial-correlation"><span class="toc-section-number">3.6.1</span> Point-Biserial Correlation</a></li>
<li><a href="categorical-and-continuous.html#logistic-regression" id="toc-logistic-regression"><span class="toc-section-number">3.6.2</span> Logistic Regression</a></li>
</ul></li>
<li><a href="two-discrete.html#two-discrete" id="toc-two-discrete"><span class="toc-section-number">3.7</span> Two Discrete</a>
<ul>
<li><a href="two-discrete.html#distance-metrics" id="toc-distance-metrics"><span class="toc-section-number">3.7.1</span> Distance Metrics</a></li>
<li><a href="two-discrete.html#statistical-metrics" id="toc-statistical-metrics"><span class="toc-section-number">3.7.2</span> Statistical Metrics</a></li>
<li><a href="two-discrete.html#ordinal-association-rank-correlation" id="toc-ordinal-association-rank-correlation"><span class="toc-section-number">3.7.3</span> Ordinal Association (Rank correlation)</a></li>
<li><a href="two-discrete.html#summary-1" id="toc-summary-1"><span class="toc-section-number">3.7.4</span> Summary</a></li>
<li><a href="two-discrete.html#visualization" id="toc-visualization"><span class="toc-section-number">3.7.5</span> Visualization</a></li>
</ul></li>
</ul></li>
<li><a href="basic-statistical-inference.html#basic-statistical-inference" id="toc-basic-statistical-inference"><span class="toc-section-number">4</span> Basic Statistical Inference</a>
<ul>
<li><a href="one-sample-inference.html#one-sample-inference" id="toc-one-sample-inference"><span class="toc-section-number">4.1</span> One Sample Inference</a>
<ul>
<li><a href="one-sample-inference.html#the-mean" id="toc-the-mean"><span class="toc-section-number">4.1.1</span> The Mean</a></li>
<li><a href="one-sample-inference.html#single-variance" id="toc-single-variance"><span class="toc-section-number">4.1.2</span> Single Variance</a></li>
<li><a href="one-sample-inference.html#single-proportion-p" id="toc-single-proportion-p"><span class="toc-section-number">4.1.3</span> Single Proportion (p)</a></li>
<li><a href="one-sample-inference.html#power" id="toc-power"><span class="toc-section-number">4.1.4</span> Power</a></li>
<li><a href="one-sample-inference.html#sample-size" id="toc-sample-size"><span class="toc-section-number">4.1.5</span> Sample Size</a></li>
<li><a href="one-sample-inference.html#note" id="toc-note"><span class="toc-section-number">4.1.6</span> Note</a></li>
<li><a href="one-sample-inference.html#one-sample-non-parametric-methods" id="toc-one-sample-non-parametric-methods"><span class="toc-section-number">4.1.7</span> One-sample Non-parametric Methods</a></li>
</ul></li>
<li><a href="two-sample-inference.html#two-sample-inference" id="toc-two-sample-inference"><span class="toc-section-number">4.2</span> Two Sample Inference</a>
<ul>
<li><a href="two-sample-inference.html#means" id="toc-means"><span class="toc-section-number">4.2.1</span> Means</a></li>
<li><a href="two-sample-inference.html#variances" id="toc-variances"><span class="toc-section-number">4.2.2</span> Variances</a></li>
<li><a href="two-sample-inference.html#power-1" id="toc-power-1"><span class="toc-section-number">4.2.3</span> Power</a></li>
<li><a href="two-sample-inference.html#sample-size-1" id="toc-sample-size-1"><span class="toc-section-number">4.2.4</span> Sample Size</a></li>
<li><a href="two-sample-inference.html#matched-pair-designs" id="toc-matched-pair-designs"><span class="toc-section-number">4.2.5</span> Matched Pair Designs</a></li>
<li><a href="two-sample-inference.html#nonparametric-tests-for-two-samples" id="toc-nonparametric-tests-for-two-samples"><span class="toc-section-number">4.2.6</span> Nonparametric Tests for Two Samples</a></li>
</ul></li>
<li><a href="categorical-data-analysis.html#categorical-data-analysis" id="toc-categorical-data-analysis"><span class="toc-section-number">4.3</span> Categorical Data Analysis</a>
<ul>
<li><a href="categorical-data-analysis.html#inferences-for-small-samples" id="toc-inferences-for-small-samples"><span class="toc-section-number">4.3.1</span> Inferences for Small Samples</a></li>
<li><a href="categorical-data-analysis.html#test-of-association" id="toc-test-of-association"><span class="toc-section-number">4.3.2</span> Test of Association</a></li>
<li><a href="categorical-data-analysis.html#ordinal-association" id="toc-ordinal-association"><span class="toc-section-number">4.3.3</span> Ordinal Association</a></li>
</ul></li>
<li><a href="divergence-metrics-and-test-for-comparing-distributions.html#divergence-metrics-and-test-for-comparing-distributions" id="toc-divergence-metrics-and-test-for-comparing-distributions"><span class="toc-section-number">4.4</span> Divergence Metrics and Test for Comparing Distributions</a>
<ul>
<li><a href="divergence-metrics-and-test-for-comparing-distributions.html#kullback-leibler-divergence" id="toc-kullback-leibler-divergence"><span class="toc-section-number">4.4.1</span> Kullback-Leibler Divergence</a></li>
<li><a href="divergence-metrics-and-test-for-comparing-distributions.html#jensen-shannon-divergence" id="toc-jensen-shannon-divergence"><span class="toc-section-number">4.4.2</span> Jensen-Shannon Divergence</a></li>
<li><a href="divergence-metrics-and-test-for-comparing-distributions.html#wasserstein-distance" id="toc-wasserstein-distance"><span class="toc-section-number">4.4.3</span> Wasserstein Distance</a></li>
<li><a href="divergence-metrics-and-test-for-comparing-distributions.html#kolmogorov-smirnov-test-1" id="toc-kolmogorov-smirnov-test-1"><span class="toc-section-number">4.4.4</span> Kolmogorov-Smirnov Test</a></li>
</ul></li>
</ul></li>
<li><a href="#part-ii.-regression" id="toc-part-ii.-regression">(PART*) II. REGRESSION</a></li>
<li><a href="linear-regression.html#linear-regression" id="toc-linear-regression"><span class="toc-section-number">5</span> Linear Regression</a>
<ul>
<li><a href="ordinary-least-squares.html#ordinary-least-squares" id="toc-ordinary-least-squares"><span class="toc-section-number">5.1</span> Ordinary Least Squares</a>
<ul>
<li><a href="ordinary-least-squares.html#simple-regression-basic-model" id="toc-simple-regression-basic-model"><span class="toc-section-number">5.1.1</span> Simple Regression (Basic Model)</a></li>
<li><a href="ordinary-least-squares.html#multiple-linear-regression" id="toc-multiple-linear-regression"><span class="toc-section-number">5.1.2</span> Multiple Linear Regression</a></li>
<li><a href="ordinary-least-squares.html#ols-assumptions" id="toc-ols-assumptions"><span class="toc-section-number">5.1.3</span> OLS Assumptions</a></li>
<li><a href="ordinary-least-squares.html#theorems" id="toc-theorems"><span class="toc-section-number">5.1.4</span> Theorems</a></li>
<li><a href="ordinary-least-squares.html#variable-selection" id="toc-variable-selection"><span class="toc-section-number">5.1.5</span> Variable Selection</a></li>
<li><a href="ordinary-least-squares.html#diagnostics-1" id="toc-diagnostics-1"><span class="toc-section-number">5.1.6</span> Diagnostics</a></li>
<li><a href="ordinary-least-squares.html#model-validation" id="toc-model-validation"><span class="toc-section-number">5.1.7</span> Model Validation</a></li>
<li><a href="ordinary-least-squares.html#finite-sample-properties" id="toc-finite-sample-properties"><span class="toc-section-number">5.1.8</span> Finite Sample Properties</a></li>
<li><a href="ordinary-least-squares.html#large-sample-properties" id="toc-large-sample-properties"><span class="toc-section-number">5.1.9</span> Large Sample Properties</a></li>
</ul></li>
<li><a href="feasible-generalized-least-squares.html#feasible-generalized-least-squares" id="toc-feasible-generalized-least-squares"><span class="toc-section-number">5.2</span> Feasible Generalized Least Squares</a>
<ul>
<li><a href="feasible-generalized-least-squares.html#heteroskedasticity" id="toc-heteroskedasticity"><span class="toc-section-number">5.2.1</span> Heteroskedasticity</a></li>
<li><a href="feasible-generalized-least-squares.html#serial-correlation" id="toc-serial-correlation"><span class="toc-section-number">5.2.2</span> Serial Correlation</a></li>
</ul></li>
<li><a href="weighted-least-squares.html#weighted-least-squares" id="toc-weighted-least-squares"><span class="toc-section-number">5.3</span> Weighted Least Squares</a></li>
<li><a href="generalized-least-squares.html#generalized-least-squares" id="toc-generalized-least-squares"><span class="toc-section-number">5.4</span> Generalized Least Squares</a></li>
<li><a href="feasiable-prais-winsten.html#feasiable-prais-winsten" id="toc-feasiable-prais-winsten"><span class="toc-section-number">5.5</span> Feasiable Prais Winsten</a></li>
<li><a href="feasible-group-level-random-effects.html#feasible-group-level-random-effects" id="toc-feasible-group-level-random-effects"><span class="toc-section-number">5.6</span> Feasible group level Random Effects</a></li>
<li><a href="ridge-regression.html#ridge-regression" id="toc-ridge-regression"><span class="toc-section-number">5.7</span> Ridge Regression</a></li>
<li><a href="principal-component-regression.html#principal-component-regression" id="toc-principal-component-regression"><span class="toc-section-number">5.8</span> Principal Component Regression</a></li>
<li><a href="robust-regression.html#robust-regression" id="toc-robust-regression"><span class="toc-section-number">5.9</span> Robust Regression</a>
<ul>
<li><a href="robust-regression.html#least-absolute-residuals-lar-regression" id="toc-least-absolute-residuals-lar-regression"><span class="toc-section-number">5.9.1</span> Least Absolute Residuals (LAR) Regression</a></li>
<li><a href="robust-regression.html#least-median-of-squares-lms-regression" id="toc-least-median-of-squares-lms-regression"><span class="toc-section-number">5.9.2</span> Least Median of Squares (LMS) Regression</a></li>
<li><a href="robust-regression.html#iteratively-reweighted-least-squares-irls-robust-regression" id="toc-iteratively-reweighted-least-squares-irls-robust-regression"><span class="toc-section-number">5.9.3</span> Iteratively Reweighted Least Squares (IRLS) Robust Regression</a></li>
</ul></li>
<li><a href="maximum-likelihood-regression.html#maximum-likelihood-regression" id="toc-maximum-likelihood-regression"><span class="toc-section-number">5.10</span> Maximum Likelihood</a>
<ul>
<li><a href="maximum-likelihood-regression.html#motivation-for-mle" id="toc-motivation-for-mle"><span class="toc-section-number">5.10.1</span> Motivation for MLE</a></li>
<li><a href="maximum-likelihood-regression.html#assumption" id="toc-assumption"><span class="toc-section-number">5.10.2</span> Assumption</a></li>
<li><a href="maximum-likelihood-regression.html#properties" id="toc-properties"><span class="toc-section-number">5.10.3</span> Properties</a></li>
<li><a href="maximum-likelihood-regression.html#compare-to-ols" id="toc-compare-to-ols"><span class="toc-section-number">5.10.4</span> Compare to OLS</a></li>
<li><a href="maximum-likelihood-regression.html#application" id="toc-application"><span class="toc-section-number">5.10.5</span> Application</a></li>
</ul></li>
</ul></li>
<li><a href="non-linear-regression.html#non-linear-regression" id="toc-non-linear-regression"><span class="toc-section-number">6</span> Non-linear Regression</a>
<ul>
<li><a href="inference-1.html#inference-1" id="toc-inference-1"><span class="toc-section-number">6.1</span> Inference</a>
<ul>
<li><a href="inference-1.html#linear-function-of-the-parameters" id="toc-linear-function-of-the-parameters"><span class="toc-section-number">6.1.1</span> Linear Function of the Parameters</a></li>
<li><a href="inference-1.html#nonlinear" id="toc-nonlinear"><span class="toc-section-number">6.1.2</span> Nonlinear</a></li>
</ul></li>
<li><a href="non-linear-least-squares.html#non-linear-least-squares" id="toc-non-linear-least-squares"><span class="toc-section-number">6.2</span> Non-linear Least Squares</a>
<ul>
<li><a href="non-linear-least-squares.html#alternative-of-gauss-newton-algorithm" id="toc-alternative-of-gauss-newton-algorithm"><span class="toc-section-number">6.2.1</span> Alternative of Gauss-Newton Algorithm</a></li>
<li><a href="non-linear-least-squares.html#practical-considerations" id="toc-practical-considerations"><span class="toc-section-number">6.2.2</span> Practical Considerations</a></li>
<li><a href="non-linear-least-squares.html#modelestiamtion-adequcy" id="toc-modelestiamtion-adequcy"><span class="toc-section-number">6.2.3</span> Model/Estiamtion Adequcy</a></li>
<li><a href="non-linear-least-squares.html#application-1" id="toc-application-1"><span class="toc-section-number">6.2.4</span> Application</a></li>
</ul></li>
</ul></li>
<li><a href="generalized-linear-models.html#generalized-linear-models" id="toc-generalized-linear-models"><span class="toc-section-number">7</span> Generalized Linear Models</a>
<ul>
<li><a href="logistic-regression-1.html#logistic-regression-1" id="toc-logistic-regression-1"><span class="toc-section-number">7.1</span> Logistic Regression</a>
<ul>
<li><a href="logistic-regression-1.html#application-2" id="toc-application-2"><span class="toc-section-number">7.1.1</span> Application</a></li>
</ul></li>
<li><a href="probit-regression.html#probit-regression" id="toc-probit-regression"><span class="toc-section-number">7.2</span> Probit Regression</a></li>
<li><a href="binomial-regression.html#binomial-regression" id="toc-binomial-regression"><span class="toc-section-number">7.3</span> Binomial Regression</a></li>
<li><a href="poisson-regression.html#poisson-regression" id="toc-poisson-regression"><span class="toc-section-number">7.4</span> Poisson Regression</a>
<ul>
<li><a href="poisson-regression.html#application-3" id="toc-application-3"><span class="toc-section-number">7.4.1</span> Application</a></li>
</ul></li>
<li><a href="negative-binomial-regression.html#negative-binomial-regression" id="toc-negative-binomial-regression"><span class="toc-section-number">7.5</span> Negative Binomial Regression</a></li>
<li><a href="multinomial.html#multinomial" id="toc-multinomial"><span class="toc-section-number">7.6</span> Multinomial</a></li>
<li><a href="generalization.html#generalization" id="toc-generalization"><span class="toc-section-number">7.7</span> Generalization</a>
<ul>
<li><a href="generalization.html#estimation-1" id="toc-estimation-1"><span class="toc-section-number">7.7.1</span> Estimation</a></li>
<li><a href="generalization.html#inference-2" id="toc-inference-2"><span class="toc-section-number">7.7.2</span> Inference</a></li>
<li><a href="generalization.html#deviance" id="toc-deviance"><span class="toc-section-number">7.7.3</span> Deviance</a></li>
<li><a href="generalization.html#diagnostic-plots" id="toc-diagnostic-plots"><span class="toc-section-number">7.7.4</span> Diagnostic Plots</a></li>
<li><a href="generalization.html#goodness-of-fit" id="toc-goodness-of-fit"><span class="toc-section-number">7.7.5</span> Goodness of Fit</a></li>
<li><a href="generalization.html#over-dispersion" id="toc-over-dispersion"><span class="toc-section-number">7.7.6</span> Over-Dispersion</a></li>
</ul></li>
</ul></li>
<li><a href="linear-mixed-models.html#linear-mixed-models" id="toc-linear-mixed-models"><span class="toc-section-number">8</span> Linear Mixed Models</a>
<ul>
<li><a href="dependent-data.html#dependent-data" id="toc-dependent-data"><span class="toc-section-number">8.1</span> Dependent Data</a>
<ul>
<li><a href="dependent-data.html#random-intercepts-model" id="toc-random-intercepts-model"><span class="toc-section-number">8.1.1</span> Random-Intercepts Model</a></li>
<li><a href="dependent-data.html#covariance-models" id="toc-covariance-models"><span class="toc-section-number">8.1.2</span> Covariance Models</a></li>
</ul></li>
<li><a href="estimation-2.html#estimation-2" id="toc-estimation-2"><span class="toc-section-number">8.2</span> Estimation</a>
<ul>
<li><a href="estimation-2.html#estimating-mathbfv" id="toc-estimating-mathbfv"><span class="toc-section-number">8.2.1</span> Estimating <span class="math inline">\(\mathbf{V}\)</span></a></li>
</ul></li>
<li><a href="inference-3.html#inference-3" id="toc-inference-3"><span class="toc-section-number">8.3</span> Inference</a>
<ul>
<li><a href="inference-3.html#parameters-beta" id="toc-parameters-beta"><span class="toc-section-number">8.3.1</span> Parameters <span class="math inline">\(\beta\)</span></a></li>
<li><a href="inference-3.html#variance-components" id="toc-variance-components"><span class="toc-section-number">8.3.2</span> Variance Components</a></li>
</ul></li>
<li><a href="information-criteria.html#information-criteria" id="toc-information-criteria"><span class="toc-section-number">8.4</span> Information Criteria</a>
<ul>
<li><a href="information-criteria.html#akaikes-information-criteria-aic" id="toc-akaikes-information-criteria-aic"><span class="toc-section-number">8.4.1</span> Akaike’s Information Criteria (AIC)</a></li>
<li><a href="information-criteria.html#corrected-aic-aicc" id="toc-corrected-aic-aicc"><span class="toc-section-number">8.4.2</span> Corrected AIC (AICC)</a></li>
<li><a href="information-criteria.html#bayesian-information-criteria-bic" id="toc-bayesian-information-criteria-bic"><span class="toc-section-number">8.4.3</span> Bayesian Information Criteria (BIC)</a></li>
</ul></li>
<li><a href="split-plot-designs.html#split-plot-designs" id="toc-split-plot-designs"><span class="toc-section-number">8.5</span> Split-Plot Designs</a>
<ul>
<li><a href="split-plot-designs.html#application-4" id="toc-application-4"><span class="toc-section-number">8.5.1</span> Application</a></li>
</ul></li>
<li><a href="repeated-measures-in-mixed-models.html#repeated-measures-in-mixed-models" id="toc-repeated-measures-in-mixed-models"><span class="toc-section-number">8.6</span> Repeated Measures in Mixed Models</a></li>
<li><a href="unbalanced-or-unequally-spaced-data.html#unbalanced-or-unequally-spaced-data" id="toc-unbalanced-or-unequally-spaced-data"><span class="toc-section-number">8.7</span> Unbalanced or Unequally Spaced Data</a></li>
<li><a href="application-5.html#application-5" id="toc-application-5"><span class="toc-section-number">8.8</span> Application</a>
<ul>
<li><a href="application-5.html#example-1-pulps" id="toc-example-1-pulps"><span class="toc-section-number">8.8.1</span> Example 1 (Pulps)</a></li>
<li><a href="application-5.html#example-2-rats" id="toc-example-2-rats"><span class="toc-section-number">8.8.2</span> Example 2 (Rats)</a></li>
<li><a href="application-5.html#example-3-agridat" id="toc-example-3-agridat"><span class="toc-section-number">8.8.3</span> Example 3 (Agridat)</a></li>
</ul></li>
</ul></li>
<li><a href="nonlinear-and-generalized-linear-mixed-models.html#nonlinear-and-generalized-linear-mixed-models" id="toc-nonlinear-and-generalized-linear-mixed-models"><span class="toc-section-number">9</span> Nonlinear and Generalized Linear Mixed Models</a>
<ul>
<li><a href="estimation-3.html#estimation-3" id="toc-estimation-3"><span class="toc-section-number">9.1</span> Estimation</a>
<ul>
<li><a href="estimation-3.html#estimation-by-numerical-integration" id="toc-estimation-by-numerical-integration"><span class="toc-section-number">9.1.1</span> Estimation by Numerical Integration</a></li>
<li><a href="estimation-3.html#estimation-by-linearization" id="toc-estimation-by-linearization"><span class="toc-section-number">9.1.2</span> Estimation by Linearization</a></li>
<li><a href="estimation-3.html#estimation-by-bayesian-hierarchical-models" id="toc-estimation-by-bayesian-hierarchical-models"><span class="toc-section-number">9.1.3</span> Estimation by Bayesian Hierarchical Models</a></li>
</ul></li>
<li><a href="application-6.html#application-6" id="toc-application-6"><span class="toc-section-number">9.2</span> Application</a>
<ul>
<li><a href="application-6.html#binomial-cbpp-data" id="toc-binomial-cbpp-data"><span class="toc-section-number">9.2.1</span> Binomial (CBPP Data)</a></li>
<li><a href="application-6.html#count-owl-data" id="toc-count-owl-data"><span class="toc-section-number">9.2.2</span> Count (Owl Data)</a></li>
<li><a href="application-6.html#binomial-1" id="toc-binomial-1"><span class="toc-section-number">9.2.3</span> Binomial</a></li>
<li><a href="application-6.html#example-from-schabenberger_2001-section-8.4.1" id="toc-example-from-schabenberger_2001-section-8.4.1"><span class="toc-section-number">9.2.4</span> Example from <span class="citation">(<span>Schabenberger and Pierce 2001</span>)</span> section 8.4.1</a></li>
</ul></li>
<li><a href="summary-2.html#summary-2" id="toc-summary-2"><span class="toc-section-number">9.3</span> Summary</a></li>
</ul></li>
<li><a href="#part-iii.-ramifications" id="toc-part-iii.-ramifications">(PART*) III. RAMIFICATIONS</a></li>
<li><a href="model-specification.html#model-specification" id="toc-model-specification"><span class="toc-section-number">10</span> Model Specification</a>
<ul>
<li><a href="nested-model.html#nested-model" id="toc-nested-model"><span class="toc-section-number">10.1</span> Nested Model</a>
<ul>
<li><a href="nested-model.html#chow-test" id="toc-chow-test"><span class="toc-section-number">10.1.1</span> Chow test</a></li>
</ul></li>
<li><a href="non-nested-model.html#non-nested-model" id="toc-non-nested-model"><span class="toc-section-number">10.2</span> Non-Nested Model</a>
<ul>
<li><a href="non-nested-model.html#davidson-mackinnon-test" id="toc-davidson-mackinnon-test"><span class="toc-section-number">10.2.1</span> Davidson-Mackinnon test</a></li>
</ul></li>
<li><a href="heteroskedasticity-1.html#heteroskedasticity-1" id="toc-heteroskedasticity-1"><span class="toc-section-number">10.3</span> Heteroskedasticity</a>
<ul>
<li><a href="heteroskedasticity-1.html#breusch-pagan-test" id="toc-breusch-pagan-test"><span class="toc-section-number">10.3.1</span> Breusch-Pagan test</a></li>
<li><a href="heteroskedasticity-1.html#white-test" id="toc-white-test"><span class="toc-section-number">10.3.2</span> White test</a></li>
</ul></li>
</ul></li>
<li><a href="imputation-missing-data.html#imputation-missing-data" id="toc-imputation-missing-data"><span class="toc-section-number">11</span> Imputation (Missing Data)</a>
<ul>
<li><a href="assumptions-1.html#assumptions-1" id="toc-assumptions-1"><span class="toc-section-number">11.1</span> Assumptions</a>
<ul>
<li><a href="assumptions-1.html#missing-completely-at-random-mcar" id="toc-missing-completely-at-random-mcar"><span class="toc-section-number">11.1.1</span> Missing Completely at Random (MCAR)</a></li>
<li><a href="assumptions-1.html#missing-at-random-mar" id="toc-missing-at-random-mar"><span class="toc-section-number">11.1.2</span> Missing at Random (MAR)</a></li>
<li><a href="assumptions-1.html#ignorable" id="toc-ignorable"><span class="toc-section-number">11.1.3</span> Ignorable</a></li>
<li><a href="assumptions-1.html#nonignorable" id="toc-nonignorable"><span class="toc-section-number">11.1.4</span> Nonignorable</a></li>
</ul></li>
<li><a href="solutions-to-missing-data.html#solutions-to-missing-data" id="toc-solutions-to-missing-data"><span class="toc-section-number">11.2</span> Solutions to Missing data</a>
<ul>
<li><a href="solutions-to-missing-data.html#listwise-deletion" id="toc-listwise-deletion"><span class="toc-section-number">11.2.1</span> Listwise Deletion</a></li>
<li><a href="solutions-to-missing-data.html#pairwise-deletion" id="toc-pairwise-deletion"><span class="toc-section-number">11.2.2</span> Pairwise Deletion</a></li>
<li><a href="solutions-to-missing-data.html#dummy-variable-adjustment" id="toc-dummy-variable-adjustment"><span class="toc-section-number">11.2.3</span> Dummy Variable Adjustment</a></li>
<li><a href="solutions-to-missing-data.html#imputation" id="toc-imputation"><span class="toc-section-number">11.2.4</span> Imputation</a></li>
<li><a href="solutions-to-missing-data.html#other-methods" id="toc-other-methods"><span class="toc-section-number">11.2.5</span> Other methods</a></li>
</ul></li>
<li><a href="criteria-for-choosing-an-effective-approach.html#criteria-for-choosing-an-effective-approach" id="toc-criteria-for-choosing-an-effective-approach"><span class="toc-section-number">11.3</span> Criteria for Choosing an Effective Approach</a></li>
<li><a href="another-perspective.html#another-perspective" id="toc-another-perspective"><span class="toc-section-number">11.4</span> Another Perspective</a></li>
<li><a href="diagnosing-the-mechanism.html#diagnosing-the-mechanism" id="toc-diagnosing-the-mechanism"><span class="toc-section-number">11.5</span> Diagnosing the Mechanism</a>
<ul>
<li><a href="diagnosing-the-mechanism.html#mar-vs.-mnar" id="toc-mar-vs.-mnar"><span class="toc-section-number">11.5.1</span> MAR vs. MNAR</a></li>
<li><a href="diagnosing-the-mechanism.html#mcar-vs.-mar" id="toc-mcar-vs.-mar"><span class="toc-section-number">11.5.2</span> MCAR vs. MAR</a></li>
</ul></li>
<li><a href="application-7.html#application-7" id="toc-application-7"><span class="toc-section-number">11.6</span> Application</a>
<ul>
<li><a href="application-7.html#imputation-with-mean-median-mode" id="toc-imputation-with-mean-median-mode"><span class="toc-section-number">11.6.1</span> Imputation with mean / median / mode</a></li>
<li><a href="application-7.html#knn" id="toc-knn"><span class="toc-section-number">11.6.2</span> KNN</a></li>
<li><a href="application-7.html#rpart" id="toc-rpart"><span class="toc-section-number">11.6.3</span> rpart</a></li>
<li><a href="application-7.html#mice-multivariate-imputation-via-chained-equations" id="toc-mice-multivariate-imputation-via-chained-equations"><span class="toc-section-number">11.6.4</span> MICE (Multivariate Imputation via Chained Equations)</a></li>
<li><a href="application-7.html#amelia" id="toc-amelia"><span class="toc-section-number">11.6.5</span> Amelia</a></li>
<li><a href="application-7.html#missforest" id="toc-missforest"><span class="toc-section-number">11.6.6</span> missForest</a></li>
<li><a href="application-7.html#hmisc" id="toc-hmisc"><span class="toc-section-number">11.6.7</span> Hmisc</a></li>
<li><a href="application-7.html#mi" id="toc-mi"><span class="toc-section-number">11.6.8</span> mi</a></li>
</ul></li>
</ul></li>
<li><a href="data.html#data" id="toc-data"><span class="toc-section-number">12</span> Data</a>
<ul>
<li><a href="cross-sectional.html#cross-sectional" id="toc-cross-sectional"><span class="toc-section-number">12.1</span> Cross-Sectional</a></li>
<li><a href="time-series-1.html#time-series-1" id="toc-time-series-1"><span class="toc-section-number">12.2</span> Time Series</a>
<ul>
<li><a href="time-series-1.html#deterministic-time-trend" id="toc-deterministic-time-trend"><span class="toc-section-number">12.2.1</span> Deterministic Time trend</a></li>
<li><a href="time-series-1.html#feedback-effect" id="toc-feedback-effect"><span class="toc-section-number">12.2.2</span> Feedback Effect</a></li>
<li><a href="time-series-1.html#dynamic-specification" id="toc-dynamic-specification"><span class="toc-section-number">12.2.3</span> Dynamic Specification</a></li>
<li><a href="time-series-1.html#dynamically-complete" id="toc-dynamically-complete"><span class="toc-section-number">12.2.4</span> Dynamically Complete</a></li>
<li><a href="time-series-1.html#highly-persistent-data" id="toc-highly-persistent-data"><span class="toc-section-number">12.2.5</span> Highly Persistent Data</a></li>
</ul></li>
<li><a href="repeated-cross-sections.html#repeated-cross-sections" id="toc-repeated-cross-sections"><span class="toc-section-number">12.3</span> Repeated Cross Sections</a>
<ul>
<li><a href="repeated-cross-sections.html#pooled-cross-section" id="toc-pooled-cross-section"><span class="toc-section-number">12.3.1</span> Pooled Cross Section</a></li>
</ul></li>
<li><a href="panel-data.html#panel-data" id="toc-panel-data"><span class="toc-section-number">12.4</span> Panel Data</a>
<ul>
<li><a href="panel-data.html#pooled-ols-estimator" id="toc-pooled-ols-estimator"><span class="toc-section-number">12.4.1</span> Pooled OLS Estimator</a></li>
<li><a href="panel-data.html#individual-specific-effects-model" id="toc-individual-specific-effects-model"><span class="toc-section-number">12.4.2</span> Individual-specific effects model</a></li>
<li><a href="panel-data.html#tests-for-assumptions" id="toc-tests-for-assumptions"><span class="toc-section-number">12.4.3</span> Tests for Assumptions</a></li>
<li><a href="panel-data.html#model-selection" id="toc-model-selection"><span class="toc-section-number">12.4.4</span> Model Selection</a></li>
<li><a href="panel-data.html#summary-3" id="toc-summary-3"><span class="toc-section-number">12.4.5</span> Summary</a></li>
<li><a href="panel-data.html#application-8" id="toc-application-8"><span class="toc-section-number">12.4.6</span> Application</a></li>
<li><a href="panel-data.html#other-estimators" id="toc-other-estimators"><span class="toc-section-number">12.4.7</span> Other Estimators</a></li>
</ul></li>
</ul></li>
<li><a href="hypothesis-testing.html#hypothesis-testing" id="toc-hypothesis-testing"><span class="toc-section-number">13</span> Hypothesis Testing</a>
<ul>
<li><a href="types-of-hypothesis-testing.html#types-of-hypothesis-testing" id="toc-types-of-hypothesis-testing"><span class="toc-section-number">13.1</span> Types of hypothesis testing</a></li>
<li><a href="wald-test.html#wald-test" id="toc-wald-test"><span class="toc-section-number">13.2</span> Wald test</a>
<ul>
<li><a href="wald-test.html#multiple-hypothesis" id="toc-multiple-hypothesis"><span class="toc-section-number">13.2.1</span> Multiple Hypothesis</a></li>
<li><a href="wald-test.html#linear-combination" id="toc-linear-combination"><span class="toc-section-number">13.2.2</span> Linear Combination</a></li>
<li><a href="wald-test.html#estimate-difference-in-coefficients" id="toc-estimate-difference-in-coefficients"><span class="toc-section-number">13.2.3</span> Estimate Difference in Coefficients</a></li>
<li><a href="wald-test.html#application-9" id="toc-application-9"><span class="toc-section-number">13.2.4</span> Application</a></li>
<li><a href="wald-test.html#nonlinear-1" id="toc-nonlinear-1"><span class="toc-section-number">13.2.5</span> Nonlinear</a></li>
</ul></li>
<li><a href="the-likelihood-ratio-test.html#the-likelihood-ratio-test" id="toc-the-likelihood-ratio-test"><span class="toc-section-number">13.3</span> The likelihood ratio test</a></li>
<li><a href="lagrange-multiplier-score.html#lagrange-multiplier-score" id="toc-lagrange-multiplier-score"><span class="toc-section-number">13.4</span> Lagrange Multiplier (Score)</a></li>
</ul></li>
<li><a href="prediction-and-estimation.html#prediction-and-estimation" id="toc-prediction-and-estimation"><span class="toc-section-number">14</span> Prediction and Estimation</a></li>
<li><a href="moderation.html#moderation" id="toc-moderation"><span class="toc-section-number">15</span> Moderation</a>
<ul>
<li><a href="emmeans-package.html#emmeans-package" id="toc-emmeans-package"><span class="toc-section-number">15.1</span> emmeans package</a>
<ul>
<li><a href="emmeans-package.html#continuous-by-continuous" id="toc-continuous-by-continuous"><span class="toc-section-number">15.1.1</span> Continuous by continuous</a></li>
<li><a href="emmeans-package.html#continuous-by-categorical" id="toc-continuous-by-categorical"><span class="toc-section-number">15.1.2</span> Continuous by categorical</a></li>
<li><a href="emmeans-package.html#categorical-by-categorical" id="toc-categorical-by-categorical"><span class="toc-section-number">15.1.3</span> Categorical by categorical</a></li>
</ul></li>
<li><a href="probmod-package.html#probmod-package" id="toc-probmod-package"><span class="toc-section-number">15.2</span> probmod package</a></li>
<li><a href="interactions-package.html#interactions-package" id="toc-interactions-package"><span class="toc-section-number">15.3</span> interactions package</a>
<ul>
<li><a href="interactions-package.html#continuous-interaction" id="toc-continuous-interaction"><span class="toc-section-number">15.3.1</span> Continuous interaction</a></li>
<li><a href="interactions-package.html#categorical-interaction" id="toc-categorical-interaction"><span class="toc-section-number">15.3.2</span> Categorical interaction</a></li>
</ul></li>
<li><a href="interactionr-package.html#interactionr-package" id="toc-interactionr-package"><span class="toc-section-number">15.4</span> interactionR package</a></li>
<li><a href="sjplot-package.html#sjplot-package" id="toc-sjplot-package"><span class="toc-section-number">15.5</span> sjPlot package</a></li>
</ul></li>
<li><a href="#part-iv.-causal-inference" id="toc-part-iv.-causal-inference">(PART*) IV. CAUSAL INFERENCE</a></li>
<li><a href="causal-inference.html#causal-inference" id="toc-causal-inference"><span class="toc-section-number">16</span> Causal Inference</a>
<ul>
<li><a href="treatment-effect-types.html#treatment-effect-types" id="toc-treatment-effect-types"><span class="toc-section-number">16.1</span> Treatment effect types</a>
<ul>
<li><a href="treatment-effect-types.html#average-treatment-effects" id="toc-average-treatment-effects"><span class="toc-section-number">16.1.1</span> Average Treatment Effects</a></li>
<li><a href="treatment-effect-types.html#conditional-average-treatment-effects" id="toc-conditional-average-treatment-effects"><span class="toc-section-number">16.1.2</span> Conditional Average Treatment Effects</a></li>
<li><a href="treatment-effect-types.html#intent-to-treat-effects" id="toc-intent-to-treat-effects"><span class="toc-section-number">16.1.3</span> Intent-to-treat Effects</a></li>
<li><a href="treatment-effect-types.html#local-average-treatment-effects" id="toc-local-average-treatment-effects"><span class="toc-section-number">16.1.4</span> Local Average Treatment Effects</a></li>
<li><a href="treatment-effect-types.html#population-vs.-sample-average-treatment-effects" id="toc-population-vs.-sample-average-treatment-effects"><span class="toc-section-number">16.1.5</span> Population vs. Sample Average Treatment Effects</a></li>
<li><a href="treatment-effect-types.html#average-treatment-effects-on-the-treated-and-control" id="toc-average-treatment-effects-on-the-treated-and-control"><span class="toc-section-number">16.1.6</span> Average Treatment Effects on the Treated and Control</a></li>
<li><a href="treatment-effect-types.html#quantile-average-treatment-effects" id="toc-quantile-average-treatment-effects"><span class="toc-section-number">16.1.7</span> Quantile Average Treatment Effects</a></li>
<li><a href="treatment-effect-types.html#mediation-effects" id="toc-mediation-effects"><span class="toc-section-number">16.1.8</span> Mediation Effects</a></li>
<li><a href="treatment-effect-types.html#log-odds-treatment-effects" id="toc-log-odds-treatment-effects"><span class="toc-section-number">16.1.9</span> Log-odds Treatment Effects</a></li>
</ul></li>
</ul></li>
<li><a href="#part-a.-experimental-design" id="toc-part-a.-experimental-design">(PART*) A. EXPERIMENTAL DESIGN</a></li>
<li><a href="experimental-design.html#experimental-design" id="toc-experimental-design"><span class="toc-section-number">17</span> Experimental Design</a>
<ul>
<li><a href="semi-random-experiment.html#semi-random-experiment" id="toc-semi-random-experiment"><span class="toc-section-number">17.1</span> Semi-random Experiment</a></li>
<li><a href="rerandomization.html#rerandomization" id="toc-rerandomization"><span class="toc-section-number">17.2</span> Rerandomization</a></li>
</ul></li>
<li><a href="sampling.html#sampling" id="toc-sampling"><span class="toc-section-number">18</span> Sampling</a>
<ul>
<li><a href="simple-sampling.html#simple-sampling" id="toc-simple-sampling"><span class="toc-section-number">18.1</span> Simple Sampling</a></li>
<li><a href="stratified-sampling.html#stratified-sampling" id="toc-stratified-sampling"><span class="toc-section-number">18.2</span> Stratified Sampling</a></li>
<li><a href="unequal-probability-sampling.html#unequal-probability-sampling" id="toc-unequal-probability-sampling"><span class="toc-section-number">18.3</span> Unequal Probability Sampling</a></li>
<li><a href="balanced-sampling.html#balanced-sampling" id="toc-balanced-sampling"><span class="toc-section-number">18.4</span> Balanced Sampling</a>
<ul>
<li><a href="balanced-sampling.html#cube" id="toc-cube"><span class="toc-section-number">18.4.1</span> Cube</a></li>
<li><a href="balanced-sampling.html#stratification" id="toc-stratification"><span class="toc-section-number">18.4.2</span> Stratification</a></li>
<li><a href="balanced-sampling.html#cluster-1" id="toc-cluster-1"><span class="toc-section-number">18.4.3</span> Cluster</a></li>
<li><a href="balanced-sampling.html#two-stage" id="toc-two-stage"><span class="toc-section-number">18.4.4</span> Two-stage</a></li>
</ul></li>
</ul></li>
<li><a href="analysis-of-variance-anova.html#analysis-of-variance-anova" id="toc-analysis-of-variance-anova"><span class="toc-section-number">19</span> Analysis of Variance (ANOVA)</a>
<ul>
<li><a href="completely-randomized-design-crd.html#completely-randomized-design-crd" id="toc-completely-randomized-design-crd"><span class="toc-section-number">19.1</span> Completely Randomized Design (CRD)</a>
<ul>
<li><a href="completely-randomized-design-crd.html#single-factor-fixed-effects-model" id="toc-single-factor-fixed-effects-model"><span class="toc-section-number">19.1.1</span> Single Factor Fixed Effects Model</a></li>
<li><a href="completely-randomized-design-crd.html#single-factor-random-effects-model" id="toc-single-factor-random-effects-model"><span class="toc-section-number">19.1.2</span> Single Factor Random Effects Model</a></li>
<li><a href="completely-randomized-design-crd.html#two-factor-fixed-effect-anova" id="toc-two-factor-fixed-effect-anova"><span class="toc-section-number">19.1.3</span> Two Factor Fixed Effect ANOVA</a></li>
<li><a href="completely-randomized-design-crd.html#two-way-random-effects-anova" id="toc-two-way-random-effects-anova"><span class="toc-section-number">19.1.4</span> Two-Way Random Effects ANOVA</a></li>
<li><a href="completely-randomized-design-crd.html#two-way-mixed-effects-anova" id="toc-two-way-mixed-effects-anova"><span class="toc-section-number">19.1.5</span> Two-Way Mixed Effects ANOVA</a></li>
</ul></li>
<li><a href="nonparametric-anova.html#nonparametric-anova" id="toc-nonparametric-anova"><span class="toc-section-number">19.2</span> Nonparametric ANOVA</a>
<ul>
<li><a href="nonparametric-anova.html#kruskal-wallis" id="toc-kruskal-wallis"><span class="toc-section-number">19.2.1</span> Kruskal-Wallis</a></li>
<li><a href="nonparametric-anova.html#friedman-test" id="toc-friedman-test"><span class="toc-section-number">19.2.2</span> Friedman Test</a></li>
</ul></li>
<li><a href="sample-size-planning-for-anova.html#sample-size-planning-for-anova" id="toc-sample-size-planning-for-anova"><span class="toc-section-number">19.3</span> Sample Size Planning for ANOVA</a>
<ul>
<li><a href="sample-size-planning-for-anova.html#balanced-designs" id="toc-balanced-designs"><span class="toc-section-number">19.3.1</span> Balanced Designs</a></li>
<li><a href="sample-size-planning-for-anova.html#randomized-block-experiments" id="toc-randomized-block-experiments"><span class="toc-section-number">19.3.2</span> Randomized Block Experiments</a></li>
</ul></li>
<li><a href="randomized-block-designs.html#randomized-block-designs" id="toc-randomized-block-designs"><span class="toc-section-number">19.4</span> Randomized Block Designs</a>
<ul>
<li><a href="randomized-block-designs.html#tukey-test-of-additivity" id="toc-tukey-test-of-additivity"><span class="toc-section-number">19.4.1</span> Tukey Test of Additivity</a></li>
</ul></li>
<li><a href="nested-designs.html#nested-designs" id="toc-nested-designs"><span class="toc-section-number">19.5</span> Nested Designs</a>
<ul>
<li><a href="nested-designs.html#two-factor-nested-designs" id="toc-two-factor-nested-designs"><span class="toc-section-number">19.5.1</span> Two-Factor Nested Designs</a></li>
</ul></li>
<li><a href="single-factor-covariance-model.html#single-factor-covariance-model" id="toc-single-factor-covariance-model"><span class="toc-section-number">19.6</span> Single Factor Covariance Model</a></li>
</ul></li>
<li><a href="multivariate-methods.html#multivariate-methods" id="toc-multivariate-methods"><span class="toc-section-number">20</span> Multivariate Methods</a>
<ul>
<li><a href="multivariate-methods.html#properties-of-mvn" id="toc-properties-of-mvn"><span class="toc-section-number">20.0.1</span> Properties of MVN</a></li>
<li><a href="multivariate-methods.html#mean-vector-inference" id="toc-mean-vector-inference"><span class="toc-section-number">20.0.2</span> Mean Vector Inference</a></li>
<li><a href="multivariate-methods.html#general-hypothesis-testing" id="toc-general-hypothesis-testing"><span class="toc-section-number">20.0.3</span> General Hypothesis Testing</a></li>
<li><a href="manova.html#manova" id="toc-manova"><span class="toc-section-number">20.1</span> MANOVA</a>
<ul>
<li><a href="manova.html#testing-general-hypotheses" id="toc-testing-general-hypotheses"><span class="toc-section-number">20.1.1</span> Testing General Hypotheses</a></li>
<li><a href="manova.html#profile-analysis" id="toc-profile-analysis"><span class="toc-section-number">20.1.2</span> Profile Analysis</a></li>
<li><a href="manova.html#summary-5" id="toc-summary-5"><span class="toc-section-number">20.1.3</span> Summary</a></li>
</ul></li>
<li><a href="principal-components.html#principal-components" id="toc-principal-components"><span class="toc-section-number">20.2</span> Principal Components</a>
<ul>
<li><a href="principal-components.html#population-principal-components" id="toc-population-principal-components"><span class="toc-section-number">20.2.1</span> Population Principal Components</a></li>
<li><a href="principal-components.html#sample-principal-components" id="toc-sample-principal-components"><span class="toc-section-number">20.2.2</span> Sample Principal Components</a></li>
<li><a href="principal-components.html#application-10" id="toc-application-10"><span class="toc-section-number">20.2.3</span> Application</a></li>
</ul></li>
<li><a href="factor-analysis.html#factor-analysis" id="toc-factor-analysis"><span class="toc-section-number">20.3</span> Factor Analysis</a>
<ul>
<li><a href="factor-analysis.html#methods-of-estimation" id="toc-methods-of-estimation"><span class="toc-section-number">20.3.1</span> Methods of Estimation</a></li>
<li><a href="factor-analysis.html#factor-rotation" id="toc-factor-rotation"><span class="toc-section-number">20.3.2</span> Factor Rotation</a></li>
<li><a href="factor-analysis.html#estimation-of-factor-scores" id="toc-estimation-of-factor-scores"><span class="toc-section-number">20.3.3</span> Estimation of Factor Scores</a></li>
<li><a href="factor-analysis.html#model-diagnostic" id="toc-model-diagnostic"><span class="toc-section-number">20.3.4</span> Model Diagnostic</a></li>
<li><a href="factor-analysis.html#application-11" id="toc-application-11"><span class="toc-section-number">20.3.5</span> Application</a></li>
</ul></li>
<li><a href="discriminant-analysis.html#discriminant-analysis" id="toc-discriminant-analysis"><span class="toc-section-number">20.4</span> Discriminant Analysis</a>
<ul>
<li><a href="discriminant-analysis.html#known-populations" id="toc-known-populations"><span class="toc-section-number">20.4.1</span> Known Populations</a></li>
<li><a href="discriminant-analysis.html#probabilities-of-misclassification" id="toc-probabilities-of-misclassification"><span class="toc-section-number">20.4.2</span> Probabilities of Misclassification</a></li>
<li><a href="discriminant-analysis.html#unknown-populations-nonparametric-discrimination" id="toc-unknown-populations-nonparametric-discrimination"><span class="toc-section-number">20.4.3</span> Unknown Populations/ Nonparametric Discrimination</a></li>
<li><a href="discriminant-analysis.html#application-12" id="toc-application-12"><span class="toc-section-number">20.4.4</span> Application</a></li>
</ul></li>
</ul></li>
<li><a href="#part-b.-quasi-experimental-design" id="toc-part-b.-quasi-experimental-design">(PART*) B. QUASI-EXPERIMENTAL DESIGN</a></li>
<li><a href="quasi-experimental.html#quasi-experimental" id="toc-quasi-experimental"><span class="toc-section-number">21</span> Quasi-experimental</a></li>
<li><a href="regression-discontinuity.html#regression-discontinuity" id="toc-regression-discontinuity"><span class="toc-section-number">22</span> Regression Discontinuity</a>
<ul>
<li><a href="specification-checks.html#specification-checks" id="toc-specification-checks"><span class="toc-section-number">22.1</span> Specification Checks</a>
<ul>
<li><a href="specification-checks.html#balance-checks" id="toc-balance-checks"><span class="toc-section-number">22.1.1</span> Balance Checks</a></li>
<li><a href="specification-checks.html#sortingbunchingmanipulation" id="toc-sortingbunchingmanipulation"><span class="toc-section-number">22.1.2</span> Sorting/Bunching/Manipulation</a></li>
<li><a href="specification-checks.html#placebo-tests" id="toc-placebo-tests"><span class="toc-section-number">22.1.3</span> Placebo Tests</a></li>
<li><a href="specification-checks.html#sensitivity-to-bandwidth-choice" id="toc-sensitivity-to-bandwidth-choice"><span class="toc-section-number">22.1.4</span> Sensitivity to Bandwidth Choice</a></li>
<li><a href="specification-checks.html#fuzzy-rd-design" id="toc-fuzzy-rd-design"><span class="toc-section-number">22.1.5</span> Fuzzy RD Design</a></li>
<li><a href="specification-checks.html#regression-kink-design" id="toc-regression-kink-design"><span class="toc-section-number">22.1.6</span> Regression Kink Design</a></li>
<li><a href="specification-checks.html#mutli-cutoff-multi-score-geographic-rd" id="toc-mutli-cutoff-multi-score-geographic-rd"><span class="toc-section-number">22.1.7</span> Mutli-cutoff, Multi-score, geographic RD</a></li>
</ul></li>
<li><a href="steps-for-sharp-rd.html#steps-for-sharp-rd" id="toc-steps-for-sharp-rd"><span class="toc-section-number">22.2</span> Steps for Sharp RD</a></li>
<li><a href="steps-for-fuzzy-rd.html#steps-for-fuzzy-rd" id="toc-steps-for-fuzzy-rd"><span class="toc-section-number">22.3</span> Steps for Fuzzy RD</a></li>
<li><a href="steps-for-rdit-regression-discontinuity-in-time.html#steps-for-rdit-regression-discontinuity-in-time" id="toc-steps-for-rdit-regression-discontinuity-in-time"><span class="toc-section-number">22.4</span> Steps for RDiT (Regression Discontinuity in Time)</a></li>
<li><a href="evaluation-of-an-rd.html#evaluation-of-an-rd" id="toc-evaluation-of-an-rd"><span class="toc-section-number">22.5</span> Evaluation of an RD</a></li>
<li><a href="applications.html#applications" id="toc-applications"><span class="toc-section-number">22.6</span> Applications</a>
<ul>
<li><a href="applications.html#example-1-1" id="toc-example-1-1"><span class="toc-section-number">22.6.1</span> Example 1</a></li>
<li><a href="applications.html#example-2" id="toc-example-2"><span class="toc-section-number">22.6.2</span> Example 2</a></li>
<li><a href="applications.html#example-3" id="toc-example-3"><span class="toc-section-number">22.6.3</span> Example 3</a></li>
<li><a href="applications.html#example-4" id="toc-example-4"><span class="toc-section-number">22.6.4</span> Example 4</a></li>
</ul></li>
</ul></li>
<li><a href="difference-in-differences.html#difference-in-differences" id="toc-difference-in-differences"><span class="toc-section-number">23</span> Difference-in-differences</a>
<ul>
<li><a href="simple-dif-n-dif.html#simple-dif-n-dif" id="toc-simple-dif-n-dif"><span class="toc-section-number">23.1</span> Simple Dif-n-dif</a>
<ul>
<li><a href="simple-dif-n-dif.html#assumptions-2" id="toc-assumptions-2"><span class="toc-section-number">23.1.1</span> Assumptions</a></li>
<li><a href="simple-dif-n-dif.html#examples" id="toc-examples"><span class="toc-section-number">23.1.2</span> Examples</a></li>
</ul></li>
<li><a href="multiple-periods-and-variation-in-treatment-timing.html#multiple-periods-and-variation-in-treatment-timing" id="toc-multiple-periods-and-variation-in-treatment-timing"><span class="toc-section-number">23.2</span> Multiple periods and variation in treatment timing</a></li>
<li><a href="staggered-dif-n-dif.html#staggered-dif-n-dif" id="toc-staggered-dif-n-dif"><span class="toc-section-number">23.3</span> Staggered Dif-n-dif</a>
<ul>
<li><a href="staggered-dif-n-dif.html#example-by-doleac2020" id="toc-example-by-doleac2020"><span class="toc-section-number">23.3.1</span> Example by <span class="citation">Doleac and Hansen (<span>2020</span>)</span></a></li>
</ul></li>
<li><a href="two-way-fixed-effects.html#two-way-fixed-effects" id="toc-two-way-fixed-effects"><span class="toc-section-number">23.4</span> Two-way Fixed-effects</a></li>
</ul></li>
<li><a href="synthetic-control.html#synthetic-control" id="toc-synthetic-control"><span class="toc-section-number">24</span> Synthetic Control</a>
<ul>
<li><a href="synthetic-control.html#example-1-2" id="toc-example-1-2"><span class="toc-section-number">24.0.1</span> Example 1</a></li>
<li><a href="synthetic-control.html#example-2-1" id="toc-example-2-1"><span class="toc-section-number">24.0.2</span> Example 2</a></li>
<li><a href="synthetic-control.html#example-3-1" id="toc-example-3-1"><span class="toc-section-number">24.0.3</span> Example 3</a></li>
<li><a href="synthetic-control.html#example-4-1" id="toc-example-4-1"><span class="toc-section-number">24.0.4</span> Example 4</a></li>
<li><a href="synthetic-difference-in-differences.html#synthetic-difference-in-differences" id="toc-synthetic-difference-in-differences"><span class="toc-section-number">24.1</span> Synthetic Difference-in-differences</a></li>
</ul></li>
<li><a href="event-studies.html#event-studies" id="toc-event-studies"><span class="toc-section-number">25</span> Event Studies</a>
<ul>
<li><a href="other-issues.html#other-issues" id="toc-other-issues"><span class="toc-section-number">25.1</span> Other Issues</a>
<ul>
<li><a href="other-issues.html#economic-significance" id="toc-economic-significance"><span class="toc-section-number">25.1.1</span> Economic significance</a></li>
<li><a href="other-issues.html#statistical-power" id="toc-statistical-power"><span class="toc-section-number">25.1.2</span> Statistical Power</a></li>
<li><a href="other-issues.html#testing" id="toc-testing"><span class="toc-section-number">25.1.3</span> Testing</a></li>
<li><a href="other-issues.html#confounders" id="toc-confounders"><span class="toc-section-number">25.1.4</span> Confounders</a></li>
<li><a href="other-issues.html#biases" id="toc-biases"><span class="toc-section-number">25.1.5</span> Biases</a></li>
<li><a href="other-issues.html#long-run-event-studies" id="toc-long-run-event-studies"><span class="toc-section-number">25.1.6</span> Long-run event studies</a></li>
</ul></li>
<li><a href="aggregation.html#aggregation" id="toc-aggregation"><span class="toc-section-number">25.2</span> Aggregation</a>
<ul>
<li><a href="aggregation.html#over-time" id="toc-over-time"><span class="toc-section-number">25.2.1</span> Over Time</a></li>
<li><a href="aggregation.html#across-firms-over-time" id="toc-across-firms-over-time"><span class="toc-section-number">25.2.2</span> Across Firms + Over Time</a></li>
</ul></li>
<li><a href="heterogeneity-in-the-event-effect.html#heterogeneity-in-the-event-effect" id="toc-heterogeneity-in-the-event-effect"><span class="toc-section-number">25.3</span> Heterogeneity in the event effect</a></li>
<li><a href="expected-return-calculation.html#expected-return-calculation" id="toc-expected-return-calculation"><span class="toc-section-number">25.4</span> Expected Return Calculation</a>
<ul>
<li><a href="expected-return-calculation.html#statistical-models" id="toc-statistical-models"><span class="toc-section-number">25.4.1</span> Statistical Models</a></li>
<li><a href="expected-return-calculation.html#economic-model" id="toc-economic-model"><span class="toc-section-number">25.4.2</span> Economic Model</a></li>
</ul></li>
<li><a href="application-13.html#application-13" id="toc-application-13"><span class="toc-section-number">25.5</span> Application</a></li>
</ul></li>
<li><a href="matching-methods.html#matching-methods" id="toc-matching-methods"><span class="toc-section-number">26</span> Matching Methods</a>
<ul>
<li><a href="matchit.html#matchit" id="toc-matchit"><span class="toc-section-number">26.1</span> MatchIt</a></li>
<li><a href="matchingfrontier.html#matchingfrontier" id="toc-matchingfrontier"><span class="toc-section-number">26.2</span> MatchingFrontier</a></li>
<li><a href="propensity-scores.html#propensity-scores" id="toc-propensity-scores"><span class="toc-section-number">26.3</span> Propensity Scores</a></li>
<li><a href="mahalanobis-distance.html#mahalanobis-distance" id="toc-mahalanobis-distance"><span class="toc-section-number">26.4</span> Mahalanobis Distance</a></li>
<li><a href="coarsened-exact-matching.html#coarsened-exact-matching" id="toc-coarsened-exact-matching"><span class="toc-section-number">26.5</span> Coarsened Exact Matching</a></li>
<li><a href="genetic-matching.html#genetic-matching" id="toc-genetic-matching"><span class="toc-section-number">26.6</span> Genetic Matching</a></li>
<li><a href="matching-for-time-series-cross-section-data.html#matching-for-time-series-cross-section-data" id="toc-matching-for-time-series-cross-section-data"><span class="toc-section-number">26.7</span> Matching for time series-cross-section data</a></li>
</ul></li>
<li><a href="interrupted-time-series.html#interrupted-time-series" id="toc-interrupted-time-series"><span class="toc-section-number">27</span> Interrupted Time Series</a></li>
<li><a href="#part-c.-other-concerns" id="toc-part-c.-other-concerns">(PART*) C. OTHER CONCERNS</a></li>
<li><a href="endogeneity.html#endogeneity" id="toc-endogeneity"><span class="toc-section-number">28</span> Endogeneity</a>
<ul>
<li><a href="measurement-error.html#measurement-error" id="toc-measurement-error"><span class="toc-section-number">28.1</span> Measurement Error</a>
<ul>
<li><a href="measurement-error.html#classical-measurement-errors" id="toc-classical-measurement-errors"><span class="toc-section-number">28.1.1</span> Classical Measurement Errors</a></li>
<li><a href="measurement-error.html#non-classical-measurement-errors" id="toc-non-classical-measurement-errors"><span class="toc-section-number">28.1.2</span> Non-classical Measurement Errors</a></li>
<li><a href="measurement-error.html#solution-to-measurement-errors" id="toc-solution-to-measurement-errors"><span class="toc-section-number">28.1.3</span> Solution to Measurement Errors</a></li>
</ul></li>
<li><a href="simultaneity.html#simultaneity" id="toc-simultaneity"><span class="toc-section-number">28.2</span> Simultaneity</a></li>
<li><a href="endogenous-treatment.html#endogenous-treatment" id="toc-endogenous-treatment"><span class="toc-section-number">28.3</span> Endogenous Treatment</a>
<ul>
<li><a href="endogenous-treatment.html#instrumental-variable" id="toc-instrumental-variable"><span class="toc-section-number">28.3.1</span> Instrumental Variable</a></li>
<li><a href="endogenous-treatment.html#internal-instrumental-variable" id="toc-internal-instrumental-variable"><span class="toc-section-number">28.3.2</span> Internal instrumental variable</a></li>
<li><a href="endogenous-treatment.html#proxy-variables" id="toc-proxy-variables"><span class="toc-section-number">28.3.3</span> Proxy Variables</a></li>
</ul></li>
<li><a href="endogenous-sample-selection.html#endogenous-sample-selection" id="toc-endogenous-sample-selection"><span class="toc-section-number">28.4</span> Endogenous Sample Selection</a>
<ul>
<li><a href="endogenous-sample-selection.html#tobit-2" id="toc-tobit-2"><span class="toc-section-number">28.4.1</span> Tobit-2</a></li>
<li><a href="endogenous-sample-selection.html#tobit-5" id="toc-tobit-5"><span class="toc-section-number">28.4.2</span> Tobit-5</a></li>
</ul></li>
</ul></li>
<li><a href="mediation.html#mediation" id="toc-mediation"><span class="toc-section-number">29</span> Mediation</a>
<ul>
<li><a href="traditional.html#traditional" id="toc-traditional"><span class="toc-section-number">29.1</span> Traditional</a>
<ul>
<li><a href="traditional.html#example-1-mediation-traditional" id="toc-example-1-mediation-traditional"><span class="toc-section-number">29.1.1</span> Example 1</a></li>
</ul></li>
<li><a href="model-based-causal-mediation-analysis.html#model-based-causal-mediation-analysis" id="toc-model-based-causal-mediation-analysis"><span class="toc-section-number">29.2</span> Model-based causal mediation analysis</a></li>
</ul></li>
<li><a href="directed-acyclic-graph.html#directed-acyclic-graph" id="toc-directed-acyclic-graph"><span class="toc-section-number">30</span> Directed Acyclic Graph</a></li>
<li><a href="#part-v.-miscellaneous" id="toc-part-v.-miscellaneous">(PART*) V. MISCELLANEOUS</a></li>
<li><a href="report.html#report" id="toc-report"><span class="toc-section-number">31</span> Report</a>
<ul>
<li><a href="one-summary-table.html#one-summary-table" id="toc-one-summary-table"><span class="toc-section-number">31.1</span> One summary table</a></li>
<li><a href="model-comparison.html#model-comparison" id="toc-model-comparison"><span class="toc-section-number">31.2</span> Model Comparison</a></li>
<li><a href="changes-in-an-estimate.html#changes-in-an-estimate" id="toc-changes-in-an-estimate"><span class="toc-section-number">31.3</span> Changes in an estimate</a></li>
</ul></li>
<li><a href="exploratory-data-analysis.html#exploratory-data-analysis" id="toc-exploratory-data-analysis"><span class="toc-section-number">32</span> Exploratory Data Analysis</a></li>
<li><a href="sensitivity-analysis-robustness-check.html#sensitivity-analysis-robustness-check" id="toc-sensitivity-analysis-robustness-check"><span class="toc-section-number">33</span> Sensitivity Analysis/ Robustness Check</a>
<ul>
<li><a href="specification-curve.html#specification-curve" id="toc-specification-curve"><span class="toc-section-number">33.1</span> Specification curve</a>
<ul>
<li><a href="specification-curve.html#starbility" id="toc-starbility"><span class="toc-section-number">33.1.1</span> starbility</a></li>
<li><a href="specification-curve.html#rdfanalysis" id="toc-rdfanalysis"><span class="toc-section-number">33.1.2</span> rdfanalysis</a></li>
</ul></li>
<li><a href="coefficient-stability.html#coefficient-stability" id="toc-coefficient-stability"><span class="toc-section-number">33.2</span> Coefficient stability</a></li>
</ul></li>
<li><a href="#appendix-appendix" id="toc-appendix-appendix">(APPENDIX) APPENDIX</a></li>
<li><a href="appendix.html#appendix" id="toc-appendix"><span class="toc-section-number">34</span> Appendix</a>
<ul>
<li><a href="git.html#git" id="toc-git"><span class="toc-section-number">34.1</span> Git</a></li>
<li><a href="short-cut.html#short-cut" id="toc-short-cut"><span class="toc-section-number">34.2</span> Short-cut</a></li>
<li><a href="function-short-cut.html#function-short-cut" id="toc-function-short-cut"><span class="toc-section-number">34.3</span> Function short-cut</a></li>
<li><a href="citation.html#citation" id="toc-citation"><span class="toc-section-number">34.4</span> Citation</a></li>
<li><a href="install-all-necessary-packageslibaries-on-your-local-machine.html#install-all-necessary-packageslibaries-on-your-local-machine" id="toc-install-all-necessary-packageslibaries-on-your-local-machine"><span class="toc-section-number">34.5</span> Install all necessary packages/libaries on your local machine</a></li>
</ul></li>
<li><a href="bookdown-cheat-sheet.html#bookdown-cheat-sheet" id="toc-bookdown-cheat-sheet"><span class="toc-section-number">35</span> Bookdown cheat sheet</a>
<ul>
<li><a href="operation.html#operation" id="toc-operation"><span class="toc-section-number">35.1</span> Operation</a></li>
<li><a href="math-expresssion-syntax.html#math-expresssion-syntax" id="toc-math-expresssion-syntax"><span class="toc-section-number">35.2</span> Math Expresssion/ Syntax</a>
<ul>
<li><a href="math-expresssion-syntax.html#statistics-notation" id="toc-statistics-notation"><span class="toc-section-number">35.2.1</span> Statistics Notation</a></li>
</ul></li>
<li><a href="table.html#table" id="toc-table"><span class="toc-section-number">35.3</span> Table</a></li>
</ul></li>
<li><a href="references.html#references" id="toc-references">References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Guide on Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="solutions-to-missing-data" class="section level2" number="11.2">
<h2><span class="header-section-number">11.2</span> Solutions to Missing data</h2>
<div id="listwise-deletion" class="section level3" number="11.2.1">
<h3><span class="header-section-number">11.2.1</span> Listwise Deletion</h3>
<p>Also known as complete case deletion only where you only retain cases with complete data for all features.</p>
<p>Advantages:</p>
<ul>
<li><p>Can be applied to any statistical test (SEM, multi-level regression, etc.)</p></li>
<li><p>In the case of MCAR, both the parameters estimates and its standard errors are unbiased.</p></li>
<li><p>In the case of MAR among independent variables (not depend on the values of dependent variables), then listwise deletion parameter estimates can still be unbiased. <span class="citation">(<a href="#ref-Little_1992" role="doc-biblioref">Little 1992</a>)</span> For example, you have a model <span class="math inline">\(y=\beta_{0}+\beta_1X_1 + \beta_2X_2 +\epsilon\)</span> if the probability of missing data on X1 is independent of Y, but dependent on the value of X1 and X2, then the model estimates are still unbiased.</p>
<ul>
<li>The missing data mechanism the depends on the values of the independent variables are the same as stratified sampling. And stratified sampling does not bias your estimates</li>
<li>In the case of logistic regression, if the probability of missing data on any variable depends on the value of the dependent variable, but independent of the value of the independent variables, then the listwise deletion will yield biased intercept estimate, but consistent estimates of the slope and their standard errors <span class="citation">(<a href="#ref-Vach_1994" role="doc-biblioref">Vach 1994</a>)</span>. However, logistic regression will still fail if the probability of missing data is dependent on both the value of the dependent and independent variables.</li>
<li>Under regression analysis, listwise deletion is more robust than maximum likelihood and multiple imputation when MAR assumption is violated.</li>
</ul></li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>It will yield a larger standard errors than other more sophisticated methods discussed later.</li>
<li>If the data are not MCAR, but MAR, then your listwise deletion can yield biased estimates.</li>
<li>In other cases than regression analysis, other sophisticated methods can yield better estimates compared to listwise deletion.</li>
</ul>
</div>
<div id="pairwise-deletion" class="section level3" number="11.2.2">
<h3><span class="header-section-number">11.2.2</span> Pairwise Deletion</h3>
<p>This method could only be used in the case of linear models such as linear regression, factor analysis, or SEM. The premise of this method based on that the coefficient estimates are calculated based on the means, standard deviations, and correlation matrix. Compared to listwise deletion, we still utilized as many correlation between variables as possible to compute the correlation matrix.</p>
<p>Advantages:</p>
<ul>
<li><p>If the true missing data mechanism is MCAR, pair wise deletion will yield consistent estimates, and unbiased in large samples</p></li>
<li><p>Compared to listwise deletion: <span class="citation">(<a href="#ref-Glasser_1964" role="doc-biblioref">Glasser 1964</a>)</span></p>
<ul>
<li>If the correlation among variables are low, pairwise deletion is more efficient estimates than listwise</li>
<li>If the correlations among variables are high, listwise deletion is more efficient than pairwise.</li>
</ul></li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>If the data mechanism is MAR, pairwise deletion will yield biased estimates.</li>
<li>In small sample, sometimes covariance matrix might not be positive definite, which means coefficients estimates cannot be calculated.</li>
</ul>
<p><strong>Note</strong>: You need to read carefully on how your software specify the sample size because it will alter the standard errors.</p>
</div>
<div id="dummy-variable-adjustment" class="section level3" number="11.2.3">
<h3><span class="header-section-number">11.2.3</span> Dummy Variable Adjustment</h3>
<p>Also known as Missing Indicator Method or Proxy Variable</p>
<p>Add another variable in the database to indicate whether a value is missing.</p>
<p>Create 2 variables</p>
<span class="math display">\[\begin{equation}
D=
\begin{cases}
1 &amp; \text{data on X are missing} \\
0 &amp; \text{otherwise}\\
\end{cases}
\end{equation}\]</span>
<span class="math display">\[\begin{equation}
X^* =
\begin{cases}
X &amp; \text{data are available} \\
c &amp; \text{data are missing}\\
\end{cases}
\end{equation}\]</span>
<p><strong>Note</strong>: A typical choice for c is usually the mean of X</p>
<p>Interpretation:</p>
<ul>
<li>Coefficient of D is the the difference in the expected value of Y between the group with data and the group without data on X.</li>
<li>Coefficient of X* is the effect of the group with data on Y</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>This method yields bias estimates of the coefficient even in the case of MCAR <span class="citation">(<a href="#ref-Jones_1996" role="doc-biblioref">Jones 1996</a>)</span></li>
</ul>
</div>
<div id="imputation" class="section level3" number="11.2.4">
<h3><span class="header-section-number">11.2.4</span> Imputation</h3>
<div id="mean-mode-median-imputation" class="section level4" number="11.2.4.1">
<h4><span class="header-section-number">11.2.4.1</span> Mean, Mode, Median Imputation</h4>
<ul>
<li><p>Bad:</p>
<ul>
<li>Mean imputation does not preserve the relationships among variables</li>
<li>Mean imputation leads to An Underestimate of Standard Errors → you’re making Type I errors without realizing it.</li>
<li>Biased estimates of variances and covariances <span class="citation">(<a href="#ref-Haitovsky_1968" role="doc-biblioref">Haitovsky 1968</a>)</span></li>
<li>In high-dimensions, mean substitution cannot account for dependence structure among features.</li>
</ul></li>
</ul>
</div>
<div id="maximum-likelihood" class="section level4" number="11.2.4.2">
<h4><span class="header-section-number">11.2.4.2</span> Maximum Likelihood</h4>
<p>When missing data are MAR and monotonic (such as in the case of panel studies), ML can be adequately in estimating coefficients.</p>
<p>Monotonic means that if you are missing data on X1, then that observation also has missing data on all other variables that come after it.</p>
<p>ML can generally handle linear models, log-linear model, but beyond that, ML still lacks both theory and software to implement.</p>
<div id="expectation-maximization-algorithm-em-algorithm" class="section level5" number="11.2.4.2.1">
<h5><span class="header-section-number">11.2.4.2.1</span> Expectation-Maximization Algorithm (EM Algorithm)</h5>
<p>An iterative process:</p>
<ol style="list-style-type: decimal">
<li>Other variables are used to impute a value (Expectation).</li>
<li>Check whether the value is most likely (Maximization).</li>
<li>If not, it re-imputes a more likely value.</li>
</ol>
<p>You start your regression with your estimates based on either listwise deletion or pairwise deletion. After regressing missing variables on available variables, you obtain a regression model. Plug the missing data back into the original model, with modified variances and covariances For example, if you have missing data on <span class="math inline">\(X_{ij}\)</span> you would regress it on available data of <span class="math inline">\(X_{i(j)}\)</span>, then plug the expected value of <span class="math inline">\(X_{ij}\)</span> back with its <span class="math inline">\(X_{ij}^2\)</span> turn into <span class="math inline">\(X_{ij}^2 + s_{j(j)}^2\)</span> where <span class="math inline">\(s_{j(j)}^2\)</span> stands for the residual variance from regressing <span class="math inline">\(X_{ij}\)</span> on <span class="math inline">\(X_{i(j)}\)</span> With the new estimated model, you rerun the process until the estimates converge.</p>
<p>Advantages:</p>
<ol style="list-style-type: decimal">
<li>easy to use</li>
<li>preserves the relationship with other variables (important if you use Factor Analysis or Linear Regression later on), but best in the case of Factor Analysis, which doesn’t require standard error of individuals item.</li>
</ol>
<p>Disadvantages:</p>
<ol style="list-style-type: decimal">
<li>Standard errors of the coefficients are incorrect (biased usually downward - underestimate)</li>
<li>Models with overidentification, the estimates will not be efficient</li>
</ol>
</div>
<div id="direct-ml-raw-maximum-likelihood" class="section level5" number="11.2.4.2.2">
<h5><span class="header-section-number">11.2.4.2.2</span> Direct ML (raw maximum likelihood)</h5>
<p>Advantages</p>
<ol style="list-style-type: decimal">
<li>efficient estimates and correct standard errors.</li>
</ol>
<p>Disadvantages:</p>
<ol style="list-style-type: decimal">
<li>Hard to implements</li>
</ol>
</div>
</div>
<div id="multiple-imputation" class="section level4" number="11.2.4.3">
<h4><span class="header-section-number">11.2.4.3</span> Multiple Imputation</h4>
<p>MI is designed to use “the Bayesian model-based approach to <em>create</em> procedures, and the frequentist (randomization-based approach) to <em>evaluate</em> procedures”. <span class="citation">(<a href="#ref-Rubin_1996" role="doc-biblioref">Rubin 1996</a>)</span></p>
<p>MI estimates have the same properties as <a href="solutions-to-missing-data.html#maximum-likelihood">ML</a> when the data is <a href="assumptions-1.html#missing-at-random-mar">MAR</a></p>
<ul>
<li>Consistent</li>
<li>Asymptotically efficient</li>
<li>Asymptotically normal</li>
</ul>
<p>MI can be applied to any type of model, unlike <a href="solutions-to-missing-data.html#maximum-likelihood">Maximum Likelihood</a> that is only limited to a small set of models.</p>
<p>A drawback of MI is that it will produce slightly different estimates every time you run it. To avoid such problem, you can set seed when doing your analysis to ensure its reproducibility.</p>
<div id="single-random-imputation" class="section level5" number="11.2.4.3.1">
<h5><span class="header-section-number">11.2.4.3.1</span> Single Random Imputation</h5>
<p>Random draws form the residual distribution of each imputed variable and add those random numbers to the imputed values.</p>
<p>For example, if we have missing data on X, and it’s MCAR, then</p>
<ol style="list-style-type: decimal">
<li><p>regress X on Y (<a href="solutions-to-missing-data.html#listwise-deletion">Listwise Deletion</a> method) to get its residual distribution.</p></li>
<li><p>For every missing value on X, we substitute with <span class="math inline">\(\tilde{x_i}=\hat{x_i} + \rho u_i\)</span> where</p>
<ul>
<li><span class="math inline">\(u_i\)</span> is a random draw from a standard normal distribution</li>
<li><span class="math inline">\(x_i\)</span> is the predicted value from the regression of X and Y</li>
<li><span class="math inline">\(\rho\)</span> is the standard deviation of the residual distribution of X regressed on Y.</li>
</ul></li>
</ol>
<p>However, the model you run with the imputed data still thinks that your data are collected, not imputed, which leads your standard error estimates to be too low and test statistics too high.</p>
<p>To address this problem, we need to repeat the imputation process which leads us to repeated imputation or multiple random imputation.</p>
</div>
<div id="repeated-imputation" class="section level5" number="11.2.4.3.2">
<h5><span class="header-section-number">11.2.4.3.2</span> Repeated Imputation</h5>
<p>“Repeated imputations are draws from the posterior predictive distribution of the missing values under a specific model , a particular Bayesian model for both the data and the missing mechanism”.<span class="citation">(<a href="#ref-Rubin_1996" role="doc-biblioref">Rubin 1996</a>)</span></p>
<p>Repeated imputation, also known as, multiple random imputation, allows us to have multiple “completed” data sets. The variability across imputations will adjust the standard errors upward.</p>
<p>The estimate of the standard error of <span class="math inline">\(\bar{r}\)</span> (mean correlation estimates between X and Y) is <span class="math display">\[
SE(\bar{r})=\sqrt{\frac{1}{M}\sum_{k}s_k^2+ (1+\frac{1}{M})(\frac{1}{M-1})\sum_{k}(r_k-\bar{r})^2}
\]</span> where M is the number of replications, <span class="math inline">\(r_k\)</span> is the the correlation in replication k, <span class="math inline">\(s_k\)</span> is the estimated standard error in replication k.</p>
<p>However, this method still considers the parameter in predicting <span class="math inline">\(\tilde{x}\)</span> is still fixed, which means we assume that we are using the true parameters to predict <span class="math inline">\(\tilde{x}\)</span>. To overcome this challenge, we need to introduce variability into our model for <span class="math inline">\(\tilde{x}\)</span> by treating the parameters as a random variables and use Bayesian posterior distribution of the parameters to predict the parameters.</p>
<p>However, if your sample is large and the proportion of missing data is small, the extra Bayesian step might not be necessary. If your sample is small or the proportion of missing data is large, the extra Bayesian step is necessary.</p>
<p>Two algorithms to get random draws of the regression parameters from its posterior distribution:</p>
<ul>
<li><a href="solutions-to-missing-data.html#data-augmentation">Data Augmentation</a></li>
<li>Sampling importance/resampling (SIR)</li>
</ul>
<p>Authors have argued for SIR superiority due to its computer time <span class="citation">(<a href="#ref-King_2001" role="doc-biblioref">G. King et al. 2001</a>)</span></p>
<div id="data-augmentation" class="section level6" number="11.2.4.3.2.1">
<h6><span class="header-section-number">11.2.4.3.2.1</span> Data Augmentation</h6>
<p>Steps for data augmentation:</p>
<ol style="list-style-type: decimal">
<li>Choose starting values for the parameters (e.g., for multivariate normal, choose means and covariance matrix). These values can come from previous values, expert knowledge, or from listwise deletion or pairwise deletion or EM estimation.</li>
<li>Based on the current values of means and covariances calculate the coefficients estimates for the equation that variable with missing data is regressed on all other variables (or variables that you think will help predict the missing values, could also be variables that are not in the final estimation model)</li>
<li>Use the estimates in step (2) to predict values for missing values. For each predicted value, add a random error from the residual normal distribution for that variable.</li>
<li>From the “complete” data set, recalculate the means and covariance matrix. And take a random draw from the posterior distribution of the means and covariances with Jeffreys’ prior.</li>
<li>Using the random draw from step (4), repeat step (2) to (4) until the means and covariances stabilize (converged).</li>
</ol>
<p>The iterative process allows us to get random draws from the joint posterior distribution of both data nd parameters, given the observed data.</p>
<p>Rules of thumb regarding convergence:</p>
<ul>
<li>The higher the proportion of missing, the more iterations</li>
<li>the rate of convergence for EM algorithm should be the minimum threshold for DA.</li>
<li>You can also check if your distribution has been converged by diagnostic statistics Can check <a href="https://bookdown.org/mike/bayesian_analysis/diag.html">Bayesian Diagnostics</a> for some introduction.</li>
</ul>
<p>Types of chains</p>
<ol style="list-style-type: decimal">
<li><p><strong>Parallel</strong>: Run a separate chain of iterations for each of data set. Different starting values are encouraged. For example, one could use bootstrap to generate different data set with replacement, and for each data set, calculate the starting values by EM estimates.</p>
<ul>
<li>Pro: Run faster, and less likely to have dependence in the resulting data sets.</li>
<li>Con: Sometimes it will not converge</li>
</ul></li>
<li><p><strong>Sequential</strong> one long chain of data augmentation cycles. After burn-in and thinning, you will have to data sets</p>
<ul>
<li>Pro: Converged to the true posterior distribution is more likely.</li>
<li>Con: The resulting data sets are likely to be dependent. Remedies can be thinning and burn-in.</li>
</ul></li>
</ol>
<p><strong>Note on Non-normal or categorical data</strong> The normal-based methods still work well, but you will need to do some transformation. For example,</p>
<ul>
<li>If the data is skewed, then log-transform, then impute, the exponentiate to have the missing data back to its original metric.</li>
<li>If the data is proportion, logit-transform, impute, then de-transform the missing data.</li>
</ul>
<p>If you want to impute non-linear relationship, such as interaction between 2 variables and 1 variable is categorical. You can do separate imputation for different levels of that variable separately, then combined for the final analysis.</p>
<ul>
<li>If all variables that have missing data are categorical, then <strong>unrestricted multinomial model</strong> or <strong>log-linear model</strong> is recommended.</li>
<li>If a single categorical variable, <strong>logistic (logit) regression</strong> would be sufficient.</li>
</ul>
</div>
</div>
</div>
<div id="nonparametric-semiparametric-methods" class="section level4" number="11.2.4.4">
<h4><span class="header-section-number">11.2.4.4</span> Nonparametric/ Semiparametric Methods</h4>
<div id="hot-deck-imputation" class="section level5" number="11.2.4.4.1">
<h5><span class="header-section-number">11.2.4.4.1</span> Hot Deck Imputation</h5>
<ul>
<li>Used by U.S. Census Bureau for public datasets</li>
<li>approximate Bayesian bootstrap</li>
<li>A randomly chosen value from an individual in the sample who has similar values on other variables. In other words, find all the sample subjects who are similar on other variables, then randomly choose one of their values on the missing variable.</li>
</ul>
<p>When we have <span class="math inline">\(n_1\)</span> cases with complete data on Y and <span class="math inline">\(n_0\)</span> cases with missing data on Y</p>
<ul>
<li>Step 1: From <span class="math inline">\(n_1\)</span>, take a random sample (with replacement) of <span class="math inline">\(n_1\)</span> cases</li>
<li>Step 2: From the retrieved sample take a random sample (with replacement) of <span class="math inline">\(n_0\)</span> cases</li>
<li>Step 3: Assign the <span class="math inline">\(n_0\)</span> cases in step 2 to <span class="math inline">\(n_0\)</span> missing data cases.</li>
<li>Step 4: Repeat the process for every variable.</li>
<li>Step 5: For multiple imputation, repeat the four steps multiple times.</li>
</ul>
<p>Note:</p>
<ul>
<li><p>If we skip step 1, it reduce variability for estimating standard errors.</p></li>
<li><p>Good:</p>
<ul>
<li>Constrained to only possible values.</li>
<li>Since the value is picked at random, it adds some variability, which might come in handy when calculating standard errors.</li>
</ul></li>
<li><p>Challenge: how can you define “similar” here.</p></li>
</ul>
</div>
<div id="cold-deck-imputation" class="section level5" number="11.2.4.4.2">
<h5><span class="header-section-number">11.2.4.4.2</span> Cold Deck Imputation</h5>
<p>Contrary to Hot Deck, Cold Deck choose value systematically from an observation that has similar values on other variables, which remove the random variation that we want.</p>
<p>Donor samples of “cold-deck” imputation come from a different data set.</p>
</div>
<div id="predictive-mean-matching" class="section level5" number="11.2.4.4.3">
<h5><span class="header-section-number">11.2.4.4.3</span> Predictive Mean Matching</h5>
<p>Steps:</p>
<ol style="list-style-type: decimal">
<li>Regress Y on X (matrix of covariates) for the <span class="math inline">\(n_1\)</span> (i.e., non-missing cases) to get coefficients <span class="math inline">\(b\)</span> (a <span class="math inline">\(k \times 1\)</span> vector) and residual variance estimates <span class="math inline">\(s^2\)</span></li>
<li>Draw randomly from the posterior predictive distribution of the residual variance (assuming a noninformative prior) by calculating <span class="math inline">\(\frac{(n_1-k)s^2}{\chi^2}\)</span>, where <span class="math inline">\(\chi^2\)</span> is a random draw from a <span class="math inline">\(\chi^2_{n_1-k}\)</span> and let <span class="math inline">\(s^2_{[1]}\)</span> be an i-th random draw</li>
<li>Randomly draw from the posterior distribution of the coefficients <span class="math inline">\(b\)</span>, by drawing from <span class="math inline">\(MVN(b, s^2_{[1]}(X&#39;X)^{-1})\)</span>, where X is an <span class="math inline">\(n_1 \times k\)</span> matrix of X values. Then we have <span class="math inline">\(b_{1}\)</span></li>
<li>Using step 1, we can calculate standardized residuals for <span class="math inline">\(n_1\)</span> cases: <span class="math inline">\(e_i = \frac{y_i - bx_i}{\sqrt{s^2(1-k/n_1)}}\)</span></li>
<li>Randomly draw a sample (with replacement) of <span class="math inline">\(n_0\)</span> from the <span class="math inline">\(n_1\)</span> residuals in step 4</li>
<li>With <span class="math inline">\(n_0\)</span> cases, we can calculate imputed values of Y: <span class="math inline">\(y_i = b_{[1]}x_i + s_{[1]}e_i\)</span> where <span class="math inline">\(e_i\)</span> are taken from step 5, and <span class="math inline">\(b_{[1]}\)</span> taken from step 3, and <span class="math inline">\(s_{[1]}\)</span> taken from step 2.</li>
<li>Repeat steps 2 through 6 except for step 4.</li>
</ol>
<p>Notes:</p>
<ul>
<li>can be used for multiple variables where each variable is imputed using all other variables as predictor.</li>
<li>can also be used for heteroskedasticity in imputed values.</li>
</ul>
<p>Example from <a href="https://statisticsglobe.com/predictive-mean-matching-imputation-method/">Statistics Globle</a></p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb220-1"><a href="solutions-to-missing-data.html#cb220-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">918273</span>)                                <span class="co"># Seed</span></span>
<span id="cb220-2"><a href="solutions-to-missing-data.html#cb220-2" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">3000</span>                                       <span class="co"># Sample size</span></span>
<span id="cb220-3"><a href="solutions-to-missing-data.html#cb220-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fu">runif</span>(N, <span class="sc">-</span><span class="dv">10</span>, <span class="dv">10</span>))                   <span class="co"># Target variable Y</span></span>
<span id="cb220-4"><a href="solutions-to-missing-data.html#cb220-4" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> y <span class="sc">+</span> <span class="fu">round</span>(<span class="fu">runif</span>(N, <span class="dv">0</span>, <span class="dv">50</span>))                <span class="co"># Auxiliary variable 1</span></span>
<span id="cb220-5"><a href="solutions-to-missing-data.html#cb220-5" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">round</span>(y <span class="sc">+</span> <span class="fl">0.25</span> <span class="sc">*</span> x1 <span class="sc">+</span> <span class="fu">rnorm</span>(N, <span class="sc">-</span> <span class="dv">3</span>, <span class="dv">15</span>))  <span class="co"># Auxiliary variable 2</span></span>
<span id="cb220-6"><a href="solutions-to-missing-data.html#cb220-6" aria-hidden="true" tabindex="-1"></a>x3 <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fl">0.1</span> <span class="sc">*</span> x1 <span class="sc">+</span> <span class="fu">rpois</span>(N, <span class="dv">2</span>))             <span class="co"># Auxiliary variable 3</span></span>
<span id="cb220-7"><a href="solutions-to-missing-data.html#cb220-7" aria-hidden="true" tabindex="-1"></a>x4 <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">round</span>(<span class="fl">0.02</span> <span class="sc">*</span> y <span class="sc">+</span> <span class="fu">runif</span>(N)))     <span class="co"># Auxiliary variable 4 (categorical variable)</span></span>
<span id="cb220-8"><a href="solutions-to-missing-data.html#cb220-8" aria-hidden="true" tabindex="-1"></a>y[<span class="fu">rbinom</span>(N, <span class="dv">1</span>, <span class="fl">0.2</span>) <span class="sc">==</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="cn">NA</span>                 <span class="co"># Insert 20% missing data in Y</span></span>
<span id="cb220-9"><a href="solutions-to-missing-data.html#cb220-9" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, x1, x2, x3, x4)           <span class="co"># Store data in dataset</span></span>
<span id="cb220-10"><a href="solutions-to-missing-data.html#cb220-10" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(data)                                      <span class="co"># First 6 rows of our data</span></span>
<span id="cb220-11"><a href="solutions-to-missing-data.html#cb220-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;    y x1  x2 x3 x4</span></span>
<span id="cb220-12"><a href="solutions-to-missing-data.html#cb220-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1  8 38  -3  6  1</span></span>
<span id="cb220-13"><a href="solutions-to-missing-data.html#cb220-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2  1 50  -9  5  0</span></span>
<span id="cb220-14"><a href="solutions-to-missing-data.html#cb220-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 3  5 43  20  5  1</span></span>
<span id="cb220-15"><a href="solutions-to-missing-data.html#cb220-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 4 NA  9  13  3  0</span></span>
<span id="cb220-16"><a href="solutions-to-missing-data.html#cb220-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 5 -4 40 -10  6  0</span></span>
<span id="cb220-17"><a href="solutions-to-missing-data.html#cb220-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 6 NA 29  -6  5  1</span></span>
<span id="cb220-18"><a href="solutions-to-missing-data.html#cb220-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-19"><a href="solutions-to-missing-data.html#cb220-19" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;mice&quot;</span>)                                 <span class="co"># Load mice package</span></span>
<span id="cb220-20"><a href="solutions-to-missing-data.html#cb220-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-21"><a href="solutions-to-missing-data.html#cb220-21" aria-hidden="true" tabindex="-1"></a><span class="do">##### Impute data via predictive mean matching (single imputation)#####</span></span>
<span id="cb220-22"><a href="solutions-to-missing-data.html#cb220-22" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb220-23"><a href="solutions-to-missing-data.html#cb220-23" aria-hidden="true" tabindex="-1"></a>imp_single <span class="ot">&lt;-</span> <span class="fu">mice</span>(data, <span class="at">m =</span> <span class="dv">1</span>, <span class="at">method =</span> <span class="st">&quot;pmm&quot;</span>) <span class="co"># Impute missing values</span></span>
<span id="cb220-24"><a href="solutions-to-missing-data.html#cb220-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb220-25"><a href="solutions-to-missing-data.html#cb220-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  iter imp variable</span></span>
<span id="cb220-26"><a href="solutions-to-missing-data.html#cb220-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   1   1  y</span></span>
<span id="cb220-27"><a href="solutions-to-missing-data.html#cb220-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   2   1  y</span></span>
<span id="cb220-28"><a href="solutions-to-missing-data.html#cb220-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   3   1  y</span></span>
<span id="cb220-29"><a href="solutions-to-missing-data.html#cb220-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   4   1  y</span></span>
<span id="cb220-30"><a href="solutions-to-missing-data.html#cb220-30" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   5   1  y</span></span>
<span id="cb220-31"><a href="solutions-to-missing-data.html#cb220-31" aria-hidden="true" tabindex="-1"></a>data_imp_single <span class="ot">&lt;-</span> <span class="fu">complete</span>(imp_single)         <span class="co"># Store imputed data</span></span>
<span id="cb220-32"><a href="solutions-to-missing-data.html#cb220-32" aria-hidden="true" tabindex="-1"></a><span class="co"># head(data_imp_single)</span></span>
<span id="cb220-33"><a href="solutions-to-missing-data.html#cb220-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-34"><a href="solutions-to-missing-data.html#cb220-34" aria-hidden="true" tabindex="-1"></a><span class="co"># SInce single imputation underestiamtes stnadard errors, we use multiple imputaiton</span></span>
<span id="cb220-35"><a href="solutions-to-missing-data.html#cb220-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-36"><a href="solutions-to-missing-data.html#cb220-36" aria-hidden="true" tabindex="-1"></a><span class="do">##### Predictive mean matching (multiple imputation)#####</span></span>
<span id="cb220-37"><a href="solutions-to-missing-data.html#cb220-37" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb220-38"><a href="solutions-to-missing-data.html#cb220-38" aria-hidden="true" tabindex="-1"></a>imp_multi <span class="ot">&lt;-</span> <span class="fu">mice</span>(data, <span class="at">m =</span> <span class="dv">5</span>, <span class="at">method =</span> <span class="st">&quot;pmm&quot;</span>)  <span class="co"># Impute missing values multiple times</span></span>
<span id="cb220-39"><a href="solutions-to-missing-data.html#cb220-39" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb220-40"><a href="solutions-to-missing-data.html#cb220-40" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  iter imp variable</span></span>
<span id="cb220-41"><a href="solutions-to-missing-data.html#cb220-41" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   1   1  y</span></span>
<span id="cb220-42"><a href="solutions-to-missing-data.html#cb220-42" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   1   2  y</span></span>
<span id="cb220-43"><a href="solutions-to-missing-data.html#cb220-43" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   1   3  y</span></span>
<span id="cb220-44"><a href="solutions-to-missing-data.html#cb220-44" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   1   4  y</span></span>
<span id="cb220-45"><a href="solutions-to-missing-data.html#cb220-45" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   1   5  y</span></span>
<span id="cb220-46"><a href="solutions-to-missing-data.html#cb220-46" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   2   1  y</span></span>
<span id="cb220-47"><a href="solutions-to-missing-data.html#cb220-47" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   2   2  y</span></span>
<span id="cb220-48"><a href="solutions-to-missing-data.html#cb220-48" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   2   3  y</span></span>
<span id="cb220-49"><a href="solutions-to-missing-data.html#cb220-49" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   2   4  y</span></span>
<span id="cb220-50"><a href="solutions-to-missing-data.html#cb220-50" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   2   5  y</span></span>
<span id="cb220-51"><a href="solutions-to-missing-data.html#cb220-51" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   3   1  y</span></span>
<span id="cb220-52"><a href="solutions-to-missing-data.html#cb220-52" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   3   2  y</span></span>
<span id="cb220-53"><a href="solutions-to-missing-data.html#cb220-53" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   3   3  y</span></span>
<span id="cb220-54"><a href="solutions-to-missing-data.html#cb220-54" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   3   4  y</span></span>
<span id="cb220-55"><a href="solutions-to-missing-data.html#cb220-55" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   3   5  y</span></span>
<span id="cb220-56"><a href="solutions-to-missing-data.html#cb220-56" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   4   1  y</span></span>
<span id="cb220-57"><a href="solutions-to-missing-data.html#cb220-57" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   4   2  y</span></span>
<span id="cb220-58"><a href="solutions-to-missing-data.html#cb220-58" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   4   3  y</span></span>
<span id="cb220-59"><a href="solutions-to-missing-data.html#cb220-59" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   4   4  y</span></span>
<span id="cb220-60"><a href="solutions-to-missing-data.html#cb220-60" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   4   5  y</span></span>
<span id="cb220-61"><a href="solutions-to-missing-data.html#cb220-61" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   5   1  y</span></span>
<span id="cb220-62"><a href="solutions-to-missing-data.html#cb220-62" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   5   2  y</span></span>
<span id="cb220-63"><a href="solutions-to-missing-data.html#cb220-63" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   5   3  y</span></span>
<span id="cb220-64"><a href="solutions-to-missing-data.html#cb220-64" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   5   4  y</span></span>
<span id="cb220-65"><a href="solutions-to-missing-data.html#cb220-65" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   5   5  y</span></span>
<span id="cb220-66"><a href="solutions-to-missing-data.html#cb220-66" aria-hidden="true" tabindex="-1"></a>data_imp_multi_all <span class="ot">&lt;-</span> <span class="fu">complete</span>(imp_multi,       <span class="co"># Store multiply imputed data</span></span>
<span id="cb220-67"><a href="solutions-to-missing-data.html#cb220-67" aria-hidden="true" tabindex="-1"></a>                           <span class="st">&quot;repeated&quot;</span>,</span>
<span id="cb220-68"><a href="solutions-to-missing-data.html#cb220-68" aria-hidden="true" tabindex="-1"></a>                           <span class="at">include =</span> <span class="cn">TRUE</span>)</span>
<span id="cb220-69"><a href="solutions-to-missing-data.html#cb220-69" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb220-70"><a href="solutions-to-missing-data.html#cb220-70" aria-hidden="true" tabindex="-1"></a>data_imp_multi <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(                   <span class="co"># Combine imputed Y and X1-X4 (for convenience)</span></span>
<span id="cb220-71"><a href="solutions-to-missing-data.html#cb220-71" aria-hidden="true" tabindex="-1"></a>  data_imp_multi_all[ , <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>], data[, <span class="dv">2</span><span class="sc">:</span><span class="dv">5</span>])</span>
<span id="cb220-72"><a href="solutions-to-missing-data.html#cb220-72" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(data_imp_multi)                            <span class="co"># First 6 rows of our multiply imputed data</span></span>
<span id="cb220-73"><a href="solutions-to-missing-data.html#cb220-73" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   y.0 y.1 y.2 y.3 y.4 y.5 x1  x2 x3 x4</span></span>
<span id="cb220-74"><a href="solutions-to-missing-data.html#cb220-74" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1   8   8   8   8   8   8 38  -3  6  1</span></span>
<span id="cb220-75"><a href="solutions-to-missing-data.html#cb220-75" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2   1   1   1   1   1   1 50  -9  5  0</span></span>
<span id="cb220-76"><a href="solutions-to-missing-data.html#cb220-76" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 3   5   5   5   5   5   5 43  20  5  1</span></span>
<span id="cb220-77"><a href="solutions-to-missing-data.html#cb220-77" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 4  NA  -6  -4  -4  -1  -3  9  13  3  0</span></span>
<span id="cb220-78"><a href="solutions-to-missing-data.html#cb220-78" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 5  -4  -4  -4  -4  -4  -4 40 -10  6  0</span></span>
<span id="cb220-79"><a href="solutions-to-missing-data.html#cb220-79" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 6  NA  -8   5  -4   1   4 29  -6  5  1</span></span></code></pre></div>
<p>Example from <a href="https://stats.idre.ucla.edu/r/faq/how-do-i-perform-multiple-imputation-using-predictive-mean-matching-in-r/">UCLA Statistical Consulting</a> <span class="citation">(<a href="#ref-newtest" role="doc-biblioref">Bruin 2011</a>)</span></p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="solutions-to-missing-data.html#cb221-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mice)</span>
<span id="cb221-2"><a href="solutions-to-missing-data.html#cb221-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(VIM)</span>
<span id="cb221-3"><a href="solutions-to-missing-data.html#cb221-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lattice)</span>
<span id="cb221-4"><a href="solutions-to-missing-data.html#cb221-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb221-5"><a href="solutions-to-missing-data.html#cb221-5" aria-hidden="true" tabindex="-1"></a><span class="do">## set observations to NA</span></span>
<span id="cb221-6"><a href="solutions-to-missing-data.html#cb221-6" aria-hidden="true" tabindex="-1"></a>anscombe <span class="ot">&lt;-</span> <span class="fu">within</span>(anscombe, {</span>
<span id="cb221-7"><a href="solutions-to-missing-data.html#cb221-7" aria-hidden="true" tabindex="-1"></a>    y1[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="cn">NA</span></span>
<span id="cb221-8"><a href="solutions-to-missing-data.html#cb221-8" aria-hidden="true" tabindex="-1"></a>    y4[<span class="dv">3</span><span class="sc">:</span><span class="dv">5</span>] <span class="ot">&lt;-</span> <span class="cn">NA</span></span>
<span id="cb221-9"><a href="solutions-to-missing-data.html#cb221-9" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb221-10"><a href="solutions-to-missing-data.html#cb221-10" aria-hidden="true" tabindex="-1"></a><span class="do">## view</span></span>
<span id="cb221-11"><a href="solutions-to-missing-data.html#cb221-11" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(anscombe)</span>
<span id="cb221-12"><a href="solutions-to-missing-data.html#cb221-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   x1 x2 x3 x4   y1   y2    y3   y4</span></span>
<span id="cb221-13"><a href="solutions-to-missing-data.html#cb221-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 10 10 10  8   NA 9.14  7.46 6.58</span></span>
<span id="cb221-14"><a href="solutions-to-missing-data.html#cb221-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2  8  8  8  8   NA 8.14  6.77 5.76</span></span>
<span id="cb221-15"><a href="solutions-to-missing-data.html#cb221-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 3 13 13 13  8   NA 8.74 12.74   NA</span></span>
<span id="cb221-16"><a href="solutions-to-missing-data.html#cb221-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 4  9  9  9  8 8.81 8.77  7.11   NA</span></span>
<span id="cb221-17"><a href="solutions-to-missing-data.html#cb221-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 5 11 11 11  8 8.33 9.26  7.81   NA</span></span>
<span id="cb221-18"><a href="solutions-to-missing-data.html#cb221-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 6 14 14 14  8 9.96 8.10  8.84 7.04</span></span>
<span id="cb221-19"><a href="solutions-to-missing-data.html#cb221-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb221-20"><a href="solutions-to-missing-data.html#cb221-20" aria-hidden="true" tabindex="-1"></a><span class="do">## check missing data patterns</span></span>
<span id="cb221-21"><a href="solutions-to-missing-data.html#cb221-21" aria-hidden="true" tabindex="-1"></a><span class="fu">md.pattern</span>(anscombe)</span></code></pre></div>
<p><img src="11-imputation_files/figure-html/unnamed-chunk-2-1.png" width="90%" style="display: block; margin: auto;" /></p>
<pre><code>#&gt;   x1 x2 x3 x4 y2 y3 y1 y4  
#&gt; 6  1  1  1  1  1  1  1  1 0
#&gt; 2  1  1  1  1  1  1  1  0 1
#&gt; 2  1  1  1  1  1  1  0  1 1
#&gt; 1  1  1  1  1  1  1  0  0 2
#&gt;    0  0  0  0  0  0  3  3 6

## Number of observations per patterns for all pairs of variables
p &lt;- md.pairs(anscombe)
p # rr = number of observations where both pairs of values are observed
#&gt; $rr
#&gt;    x1 x2 x3 x4 y1 y2 y3 y4
#&gt; x1 11 11 11 11  8 11 11  8
#&gt; x2 11 11 11 11  8 11 11  8
#&gt; x3 11 11 11 11  8 11 11  8
#&gt; x4 11 11 11 11  8 11 11  8
#&gt; y1  8  8  8  8  8  8  8  6
#&gt; y2 11 11 11 11  8 11 11  8
#&gt; y3 11 11 11 11  8 11 11  8
#&gt; y4  8  8  8  8  6  8  8  8
#&gt; 
#&gt; $rm
#&gt;    x1 x2 x3 x4 y1 y2 y3 y4
#&gt; x1  0  0  0  0  3  0  0  3
#&gt; x2  0  0  0  0  3  0  0  3
#&gt; x3  0  0  0  0  3  0  0  3
#&gt; x4  0  0  0  0  3  0  0  3
#&gt; y1  0  0  0  0  0  0  0  2
#&gt; y2  0  0  0  0  3  0  0  3
#&gt; y3  0  0  0  0  3  0  0  3
#&gt; y4  0  0  0  0  2  0  0  0
#&gt; 
#&gt; $mr
#&gt;    x1 x2 x3 x4 y1 y2 y3 y4
#&gt; x1  0  0  0  0  0  0  0  0
#&gt; x2  0  0  0  0  0  0  0  0
#&gt; x3  0  0  0  0  0  0  0  0
#&gt; x4  0  0  0  0  0  0  0  0
#&gt; y1  3  3  3  3  0  3  3  2
#&gt; y2  0  0  0  0  0  0  0  0
#&gt; y3  0  0  0  0  0  0  0  0
#&gt; y4  3  3  3  3  2  3  3  0
#&gt; 
#&gt; $mm
#&gt;    x1 x2 x3 x4 y1 y2 y3 y4
#&gt; x1  0  0  0  0  0  0  0  0
#&gt; x2  0  0  0  0  0  0  0  0
#&gt; x3  0  0  0  0  0  0  0  0
#&gt; x4  0  0  0  0  0  0  0  0
#&gt; y1  0  0  0  0  3  0  0  1
#&gt; y2  0  0  0  0  0  0  0  0
#&gt; y3  0  0  0  0  0  0  0  0
#&gt; y4  0  0  0  0  1  0  0  3
# rm = the number of observations where both variables are missing values
# mr = the number of observations where the first variable’s value (e.g. the row variable) is observed and second (or column) variable is missing
# mm = the number of observations where the second variable’s value (e.g. the col variable) is observed and first (or row) variable is missing

## Margin plot of y1 and y4
marginplot(anscombe[c(5, 8)], col = c(&quot;blue&quot;, &quot;red&quot;, &quot;orange&quot;))</code></pre>
<p><img src="11-imputation_files/figure-html/unnamed-chunk-2-2.png" width="90%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb223-1"><a href="solutions-to-missing-data.html#cb223-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb223-2"><a href="solutions-to-missing-data.html#cb223-2" aria-hidden="true" tabindex="-1"></a><span class="do">## 5 imputations for all missing values</span></span>
<span id="cb223-3"><a href="solutions-to-missing-data.html#cb223-3" aria-hidden="true" tabindex="-1"></a>imp1 <span class="ot">&lt;-</span> <span class="fu">mice</span>(anscombe, <span class="at">m =</span> <span class="dv">5</span>)</span>
<span id="cb223-4"><a href="solutions-to-missing-data.html#cb223-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb223-5"><a href="solutions-to-missing-data.html#cb223-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  iter imp variable</span></span>
<span id="cb223-6"><a href="solutions-to-missing-data.html#cb223-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   1   1  y1  y4</span></span>
<span id="cb223-7"><a href="solutions-to-missing-data.html#cb223-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   1   2  y1  y4</span></span>
<span id="cb223-8"><a href="solutions-to-missing-data.html#cb223-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   1   3  y1  y4</span></span>
<span id="cb223-9"><a href="solutions-to-missing-data.html#cb223-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   1   4  y1  y4</span></span>
<span id="cb223-10"><a href="solutions-to-missing-data.html#cb223-10" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   1   5  y1  y4</span></span>
<span id="cb223-11"><a href="solutions-to-missing-data.html#cb223-11" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   2   1  y1  y4</span></span>
<span id="cb223-12"><a href="solutions-to-missing-data.html#cb223-12" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   2   2  y1  y4</span></span>
<span id="cb223-13"><a href="solutions-to-missing-data.html#cb223-13" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   2   3  y1  y4</span></span>
<span id="cb223-14"><a href="solutions-to-missing-data.html#cb223-14" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   2   4  y1  y4</span></span>
<span id="cb223-15"><a href="solutions-to-missing-data.html#cb223-15" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   2   5  y1  y4</span></span>
<span id="cb223-16"><a href="solutions-to-missing-data.html#cb223-16" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   3   1  y1  y4</span></span>
<span id="cb223-17"><a href="solutions-to-missing-data.html#cb223-17" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   3   2  y1  y4</span></span>
<span id="cb223-18"><a href="solutions-to-missing-data.html#cb223-18" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   3   3  y1  y4</span></span>
<span id="cb223-19"><a href="solutions-to-missing-data.html#cb223-19" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   3   4  y1  y4</span></span>
<span id="cb223-20"><a href="solutions-to-missing-data.html#cb223-20" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   3   5  y1  y4</span></span>
<span id="cb223-21"><a href="solutions-to-missing-data.html#cb223-21" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   4   1  y1  y4</span></span>
<span id="cb223-22"><a href="solutions-to-missing-data.html#cb223-22" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   4   2  y1  y4</span></span>
<span id="cb223-23"><a href="solutions-to-missing-data.html#cb223-23" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   4   3  y1  y4</span></span>
<span id="cb223-24"><a href="solutions-to-missing-data.html#cb223-24" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   4   4  y1  y4</span></span>
<span id="cb223-25"><a href="solutions-to-missing-data.html#cb223-25" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   4   5  y1  y4</span></span>
<span id="cb223-26"><a href="solutions-to-missing-data.html#cb223-26" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   5   1  y1  y4</span></span>
<span id="cb223-27"><a href="solutions-to-missing-data.html#cb223-27" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   5   2  y1  y4</span></span>
<span id="cb223-28"><a href="solutions-to-missing-data.html#cb223-28" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   5   3  y1  y4</span></span>
<span id="cb223-29"><a href="solutions-to-missing-data.html#cb223-29" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   5   4  y1  y4</span></span>
<span id="cb223-30"><a href="solutions-to-missing-data.html#cb223-30" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   5   5  y1  y4</span></span>
<span id="cb223-31"><a href="solutions-to-missing-data.html#cb223-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb223-32"><a href="solutions-to-missing-data.html#cb223-32" aria-hidden="true" tabindex="-1"></a><span class="do">## linear regression for each imputed data set - 5 regression are run</span></span>
<span id="cb223-33"><a href="solutions-to-missing-data.html#cb223-33" aria-hidden="true" tabindex="-1"></a>fitm <span class="ot">&lt;-</span> <span class="fu">with</span>(imp1, <span class="fu">lm</span>(y1 <span class="sc">~</span> y4 <span class="sc">+</span> x1))</span>
<span id="cb223-34"><a href="solutions-to-missing-data.html#cb223-34" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fitm)</span>
<span id="cb223-35"><a href="solutions-to-missing-data.html#cb223-35" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; # A tibble: 15 x 6</span></span>
<span id="cb223-36"><a href="solutions-to-missing-data.html#cb223-36" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;    term        estimate std.error statistic p.value  nobs</span></span>
<span id="cb223-37"><a href="solutions-to-missing-data.html#cb223-37" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;    &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;</span></span>
<span id="cb223-38"><a href="solutions-to-missing-data.html#cb223-38" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  1 (Intercept)    6.44      1.77       3.63 0.00664    11</span></span>
<span id="cb223-39"><a href="solutions-to-missing-data.html#cb223-39" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  2 y4            -0.381     0.162     -2.35 0.0468     11</span></span>
<span id="cb223-40"><a href="solutions-to-missing-data.html#cb223-40" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  3 x1             0.458     0.101      4.56 0.00186    11</span></span>
<span id="cb223-41"><a href="solutions-to-missing-data.html#cb223-41" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  4 (Intercept)    6.17      2.02       3.06 0.0156     11</span></span>
<span id="cb223-42"><a href="solutions-to-missing-data.html#cb223-42" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  5 y4            -0.351     0.183     -1.92 0.0914     11</span></span>
<span id="cb223-43"><a href="solutions-to-missing-data.html#cb223-43" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  6 x1             0.443     0.117      3.79 0.00535    11</span></span>
<span id="cb223-44"><a href="solutions-to-missing-data.html#cb223-44" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  7 (Intercept)    6.47      2.02       3.21 0.0125     11</span></span>
<span id="cb223-45"><a href="solutions-to-missing-data.html#cb223-45" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  8 y4            -0.360     0.186     -1.93 0.0893     11</span></span>
<span id="cb223-46"><a href="solutions-to-missing-data.html#cb223-46" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  9 x1             0.430     0.114      3.77 0.00549    11</span></span>
<span id="cb223-47"><a href="solutions-to-missing-data.html#cb223-47" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 10 (Intercept)    6.51      2.63       2.48 0.0384     11</span></span>
<span id="cb223-48"><a href="solutions-to-missing-data.html#cb223-48" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 11 y4            -0.358     0.242     -1.48 0.178      11</span></span>
<span id="cb223-49"><a href="solutions-to-missing-data.html#cb223-49" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 12 x1             0.433     0.149      2.91 0.0195     11</span></span>
<span id="cb223-50"><a href="solutions-to-missing-data.html#cb223-50" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 13 (Intercept)    6.15      2.37       2.60 0.0318     11</span></span>
<span id="cb223-51"><a href="solutions-to-missing-data.html#cb223-51" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 14 y4            -0.362     0.219     -1.65 0.137      11</span></span>
<span id="cb223-52"><a href="solutions-to-missing-data.html#cb223-52" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 15 x1             0.478     0.138      3.46 0.00855    11</span></span>
<span id="cb223-53"><a href="solutions-to-missing-data.html#cb223-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb223-54"><a href="solutions-to-missing-data.html#cb223-54" aria-hidden="true" tabindex="-1"></a><span class="do">## pool coefficients and standard errors across all 5 regression models</span></span>
<span id="cb223-55"><a href="solutions-to-missing-data.html#cb223-55" aria-hidden="true" tabindex="-1"></a><span class="fu">pool</span>(fitm)</span>
<span id="cb223-56"><a href="solutions-to-missing-data.html#cb223-56" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Class: mipo    m = 5 </span></span>
<span id="cb223-57"><a href="solutions-to-missing-data.html#cb223-57" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;          term m   estimate       ubar            b          t dfcom       df</span></span>
<span id="cb223-58"><a href="solutions-to-missing-data.html#cb223-58" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 (Intercept) 5  6.3487398 4.76263432 0.0300416474 4.79868430     8 6.495687</span></span>
<span id="cb223-59"><a href="solutions-to-missing-data.html#cb223-59" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2          y4 5 -0.3623970 0.04025189 0.0001255017 0.04040250     8 6.520908</span></span>
<span id="cb223-60"><a href="solutions-to-missing-data.html#cb223-60" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 3          x1 5  0.4485592 0.01560878 0.0003990336 0.01608762     8 6.341712</span></span>
<span id="cb223-61"><a href="solutions-to-missing-data.html#cb223-61" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;           riv      lambda       fmi</span></span>
<span id="cb223-62"><a href="solutions-to-missing-data.html#cb223-62" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 0.007569335 0.007512471 0.2165521</span></span>
<span id="cb223-63"><a href="solutions-to-missing-data.html#cb223-63" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2 0.003741490 0.003727544 0.2130085</span></span>
<span id="cb223-64"><a href="solutions-to-missing-data.html#cb223-64" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 3 0.030677620 0.029764516 0.2374856</span></span>
<span id="cb223-65"><a href="solutions-to-missing-data.html#cb223-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb223-66"><a href="solutions-to-missing-data.html#cb223-66" aria-hidden="true" tabindex="-1"></a><span class="do">## output parameter estimates</span></span>
<span id="cb223-67"><a href="solutions-to-missing-data.html#cb223-67" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">pool</span>(fitm))</span>
<span id="cb223-68"><a href="solutions-to-missing-data.html#cb223-68" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;          term   estimate std.error statistic       df    p.value</span></span>
<span id="cb223-69"><a href="solutions-to-missing-data.html#cb223-69" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 (Intercept)  6.3487398 2.1905899  2.898187 6.495687 0.02504369</span></span>
<span id="cb223-70"><a href="solutions-to-missing-data.html#cb223-70" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 2          y4 -0.3623970 0.2010037 -1.802937 6.520908 0.11751237</span></span>
<span id="cb223-71"><a href="solutions-to-missing-data.html#cb223-71" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 3          x1  0.4485592 0.1268370  3.536502 6.341712 0.01119463</span></span></code></pre></div>
</div>
<div id="stochastic-imputation" class="section level5" number="11.2.4.4.4">
<h5><span class="header-section-number">11.2.4.4.4</span> Stochastic Imputation</h5>
<p><code>Regression imputation + random residual = Stochastic Imputation</code></p>
<p>Most multiple imputation is based off of some form of stochastic regression imputation.</p>
<p>Good:</p>
<ul>
<li>Has all the advantage of <a href="solutions-to-missing-data.html#regression-imputation">Regression Imputation</a></li>
<li>and also has the random components</li>
</ul>
<p>Bad:</p>
<ul>
<li>might lead to implausible values (e.g. negative values)</li>
<li>can’t handle heteroskadastic data</li>
</ul>
<p><strong>Note</strong><br />
Multiple Imputation usually based on some form of stochastic regression imputation.</p>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="solutions-to-missing-data.html#cb224-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Income data</span></span>
<span id="cb224-2"><a href="solutions-to-missing-data.html#cb224-2" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb224-3"><a href="solutions-to-missing-data.html#cb224-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">91919</span>)                              <span class="co"># Set seed</span></span>
<span id="cb224-4"><a href="solutions-to-missing-data.html#cb224-4" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1000</span>                                    <span class="co"># Sample size</span></span>
<span id="cb224-5"><a href="solutions-to-missing-data.html#cb224-5" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb224-6"><a href="solutions-to-missing-data.html#cb224-6" aria-hidden="true" tabindex="-1"></a>income <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fu">rnorm</span>(N, <span class="dv">0</span>, <span class="dv">500</span>))            <span class="co"># Create some synthetic income data</span></span>
<span id="cb224-7"><a href="solutions-to-missing-data.html#cb224-7" aria-hidden="true" tabindex="-1"></a>income[income <span class="sc">&lt;</span> <span class="dv">0</span>] <span class="ot">&lt;-</span> income[income <span class="sc">&lt;</span> <span class="dv">0</span>] <span class="sc">*</span> (<span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb224-8"><a href="solutions-to-missing-data.html#cb224-8" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb224-9"><a href="solutions-to-missing-data.html#cb224-9" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> income <span class="sc">+</span> <span class="fu">rnorm</span>(N, <span class="dv">1000</span>, <span class="dv">1500</span>)          <span class="co"># Auxiliary variables</span></span>
<span id="cb224-10"><a href="solutions-to-missing-data.html#cb224-10" aria-hidden="true" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> income <span class="sc">+</span> <span class="fu">rnorm</span>(N, <span class="sc">-</span> <span class="dv">5000</span>, <span class="dv">2000</span>)</span>
<span id="cb224-11"><a href="solutions-to-missing-data.html#cb224-11" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb224-12"><a href="solutions-to-missing-data.html#cb224-12" aria-hidden="true" tabindex="-1"></a>income[<span class="fu">rbinom</span>(N, <span class="dv">1</span>, <span class="fl">0.1</span>) <span class="sc">==</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="cn">NA</span>         <span class="co"># Create 10% missingness in income</span></span>
<span id="cb224-13"><a href="solutions-to-missing-data.html#cb224-13" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb224-14"><a href="solutions-to-missing-data.html#cb224-14" aria-hidden="true" tabindex="-1"></a>data_inc_miss <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(income, x1, x2)</span></code></pre></div>
<p>Single stochastic regression imputation</p>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb225-1"><a href="solutions-to-missing-data.html#cb225-1" aria-hidden="true" tabindex="-1"></a>imp_inc_sri <span class="ot">&lt;-</span> <span class="fu">mice</span>(data_inc_miss, <span class="at">method =</span> <span class="st">&quot;norm.nob&quot;</span>, <span class="at">m =</span> <span class="dv">1</span>)</span>
<span id="cb225-2"><a href="solutions-to-missing-data.html#cb225-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb225-3"><a href="solutions-to-missing-data.html#cb225-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  iter imp variable</span></span>
<span id="cb225-4"><a href="solutions-to-missing-data.html#cb225-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   1   1  income</span></span>
<span id="cb225-5"><a href="solutions-to-missing-data.html#cb225-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   2   1  income</span></span>
<span id="cb225-6"><a href="solutions-to-missing-data.html#cb225-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   3   1  income</span></span>
<span id="cb225-7"><a href="solutions-to-missing-data.html#cb225-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   4   1  income</span></span>
<span id="cb225-8"><a href="solutions-to-missing-data.html#cb225-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   5   1  income</span></span>
<span id="cb225-9"><a href="solutions-to-missing-data.html#cb225-9" aria-hidden="true" tabindex="-1"></a>data_inc_sri <span class="ot">&lt;-</span> <span class="fu">complete</span>(imp_inc_sri)</span></code></pre></div>
<p>Single predictive mean matching</p>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="solutions-to-missing-data.html#cb226-1" aria-hidden="true" tabindex="-1"></a>imp_inc_pmm <span class="ot">&lt;-</span> <span class="fu">mice</span>(data_inc_miss, <span class="at">method =</span> <span class="st">&quot;pmm&quot;</span>, <span class="at">m =</span> <span class="dv">1</span>)</span>
<span id="cb226-2"><a href="solutions-to-missing-data.html#cb226-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb226-3"><a href="solutions-to-missing-data.html#cb226-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  iter imp variable</span></span>
<span id="cb226-4"><a href="solutions-to-missing-data.html#cb226-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   1   1  income</span></span>
<span id="cb226-5"><a href="solutions-to-missing-data.html#cb226-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   2   1  income</span></span>
<span id="cb226-6"><a href="solutions-to-missing-data.html#cb226-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   3   1  income</span></span>
<span id="cb226-7"><a href="solutions-to-missing-data.html#cb226-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   4   1  income</span></span>
<span id="cb226-8"><a href="solutions-to-missing-data.html#cb226-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   5   1  income</span></span>
<span id="cb226-9"><a href="solutions-to-missing-data.html#cb226-9" aria-hidden="true" tabindex="-1"></a>data_inc_pmm <span class="ot">&lt;-</span> <span class="fu">complete</span>(imp_inc_pmm)</span></code></pre></div>
<p>Stochastic regression imputation contains negative values</p>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb227-1"><a href="solutions-to-missing-data.html#cb227-1" aria-hidden="true" tabindex="-1"></a>data_inc_sri<span class="sc">$</span>income[data_inc_sri<span class="sc">$</span>income <span class="sc">&lt;</span> <span class="dv">0</span>]</span>
<span id="cb227-2"><a href="solutions-to-missing-data.html#cb227-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1]  -66.055957  -96.980053  -28.921432   -4.175686  -54.480798  -27.207102</span></span>
<span id="cb227-3"><a href="solutions-to-missing-data.html#cb227-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [7] -143.603500  -80.960488</span></span>
<span id="cb227-4"><a href="solutions-to-missing-data.html#cb227-4" aria-hidden="true" tabindex="-1"></a>data_inc_pmm<span class="sc">$</span>income[data_inc_pmm<span class="sc">$</span>income <span class="sc">&lt;</span> <span class="dv">0</span>] <span class="co"># No values below 0</span></span>
<span id="cb227-5"><a href="solutions-to-missing-data.html#cb227-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; numeric(0)</span></span></code></pre></div>
<p>Proof for heteroskadastic data</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="solutions-to-missing-data.html#cb228-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Heteroscedastic data</span></span>
<span id="cb228-2"><a href="solutions-to-missing-data.html#cb228-2" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb228-3"><a href="solutions-to-missing-data.html#cb228-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">654654</span>)                             <span class="co"># Set seed</span></span>
<span id="cb228-4"><a href="solutions-to-missing-data.html#cb228-4" aria-hidden="true" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5000</span>                                  <span class="co"># Sample size</span></span>
<span id="cb228-5"><a href="solutions-to-missing-data.html#cb228-5" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb228-6"><a href="solutions-to-missing-data.html#cb228-6" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb228-7"><a href="solutions-to-missing-data.html#cb228-7" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb228-8"><a href="solutions-to-missing-data.html#cb228-8" aria-hidden="true" tabindex="-1"></a>sigma2 <span class="ot">&lt;-</span> N<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb228-9"><a href="solutions-to-missing-data.html#cb228-9" aria-hidden="true" tabindex="-1"></a>eps <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(N, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fu">sqrt</span>(sigma2))</span>
<span id="cb228-10"><a href="solutions-to-missing-data.html#cb228-10" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb228-11"><a href="solutions-to-missing-data.html#cb228-11" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> a <span class="sc">+</span> b <span class="sc">*</span> N <span class="sc">+</span> eps                         <span class="co"># Heteroscedastic variable</span></span>
<span id="cb228-12"><a href="solutions-to-missing-data.html#cb228-12" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">30</span> <span class="sc">*</span> N <span class="sc">+</span> <span class="fu">rnorm</span>(N[<span class="fu">length</span>(N)], <span class="dv">1000</span>, <span class="dv">200</span>) <span class="co"># Correlated variable</span></span>
<span id="cb228-13"><a href="solutions-to-missing-data.html#cb228-13" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb228-14"><a href="solutions-to-missing-data.html#cb228-14" aria-hidden="true" tabindex="-1"></a>y[<span class="fu">rbinom</span>(N[<span class="fu">length</span>(N)], <span class="dv">1</span>, <span class="fl">0.3</span>) <span class="sc">==</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="cn">NA</span>   <span class="co"># 30% missings</span></span>
<span id="cb228-15"><a href="solutions-to-missing-data.html#cb228-15" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb228-16"><a href="solutions-to-missing-data.html#cb228-16" aria-hidden="true" tabindex="-1"></a>data_het_miss <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, x)</span></code></pre></div>
<p>Single stochastic regression imputation</p>
<div class="sourceCode" id="cb229"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb229-1"><a href="solutions-to-missing-data.html#cb229-1" aria-hidden="true" tabindex="-1"></a>imp_het_sri <span class="ot">&lt;-</span> <span class="fu">mice</span>(data_het_miss, <span class="at">method =</span> <span class="st">&quot;norm.nob&quot;</span>, <span class="at">m =</span> <span class="dv">1</span>)</span>
<span id="cb229-2"><a href="solutions-to-missing-data.html#cb229-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb229-3"><a href="solutions-to-missing-data.html#cb229-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  iter imp variable</span></span>
<span id="cb229-4"><a href="solutions-to-missing-data.html#cb229-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   1   1  y</span></span>
<span id="cb229-5"><a href="solutions-to-missing-data.html#cb229-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   2   1  y</span></span>
<span id="cb229-6"><a href="solutions-to-missing-data.html#cb229-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   3   1  y</span></span>
<span id="cb229-7"><a href="solutions-to-missing-data.html#cb229-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   4   1  y</span></span>
<span id="cb229-8"><a href="solutions-to-missing-data.html#cb229-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   5   1  y</span></span>
<span id="cb229-9"><a href="solutions-to-missing-data.html#cb229-9" aria-hidden="true" tabindex="-1"></a>data_het_sri <span class="ot">&lt;-</span> <span class="fu">complete</span>(imp_het_sri)</span></code></pre></div>
<p>Single predictive mean matching</p>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="solutions-to-missing-data.html#cb230-1" aria-hidden="true" tabindex="-1"></a>imp_het_pmm <span class="ot">&lt;-</span> <span class="fu">mice</span>(data_het_miss, <span class="at">method =</span> <span class="st">&quot;pmm&quot;</span>, <span class="at">m =</span> <span class="dv">1</span>)</span>
<span id="cb230-2"><a href="solutions-to-missing-data.html#cb230-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb230-3"><a href="solutions-to-missing-data.html#cb230-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  iter imp variable</span></span>
<span id="cb230-4"><a href="solutions-to-missing-data.html#cb230-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   1   1  y</span></span>
<span id="cb230-5"><a href="solutions-to-missing-data.html#cb230-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   2   1  y</span></span>
<span id="cb230-6"><a href="solutions-to-missing-data.html#cb230-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   3   1  y</span></span>
<span id="cb230-7"><a href="solutions-to-missing-data.html#cb230-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   4   1  y</span></span>
<span id="cb230-8"><a href="solutions-to-missing-data.html#cb230-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;   5   1  y</span></span>
<span id="cb230-9"><a href="solutions-to-missing-data.html#cb230-9" aria-hidden="true" tabindex="-1"></a>data_het_pmm <span class="ot">&lt;-</span> <span class="fu">complete</span>(imp_het_pmm)</span></code></pre></div>
<p>Comparison between predictive mean matching and stochastic regression imputation</p>
<div class="sourceCode" id="cb231"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb231-1"><a href="solutions-to-missing-data.html#cb231-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))                              <span class="co"># Both plots in one graphic</span></span>
<span id="cb231-2"><a href="solutions-to-missing-data.html#cb231-2" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb231-3"><a href="solutions-to-missing-data.html#cb231-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x[<span class="sc">!</span><span class="fu">is.na</span>(data_het_sri<span class="sc">$</span>y)],                   <span class="co"># Plot of observed values</span></span>
<span id="cb231-4"><a href="solutions-to-missing-data.html#cb231-4" aria-hidden="true" tabindex="-1"></a>     data_het_sri<span class="sc">$</span>y[<span class="sc">!</span><span class="fu">is.na</span>(data_het_sri<span class="sc">$</span>y)],</span>
<span id="cb231-5"><a href="solutions-to-missing-data.html#cb231-5" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb231-6"><a href="solutions-to-missing-data.html#cb231-6" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;X&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Y&quot;</span>)</span>
<span id="cb231-7"><a href="solutions-to-missing-data.html#cb231-7" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x[<span class="fu">is.na</span>(y)], data_het_sri<span class="sc">$</span>y[<span class="fu">is.na</span>(y)],     <span class="co"># Plot of missing values</span></span>
<span id="cb231-8"><a href="solutions-to-missing-data.html#cb231-8" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb231-9"><a href="solutions-to-missing-data.html#cb231-9" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="st">&quot;Stochastic Regression Imputation&quot;</span>,         <span class="co"># Title of plot</span></span>
<span id="cb231-10"><a href="solutions-to-missing-data.html#cb231-10" aria-hidden="true" tabindex="-1"></a>      <span class="at">line =</span> <span class="fl">0.5</span>)</span>
<span id="cb231-11"><a href="solutions-to-missing-data.html#cb231-11" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x, data_het_sri),                   <span class="co"># Regression line</span></span>
<span id="cb231-12"><a href="solutions-to-missing-data.html#cb231-12" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&quot;#1b98e0&quot;</span>, <span class="at">lwd =</span> <span class="fl">2.5</span>)</span>
<span id="cb231-13"><a href="solutions-to-missing-data.html#cb231-13" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>,                                 <span class="co"># Legend</span></span>
<span id="cb231-14"><a href="solutions-to-missing-data.html#cb231-14" aria-hidden="true" tabindex="-1"></a>       <span class="fu">c</span>(<span class="st">&quot;Observed Values&quot;</span>, <span class="st">&quot;Imputed Values&quot;</span>, <span class="st">&quot;Regression Y ~ X&quot;</span>),</span>
<span id="cb231-15"><a href="solutions-to-missing-data.html#cb231-15" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="cn">NA</span>),</span>
<span id="cb231-16"><a href="solutions-to-missing-data.html#cb231-16" aria-hidden="true" tabindex="-1"></a>       <span class="at">lty =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="dv">1</span>),</span>
<span id="cb231-17"><a href="solutions-to-missing-data.html#cb231-17" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;#1b98e0&quot;</span>))</span>
<span id="cb231-18"><a href="solutions-to-missing-data.html#cb231-18" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb231-19"><a href="solutions-to-missing-data.html#cb231-19" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x[<span class="sc">!</span><span class="fu">is.na</span>(data_het_pmm<span class="sc">$</span>y)],                   <span class="co"># Plot of observed values</span></span>
<span id="cb231-20"><a href="solutions-to-missing-data.html#cb231-20" aria-hidden="true" tabindex="-1"></a>     data_het_pmm<span class="sc">$</span>y[<span class="sc">!</span><span class="fu">is.na</span>(data_het_pmm<span class="sc">$</span>y)],</span>
<span id="cb231-21"><a href="solutions-to-missing-data.html#cb231-21" aria-hidden="true" tabindex="-1"></a>     <span class="at">main =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb231-22"><a href="solutions-to-missing-data.html#cb231-22" aria-hidden="true" tabindex="-1"></a>     <span class="at">xlab =</span> <span class="st">&quot;X&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Y&quot;</span>)</span>
<span id="cb231-23"><a href="solutions-to-missing-data.html#cb231-23" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x[<span class="fu">is.na</span>(y)], data_het_pmm<span class="sc">$</span>y[<span class="fu">is.na</span>(y)],     <span class="co"># Plot of missing values</span></span>
<span id="cb231-24"><a href="solutions-to-missing-data.html#cb231-24" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb231-25"><a href="solutions-to-missing-data.html#cb231-25" aria-hidden="true" tabindex="-1"></a><span class="fu">title</span>(<span class="st">&quot;Predictive Mean Matching&quot;</span>,                 <span class="co"># Title of plot</span></span>
<span id="cb231-26"><a href="solutions-to-missing-data.html#cb231-26" aria-hidden="true" tabindex="-1"></a>      <span class="at">line =</span> <span class="fl">0.5</span>)</span>
<span id="cb231-27"><a href="solutions-to-missing-data.html#cb231-27" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(y <span class="sc">~</span> x, data_het_pmm),</span>
<span id="cb231-28"><a href="solutions-to-missing-data.html#cb231-28" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&quot;#1b98e0&quot;</span>, <span class="at">lwd =</span> <span class="fl">2.5</span>)</span>
<span id="cb231-29"><a href="solutions-to-missing-data.html#cb231-29" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>,                                 <span class="co"># Legend</span></span>
<span id="cb231-30"><a href="solutions-to-missing-data.html#cb231-30" aria-hidden="true" tabindex="-1"></a>       <span class="fu">c</span>(<span class="st">&quot;Observed Values&quot;</span>, <span class="st">&quot;Imputed Values&quot;</span>, <span class="st">&quot;Regression Y ~ X&quot;</span>),</span>
<span id="cb231-31"><a href="solutions-to-missing-data.html#cb231-31" aria-hidden="true" tabindex="-1"></a>       <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="cn">NA</span>),</span>
<span id="cb231-32"><a href="solutions-to-missing-data.html#cb231-32" aria-hidden="true" tabindex="-1"></a>       <span class="at">lty =</span> <span class="fu">c</span>(<span class="cn">NA</span>, <span class="cn">NA</span>, <span class="dv">1</span>),</span>
<span id="cb231-33"><a href="solutions-to-missing-data.html#cb231-33" aria-hidden="true" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;#1b98e0&quot;</span>))</span>
<span id="cb231-34"><a href="solutions-to-missing-data.html#cb231-34" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb231-35"><a href="solutions-to-missing-data.html#cb231-35" aria-hidden="true" tabindex="-1"></a><span class="fu">mtext</span>(<span class="st">&quot;Imputation of Heteroscedastic Data&quot;</span>,       <span class="co"># Main title of plot</span></span>
<span id="cb231-36"><a href="solutions-to-missing-data.html#cb231-36" aria-hidden="true" tabindex="-1"></a>      <span class="at">side =</span> <span class="dv">3</span>, <span class="at">line =</span> <span class="sc">-</span> <span class="fl">1.5</span>, <span class="at">outer =</span> <span class="cn">TRUE</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="11-imputation_files/figure-html/unnamed-chunk-10-1.png" width="90%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="regression-imputation" class="section level4" number="11.2.4.5">
<h4><span class="header-section-number">11.2.4.5</span> Regression Imputation</h4>
<p>Also known as conditional mean imputation Missing value is based (regress) on other variables.</p>
<ul>
<li><p>Good:</p>
<ul>
<li><p>Maintain the relationship with other variables (i.e., preserve dependence structure among features, unlike <a href="solutions-to-missing-data.html#mean-mode-median-imputation">11.2.4.1</a>).</p></li>
<li><p>If the data are MCAR, least-squares coefficients estimates will be consistent, and approximately unbiased in large samples <span class="citation">(<a href="#ref-Gourieroux_1981" role="doc-biblioref">Gourieroux and Monfort 1981</a>)</span></p>
<ul>
<li>Can have improvement on efficiency by using weighted least squares <span class="citation">(<a href="#ref-Beale_1975" role="doc-biblioref">Beale and Little 1975</a>)</span> or generalized least squares <span class="citation">(<a href="#ref-Gourieroux_1981" role="doc-biblioref">Gourieroux and Monfort 1981</a>)</span>.</li>
</ul></li>
</ul></li>
<li><p>Bad:</p>
<ul>
<li>No variability left. treated data as if they were collected.</li>
<li>Underestimate the standard errors and overestimate test statistics</li>
</ul></li>
</ul>
</div>
<div id="interpolation-and-extrapolation" class="section level4" number="11.2.4.6">
<h4><span class="header-section-number">11.2.4.6</span> Interpolation and Extrapolation</h4>
<p>An estimated value from other observations from the same individual. It usually only works in longitudinal data.</p>
</div>
<div id="k-nearest-neighbor-knn-imputation" class="section level4" number="11.2.4.7">
<h4><span class="header-section-number">11.2.4.7</span> K-nearest neighbor (KNN) imputation</h4>
<p>The above methods are model-based imputation (regression).<br />
This is an example of neighbor-based imputation (K-nearest neighbor).</p>
<p>For every observation that needs to be imputed, the algorithm identifies ‘k’ closest observations based on some types distance (e.g., Euclidean) and computes the weighted average (weighted based on distance) of these ‘k’ obs.</p>
<p>For a discrete variable, it uses the most frequent value among the k nearest neighbors.</p>
<ul>
<li>Distance metrics: Hamming distance.</li>
</ul>
<p>For a continuous variable, it uses the mean or mode.</p>
<ul>
<li><p>Distance metrics:</p>
<ul>
<li>Euclidean</li>
<li>Mahalanobis</li>
<li>Manhattan</li>
</ul></li>
</ul>
</div>
<div id="bayesian-ridge-regression-implementation" class="section level4" number="11.2.4.8">
<h4><span class="header-section-number">11.2.4.8</span> Bayesian Ridge regression implementation</h4>
</div>
<div id="matrix-completion" class="section level4" number="11.2.4.9">
<h4><span class="header-section-number">11.2.4.9</span> Matrix Completion</h4>
<p>Impute items missing at random while accounting for dependence between features by using principal components, which is known as <strong>matrix completion</strong> <span class="citation">(<a href="#ref-james2013" role="doc-biblioref">James et al. 2013</a>, Sec 12.3)</span></p>
<p>Consider an <span class="math inline">\(n \times p\)</span> feature matrix, <span class="math inline">\(\mathbf{X}\)</span>, with element <span class="math inline">\(x_{ij}\)</span>, some of which are missing.</p>
<p>Similar to <a href="principal-components.html#principal-components">20.2</a>, we can approximate the matrix <span class="math inline">\(\mathbf{X}\)</span> in terms of its leading PCs.</p>
<p>We consider the <span class="math inline">\(M\)</span> principal components that optimize</p>
<p><span class="math display">\[
\underset{\mathbf{A} \in R^{n \times M}, \mathbf{B} \in R^{p \times M}}{\operatorname{min}} \{ \sum_{(i,j) \in \cal{O}} (x_{ij} - \sum_{m=1}^M a_{im}b_{jm})^2 \}
\]</span></p>
<p>where <span class="math inline">\(\cal{O}\)</span> is the set of all observed pairs indices <span class="math inline">\((i,j)\)</span>, a subset of the possible <span class="math inline">\(n \times p\)</span> pairs</p>
<p>Once this minimization is solved,</p>
<ul>
<li><p>One can impute a missing observation, <span class="math inline">\(x_{ij}\)</span>, with <span class="math inline">\(\hat{x}_{ij} = \sum_{m=1}^M \hat{a}_{im}\hat{b}_{jm}\)</span> where <span class="math inline">\(\hat{a}_{im}, \hat{b}_{jm}\)</span> are the <span class="math inline">\((i,m)\)</span> and <span class="math inline">\((j.m)\)</span> elements, respectively, of the matrices <span class="math inline">\(\hat{\mathbf{A}}\)</span> and <span class="math inline">\(\hat{\mathbf{B}}\)</span> from the minimization, and</p></li>
<li><p>One can approximately recover the <span class="math inline">\(M\)</span> principal component scores and loadings, as we did when the data were complete</p></li>
</ul>
<p>The challenge here is to solve this minimization problem: the eigen-decomposition non longer applies (as in <a href="principal-components.html#principal-components">20.2</a></p>
<p>Hence, we have to use iterative algorithm <span class="citation">(<a href="#ref-james2013" role="doc-biblioref">James et al. 2013</a> Alg 12.1)</span></p>
<ol style="list-style-type: decimal">
<li>Create a complete data matrix <span class="math inline">\(\tilde{\mathbf{X}}\)</span> of dimension <span class="math inline">\(n \times p\)</span> of which the <span class="math inline">\((i,j)\)</span> element equals</li>
</ol>
<p><span class="math display">\[
\tilde{x}_{ij} =
\begin{cases}
x_{ij} &amp; \text{if } (i,j) \in \cal{O} \\
\bar{x}_{j} &amp; \text{if } (i,j) \notin \cal{O}
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(\bar{x}_j\)</span> is the average of the observed values for the <span class="math inline">\(j\)</span>th variable in the incomplete data matrix <span class="math inline">\(\mathbf{X}\)</span></p>
<p><span class="math inline">\(\cal{O}\)</span> indexes the observations that are observed in <span class="math inline">\(\mathbf{X}\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Repeat these 3 steps until some objectives are met</li>
</ol>
<p>a. Solve</p>
<p><span class="math display">\[
\underset{\mathbf{A} \in R^{n \times M}, \mathbf{B} \in R^{p \times M}}{\operatorname{min}} \{ \sum_{(i,j) \in \cal{O}} (x_{ij} - \sum_{m=1}^M a_{im}b_{jm})^2 \}
\]</span></p>
<p>by computing the principal components of <span class="math inline">\(\tilde{\mathbf{X}}\)</span></p>
<p>b. For each element <span class="math inline">\((i,j) \notin \cal{O}\)</span>, set <span class="math inline">\(\tilde{x}_{ij} \leftarrow \sum_{m=1}^M \hat{a}_{im}\hat{b}_{jm}\)</span></p>
<p>c. Compute the objective</p>
<p><span class="math display">\[
\sum_{(i,j \in \cal{O})} (x_{ij} - \sum_{m=1}^M \hat{a}_{im} \hat{b}_{jm})^2
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Return the estimated missing entries <span class="math inline">\(\tilde{x}_{ij}, (i,j) \notin \cal{O}\)</span></li>
</ol>
<p><br></p>
</div>
</div>
<div id="other-methods" class="section level3" number="11.2.5">
<h3><span class="header-section-number">11.2.5</span> Other methods</h3>
<ul>
<li>For panel data, or clustered data, use <code>pan</code> package by Schafer (1997)</li>
</ul>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Beale_1975" class="csl-entry">
Beale, E. M. L., and R. J. A. Little. 1975. <span>“Missing Values in Multivariate Analysis.”</span> <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 37 (1): 129–45. <a href="https://doi.org/10.1111/j.2517-6161.1975.tb01037.x">https://doi.org/10.1111/j.2517-6161.1975.tb01037.x</a>.
</div>
<div id="ref-newtest" class="csl-entry">
Bruin, J. 2011. <span>“Newtest: Command to Compute New Test <span>@ONLINE</span>,”</span> February. <a href="https://stats.idre.ucla.edu/stata/ado/analysis/">https://stats.idre.ucla.edu/stata/ado/analysis/</a>.
</div>
<div id="ref-Glasser_1964" class="csl-entry">
Glasser, M. 1964. <span>“Linear Regression Analysis with Missing Observations Among the Independent Variables.”</span> <em>Journal of the American Statistical Association</em> 59 (307): 834–44. <a href="https://doi.org/10.1080/01621459.1964.10480730">https://doi.org/10.1080/01621459.1964.10480730</a>.
</div>
<div id="ref-Gourieroux_1981" class="csl-entry">
Gourieroux, Christian, and Alain Monfort. 1981. <span>“On the Problem of Missing Data in Linear Models.”</span> <em>The Review of Economic Studies</em> 48 (4): 579. <a href="https://doi.org/10.2307/2297197">https://doi.org/10.2307/2297197</a>.
</div>
<div id="ref-Haitovsky_1968" class="csl-entry">
Haitovsky, Yoel. 1968. <span>“Missing Data in Regression Analysis.”</span> <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 30 (1): 67–82. <a href="https://doi.org/10.1111/j.2517-6161.1968.tb01507.x">https://doi.org/10.1111/j.2517-6161.1968.tb01507.x</a>.
</div>
<div id="ref-james2013" class="csl-entry">
James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. <span>“Statistical Learning.”</span> In, 15–57. Springer New York. <a href="https://doi.org/10.1007/978-1-4614-7138-7_2">https://doi.org/10.1007/978-1-4614-7138-7_2</a>.
</div>
<div id="ref-Jones_1996" class="csl-entry">
Jones, Michael P. 1996. <span>“Indicator and Stratification Methods for Missing Explanatory Variables in Multiple Linear Regression.”</span> <em>Journal of the American Statistical Association</em> 91 (433): 222–30. <a href="https://doi.org/10.1080/01621459.1996.10476680">https://doi.org/10.1080/01621459.1996.10476680</a>.
</div>
<div id="ref-King_2001" class="csl-entry">
King, Gary, James Honaker, Anne Joseph, and Kenneth Scheve. 2001. <span>“Analyzing Incomplete Political Science Data: An Alternative Algorithm for Multiple Imputation.”</span> <em>American Political Science Review</em> 95 (1): 49–69. <a href="https://doi.org/10.1017/s0003055401000235">https://doi.org/10.1017/s0003055401000235</a>.
</div>
<div id="ref-Little_1992" class="csl-entry">
Little, Roderick J. A. 1992. <span>“Regression with Missing x<span></span>s: A Review.”</span> <em>Journal of the American Statistical Association</em> 87 (420): 1227. <a href="https://doi.org/10.2307/2290664">https://doi.org/10.2307/2290664</a>.
</div>
<div id="ref-Rubin_1996" class="csl-entry">
———. 1996. <span>“Multiple Imputation After 18+ Years.”</span> <em>Journal of the American Statistical Association</em> 91 (434): 473–89. <a href="https://doi.org/10.1080/01621459.1996.10476908">https://doi.org/10.1080/01621459.1996.10476908</a>.
</div>
<div id="ref-Vach_1994" class="csl-entry">
Vach, Werner. 1994. <em>Logistic Regression with Missing Values in the Covariates</em>. Springer New York. <a href="https://doi.org/10.1007/978-1-4612-2650-5">https://doi.org/10.1007/978-1-4612-2650-5</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="assumptions-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="criteria-for-choosing-an-effective-approach.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mikenguyen13/data_analysis/edit/main/11-imputation.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/mikenguyen13/data_analysis/blob/main/11-imputation.Rmd",
"text": null
},
"download": ["data_analysis.pdf", "data_analysis.epub", "data_analysis.mobi"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true,
"sharing": {
"facebook": true,
"github": true,
"twitter": true,
"linkedin": true
},
"info": true,
"edit": "https://github.com/mikenguyen13/data_analysis/edit/main/%s"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
