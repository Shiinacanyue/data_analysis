<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>11.2 Solutions to Missing data | A Guide on Data Analysis</title>
  <meta name="description" content="This is a guide on how to conduct a data analysis routine" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="11.2 Solutions to Missing data | A Guide on Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a guide on how to conduct a data analysis routine" />
  <meta name="github-repo" content="mikenguyen13/data_analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="11.2 Solutions to Missing data | A Guide on Data Analysis" />
  
  <meta name="twitter:description" content="This is a guide on how to conduct a data analysis routine" />
  

<meta name="author" content="Mike Nguyen" />


<meta name="date" content="2021-01-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="assumptions.html"/>
<link rel="next" href="criteria-for-choosing-an-effective-approach.html"/>
<script src="libs/jquery-3.5.0/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/proj4js-2.3.15/proj4.js"></script>
<link href="libs/highcharts-8.1.2/css/motion.css" rel="stylesheet" />
<script src="libs/highcharts-8.1.2/highcharts.js"></script>
<script src="libs/highcharts-8.1.2/highcharts-3d.js"></script>
<script src="libs/highcharts-8.1.2/highcharts-more.js"></script>
<script src="libs/highcharts-8.1.2/modules/stock.js"></script>
<script src="libs/highcharts-8.1.2/modules/map.js"></script>
<script src="libs/highcharts-8.1.2/modules/annotations.js"></script>
<script src="libs/highcharts-8.1.2/modules/data.js"></script>
<script src="libs/highcharts-8.1.2/modules/drilldown.js"></script>
<script src="libs/highcharts-8.1.2/modules/item-series.js"></script>
<script src="libs/highcharts-8.1.2/modules/offline-exporting.js"></script>
<script src="libs/highcharts-8.1.2/modules/overlapping-datalabels.js"></script>
<script src="libs/highcharts-8.1.2/modules/exporting.js"></script>
<script src="libs/highcharts-8.1.2/modules/export-data.js"></script>
<script src="libs/highcharts-8.1.2/modules/funnel.js"></script>
<script src="libs/highcharts-8.1.2/modules/heatmap.js"></script>
<script src="libs/highcharts-8.1.2/modules/treemap.js"></script>
<script src="libs/highcharts-8.1.2/modules/sankey.js"></script>
<script src="libs/highcharts-8.1.2/modules/dependency-wheel.js"></script>
<script src="libs/highcharts-8.1.2/modules/organization.js"></script>
<script src="libs/highcharts-8.1.2/modules/solid-gauge.js"></script>
<script src="libs/highcharts-8.1.2/modules/streamgraph.js"></script>
<script src="libs/highcharts-8.1.2/modules/sunburst.js"></script>
<script src="libs/highcharts-8.1.2/modules/vector.js"></script>
<script src="libs/highcharts-8.1.2/modules/wordcloud.js"></script>
<script src="libs/highcharts-8.1.2/modules/xrange.js"></script>
<script src="libs/highcharts-8.1.2/modules/tilemap.js"></script>
<script src="libs/highcharts-8.1.2/modules/venn.js"></script>
<script src="libs/highcharts-8.1.2/modules/gantt.js"></script>
<script src="libs/highcharts-8.1.2/modules/timeline.js"></script>
<script src="libs/highcharts-8.1.2/modules/parallel-coordinates.js"></script>
<script src="libs/highcharts-8.1.2/modules/bullet.js"></script>
<script src="libs/highcharts-8.1.2/modules/coloraxis.js"></script>
<script src="libs/highcharts-8.1.2/modules/dumbbell.js"></script>
<script src="libs/highcharts-8.1.2/modules/lollipop.js"></script>
<script src="libs/highcharts-8.1.2/modules/series-label.js"></script>
<script src="libs/highcharts-8.1.2/plugins/motion.js"></script>
<script src="libs/highcharts-8.1.2/custom/reset.js"></script>
<script src="libs/highcharts-8.1.2/modules/boost.js"></script>
<script src="libs/highchart-binding-0.8.2/highchart.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Guide on Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>2</b> Prerequisites</a><ul>
<li class="chapter" data-level="2.1" data-path="matrix-theory.html"><a href="matrix-theory.html"><i class="fa fa-check"></i><b>2.1</b> Matrix Theory</a><ul>
<li class="chapter" data-level="2.1.1" data-path="matrix-theory.html"><a href="matrix-theory.html#rank"><i class="fa fa-check"></i><b>2.1.1</b> Rank</a></li>
<li class="chapter" data-level="2.1.2" data-path="matrix-theory.html"><a href="matrix-theory.html#inverse"><i class="fa fa-check"></i><b>2.1.2</b> Inverse</a></li>
<li class="chapter" data-level="2.1.3" data-path="matrix-theory.html"><a href="matrix-theory.html#definiteness"><i class="fa fa-check"></i><b>2.1.3</b> Definiteness</a></li>
<li class="chapter" data-level="2.1.4" data-path="matrix-theory.html"><a href="matrix-theory.html#matrix-calculus"><i class="fa fa-check"></i><b>2.1.4</b> Matrix Calculus</a></li>
<li class="chapter" data-level="2.1.5" data-path="matrix-theory.html"><a href="matrix-theory.html#optimization"><i class="fa fa-check"></i><b>2.1.5</b> Optimization</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>2.2</b> Probability Theory</a><ul>
<li class="chapter" data-level="2.2.1" data-path="probability-theory.html"><a href="probability-theory.html#axiom-and-theorems-of-probability"><i class="fa fa-check"></i><b>2.2.1</b> Axiom and Theorems of Probability</a></li>
<li class="chapter" data-level="2.2.2" data-path="probability-theory.html"><a href="probability-theory.html#central-limit-theorem"><i class="fa fa-check"></i><b>2.2.2</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="2.2.3" data-path="probability-theory.html"><a href="probability-theory.html#random-variable"><i class="fa fa-check"></i><b>2.2.3</b> Random variable</a></li>
<li class="chapter" data-level="2.2.4" data-path="probability-theory.html"><a href="probability-theory.html#moment"><i class="fa fa-check"></i><b>2.2.4</b> Moment</a></li>
<li class="chapter" data-level="2.2.5" data-path="probability-theory.html"><a href="probability-theory.html#distributions"><i class="fa fa-check"></i><b>2.2.5</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="general-math.html"><a href="general-math.html"><i class="fa fa-check"></i><b>2.3</b> General Math</a><ul>
<li class="chapter" data-level="2.3.1" data-path="general-math.html"><a href="general-math.html#law-of-large-numbers"><i class="fa fa-check"></i><b>2.3.1</b> Law of large numbers</a></li>
<li class="chapter" data-level="2.3.2" data-path="general-math.html"><a href="general-math.html#law-of-iterated-expectation"><i class="fa fa-check"></i><b>2.3.2</b> Law of Iterated Expectation</a></li>
<li class="chapter" data-level="2.3.3" data-path="general-math.html"><a href="general-math.html#convergence"><i class="fa fa-check"></i><b>2.3.3</b> Convergence</a></li>
<li class="chapter" data-level="2.3.4" data-path="general-math.html"><a href="general-math.html#sufficient-statistics"><i class="fa fa-check"></i><b>2.3.4</b> Sufficient Statistics</a></li>
<li class="chapter" data-level="2.3.5" data-path="general-math.html"><a href="general-math.html#parameter-transformations"><i class="fa fa-check"></i><b>2.3.5</b> Parameter transformations</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="methods.html"><a href="methods.html"><i class="fa fa-check"></i><b>2.4</b> Methods</a></li>
</ul></li>
<li class="part"><span><b>I BASIC</b></span></li>
<li class="chapter" data-level="3" data-path="descriptive-stat.html"><a href="descriptive-stat.html"><i class="fa fa-check"></i><b>3</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="3.1" data-path="numerical-measures.html"><a href="numerical-measures.html"><i class="fa fa-check"></i><b>3.1</b> Numerical Measures</a></li>
<li class="chapter" data-level="3.2" data-path="graphical-measures.html"><a href="graphical-measures.html"><i class="fa fa-check"></i><b>3.2</b> Graphical Measures</a><ul>
<li class="chapter" data-level="3.2.1" data-path="graphical-measures.html"><a href="graphical-measures.html#shape"><i class="fa fa-check"></i><b>3.2.1</b> Shape</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="normality-assessment.html"><a href="normality-assessment.html"><i class="fa fa-check"></i><b>3.3</b> Normality Assessment</a><ul>
<li class="chapter" data-level="3.3.1" data-path="normality-assessment.html"><a href="normality-assessment.html#graphical-assessment"><i class="fa fa-check"></i><b>3.3.1</b> Graphical Assessment</a></li>
<li class="chapter" data-level="3.3.2" data-path="normality-assessment.html"><a href="normality-assessment.html#summary-statistics"><i class="fa fa-check"></i><b>3.3.2</b> Summary Statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="basic-statistical-inference.html"><a href="basic-statistical-inference.html"><i class="fa fa-check"></i><b>4</b> Basic Statistical Inference</a><ul>
<li class="chapter" data-level="4.1" data-path="one-sample-inference.html"><a href="one-sample-inference.html"><i class="fa fa-check"></i><b>4.1</b> One Sample Inference</a><ul>
<li class="chapter" data-level="4.1.1" data-path="one-sample-inference.html"><a href="one-sample-inference.html#interval-estimation-of-the-mean"><i class="fa fa-check"></i><b>4.1.1</b> Interval Estimation of the Mean</a></li>
<li class="chapter" data-level="4.1.2" data-path="one-sample-inference.html"><a href="one-sample-inference.html#interval-estimation-for-the-variance"><i class="fa fa-check"></i><b>4.1.2</b> Interval Estimation for the Variance</a></li>
<li class="chapter" data-level="4.1.3" data-path="one-sample-inference.html"><a href="one-sample-inference.html#power"><i class="fa fa-check"></i><b>4.1.3</b> Power</a></li>
<li class="chapter" data-level="4.1.4" data-path="one-sample-inference.html"><a href="one-sample-inference.html#sample-size"><i class="fa fa-check"></i><b>4.1.4</b> Sample Size</a></li>
<li class="chapter" data-level="4.1.5" data-path="one-sample-inference.html"><a href="one-sample-inference.html#note"><i class="fa fa-check"></i><b>4.1.5</b> Note</a></li>
<li class="chapter" data-level="4.1.6" data-path="one-sample-inference.html"><a href="one-sample-inference.html#one-sample-non-parametric-methods"><i class="fa fa-check"></i><b>4.1.6</b> One-sample Non-parametric Methods</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="two-sample-inference.html"><a href="two-sample-inference.html"><i class="fa fa-check"></i><b>4.2</b> Two Sample Inference</a><ul>
<li class="chapter" data-level="4.2.1" data-path="two-sample-inference.html"><a href="two-sample-inference.html#means"><i class="fa fa-check"></i><b>4.2.1</b> Means</a></li>
<li class="chapter" data-level="4.2.2" data-path="two-sample-inference.html"><a href="two-sample-inference.html#variances"><i class="fa fa-check"></i><b>4.2.2</b> Variances</a></li>
<li class="chapter" data-level="4.2.3" data-path="two-sample-inference.html"><a href="two-sample-inference.html#power-1"><i class="fa fa-check"></i><b>4.2.3</b> Power</a></li>
<li class="chapter" data-level="4.2.4" data-path="two-sample-inference.html"><a href="two-sample-inference.html#sample-size-1"><i class="fa fa-check"></i><b>4.2.4</b> Sample Size</a></li>
<li class="chapter" data-level="4.2.5" data-path="two-sample-inference.html"><a href="two-sample-inference.html#matched-pair-designs"><i class="fa fa-check"></i><b>4.2.5</b> Matched Pair Designs</a></li>
<li class="chapter" data-level="4.2.6" data-path="two-sample-inference.html"><a href="two-sample-inference.html#nonparametric-tests-for-two-samples"><i class="fa fa-check"></i><b>4.2.6</b> Nonparametric Tests for Two Samples</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html"><i class="fa fa-check"></i><b>4.3</b> Categorical Data Analysis</a><ul>
<li class="chapter" data-level="4.3.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#inferences-for-small-samples"><i class="fa fa-check"></i><b>4.3.1</b> Inferences for Small Samples</a></li>
<li class="chapter" data-level="4.3.2" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#two-way-count-data"><i class="fa fa-check"></i><b>4.3.2</b> Two-way Count Data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="confidence-intervals-hypothesis-testing.html"><a href="confidence-intervals-hypothesis-testing.html"><i class="fa fa-check"></i><b>5</b> Confidence Intervals &amp; Hypothesis Testing</a><ul>
<li class="chapter" data-level="5.1" data-path="for-the-mean-mu.html"><a href="for-the-mean-mu.html"><i class="fa fa-check"></i><b>5.1</b> For the Mean (<span class="math inline">\(\mu\)</span>)</a></li>
<li class="chapter" data-level="5.2" data-path="for-single-proportions-p.html"><a href="for-single-proportions-p.html"><i class="fa fa-check"></i><b>5.2</b> For Single Proportions (p)</a></li>
<li class="chapter" data-level="5.3" data-path="for-difference-of-two-proportions-p-1-p-2.html"><a href="for-difference-of-two-proportions-p-1-p-2.html"><i class="fa fa-check"></i><b>5.3</b> For Difference of Two Proportions (<span class="math inline">\(p_1 - p_2\)</span>)</a></li>
<li class="chapter" data-level="5.4" data-path="for-a-signle-variance-sigma2.html"><a href="for-a-signle-variance-sigma2.html"><i class="fa fa-check"></i><b>5.4</b> For a signle variance (<span class="math inline">\(\sigma^2\)</span>)</a></li>
<li class="chapter" data-level="5.5" data-path="for-two-variances-sigma2-1sigma2-2.html"><a href="for-two-variances-sigma2-1sigma2-2.html"><i class="fa fa-check"></i><b>5.5</b> For Two Variances (<span class="math inline">\(\sigma^2_1,\sigma^2_2\)</span>)</a></li>
<li class="chapter" data-level="5.6" data-path="for-difference-of-means-mu-1-mu-2-independent-samples.html"><a href="for-difference-of-means-mu-1-mu-2-independent-samples.html"><i class="fa fa-check"></i><b>5.6</b> For Difference of Means (<span class="math inline">\(\mu_1-\mu_2\)</span>), Independent Samples</a></li>
<li class="chapter" data-level="5.7" data-path="for-difference-of-means-mu-1-mu-2-paired-samples-d-x-y.html"><a href="for-difference-of-means-mu-1-mu-2-paired-samples-d-x-y.html"><i class="fa fa-check"></i><b>5.7</b> For Difference of Means (<span class="math inline">\(\mu_1 - \mu_2\)</span>), Paired Samples (D = X-Y)</a></li>
</ul></li>
<li class="part"><span><b>II REGRESSION</b></span></li>
<li class="chapter" data-level="6" data-path="regression-analysis.html"><a href="regression-analysis.html"><i class="fa fa-check"></i><b>6</b> Regression Analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>6.1</b> Linear Regression</a><ul>
<li class="chapter" data-level="6.1.1" data-path="linear-regression.html"><a href="linear-regression.html#ordinary-least-squares"><i class="fa fa-check"></i><b>6.1.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="6.1.2" data-path="linear-regression.html"><a href="linear-regression.html#feasible-generalized-least-squares"><i class="fa fa-check"></i><b>6.1.2</b> Feasible Generalized Least Squares</a></li>
<li class="chapter" data-level="6.1.3" data-path="linear-regression.html"><a href="linear-regression.html#weighted-least-squares"><i class="fa fa-check"></i><b>6.1.3</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="6.1.4" data-path="linear-regression.html"><a href="linear-regression.html#feasiable-prais-winsten"><i class="fa fa-check"></i><b>6.1.4</b> Feasiable Prais Winsten</a></li>
<li class="chapter" data-level="6.1.5" data-path="linear-regression.html"><a href="linear-regression.html#feasible-group-level-random-effects"><i class="fa fa-check"></i><b>6.1.5</b> Feasible group level Random Effects</a></li>
<li class="chapter" data-level="6.1.6" data-path="linear-regression.html"><a href="linear-regression.html#generalized-least-squares"><i class="fa fa-check"></i><b>6.1.6</b> Generalized Least Squares</a></li>
<li class="chapter" data-level="6.1.7" data-path="linear-regression.html"><a href="linear-regression.html#maximum-likelihood"><i class="fa fa-check"></i><b>6.1.7</b> Maximum Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="quantile-regression.html"><a href="quantile-regression.html"><i class="fa fa-check"></i><b>6.2</b> Quantile Regression</a><ul>
<li class="chapter" data-level="6.2.1" data-path="quantile-regression.html"><a href="quantile-regression.html#application-2"><i class="fa fa-check"></i><b>6.2.1</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="non-linear-regression.html"><a href="non-linear-regression.html"><i class="fa fa-check"></i><b>6.3</b> Non-linear Regression</a><ul>
<li class="chapter" data-level="6.3.1" data-path="non-linear-regression.html"><a href="non-linear-regression.html#non-linear-least-squares"><i class="fa fa-check"></i><b>6.3.1</b> Non-linear Least Squares</a></li>
<li class="chapter" data-level="6.3.2" data-path="non-linear-regression.html"><a href="non-linear-regression.html#genelized-method-of-moments"><i class="fa fa-check"></i><b>6.3.2</b> Genelized Method of Moments</a></li>
<li class="chapter" data-level="6.3.3" data-path="non-linear-regression.html"><a href="non-linear-regression.html#minimum-distance"><i class="fa fa-check"></i><b>6.3.3</b> Minimum Distance</a></li>
<li class="chapter" data-level="6.3.4" data-path="non-linear-regression.html"><a href="non-linear-regression.html#spline-regression"><i class="fa fa-check"></i><b>6.3.4</b> Spline Regression</a></li>
<li class="chapter" data-level="6.3.5" data-path="non-linear-regression.html"><a href="non-linear-regression.html#generalized-additive-models"><i class="fa fa-check"></i><b>6.3.5</b> Generalized Additive Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="model-specification.html"><a href="model-specification.html"><i class="fa fa-check"></i><b>7</b> Model Specification</a><ul>
<li class="chapter" data-level="7.1" data-path="nested-model.html"><a href="nested-model.html"><i class="fa fa-check"></i><b>7.1</b> Nested Model</a><ul>
<li class="chapter" data-level="7.1.1" data-path="nested-model.html"><a href="nested-model.html#chow-test"><i class="fa fa-check"></i><b>7.1.1</b> Chow test</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="non-nested-model.html"><a href="non-nested-model.html"><i class="fa fa-check"></i><b>7.2</b> Non-Nested Model</a><ul>
<li class="chapter" data-level="7.2.1" data-path="non-nested-model.html"><a href="non-nested-model.html#davidson-mackinnon-test"><i class="fa fa-check"></i><b>7.2.1</b> Davidson-Mackinnon test</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="heteroskedasticity-1.html"><a href="heteroskedasticity-1.html"><i class="fa fa-check"></i><b>7.3</b> Heteroskedasticity</a><ul>
<li class="chapter" data-level="7.3.1" data-path="heteroskedasticity-1.html"><a href="heteroskedasticity-1.html#breusch-pagan-test"><i class="fa fa-check"></i><b>7.3.1</b> Breusch-Pagan test</a></li>
<li class="chapter" data-level="7.3.2" data-path="heteroskedasticity-1.html"><a href="heteroskedasticity-1.html#white-test"><i class="fa fa-check"></i><b>7.3.2</b> White test</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="endogeneity.html"><a href="endogeneity.html"><i class="fa fa-check"></i><b>8</b> Endogeneity</a><ul>
<li class="chapter" data-level="8.1" data-path="testing-assumption.html"><a href="testing-assumption.html"><i class="fa fa-check"></i><b>8.1</b> Testing Assumption</a><ul>
<li class="chapter" data-level="8.1.1" data-path="testing-assumption.html"><a href="testing-assumption.html#test-of-endogeneity"><i class="fa fa-check"></i><b>8.1.1</b> Test of Endogeneity</a></li>
<li class="chapter" data-level="8.1.2" data-path="testing-assumption.html"><a href="testing-assumption.html#testing-instruments-assumptions"><i class="fa fa-check"></i><b>8.1.2</b> Testing Instrument’s assumptions</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="omitted-variables-bias.html"><a href="omitted-variables-bias.html"><i class="fa fa-check"></i><b>8.2</b> Omitted Variables Bias</a></li>
<li class="chapter" data-level="8.3" data-path="feedback-effect-simultaneity.html"><a href="feedback-effect-simultaneity.html"><i class="fa fa-check"></i><b>8.3</b> Feedback Effect (Simultaneity)</a></li>
<li class="chapter" data-level="8.4" data-path="endogenous-sample-design-sample-selection.html"><a href="endogenous-sample-design-sample-selection.html"><i class="fa fa-check"></i><b>8.4</b> Endogenous sample design (sample selection)</a></li>
<li class="chapter" data-level="8.5" data-path="measurement-error.html"><a href="measurement-error.html"><i class="fa fa-check"></i><b>8.5</b> Measurement Error</a></li>
<li class="chapter" data-level="8.6" data-path="proxy-variables.html"><a href="proxy-variables.html"><i class="fa fa-check"></i><b>8.6</b> Proxy Variables</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>9</b> Data</a><ul>
<li class="chapter" data-level="9.1" data-path="cross-sectional.html"><a href="cross-sectional.html"><i class="fa fa-check"></i><b>9.1</b> Cross-Sectional</a></li>
<li class="chapter" data-level="9.2" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>9.2</b> Time Series</a><ul>
<li class="chapter" data-level="9.2.1" data-path="time-series.html"><a href="time-series.html#deterministic-time-trend"><i class="fa fa-check"></i><b>9.2.1</b> Deterministic Time trend</a></li>
<li class="chapter" data-level="9.2.2" data-path="time-series.html"><a href="time-series.html#feedback-effect"><i class="fa fa-check"></i><b>9.2.2</b> Feedback Effect</a></li>
<li class="chapter" data-level="9.2.3" data-path="time-series.html"><a href="time-series.html#dynamic-specification"><i class="fa fa-check"></i><b>9.2.3</b> Dynamic Specification</a></li>
<li class="chapter" data-level="9.2.4" data-path="time-series.html"><a href="time-series.html#dynamically-complete"><i class="fa fa-check"></i><b>9.2.4</b> Dynamically Complete</a></li>
<li class="chapter" data-level="9.2.5" data-path="time-series.html"><a href="time-series.html#highly-persistent-data"><i class="fa fa-check"></i><b>9.2.5</b> Highly Persistent Data</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="repeated-cross-sections.html"><a href="repeated-cross-sections.html"><i class="fa fa-check"></i><b>9.3</b> Repeated Cross Sections</a><ul>
<li class="chapter" data-level="9.3.1" data-path="repeated-cross-sections.html"><a href="repeated-cross-sections.html#pooled-cross-section"><i class="fa fa-check"></i><b>9.3.1</b> Pooled Cross Section</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="panel-data.html"><a href="panel-data.html"><i class="fa fa-check"></i><b>9.4</b> Panel Data</a><ul>
<li class="chapter" data-level="9.4.1" data-path="panel-data.html"><a href="panel-data.html#pooled-ols-esimator"><i class="fa fa-check"></i><b>9.4.1</b> Pooled OLS Esimator</a></li>
<li class="chapter" data-level="9.4.2" data-path="panel-data.html"><a href="panel-data.html#individual-specific-effects-model"><i class="fa fa-check"></i><b>9.4.2</b> Individual-specific effects model</a></li>
<li class="chapter" data-level="9.4.3" data-path="panel-data.html"><a href="panel-data.html#tests-for-assumptions"><i class="fa fa-check"></i><b>9.4.3</b> Tests for Assumptions</a></li>
<li class="chapter" data-level="9.4.4" data-path="panel-data.html"><a href="panel-data.html#model-selection"><i class="fa fa-check"></i><b>9.4.4</b> Model Selection</a></li>
<li class="chapter" data-level="9.4.5" data-path="panel-data.html"><a href="panel-data.html#summary-1"><i class="fa fa-check"></i><b>9.4.5</b> Summary</a></li>
<li class="chapter" data-level="9.4.6" data-path="panel-data.html"><a href="panel-data.html#application-4"><i class="fa fa-check"></i><b>9.4.6</b> Application</a></li>
<li class="chapter" data-level="9.4.7" data-path="panel-data.html"><a href="panel-data.html#other-estimators"><i class="fa fa-check"></i><b>9.4.7</b> Other Estimators</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="hypothesis-testing.html"><a href="hypothesis-testing.html"><i class="fa fa-check"></i><b>10</b> Hypothesis Testing</a><ul>
<li class="chapter" data-level="10.1" data-path="types-of-hypothesis-testing.html"><a href="types-of-hypothesis-testing.html"><i class="fa fa-check"></i><b>10.1</b> Types of hypothesis testing</a></li>
<li class="chapter" data-level="10.2" data-path="wald-test.html"><a href="wald-test.html"><i class="fa fa-check"></i><b>10.2</b> Wald test</a><ul>
<li class="chapter" data-level="10.2.1" data-path="wald-test.html"><a href="wald-test.html#multiple-hypothesis"><i class="fa fa-check"></i><b>10.2.1</b> Multiple Hypothesis</a></li>
<li class="chapter" data-level="10.2.2" data-path="wald-test.html"><a href="wald-test.html#linear-combination"><i class="fa fa-check"></i><b>10.2.2</b> Linear Combination</a></li>
<li class="chapter" data-level="10.2.3" data-path="wald-test.html"><a href="wald-test.html#application-5"><i class="fa fa-check"></i><b>10.2.3</b> Application</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="the-likelihood-ratio-test.html"><a href="the-likelihood-ratio-test.html"><i class="fa fa-check"></i><b>10.3</b> The likelihood ratio test</a></li>
<li class="chapter" data-level="10.4" data-path="lagrange-multiplier-score.html"><a href="lagrange-multiplier-score.html"><i class="fa fa-check"></i><b>10.4</b> Lagrange Multiplier (Score)</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="imputation-missing-data.html"><a href="imputation-missing-data.html"><i class="fa fa-check"></i><b>11</b> Imputation (Missing Data)</a><ul>
<li class="chapter" data-level="11.1" data-path="assumptions.html"><a href="assumptions.html"><i class="fa fa-check"></i><b>11.1</b> Assumptions</a><ul>
<li class="chapter" data-level="11.1.1" data-path="assumptions.html"><a href="assumptions.html#missing-completely-at-random-mcar"><i class="fa fa-check"></i><b>11.1.1</b> Missing Completely at Random (MCAR)</a></li>
<li class="chapter" data-level="11.1.2" data-path="assumptions.html"><a href="assumptions.html#missing-at-random-mar"><i class="fa fa-check"></i><b>11.1.2</b> Missing at Random (MAR)</a></li>
<li class="chapter" data-level="11.1.3" data-path="assumptions.html"><a href="assumptions.html#ignorable"><i class="fa fa-check"></i><b>11.1.3</b> Ignorable</a></li>
<li class="chapter" data-level="11.1.4" data-path="assumptions.html"><a href="assumptions.html#nonignorable"><i class="fa fa-check"></i><b>11.1.4</b> Nonignorable</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html"><i class="fa fa-check"></i><b>11.2</b> Solutions to Missing data</a><ul>
<li class="chapter" data-level="11.2.1" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#listwise-deletion"><i class="fa fa-check"></i><b>11.2.1</b> Listwise Deletion</a></li>
<li class="chapter" data-level="11.2.2" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#pairwise-deletion"><i class="fa fa-check"></i><b>11.2.2</b> Pairwise Deletion</a></li>
<li class="chapter" data-level="11.2.3" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#dummy-variable-adjustment"><i class="fa fa-check"></i><b>11.2.3</b> Dummy Variable Adjustment</a></li>
<li class="chapter" data-level="11.2.4" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#imputation"><i class="fa fa-check"></i><b>11.2.4</b> Imputation</a></li>
<li class="chapter" data-level="11.2.5" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#heckmans-sample-selection-model"><i class="fa fa-check"></i><b>11.2.5</b> Heckman’s Sample Selection Model</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="criteria-for-choosing-an-effective-approach.html"><a href="criteria-for-choosing-an-effective-approach.html"><i class="fa fa-check"></i><b>11.3</b> Criteria for Choosing an Effective Approach</a></li>
<li class="chapter" data-level="11.4" data-path="another-perspective.html"><a href="another-perspective.html"><i class="fa fa-check"></i><b>11.4</b> Another Perspective</a></li>
<li class="chapter" data-level="11.5" data-path="diagnosing-the-mechanism.html"><a href="diagnosing-the-mechanism.html"><i class="fa fa-check"></i><b>11.5</b> Diagnosing the Mechanism</a><ul>
<li class="chapter" data-level="11.5.1" data-path="diagnosing-the-mechanism.html"><a href="diagnosing-the-mechanism.html#mar-vs.-mnar"><i class="fa fa-check"></i><b>11.5.1</b> MAR vs. MNAR</a></li>
<li class="chapter" data-level="11.5.2" data-path="diagnosing-the-mechanism.html"><a href="diagnosing-the-mechanism.html#mcar-vs.-mar"><i class="fa fa-check"></i><b>11.5.2</b> MCAR vs. MAR</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="application-6.html"><a href="application-6.html"><i class="fa fa-check"></i><b>11.6</b> Application</a><ul>
<li class="chapter" data-level="11.6.1" data-path="application-6.html"><a href="application-6.html#imputation-with-mean-median-mode"><i class="fa fa-check"></i><b>11.6.1</b> Imputation with mean / median / mode</a></li>
<li class="chapter" data-level="11.6.2" data-path="application-6.html"><a href="application-6.html#knn"><i class="fa fa-check"></i><b>11.6.2</b> KNN</a></li>
<li class="chapter" data-level="11.6.3" data-path="application-6.html"><a href="application-6.html#rpart"><i class="fa fa-check"></i><b>11.6.3</b> rpart</a></li>
<li class="chapter" data-level="11.6.4" data-path="application-6.html"><a href="application-6.html#mice-multivariate-imputation-via-chained-equations"><i class="fa fa-check"></i><b>11.6.4</b> MICE (Multivariate Imputation via Chained Equations)</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>III EXPERIMENTAL DESIGN</b></span></li>
<li class="chapter" data-level="12" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html"><i class="fa fa-check"></i><b>12</b> Analysis of Variance (ANOVA)</a><ul>
<li class="chapter" data-level="12.1" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html"><i class="fa fa-check"></i><b>12.1</b> Completely Randomized Design (CRD)</a><ul>
<li class="chapter" data-level="12.1.1" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#single-factor-one-way-anova"><i class="fa fa-check"></i><b>12.1.1</b> Single Factor (One-Way) ANOVA</a></li>
<li class="chapter" data-level="12.1.2" data-path="completely-randomized-design-crd.html"><a href="completely-randomized-design-crd.html#two-factor-fixed-effect-anova"><i class="fa fa-check"></i><b>12.1.2</b> Two Factor Fixed Effect ANOVA</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV ADVANCED</b></span></li>
<li class="chapter" data-level="13" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>13</b> Deep Learning</a><ul>
<li class="chapter" data-level="13.1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>13.1</b> Overview</a></li>
<li class="chapter" data-level="13.2" data-path="tensor-flow-and-r.html"><a href="tensor-flow-and-r.html"><i class="fa fa-check"></i><b>13.2</b> Tensor Flow and R</a><ul>
<li class="chapter" data-level="13.2.1" data-path="tensor-flow-and-r.html"><a href="tensor-flow-and-r.html#r-packages"><i class="fa fa-check"></i><b>13.2.1</b> R packages</a></li>
<li class="chapter" data-level="13.2.2" data-path="tensor-flow-and-r.html"><a href="tensor-flow-and-r.html#keras-layers"><i class="fa fa-check"></i><b>13.2.2</b> Keras layers</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="compiling-models.html"><a href="compiling-models.html"><i class="fa fa-check"></i><b>13.3</b> Compiling models</a></li>
<li class="chapter" data-level="13.4" data-path="losses-optimizers-and-metrics.html"><a href="losses-optimizers-and-metrics.html"><i class="fa fa-check"></i><b>13.4</b> Losses, Optimizers, and Metrics</a></li>
<li class="chapter" data-level="13.5" data-path="nyu.html"><a href="nyu.html"><i class="fa fa-check"></i><b>13.5</b> NYU</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="causality.html"><a href="causality.html"><i class="fa fa-check"></i><b>14</b> Causality</a></li>
<li class="chapter" data-level="15" data-path="report.html"><a href="report.html"><i class="fa fa-check"></i><b>15</b> Report</a><ul>
<li class="chapter" data-level="15.1" data-path="one-summary-table.html"><a href="one-summary-table.html"><i class="fa fa-check"></i><b>15.1</b> One summary table</a></li>
<li class="chapter" data-level="15.2" data-path="model-comparison.html"><a href="model-comparison.html"><i class="fa fa-check"></i><b>15.2</b> Model Comparison</a></li>
<li class="chapter" data-level="15.3" data-path="changes-in-an-estimate.html"><a href="changes-in-an-estimate.html"><i class="fa fa-check"></i><b>15.3</b> Changes in an estimate</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="research-methods.html"><a href="research-methods.html"><i class="fa fa-check"></i><b>16</b> Research Methods</a><ul>
<li class="chapter" data-level="16.1" data-path="but-for-world.html"><a href="but-for-world.html"><i class="fa fa-check"></i><b>16.1</b> But-for World</a></li>
</ul></li>
<li class="appendix"><span><b>APPENDIX</b></span></li>
<li class="chapter" data-level="A" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>A</b> Appendix</a><ul>
<li class="chapter" data-level="A.1" data-path="short-cut.html"><a href="short-cut.html"><i class="fa fa-check"></i><b>A.1</b> Short-cut</a></li>
<li class="chapter" data-level="A.2" data-path="function-short-cut.html"><a href="function-short-cut.html"><i class="fa fa-check"></i><b>A.2</b> Function short-cut</a></li>
<li class="chapter" data-level="A.3" data-path="citation.html"><a href="citation.html"><i class="fa fa-check"></i><b>A.3</b> Citation</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="bookdown-cheat-sheet.html"><a href="bookdown-cheat-sheet.html"><i class="fa fa-check"></i><b>B</b> Bookdown cheat sheet</a><ul>
<li class="chapter" data-level="B.1" data-path="math-expresssion-syntax.html"><a href="math-expresssion-syntax.html"><i class="fa fa-check"></i><b>B.1</b> Math Expresssion/ Syntax</a><ul>
<li class="chapter" data-level="B.1.1" data-path="math-expresssion-syntax.html"><a href="math-expresssion-syntax.html#statistics-notation"><i class="fa fa-check"></i><b>B.1.1</b> Statistics Notation</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="table.html"><a href="table.html"><i class="fa fa-check"></i><b>B.2</b> Table</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Guide on Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="solutions-to-missing-data" class="section level2">
<h2><span class="header-section-number">11.2</span> Solutions to Missing data</h2>
<div id="listwise-deletion" class="section level3">
<h3><span class="header-section-number">11.2.1</span> Listwise Deletion</h3>
<p>Advantages:</p>
<ul>
<li>Can be applied to any statistical test (SEM, multi-level regression, etc.)</li>
<li>In the case of MCAR, both the parameters estimates and its standard errors are unbiased.</li>
<li>In the case of MAR among independent variables (not depend on the values of dependent variables), then listwise deletion parameter estimates can still be unbiased. <span class="citation">(Little <a href="#ref-Little_1992" role="doc-biblioref">1992</a>)</span> For example, you have a model <span class="math inline">\(y=\beta_{0}+\beta_1X_1 + \beta_2X_2 +\epsilon\)</span> if the probability of missing data on X1 is independent of Y, but dependent on the value of X1 and X2, then the model estimates are still unbiased.
<ul>
<li>The missing data mechanism the depends on the values of the independent variables are the same as stratified sampling. And stratified sampling does not bias your estimates</li>
<li>In the case of logistic regression, if the probability of missing data on any variable depends on the value of the dependent variable, but independent of the value of the independent variables, then the listwise deletion will yield biased intercept estimate, but consistent estimates of the slope and their standard errors <span class="citation">(Vach <a href="#ref-Vach_1994" role="doc-biblioref">1994</a>)</span>. However, logistic regression will still fail if the probability of missing data is dependent on both the value of the dependent and independent variables.</li>
<li>Under regression analysis, listwise deletion is more robust than maximum likelihood and multiple imputation when MAR assumption is violated.</li>
</ul></li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>It will yield a larger standard errors than other more sophisticated methods discussed later.</li>
<li>If the data are not MCAR, but MAR, then your listwise deletion can yield biased estimates.</li>
<li>In other cases than regression analysis, other sophisticated methods can yield better estimates compared to listwise deletion.</li>
</ul>
</div>
<div id="pairwise-deletion" class="section level3">
<h3><span class="header-section-number">11.2.2</span> Pairwise Deletion</h3>
<p>This method could only be used in the case of linear models such as linear regression, factor analysis, or SEM.
The premise of this method based on that the coefficient estimates are calculated based on the means, standard deviations, and correlation matrix. Compared to listwise deletion, we still utilized as many correlation between variables as possible to compute the correlation matrix.</p>
<p>Advantages:</p>
<ul>
<li>If the true missing data mechanism is MCAR, pair wise deletion will yield consistent estimates, and unbiased in large samples</li>
<li>Compared to listwise deletion: <span class="citation">(Glasser <a href="#ref-Glasser_1964" role="doc-biblioref">1964</a>)</span>
<ul>
<li>If the correlation among variables are low, pairwise deletion is more efficient estimates than listwise</li>
<li>If the correlations among variables are high, listwise deletion is more efficient than pairwise.</li>
</ul></li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>If the data mechanism is MAR, pairwise deletion will yield biased estimates.</li>
<li>In small sample, sometimes covariance matrix might not be positive definite, which means coefficients estimates cannot be calculated.</li>
</ul>
<p><strong>Note</strong>: You need to read carefully on how your software specify the sample size because it will alter the standard errors.</p>
</div>
<div id="dummy-variable-adjustment" class="section level3">
<h3><span class="header-section-number">11.2.3</span> Dummy Variable Adjustment</h3>
<p>Also known as Missing Indicator Method or Proxy Variable</p>
<p>Add another variable in the database to indicate whether a value is missing.</p>
<p>Create 2 variables</p>
<p><span class="math display">\[\begin{equation}
D=
\begin{cases}
1 &amp; \text{data on X are missing} \\
0 &amp; \text{otherwise}\\
\end{cases}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
X^* = 
\begin{cases}
X &amp; \text{data are available} \\
c &amp; \text{data are missing}\\
\end{cases}
\end{equation}\]</span></p>
<p><strong>Note</strong>: A typical choice for c is usually the mean of X</p>
<p>Interpretation:</p>
<ul>
<li>Coefficient of D is the the difference in the expected value of Y between the group with data and the group without data on X.</li>
<li>Coefficient of X* is the effect of the group with data on Y</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>This method yields bias estimates of the coefficient even in the case of MCAR <span class="citation">(Jones <a href="#ref-Jones_1996" role="doc-biblioref">1996</a>)</span></li>
</ul>
</div>
<div id="imputation" class="section level3">
<h3><span class="header-section-number">11.2.4</span> Imputation</h3>
<div id="maximum-likelihood-1" class="section level4">
<h4><span class="header-section-number">11.2.4.1</span> Maximum Likelihood</h4>
<p>When missing data are MAR and monotonic (such as in the case of panel studies), ML can be adequately in estimating coefficients.</p>
<p>Monotonic means that if you are missing data on X1, then that observation also has missing data on all other variables that come after it.</p>
<p>ML can generally handle linear models, log-linear model, but beyond that, ML still lacks both theory and software to implement.</p>
<div id="expectation-maximization-algorithm-em-algorithm" class="section level5">
<h5><span class="header-section-number">11.2.4.1.1</span> Expectation-Maximization Algorithm (EM Algorithm)</h5>
<p>An iterative process:</p>
<ol style="list-style-type: decimal">
<li>Other variables are used to impute a value (Expectation).<br />
</li>
<li>Check whether the value is most likely (Maximization).<br />
</li>
<li>If not, it re-imputes a more likely value.</li>
</ol>
<p>You start your regression with your estimates based on either listwise deletion or pairwise deletion.
After regressing missing variables on available variables, you obtain a regression model.
Plug the missing data back into the original model, with modified variances and covariances For example, if you have missing data on <span class="math inline">\(X_{ij}\)</span> you would regress it on available data of <span class="math inline">\(X_{i(j)}\)</span>, then plug the expected value of <span class="math inline">\(X_{ij}\)</span> back with its <span class="math inline">\(X_{ij}^2\)</span> turn into <span class="math inline">\(X_{ij}^2 + s_{j(j)}^2\)</span> where <span class="math inline">\(s_{j(j)}^2\)</span> stands for the residual variance from regressing <span class="math inline">\(X_{ij}\)</span> on <span class="math inline">\(X_{i(j)}\)</span>
With the new estimated model, you rerun the process until the estimates converge.</p>
<p>Advantages:</p>
<ol style="list-style-type: decimal">
<li>easy to use</li>
<li>preserves the relationship with other variables</li>
</ol>
<p>Disadvantages:</p>
<ol style="list-style-type: decimal">
<li>Standard errors of the coefficients are incorrect (biased usually downward)</li>
<li>Models with overidentification, the estimates will not be efficient.</li>
</ol>
</div>
<div id="direct-ml-raw-maximum-likelihood" class="section level5">
<h5><span class="header-section-number">11.2.4.1.2</span> Direct ML (raw maximum likelihood)</h5>
<p>Advantages</p>
<ol style="list-style-type: decimal">
<li>efficient estimates and correct standard errors.</li>
</ol>
<p>Disadvantages:</p>
<ol style="list-style-type: decimal">
<li>Hard to implements</li>
</ol>
</div>
</div>
<div id="multiple-imputation" class="section level4">
<h4><span class="header-section-number">11.2.4.2</span> Multiple Imputation</h4>
<p>MI is designed to use “the Bayesian model-based approach to <em>create</em> procedures, and the frequentist (randomization-based approach) to <em>evaluate</em> procedures”. <span class="citation">(Rubin <a href="#ref-Rubin_1996" role="doc-biblioref">1996</a>)</span></p>
<p>MI estimates have the same properties as <a href="solutions-to-missing-data.html#maximum-likelihood-1">ML</a> when the data is <a href="assumptions.html#missing-at-random-mar">MAR</a></p>
<ul>
<li>Consistent</li>
<li>Asymptotically efficient</li>
<li>Asymptotically normal</li>
</ul>
<p>MI can be applied to any type of model, unlike <a href="solutions-to-missing-data.html#maximum-likelihood-1">Maximum Likelihood</a> that is only limited to a small set of models.</p>
<p>A drawback of MI is that it will produce slightly different estimates every time you run it. To avoid such problem, you can set seed when doing your analysis to ensure its reproducibility.</p>
<div id="single-random-imputaiton" class="section level5">
<h5><span class="header-section-number">11.2.4.2.1</span> Single Random Imputaiton</h5>
<p>Random draws form the residual distribution of each imputed variable and add those random numbers to the imputed values.</p>
<p>For example, if we have missing data on X, and it’s MCAR, then</p>
<ol style="list-style-type: decimal">
<li>regress X on Y (<a href="solutions-to-missing-data.html#listwise-deletion">Listwise Deletion</a> method) to get its residual distribution.</li>
<li>For every missing value on X, we substitute with <span class="math inline">\(\tilde{x_i}=\hat{x_i} + \rho u_i\)</span> where
<ul>
<li><span class="math inline">\(u_i\)</span> is a random draw from a standard normal distribution</li>
<li><span class="math inline">\(x_i\)</span> is the predicted value from the regression of X and Y</li>
<li><span class="math inline">\(\rho\)</span> is the standard deviation of the residual distribution of X regressed on Y.</li>
</ul></li>
</ol>
<p>However, the model you run with the imputed data still thinks that your data are collected, not imputed, which leads your standard error estimates to be too low and test statistics too high.</p>
<p>To address this problem, we need to repeat the imputation process which leads us to repeated imputation or multiple random imputation.</p>
</div>
<div id="repeated-imputation" class="section level5">
<h5><span class="header-section-number">11.2.4.2.2</span> Repeated Imputation</h5>
<p>“Repeated imputations are draws from the posterior predictive distribution of the missing values under a specific model , a particular Bayesian model for both the data and the missing mechanism”.<span class="citation">(Rubin <a href="#ref-Rubin_1996" role="doc-biblioref">1996</a>)</span></p>
<p>Repeated imputation, also known as, multiple random imputation, allows us to have multiple “completed” data sets. The variability across imputations will adjust the standard errors upward.</p>
<p>The estimate of the standard error of <span class="math inline">\(\bar{r}\)</span> (mean correlation estimates between X and Y) is
<span class="math display">\[
SE(\bar{r})=\sqrt{\frac{1}{M}\sum_{k}s_k^2+ (1+\frac{1}{M})(\frac{1}{M-1})\sum_{k}(r_k-\bar{r})^2}
\]</span>
where M is the number of replications,
<span class="math inline">\(r_k\)</span> is the the correlation in replication k,
<span class="math inline">\(s_k\)</span> is the estimated standard error in replication k.</p>
<p>However, this method still considers the parameter in predicting <span class="math inline">\(\tilde{x}\)</span> is still fixed, which means we assume that we are using the true parameters to predict <span class="math inline">\(\tilde{x}\)</span>. To overcome this challenge, we need to introduce variability into our model for <span class="math inline">\(\tilde{x}\)</span> by treating the parameters as a random variables and use Bayesian posterior distribution of the parameters to predict the parameters.</p>
<p>However, if your sample is large and the proportion of missing data is small, the extra Bayesian step might not be necessary.
If your sample is small or the proportion of missing data is large, the extra Bayesian step is necessary.</p>
<p>Two algorithms to get random draws of the regression parameters from its posterior distribution:</p>
<ul>
<li><a href="solutions-to-missing-data.html#data-augmentation">Data Augmentation</a></li>
<li>Sampling importance/resampling (SIR)</li>
</ul>
<p>Authors have argued for SIR superiority due to its computer time <span class="citation">(King et al. <a href="#ref-King_2001" role="doc-biblioref">2001</a>)</span></p>
<div id="data-augmentation" class="section level6">
<h6><span class="header-section-number">11.2.4.2.2.1</span> Data Augmentation</h6>
<p>Steps for data augmentation:</p>
<ol style="list-style-type: decimal">
<li>Choose starting values for the parameters (e.g., for multivariate normal, choose means and covariance matrix). These values can come from previous values, expert knowledge, or from listwise deletion or pairwise deletion or EM estimation.</li>
<li>Based on the current values of means and covariances calculate the coefficients estimates for the equation that variable with missing data is regressed on all other variables (or variables that you think will help predict the missing values, could also be variables that are not in the final estimation model)</li>
<li>Use the estimates in step (2) to predict values for missing values. For each predicted value, add a random error from the residual normal distribution for that variable.</li>
<li>From the “complete” data set, recalculate the means and covariance matrix. And take a random draw from the posterior distribution of the means and covariances with Jeffreys’ prior.</li>
<li>Using the random draw from step (4), repeat step (2) to (4) until the means and covariances stabilize (converged).</li>
</ol>
<p>The iterative process allows us to get random draws from the joint posterior distribution of both data nd parameters, given the observed data.</p>
<p>Rules of thumb regarding convergence:</p>
<ul>
<li>The higher the proportion of missing, the more iterations</li>
<li>the rate of convergence for EM algorithm should be the minimum threshold for DA.</li>
<li>You can also check if your distribution has been converged by diagnostic statistics Can check <a href="https://bookdown.org/mike/bayesian_analysis/diag.html">Bayesian Diagnostics</a> for some introduction.</li>
</ul>
<p>Types of chains</p>
<ol style="list-style-type: decimal">
<li><strong>Parallel</strong>: Run a separate chain of iterations for each of data set. Different starting values are encouraged. For example, one could use bootstrap to generate different data set with replacement, and for each data set, calculate the starting values by EM estimates.
<ul>
<li>Pro: Run faster, and less likely to have dependence in the resulting data sets.</li>
<li>Con: Sometimes it will not converge</li>
</ul></li>
<li><strong>Sequential</strong> one long chain of data augmentation cycles. After burn-in and thinning, you will have to data sets
<ul>
<li>Pro: Converged to the true posterior distribution is more likely.</li>
<li>Con: The resulting data sets are likely to be dependent. Remedies can be thinning and burn-in.</li>
</ul></li>
</ol>
<p><strong>Note on Non-normal or categorical data</strong>
The normal-based methods still work well, but you will need to do some transformation. For example,</p>
<ul>
<li>If the data is skewed, then log-transform, then impute, the exponentiate to have the missing data back to its original metric.</li>
<li>If the data is proportion, logit-transform, impute, then de-transform the missing data.</li>
</ul>
<p>If you want to impute non-linear relationship, such as interaction between 2 variables and 1 variable is categorical. You can do separate imputation for different levels of that variable separately, then combined for the final analysis.</p>
<ul>
<li>If all variables that have missing data are categorical, then <strong>unrestricted multinomial model</strong> or <strong>log-linear model</strong> is recommended.</li>
<li>If a single categorical variable, <strong>logistic (logit) regression</strong> would be sufficient.</li>
</ul>
</div>
</div>
</div>
</div>
<div id="heckmans-sample-selection-model" class="section level3">
<h3><span class="header-section-number">11.2.5</span> Heckman’s Sample Selection Model</h3>
<div id="mean-mode-median-imputation" class="section level4">
<h4><span class="header-section-number">11.2.5.1</span> Mean, Mode, Median Imputation</h4>
<ul>
<li><p>Bad:</p>
<ul>
<li>Mean imputation does not preserve the relationships among variables<br />
</li>
<li>Mean imputation leads to An Underestimate of Standard Errors → you’re making Type I errors without realizing it.</li>
<li>Biased estimates of variances and covariances <span class="citation">(Haitovsky <a href="#ref-Haitovsky_1968" role="doc-biblioref">1968</a>)</span> .</li>
</ul></li>
</ul>
</div>
<div id="hot-deck-imputation" class="section level4">
<h4><span class="header-section-number">11.2.5.2</span> Hot Deck Imputation</h4>
<p>A randomly chosen value from an individual in the sample who has similar values on other variables.
In other words, find all the sample subjects who are similar on other variables, then randomly choose one of their values on the missing variable.</p>
<ul>
<li>Good:
<ul>
<li>Constrained to only possible values.<br />
</li>
<li>Since the value is picked at random, it adds some variability, which might come in handy when calculating standard errors.</li>
</ul></li>
</ul>
</div>
<div id="cold-deck-imputation" class="section level4">
<h4><span class="header-section-number">11.2.5.3</span> Cold Deck Imputation</h4>
<p>Contrary to Hot Deck, Cold Deck choose value systematically from an observation that has similar values on other variables, which remove the random variation that we want.</p>
<ol start="9" style="list-style-type: decimal">
<li>Bayesian ridge regression implementation</li>
</ol>
</div>
<div id="regression-imputation" class="section level4">
<h4><span class="header-section-number">11.2.5.4</span> Regression Imputation</h4>
<p>Also known as conditional mean imputation
Missing value is based (regress) on other variables.</p>
<ul>
<li><p>Good:</p>
<ul>
<li>Maintain the relationship with other variables<br />
</li>
<li>If the data are MCAR, least-squares coefficients estimates will be consistent, and approximately unbiased in large samples <span class="citation">(Gourieroux and Monfort <a href="#ref-Gourieroux_1981" role="doc-biblioref">1981</a>)</span>
<ul>
<li>Can have improvement on efficiency by using weighted least squares <span class="citation">(Beale and Little <a href="#ref-Beale_1975" role="doc-biblioref">1975</a>)</span> or generalized least squares <span class="citation">(Gourieroux and Monfort <a href="#ref-Gourieroux_1981" role="doc-biblioref">1981</a>)</span>.</li>
</ul></li>
</ul></li>
<li><p>Bad:</p>
<ul>
<li>No variability left. treated data as if they were collected.</li>
<li>Underestimate the standard errors and overestimate test statistics</li>
</ul></li>
</ul>
</div>
<div id="stochatistc-imputation" class="section level4">
<h4><span class="header-section-number">11.2.5.5</span> Stochatistc Imputation</h4>
<p><code>Regression imputation + random residual = Stochastic Imputation</code></p>
<p>Most multiple imputation is based off of some form of stochastic regression imputation.
Good:</p>
<ul>
<li>Has all the advantage of <a href="solutions-to-missing-data.html#regression-imputation">Regression Imputation</a></li>
<li>and also has the random components</li>
</ul>
<p><strong>Note</strong><br />
Multiple Imputation usually based on some form of stochastic regression imputation.</p>
</div>
<div id="interpolation-and-extrapolation" class="section level4">
<h4><span class="header-section-number">11.2.5.6</span> Interpolation and Extrapolation</h4>
<p>An estimated value from other observations from the same individual. It usually only works in longitudinal data.</p>
</div>
<div id="k-nearest-neighbour-knn-imputation" class="section level4">
<h4><span class="header-section-number">11.2.5.7</span> K-nearest neighbour (KNN) imputation</h4>
<p>The above methods are model-based imputation (regression).<br />
This is an example of neighbor-based imputation (K-nearest neighbor).</p>
<p>For every observation that needs to be imputed, the algorithm identifies ‘k’ closest observations based on some types distance (e.g., Euclidean) and computes the weighted average (weighted based on distance) of these ‘k’ obs.</p>
<p>For a discrete variable, it uses the most frequent value among the k nearest neighbors.<br />
* Distance metrics: Hamming distance.</p>
<p>For a continuous variable, it uses the mean or mode.</p>
<ul>
<li>Distance metrics:
<ul>
<li>Euclidean<br />
</li>
<li>Mahalanobis<br />
</li>
<li>Manhattan</li>
</ul></li>
</ul>
</div>
<div id="bayesian-ridge-regression-implementation" class="section level4">
<h4><span class="header-section-number">11.2.5.8</span> Bayesian Ridge regression implementation</h4>
</div>
<div id="e-m-algorithm" class="section level4">
<h4><span class="header-section-number">11.2.5.9</span> E-M Algorithm</h4>
<p>the E-M Algorithm, which stands for Expectation-Maximization. It is an iterative procedure in which it uses other variables to impute a value (Expectation), then checks whether that is the value most likely (Maximization). If not, it re-imputes a more likely value. This goes on until it reaches the most likely value.</p>
<p>EM imputations are better than mean imputations because they preserve the relationship with other variables, which is vital if you go on to use something like Factor Analysis or Linear Regression.
EM Imputations still underestimate standard error, however. Once again, this approach is only reasonable if the standard error of individual items is not vital, like in Factor Analysis, which doesn’t have p-values.</p>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Beale_1975">
<p>Beale, E. M. L., and R. J. A. Little. 1975. “Missing Values in Multivariate Analysis.” <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 37 (1): 129–45. <a href="https://doi.org/10.1111/j.2517-6161.1975.tb01037.x">https://doi.org/10.1111/j.2517-6161.1975.tb01037.x</a>.</p>
</div>
<div id="ref-Glasser_1964">
<p>Glasser, M. 1964. “Linear Regression Analysis with Missing Observations Among the Independent Variables.” <em>Journal of the American Statistical Association</em> 59 (307): 834–44. <a href="https://doi.org/10.1080/01621459.1964.10480730">https://doi.org/10.1080/01621459.1964.10480730</a>.</p>
</div>
<div id="ref-Gourieroux_1981">
<p>Gourieroux, Christian, and Alain Monfort. 1981. “On the Problem of Missing Data in Linear Models.” <em>The Review of Economic Studies</em> 48 (4): 579. <a href="https://doi.org/10.2307/2297197">https://doi.org/10.2307/2297197</a>.</p>
</div>
<div id="ref-Haitovsky_1968">
<p>Haitovsky, Yoel. 1968. “Missing Data in Regression Analysis.” <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 30 (1): 67–82. <a href="https://doi.org/10.1111/j.2517-6161.1968.tb01507.x">https://doi.org/10.1111/j.2517-6161.1968.tb01507.x</a>.</p>
</div>
<div id="ref-Jones_1996">
<p>Jones, Michael P. 1996. “Indicator and Stratification Methods for Missing Explanatory Variables in Multiple Linear Regression.” <em>Journal of the American Statistical Association</em> 91 (433): 222–30. <a href="https://doi.org/10.1080/01621459.1996.10476680">https://doi.org/10.1080/01621459.1996.10476680</a>.</p>
</div>
<div id="ref-King_2001">
<p>King, Gary, James Honaker, Anne Joseph, and Kenneth Scheve. 2001. “Analyzing Incomplete Political Science Data: An Alternative Algorithm for Multiple Imputation.” <em>American Political Science Review</em> 95 (1): 49–69. <a href="https://doi.org/10.1017/s0003055401000235">https://doi.org/10.1017/s0003055401000235</a>.</p>
</div>
<div id="ref-Little_1992">
<p>Little, Roderick J. A. 1992. “Regression with Missing Xs: A Review.” <em>Journal of the American Statistical Association</em> 87 (420): 1227. <a href="https://doi.org/10.2307/2290664">https://doi.org/10.2307/2290664</a>.</p>
</div>
<div id="ref-Rubin_1996">
<p>Rubin, Donald B. 1996. “Multiple Imputation After 18+ Years.” <em>Journal of the American Statistical Association</em> 91 (434): 473–89. <a href="https://doi.org/10.1080/01621459.1996.10476908">https://doi.org/10.1080/01621459.1996.10476908</a>.</p>
</div>
<div id="ref-Vach_1994">
<p>Vach, Werner. 1994. <em>Logistic Regression with Missing Values in the Covariates</em>. Springer New York. <a href="https://doi.org/10.1007/978-1-4612-2650-5">https://doi.org/10.1007/978-1-4612-2650-5</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="assumptions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="criteria-for-choosing-an-effective-approach.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mikenguyen13/data_analysis/edit/main/11-imputation.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Data Analysis.pdf", "Data Analysis.epub", "Data Analysis.mobi"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true,
"sharing": {
"facebook": true,
"github": true,
"twitter": true,
"linkedin": true
},
"info": true,
"edit": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
