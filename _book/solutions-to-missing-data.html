<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6.2 Solutions to Missing data | A Guide on Data Analysis</title>
  <meta name="description" content="This is a guide on how to conduct a data analysis routine" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="6.2 Solutions to Missing data | A Guide on Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a guide on how to conduct a data analysis routine" />
  <meta name="github-repo" content="mikenguyen13/data_analysis" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6.2 Solutions to Missing data | A Guide on Data Analysis" />
  
  <meta name="twitter:description" content="This is a guide on how to conduct a data analysis routine" />
  

<meta name="author" content="Mike Nguyen" />


<meta name="date" content="2020-11-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="assumptions.html"/>
<link rel="next" href="experimental-design.html"/>
<script src="libs/jquery-3.5.0/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.2/htmlwidgets.js"></script>
<script src="libs/proj4js-2.3.15/proj4.js"></script>
<link href="libs/highcharts-8.1.2/css/motion.css" rel="stylesheet" />
<script src="libs/highcharts-8.1.2/highcharts.js"></script>
<script src="libs/highcharts-8.1.2/highcharts-3d.js"></script>
<script src="libs/highcharts-8.1.2/highcharts-more.js"></script>
<script src="libs/highcharts-8.1.2/modules/stock.js"></script>
<script src="libs/highcharts-8.1.2/modules/map.js"></script>
<script src="libs/highcharts-8.1.2/modules/annotations.js"></script>
<script src="libs/highcharts-8.1.2/modules/data.js"></script>
<script src="libs/highcharts-8.1.2/modules/drilldown.js"></script>
<script src="libs/highcharts-8.1.2/modules/item-series.js"></script>
<script src="libs/highcharts-8.1.2/modules/offline-exporting.js"></script>
<script src="libs/highcharts-8.1.2/modules/overlapping-datalabels.js"></script>
<script src="libs/highcharts-8.1.2/modules/exporting.js"></script>
<script src="libs/highcharts-8.1.2/modules/export-data.js"></script>
<script src="libs/highcharts-8.1.2/modules/funnel.js"></script>
<script src="libs/highcharts-8.1.2/modules/heatmap.js"></script>
<script src="libs/highcharts-8.1.2/modules/treemap.js"></script>
<script src="libs/highcharts-8.1.2/modules/sankey.js"></script>
<script src="libs/highcharts-8.1.2/modules/dependency-wheel.js"></script>
<script src="libs/highcharts-8.1.2/modules/organization.js"></script>
<script src="libs/highcharts-8.1.2/modules/solid-gauge.js"></script>
<script src="libs/highcharts-8.1.2/modules/streamgraph.js"></script>
<script src="libs/highcharts-8.1.2/modules/sunburst.js"></script>
<script src="libs/highcharts-8.1.2/modules/vector.js"></script>
<script src="libs/highcharts-8.1.2/modules/wordcloud.js"></script>
<script src="libs/highcharts-8.1.2/modules/xrange.js"></script>
<script src="libs/highcharts-8.1.2/modules/tilemap.js"></script>
<script src="libs/highcharts-8.1.2/modules/venn.js"></script>
<script src="libs/highcharts-8.1.2/modules/gantt.js"></script>
<script src="libs/highcharts-8.1.2/modules/timeline.js"></script>
<script src="libs/highcharts-8.1.2/modules/parallel-coordinates.js"></script>
<script src="libs/highcharts-8.1.2/modules/bullet.js"></script>
<script src="libs/highcharts-8.1.2/modules/coloraxis.js"></script>
<script src="libs/highcharts-8.1.2/modules/dumbbell.js"></script>
<script src="libs/highcharts-8.1.2/modules/lollipop.js"></script>
<script src="libs/highcharts-8.1.2/modules/series-label.js"></script>
<script src="libs/highcharts-8.1.2/plugins/motion.js"></script>
<script src="libs/highcharts-8.1.2/custom/reset.js"></script>
<script src="libs/highcharts-8.1.2/modules/boost.js"></script>
<script src="libs/highchart-binding-0.8.2/highchart.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Guide on Data Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Prerequisites</a><ul>
<li class="chapter" data-level="1.1" data-path="general-math.html"><a href="general-math.html"><i class="fa fa-check"></i><b>1.1</b> General Math</a></li>
<li class="chapter" data-level="1.2" data-path="matrix-theory.html"><a href="matrix-theory.html"><i class="fa fa-check"></i><b>1.2</b> Matrix Theory</a><ul>
<li class="chapter" data-level="1.2.1" data-path="matrix-theory.html"><a href="matrix-theory.html#rank"><i class="fa fa-check"></i><b>1.2.1</b> Rank</a></li>
<li class="chapter" data-level="1.2.2" data-path="matrix-theory.html"><a href="matrix-theory.html#inverse"><i class="fa fa-check"></i><b>1.2.2</b> Inverse</a></li>
<li class="chapter" data-level="1.2.3" data-path="matrix-theory.html"><a href="matrix-theory.html#definiteness"><i class="fa fa-check"></i><b>1.2.3</b> Definiteness</a></li>
<li class="chapter" data-level="1.2.4" data-path="matrix-theory.html"><a href="matrix-theory.html#matrix-calculus"><i class="fa fa-check"></i><b>1.2.4</b> Matrix Calculus</a></li>
<li class="chapter" data-level="1.2.5" data-path="matrix-theory.html"><a href="matrix-theory.html#optimization"><i class="fa fa-check"></i><b>1.2.5</b> Optimization</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="probability-theory.html"><a href="probability-theory.html"><i class="fa fa-check"></i><b>1.3</b> Probability Theory</a><ul>
<li class="chapter" data-level="1.3.1" data-path="probability-theory.html"><a href="probability-theory.html#axiom-and-theorems-of-probability"><i class="fa fa-check"></i><b>1.3.1</b> Axiom and Theorems of Probability</a></li>
<li class="chapter" data-level="1.3.2" data-path="probability-theory.html"><a href="probability-theory.html#random-variable"><i class="fa fa-check"></i><b>1.3.2</b> Random variable</a></li>
<li class="chapter" data-level="1.3.3" data-path="probability-theory.html"><a href="probability-theory.html#moment"><i class="fa fa-check"></i><b>1.3.3</b> Moment</a></li>
<li class="chapter" data-level="1.3.4" data-path="probability-theory.html"><a href="probability-theory.html#distributions"><i class="fa fa-check"></i><b>1.3.4</b> Distributions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a></li>
<li class="chapter" data-level="3" data-path="descriptive-stat.html"><a href="descriptive-stat.html"><i class="fa fa-check"></i><b>3</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="3.1" data-path="numerical-measures.html"><a href="numerical-measures.html"><i class="fa fa-check"></i><b>3.1</b> Numerical Measures</a></li>
<li class="chapter" data-level="3.2" data-path="graphical-measures.html"><a href="graphical-measures.html"><i class="fa fa-check"></i><b>3.2</b> Graphical Measures</a><ul>
<li class="chapter" data-level="3.2.1" data-path="graphical-measures.html"><a href="graphical-measures.html#shape"><i class="fa fa-check"></i><b>3.2.1</b> Shape</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="normality-assessment.html"><a href="normality-assessment.html"><i class="fa fa-check"></i><b>3.3</b> Normality Assessment</a><ul>
<li class="chapter" data-level="3.3.1" data-path="normality-assessment.html"><a href="normality-assessment.html#graphical-assessment"><i class="fa fa-check"></i><b>3.3.1</b> Graphical Assessment</a></li>
<li class="chapter" data-level="3.3.2" data-path="normality-assessment.html"><a href="normality-assessment.html#summary-statistics"><i class="fa fa-check"></i><b>3.3.2</b> Summary Statistics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="basic-statistical-inference.html"><a href="basic-statistical-inference.html"><i class="fa fa-check"></i><b>4</b> Basic Statistical Inference</a><ul>
<li class="chapter" data-level="4.1" data-path="one-sample-inference.html"><a href="one-sample-inference.html"><i class="fa fa-check"></i><b>4.1</b> One Sample Inference</a></li>
<li class="chapter" data-level="4.2" data-path="two-sample-inference.html"><a href="two-sample-inference.html"><i class="fa fa-check"></i><b>4.2</b> Two Sample Inference</a></li>
<li class="chapter" data-level="4.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html"><i class="fa fa-check"></i><b>4.3</b> Categorical Data Analysis</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="regression-analysis.html"><a href="regression-analysis.html"><i class="fa fa-check"></i><b>5</b> Regression Analysis</a><ul>
<li class="chapter" data-level="5.1" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>5.1</b> Linear Regression</a><ul>
<li class="chapter" data-level="5.1.1" data-path="linear-regression.html"><a href="linear-regression.html#ordinary-least-squares"><i class="fa fa-check"></i><b>5.1.1</b> Ordinary Least Squares</a></li>
<li class="chapter" data-level="5.1.2" data-path="linear-regression.html"><a href="linear-regression.html#feasible-generalized-least-squares"><i class="fa fa-check"></i><b>5.1.2</b> Feasible Generalized Least Squares</a></li>
<li class="chapter" data-level="5.1.3" data-path="linear-regression.html"><a href="linear-regression.html#weighted-least-squares"><i class="fa fa-check"></i><b>5.1.3</b> Weighted Least Squares</a></li>
<li class="chapter" data-level="5.1.4" data-path="linear-regression.html"><a href="linear-regression.html#generalized-least-squares"><i class="fa fa-check"></i><b>5.1.4</b> Generalized Least Squares</a></li>
<li class="chapter" data-level="5.1.5" data-path="linear-regression.html"><a href="linear-regression.html#maximum-likelihood"><i class="fa fa-check"></i><b>5.1.5</b> Maximum Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="non-linear-regression.html"><a href="non-linear-regression.html"><i class="fa fa-check"></i><b>5.2</b> Non-linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="imputation-missing-data.html"><a href="imputation-missing-data.html"><i class="fa fa-check"></i><b>6</b> Imputation (Missing Data)</a><ul>
<li class="chapter" data-level="6.1" data-path="assumptions.html"><a href="assumptions.html"><i class="fa fa-check"></i><b>6.1</b> Assumptions</a><ul>
<li class="chapter" data-level="6.1.1" data-path="assumptions.html"><a href="assumptions.html#missing-completely-at-random-mcar"><i class="fa fa-check"></i><b>6.1.1</b> Missing Completely at Random (MCAR)</a></li>
<li class="chapter" data-level="6.1.2" data-path="assumptions.html"><a href="assumptions.html#missing-at-random-mar"><i class="fa fa-check"></i><b>6.1.2</b> Missing at Random (MAR)</a></li>
<li class="chapter" data-level="6.1.3" data-path="assumptions.html"><a href="assumptions.html#ignorable"><i class="fa fa-check"></i><b>6.1.3</b> Ignorable</a></li>
<li class="chapter" data-level="6.1.4" data-path="assumptions.html"><a href="assumptions.html#nonignorable"><i class="fa fa-check"></i><b>6.1.4</b> Nonignorable</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html"><i class="fa fa-check"></i><b>6.2</b> Solutions to Missing data</a><ul>
<li class="chapter" data-level="6.2.1" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#listwise-deletion"><i class="fa fa-check"></i><b>6.2.1</b> Listwise Deletion</a></li>
<li class="chapter" data-level="6.2.2" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#pairwise-deletion"><i class="fa fa-check"></i><b>6.2.2</b> Pairwise Deletion</a></li>
<li class="chapter" data-level="6.2.3" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#dummy-variable-adjustment"><i class="fa fa-check"></i><b>6.2.3</b> Dummy Variable Adjustment</a></li>
<li class="chapter" data-level="6.2.4" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#imputation"><i class="fa fa-check"></i><b>6.2.4</b> Imputation</a></li>
<li class="chapter" data-level="6.2.5" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#criteria-for-choosing-an-effective-approach"><i class="fa fa-check"></i><b>6.2.5</b> Criteria for Choosing an Effective Approach</a></li>
<li class="chapter" data-level="6.2.6" data-path="solutions-to-missing-data.html"><a href="solutions-to-missing-data.html#another-perspective"><i class="fa fa-check"></i><b>6.2.6</b> Another Perspective</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="experimental-design.html"><a href="experimental-design.html"><i class="fa fa-check"></i><b>7</b> Experimental Design</a><ul>
<li class="chapter" data-level="7.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html"><i class="fa fa-check"></i><b>7.1</b> Analysis of Variance (ANOVA)</a><ul>
<li class="chapter" data-level="7.1.1" data-path="analysis-of-variance-anova.html"><a href="analysis-of-variance-anova.html#completely-randomized-design-crd"><i class="fa fa-check"></i><b>7.1.1</b> Completely Randomized Design (CRD)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>8</b> Deep Learning</a><ul>
<li class="chapter" data-level="8.1" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i><b>8.1</b> Overview</a></li>
<li class="chapter" data-level="8.2" data-path="tensor-flow-and-r.html"><a href="tensor-flow-and-r.html"><i class="fa fa-check"></i><b>8.2</b> Tensor Flow and R</a><ul>
<li class="chapter" data-level="8.2.1" data-path="tensor-flow-and-r.html"><a href="tensor-flow-and-r.html#r-packages"><i class="fa fa-check"></i><b>8.2.1</b> R packages</a></li>
<li class="chapter" data-level="8.2.2" data-path="tensor-flow-and-r.html"><a href="tensor-flow-and-r.html#keras-layers"><i class="fa fa-check"></i><b>8.2.2</b> Keras layers</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="compiling-models.html"><a href="compiling-models.html"><i class="fa fa-check"></i><b>8.3</b> Compiling models</a></li>
<li class="chapter" data-level="8.4" data-path="losses-optimizers-and-metrics.html"><a href="losses-optimizers-and-metrics.html"><i class="fa fa-check"></i><b>8.4</b> Losses, Optimizers, and Metrics</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="research-methods.html"><a href="research-methods.html"><i class="fa fa-check"></i><b>9</b> Research Methods</a><ul>
<li class="chapter" data-level="9.1" data-path="but-for-world.html"><a href="but-for-world.html"><i class="fa fa-check"></i><b>9.1</b> But-for World</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="causality.html"><a href="causality.html"><i class="fa fa-check"></i><b>10</b> Causality</a></li>
<li class="chapter" data-level="11" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>11</b> Appendix</a><ul>
<li class="chapter" data-level="11.1" data-path="short-cut.html"><a href="short-cut.html"><i class="fa fa-check"></i><b>11.1</b> Short-cut</a></li>
<li class="chapter" data-level="11.2" data-path="function-short-cut.html"><a href="function-short-cut.html"><i class="fa fa-check"></i><b>11.2</b> Function short-cut</a></li>
<li class="chapter" data-level="11.3" data-path="citation.html"><a href="citation.html"><i class="fa fa-check"></i><b>11.3</b> Citation</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="bookdown-cheat-sheet.html"><a href="bookdown-cheat-sheet.html"><i class="fa fa-check"></i><b>12</b> Bookdown cheat sheet</a><ul>
<li class="chapter" data-level="12.1" data-path="math-expresssion-syntax.html"><a href="math-expresssion-syntax.html"><i class="fa fa-check"></i><b>12.1</b> Math Expresssion/ Syntax</a><ul>
<li class="chapter" data-level="12.1.1" data-path="math-expresssion-syntax.html"><a href="math-expresssion-syntax.html#statistics-notation"><i class="fa fa-check"></i><b>12.1.1</b> Statistics Notation</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="table.html"><a href="table.html"><i class="fa fa-check"></i><b>12.2</b> Table</a></li>
<li class="chapter" data-level="12.3" data-path="heading.html"><a href="heading.html"><i class="fa fa-check"></i><b>12.3</b> heading</a></li>
<li class="chapter" data-level="12.4" data-path="id-example.html"><a href="id-example.html"><i class="fa fa-check"></i><b>12.4</b> About labelling things</a></li>
<li class="chapter" data-level="12.5" data-path="cross-references.html"><a href="cross-references.html"><i class="fa fa-check"></i><b>12.5</b> Cross-references</a></li>
<li class="chapter" data-level="12.6" data-path="figures-tables-citations.html"><a href="figures-tables-citations.html"><i class="fa fa-check"></i><b>12.6</b> Figures, tables, citations</a></li>
<li class="chapter" data-level="12.7" data-path="how-the-square-bracket-links-work.html"><a href="how-the-square-bracket-links-work.html"><i class="fa fa-check"></i><b>12.7</b> How the square bracket links work</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Guide on Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="solutions-to-missing-data" class="section level2">
<h2><span class="header-section-number">6.2</span> Solutions to Missing data</h2>
<div id="listwise-deletion" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Listwise Deletion</h3>
<p>Advantages:</p>
<ul>
<li>Can be applied to any statistical test (SEM, multi-level regression, etc.)</li>
<li>In the case of MCAR, both the parameters estimates and its standard errors are unbiased.</li>
<li>In the case of MAR among independent variables (not depend on the values of dependent variables), then listwise deletion parameter estimates can still be unbiased. <span class="citation">(Little <a href="#ref-Little_1992" role="doc-biblioref">1992</a>)</span> For example, you have a model <span class="math inline">\(y=\beta_{0}+\beta_1X_1 + \beta_2X_2 +\epsilon\)</span> if the probability of missing data on X1 is independent of Y, but dependent on the value of X1 and X2, then the model estimates are still unbiased.
<ul>
<li>The missing data mechanism the depends on the values of the independent variables are the same as stratified sampling. And stratified sampling does not bias your estimates</li>
<li>In the case of logistic regression, if the probability of missing data on any variable depends on the value of the dependent variable, but independent of the value of the independent variables, then the listwise deletion will yield biased intercept estimate, but consistent estimates of the slope and their standard errors <span class="citation">(Vach <a href="#ref-Vach_1994" role="doc-biblioref">1994</a>)</span>. However, logistic regression will still fail if the probability of missing data is dependent on both the value of the dependent and independent variables.</li>
<li>Under regression analysis, listwise deletion is more robust than maximum likelihood and multiple imputation when MAR assumption is violated.</li>
</ul></li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>It will yield a larger standard errors than other more sophisticated methods discussed later.</li>
<li>If the data are not MCAR, but MAR, then your listwise deletion can yield biased estimates.</li>
<li>In other cases than regression analysis, other sophisticated methods can yield better estimates compared to listwise deletion.</li>
</ul>
</div>
<div id="pairwise-deletion" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Pairwise Deletion</h3>
<p>This method could only be used in the case of linear models such as linear regression, factor analysis, or SEM.
The premise of this method based on that the coefficient estimates are calculated based on the means, standard deviations, and correlation matrix. Compared to listwise deletion, we still utilized as many correlation between variables as possible to compute the correlation matrix.</p>
<p>Advantages:</p>
<ul>
<li>If the true missing data mechanism is MCAR, pair wise deletion will yield consistent estimates, and unbiased in large samples</li>
<li>Compared to listwise deletion: <span class="citation">(Glasser <a href="#ref-Glasser_1964" role="doc-biblioref">1964</a>)</span>
<ul>
<li>If the correlation among variables are low, pairwise deletion is more efficient estimates than listwise</li>
<li>If the correlations among variables are high, listwise deletion is more efficient than pairwise.</li>
</ul></li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>If the data mechanism is MAR, pairwise deletion will yield biased estimates.</li>
<li>In small sample, sometimes covariance matrix might not be positive definite, which means coefficients estimates cannot be calculated.</li>
</ul>
<p><strong>Note</strong>: You need to read carefully on how your software specify the sample size because it will alter the standard errors.</p>
</div>
<div id="dummy-variable-adjustment" class="section level3">
<h3><span class="header-section-number">6.2.3</span> Dummy Variable Adjustment</h3>
<p>Also known as Missing Indicator Method or Proxy Variable</p>
<p>Add another variable in the database to indicate whether a value is missing.</p>
<p>Create 2 variables</p>
<p><span class="math display">\[\begin{equation}
D=
\begin{cases}
1 &amp; \text{data on X are missing} \\
0 &amp; \text{otherwise}\\
\end{cases}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
X* = 
\begin{cases}
X &amp; \text{data are available} \\
c &amp; \text{data are missing}\\
\end{cases}
\end{equation}\]</span></p>
<p><strong>Note</strong>: A typical choice for c is usually the mean of X</p>
<p>Interpretation:</p>
<ul>
<li>Coefficient of D is the the difference in the expected value of Y between the group with data and the group without data on X.</li>
<li>Coefficient of X* is the effect of the group with data on Y</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>This method yields bias estimates of the coefficient even in the case of MCAR <span class="citation">(Jones <a href="#ref-Jones_1996" role="doc-biblioref">1996</a>)</span></li>
</ul>
</div>
<div id="imputation" class="section level3">
<h3><span class="header-section-number">6.2.4</span> Imputation</h3>
<div id="maximum-likelihood-1" class="section level4">
<h4><span class="header-section-number">6.2.4.1</span> Maximum Likelihood</h4>
<p>When missing data are MAR and monotonic (such as in the case of panel studies), ML can be adequately in estimating coefficients.</p>
<p>Monotonic means that if you are missing data on X1, then that observation also has missing data on all other variables that come after it.</p>
<div id="expectation-maximization-algorithm-em-algorithm" class="section level5">
<h5><span class="header-section-number">6.2.4.1.1</span> Expectation-Maximization Algorithm (EM Algorithm)</h5>
<p>An iterative process:</p>
<ol style="list-style-type: decimal">
<li>Other variables are used to impute a value (Expectation).<br />
</li>
<li>Check whether the value is most likely (Maximization).<br />
</li>
<li>If not, it re-imputes a more likely value.</li>
</ol>
<p>You start your regression with your estimates based on either listwise deletion or pairwise deletion.
After regressing missing variables on available variables, you obtain a regression model.
Plug the missing data back into the original model, with modified variances and covariances For example, if you have missing data on <span class="math inline">\(X_{ij}\)</span> you would regress it on available data of <span class="math inline">\(X_{i(j)}\)</span>, then plug the expected value of <span class="math inline">\(X_{ij}\)</span> back with its <span class="math inline">\(X_{ij}^2\)</span> turn into <span class="math inline">\(X_{ij}^2 + s_{j(j)}^2\)</span> where <span class="math inline">\(s_{j(j)}^2\)</span> stands for the residual variance from regressing <span class="math inline">\(X_{ij}\)</span> on <span class="math inline">\(X_{i(j)}\)</span>
With the new estimated model, you rerun the process until the estimates converge.</p>
<p>Advantages:</p>
<ol style="list-style-type: decimal">
<li>easy to use</li>
<li>preserves the relationship with other variables</li>
</ol>
<p>Disadvantages:</p>
<ol style="list-style-type: decimal">
<li>Standard errors of the coefficients are incorrect (biased usually downward)</li>
<li>Models with overidentification, the estimates will not be efficient.</li>
</ol>
</div>
<div id="direct-ml-raw-maximum-likelihood" class="section level5">
<h5><span class="header-section-number">6.2.4.1.2</span> Direct ML (raw maximum likelihood)</h5>
<p>Advantages</p>
<ol style="list-style-type: decimal">
<li>efficient estimates and correct standard errors.</li>
</ol>
<p>Disadvantages:</p>
<ol style="list-style-type: decimal">
<li>Hard to implement</li>
</ol>
</div>
</div>
<div id="mean-mode-median-imputation" class="section level4">
<h4><span class="header-section-number">6.2.4.2</span> Mean, Mode, Median Imputation</h4>
<ul>
<li><p>Bad:</p>
<ul>
<li>Mean imputation does not preserve the relationships among variables<br />
</li>
<li>Mean imputation leads to An Underestimate of Standard Errors → you’re making Type I errors without realizing it.</li>
<li>Biased estimates of variances and covariances <span class="citation">(Haitovsky <a href="#ref-Haitovsky_1968" role="doc-biblioref">1968</a>)</span></li>
</ul></li>
</ul>
</div>
<div id="hot-deck-imputation" class="section level4">
<h4><span class="header-section-number">6.2.4.3</span> Hot Deck Imputation</h4>
<p>Randomly choose value from other observations that share similar values on other variables.</p>
<ul>
<li>Good:
<ul>
<li>Constrained to only possible values.<br />
</li>
<li>Since the value is picked at random, it adds some variability, which might come in handy when calculating standard errors.</li>
</ul></li>
</ul>
</div>
<div id="cold-deck-imputation" class="section level4">
<h4><span class="header-section-number">6.2.4.4</span> Cold Deck Imputation</h4>
<p>Contrary to Hot Deck, Cold Deck choose value systematically from an observation that has similar values on other variables, which remove the random variation that we want.</p>
</div>
<div id="regression-imputation" class="section level4">
<h4><span class="header-section-number">6.2.4.5</span> Regression Imputation</h4>
<p>Also known as conditional mean imputation
Missing value is based (regress) on other variables.</p>
<ul>
<li><p>Good:</p>
<ul>
<li>Maintain the relationship with other variables<br />
</li>
<li>If the data are MCAR, least-squares coefficients estimates will be consistent, and approximately unbiased in large samples <span class="citation">(Gourieroux and Monfort <a href="#ref-Gourieroux_1981" role="doc-biblioref">1981</a>)</span>
<ul>
<li>Can have improvement on efficiency by using weighted least squares <span class="citation">(Beale and Little <a href="#ref-Beale_1975" role="doc-biblioref">1975</a>)</span> or generalized least squares <span class="citation">(Gourieroux and Monfort <a href="#ref-Gourieroux_1981" role="doc-biblioref">1981</a>)</span>.</li>
</ul></li>
</ul></li>
<li><p>Bad:</p>
<ul>
<li>No variability left. treated data as if they were collected.</li>
<li>Underestimate the standard errors and overestimate test statistics</li>
</ul></li>
</ul>
</div>
<div id="stochatistc-imputation" class="section level4">
<h4><span class="header-section-number">6.2.4.6</span> Stochatistc Imputation</h4>
<p><code>Regression imputation + random residual = Stochastic Imputation</code></p>
<p>Most multiple imputation is based off of some form of stochastic regression imputation.</p>
</div>
<div id="interpolation-and-extrapolation" class="section level4">
<h4><span class="header-section-number">6.2.4.7</span> Interpolation and Extrapolation</h4>
<p>An estimated value from other observations from the same individual. It usually only works in longitudinal data.</p>
</div>
<div id="k-nearest-neighbour-knn-imputation" class="section level4">
<h4><span class="header-section-number">6.2.4.8</span> K-nearest neighbour (KNN) imputation</h4>
<p>The above methods are model-based imputation (regression).<br />
This is an example of neighbor-based imputation (K-nearest neighbor).</p>
<p>For a discrete variable, it uses the most frequent value among the k nearest neighbors.</p>
<ul>
<li>Distance metrics: Hamming distance.</li>
</ul>
<p>For a continuous variable, it uses the mean or mode.</p>
<ul>
<li>Distance metrics:
<ul>
<li>Euclidean<br />
</li>
<li>Mahalanobis<br />
</li>
<li>Manhattan</li>
</ul></li>
</ul>
</div>
<div id="bayesian-ridge-regression-implementation" class="section level4">
<h4><span class="header-section-number">6.2.4.9</span> Bayesian Ridge regression implementation</h4>
</div>
</div>
<div id="criteria-for-choosing-an-effective-approach" class="section level3">
<h3><span class="header-section-number">6.2.5</span> Criteria for Choosing an Effective Approach</h3>
<p>Criteria for an ideal technique in treating missing data:</p>
<ol style="list-style-type: decimal">
<li>Unbiased parameter estimates</li>
<li>Adequate power</li>
<li>Accurate standard errors (p-values, confidence intervals)</li>
</ol>
<p>The Multiple Imputation and Full Information Maximum Likelihood are the the most ideal candidate. Single imputation will generally lead to underestimation of standard errors.</p>
</div>
<div id="another-perspective" class="section level3">
<h3><span class="header-section-number">6.2.6</span> Another Perspective</h3>
<p>In the imputation world, you can go for either single or multiple imputation.<br />
Most of the imputation methods listed above are single imputation. A downfall of single imputation is its underestimation towards standard error.</p>
<p><br>
Model bias can arisen from various factors including:</p>
<ul>
<li>Imputation method<br />
</li>
<li>Missing data mechanism (MCAR vs. MAR)<br />
</li>
<li>Proportion of the missing data<br />
</li>
<li>Information available in the data set</li>
</ul>
<p>Since the imputed observations are themselves estimates, their values have corresponding random error. But when you put in that estimate as a data point, your software doesn’t know that. So it overlooks the extra source of error, resulting in too-small standard errors and too-small p-values.
So multiple imputation comes up with multiple estimates. Two of the methods listed above work as the imputation method in multiple imputation–hot deck and stochastic regression.
Because these two methods have a random component, the multiple estimates are slightly different. This re-introduces some variation that your software can incorporate in order to give your model accurate estimates of standard error.
Multiple imputation was a huge breakthrough in statistics about 20 years ago. It solves a lot of problems with missing data (though, unfortunately not all) and if done well, leads to unbiased parameter estimates and accurate standard errors.
If your rate of missing data is very, very small, it honestly doesn’t matter what technique you use. I’m talking very, very, very small (2-3%).</p>
<p>Missing Data Mechanisms
(1) Missing Completely at Random (MCAR)
- the propensity for a data point to be missing is completely random.
- There’s no relationship between whether a data point is missing and any values in the data set, missing or observed.
- The missing data are just a random subset of the data.
(2) Missing at Random. (MAR)
- the propensity for a data point to be missing is not related to the missing data, but it is related to some of the observed data. In another word, there is a systematic relationship between the propensity of missing values and the observed data, but not the missing data.
- For example, if men are more likely to tell you their weight than women, weight is MAR
- MAR requires that the cause of the missing data is unrelated to the missing values but may be related to the observed values of other variables.
- MAR means that the missing values are related to observed values on other variables. As an example of CD missing data, missing income data may be unrelated to the actual income values but are related to education. Perhaps people with more education are less likely to reveal their income than those with less education
(3) Non-Ignorable (NI)
- Another name: Missing Not at Random (MNAR): there is a relationship between the propensity of a value to be missing and its values
- For example, people with low education will be less likely to report it.
- We need to model why the data are missing and what the likely values are.
- the missing data mechanism is related to the missing values
- It commonly occurs when people do not want to reveal something very personal or unpopular about themselves
- Complete case analysis can give highly biased results for NI missing data. If proportionally more low and moderate income individuals are left in the sample because high income people are missing, an estimate of the mean income will be lower than the actual population mean.</p>
<p>Remember that there are three goals of multiple imputation, or any missing data technique: Unbiased parameter estimates in the final analysis (regression coefficients, group means, odds ratios, etc.); accurate standard errors of those parameter estimates, and therefore, accurate p-values in the analysis; and adequate power to find meaningful parameter values significant.
1. Don’t round off imputations for dummy variables. Many common imputation techniques, like MCMC, require normally distributed variables. Suggestions for imputing categorical variables were to dummy code them, impute them, then round off imputed values to 0 or 1. Recent research, however, has found that rounding off imputed values actually leads to biased parameter estimates in the analysis model. You actually get better results by leaving the imputed values at impossible values, even though it’s counter-intuitive.</p>
<ol start="2" style="list-style-type: decimal">
<li><p>Don’t transform skewed variables. Likewise, when you transform a variable to meet normality assumptions before imputing, you not only are changing the distribution of that variable but the relationship between that variable and the others you use to impute. Doing so can lead to imputing outliers, creating more bias than just imputing the skewed variable.</p></li>
<li><p>Use more imputations. The advice for years has been that 5-10 imputations are adequate. And while this is true for unbiasedness, you can get inconsistent results if you run the multiple imputation more than once. Bodner (2008) recommends having as many imputations as the percentage of missing data. Since running more imputations isn’t any more work for the data analyst, there’s no reason not to.
Bodner, T. E. 2008. “What Improves with Increased Missing Data Imputations?”
Structural Equation Modeling 15(4):651–75.</p></li>
<li><p>Create multiplicative terms before imputing. When the analysis model contains a multiplicative term, like an interaction term or a quadratic, create the multiplicative terms first, then impute. Imputing first, and then creating the multiplicative terms actually biases the regression parameters of the multiplicative term (von Hippel, 2009).
von Hippel, P.T. (2009). “How To Impute Squares, Interactions, and Other Transformed Variables.” Sociological Methodology 39.</p></li>
<li><p>Alternatives to multiple imputation aren’t usually better. Multiple imputation assumes the data are missing at random. In most tests, if an assumption is not met, there are better alternatives—a nonparametric test or an alternative type of model. This is often not true with missing data. Alternatives like listwise deletion (a.k.a. ignoring it) have more stringent assumptions. So do nonignorable missing data techniques like Heckman’s selection models.</p></li>
</ol>
<p>Do not use these ad-hoc:
Pairwise Deletion: use the available data for each part of an analysis. This has been shown to result in correlations beyond the 0,1 range and other fun statistical impossibilities.
Mean Imputation: substitute the mean of the observed values for all missing data. There are so many problems, it’s difficult to list them all, but suffice it to say, this technique never meets the above 3 criteria.
Dummy Variable: create a dummy variable that indicates whether a data point is missing, then substitute any arbitrary value for the missing data in the original variable. Use both variables in the analysis. While it does help the loss of power, it usually leads to biased results.</p>
<p>Multiple Imputation of categorical variables is bad
Paul Allison, one of my favorite authors of statistical information for researchers, did a study that showed that the most common method actually gives worse results that listwise deletion. (Did I mention I’ve used it myself?)
What is the bad method?
1. Dummy code the variable
2. Impute a continuous value. This will generally be between 0 and 1.
3. Round off to either 0 or 1, based on whether the imputed value is below or above .5.
As Allison discovered, this method generally leads to biased results, and incorrect standard errors
What to do instead?
Allison compared this approach to four others, each of which generally gave more accurate results, at least under some conditions.
1. Listwise deletion
2. Imputation of the continuous variable without rounding (just leave off step 3).
3. Logistic Regression imputation
4. Discriminant Analysis imputation
These last two generally performed best, but only work in limited situations.</p>
<p>How to Diagnose the Missing Data Mechanism
by Karen Grace-Martin 4 Comments</p>
<p>One important consideration in choosing a missing data approach is the missing data mechanism—different approaches have different assumptions about the mechanism.</p>
<p>Each of the three mechanisms describes one possible relationship between the propensity of data to be missing and values of the data, both missing and observed.</p>
<p>The Missing Data Mechanisms
Missing Completely at Random, MCAR, means there is no relationship between the missingness of the data and any values, observed or missing. Those missing data points are a random subset of the data. There is nothing systematic going on that makes some data more likely to be missing than others.</p>
<p>Missing at Random, MAR, means there is a systematic relationship between the propensity of missing values and the observed data, but not the missing data.</p>
<p>Whether an observation is missing has nothing to do with the missing values, but it does have to do with the values of an individual’s observed variables. So, for example, if men are more likely to tell you their weight than women, weight is MAR.</p>
<p>Missing Not at Random, MNAR, means there is a relationship between the propensity of a value to be missing and its values. This is a case where the people with the lowest education are missing on education or the sickest people are most likely to drop out of the study.</p>
<p>MNAR is called “non-ignorable” because the missing data mechanism itself has to be modeled as you deal with the missing data. You have to include some model for why the data are missing and what the likely values are.</p>
<p>“Missing Completely at Random” and “Missing at Random” are both considered ‘ignorable’ because we don’t have to include any information about the missing data itself when we deal with the missing data.</p>
<p>Why you need to know the mechanism you have
Multiple imputation and Maximum Likelihood assume the data are at least missing at random. So the important distinction here is whether the data are MAR as opposed to MNAR.</p>
<p>Listwise deletion, however, requires the data are MCAR in order to not introduce bias in the results.</p>
<p>As long as the distribution and percentage of missing data is no so great that it negatively affects power, listwise deletion can be a good choice for MCAR missing data. So the important distinction here is whether the data are MCAR as opposed to MAR.</p>
<p>Keep in mind that in most data sets, more than one variable will have missing data, and they may not all have the same mechanism. It’s worthwhile diagnosing the mechanism for each variable with missing data before choosing an approach.</p>
<p>I use the term diagnosing rather than testing, because you’re not going to get a straight answer without knowing the values of the missing data. Of course, if you knew those, you wouldn’t be doing any of this.</p>
<p>It’s like checking for multicollinearity or testing assumptions. Each piece of information tells you something, but there is no definitive answer.</p>
<p>You have to get at the mechanism in a number of ways and then decide if making the assumption about the mechanism is reasonable.</p>
<div id="diagnosing-the-mechanism" class="section level5">
<h5><span class="header-section-number">6.2.6.0.1</span> Diagnosing the Mechanism</h5>
<ol style="list-style-type: decimal">
<li>MAR vs. MNAR
The only true way to distinguish between MNAR and MAR is to measure some of that missing data. It’s a common practice among professional surveyors to, for example, follow-up on a paper survey with phone calls to a group of the non-respondents and ask a few key survey items. This allows you to compare respondents to non-respondents.</li>
</ol>
<p>If their responses on those key items differ by very much, that’s good evidence that the data are MNAR.</p>
<p>However in most missing data situations, we don’t have the luxury of getting a hold of the missing data. So while we can’t test it directly, we can examine patterns in the data get an idea of what’s the most likely mechanism.</p>
<p>The first thing in diagnosing randomness of the missing data is to use your substantive scientific knowledge of the data and your field. The more sensitive the issue, the less likely people are to tell you. They’re not going to tell you as much about their cocaine usage as they are about their phone usage.</p>
<p>Likewise, many fields have common research situations in which non-ignorable data is common. Educate yourself in your field’s literature.</p>
<ol start="2" style="list-style-type: decimal">
<li>MCAR vs. MAR
There is a very useful test for MCAR, Little’s test.</li>
</ol>
<p>A second technique is to create dummy variables for whether a variable is missing.</p>
<p>1 = missing
0 = observed</p>
<p>You can then run t-tests and chi-square tests between this variable and other variables in the data set to see if the missingness on this variable is related to the values of other variables.</p>
<p>For example, if women really are less likely to tell you their weight than men, a chi-square test will tell you that the percentage of missing data on the weight variable is higher for women than men.</p>

</div>
</div>
</div>
<!-- </div> -->
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Beale_1975">
<p>Beale, E. M. L., and R. J. A. Little. 1975. “Missing Values in Multivariate Analysis.” <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 37 (1): 129–45. <a href="https://doi.org/10.1111/j.2517-6161.1975.tb01037.x">https://doi.org/10.1111/j.2517-6161.1975.tb01037.x</a>.</p>
</div>
<div id="ref-Glasser_1964">
<p>Glasser, M. 1964. “Linear Regression Analysis with Missing Observations Among the Independent Variables.” <em>Journal of the American Statistical Association</em> 59 (307): 834–44. <a href="https://doi.org/10.1080/01621459.1964.10480730">https://doi.org/10.1080/01621459.1964.10480730</a>.</p>
</div>
<div id="ref-Gourieroux_1981">
<p>Gourieroux, Christian, and Alain Monfort. 1981. “On the Problem of Missing Data in Linear Models.” <em>The Review of Economic Studies</em> 48 (4): 579. <a href="https://doi.org/10.2307/2297197">https://doi.org/10.2307/2297197</a>.</p>
</div>
<div id="ref-Haitovsky_1968">
<p>Haitovsky, Yoel. 1968. “Missing Data in Regression Analysis.” <em>Journal of the Royal Statistical Society: Series B (Methodological)</em> 30 (1): 67–82. <a href="https://doi.org/10.1111/j.2517-6161.1968.tb01507.x">https://doi.org/10.1111/j.2517-6161.1968.tb01507.x</a>.</p>
</div>
<div id="ref-Jones_1996">
<p>Jones, Michael P. 1996. “Indicator and Stratification Methods for Missing Explanatory Variables in Multiple Linear Regression.” <em>Journal of the American Statistical Association</em> 91 (433): 222–30. <a href="https://doi.org/10.1080/01621459.1996.10476680">https://doi.org/10.1080/01621459.1996.10476680</a>.</p>
</div>
<div id="ref-Little_1992">
<p>Little, Roderick J. A. 1992. “Regression with Missing Xs: A Review.” <em>Journal of the American Statistical Association</em> 87 (420): 1227. <a href="https://doi.org/10.2307/2290664">https://doi.org/10.2307/2290664</a>.</p>
</div>
<div id="ref-Vach_1994">
<p>Vach, Werner. 1994. <em>Logistic Regression with Missing Values in the Covariates</em>. Springer New York. <a href="https://doi.org/10.1007/978-1-4612-2650-5">https://doi.org/10.1007/978-1-4612-2650-5</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="assumptions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="experimental-design.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mikenguyen13/data_analysis/edit/main/04-imputation.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Data Analysis.pdf", "Data Analysis.epub", "Data Analysis.mobi"],
"toc": {
"collapse": "subsection",
"scroll_highlight": true,
"sharing": {
"facebook": true,
"github": true,
"twitter": true,
"linkedin": true
},
"info": true,
"edit": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
