# Quantile Regression

Since we lose info when we summarize distribution (e.g., $Y \sim N(\mu, \sigma^2)$), quantiles can complement in summarizing a distribution.

They capture skewness, spread, tails, other aspects of the distribution's shape.

<br>

$\tau$-quantile = 100$\tau$ -th percentile:

Let $\tau \in [0,1]$ be the quantile index (or level)

$Q_\tau(Y)$ be the $\tau$-quantile of random variable $Y$

Then, $Q_\tau(Y)$ satisfies $P(Y \le Q_\tau(Y)) = \tau$ or $F_Y (Q_\tau(Y)) = \tau$

and $Q_\tau (Y) = F^{-1}_Y (\tau)$ if the CDF $F(.)$ is invertible

$$
Q_\tau(Y) \equiv \inf\{y : F_Y(y) \ge \tau \}
$$

The quantile function $Q_Y(.)$ expresses the quantiles of $Y$ as a function of $\tau$

$$
Q_Y(\tau) \equiv \inf \{ y : F_Y(y) \ge \tau \}, 0 \le \tau \le 1
$$

-   If the CDF $F_Y(.)$ is invertible, then $Q_Y(.) = F^{-1}_Y (.)$

-   If $F_Y(.)$ has a flat spot, then $Q_Y(.)$ has a jump discontinuity

-   If $F_Y(.)$ has a discontinuity (e.g., if $Y$ is discrete), then $Q_Y(.)$ has a corresponding flat spot

-   Whereas CDFs are right-continuous (with left limits), quantile function are left-continuous (with right limits)

<br>

## Prediction

Like the mean, quantiles are optimal predictors under certain "loss functions"

Let $Y$ be a discrete random variable with $P(Y=1)= P(Y = 2) = P(Y = 99) = 1/3$

**Population minimization**

Quadratic loss

$$
\theta_1 = \arg \min_{t \in \mathbb{R}} E((Y-t)^2) \\
= E(Y) = 34
$$

Absolute loss

$$
\theta_2 = \arg \min_{t \in \mathbb{R}} E(|Y-t|)
$$

Loss function $L(y,g)$ quantifies how bad it is to guess (predict) $g$ when the true value is $y$

The (infinitely) long-run average loss given fixed $g$ is the expected loss (also known as risk), where the expectation is wrt the distribution of $Y$: $E[L(Y,g)]$

Given this framework and a particular loss function, the optimal predictor minimizes risk.

Given loss function $L$, the optimal frequentist predictor minimizes risk (expected loss):

$$
g_L^* \equiv \arg \min_g E[L(Y,g)]
$$

Since the population mean is the "best" unconditional predictor of $Y$ given a certain definition of "best"

Consider quadratic loss function

$$
L_2(y,g) = \rho_2 (y-g) = (y - g)^2
$$

Then, the mean is optimal in that

$$
E(Y) = \arg \min_g E[L_2 (Y,g)]
$$

Equivalently, the mean $E(Y)$ minimizes the mean squared prediction error, where

-   $y-g$ is the prediction error

-   $(y-g)^2$ is the squared prediction

-   $E[(Y- g)^2]$ is the mean squared prediction error (MSPE).

This can be derived from the FOC:

$$
0 = \frac{d}{d g} E[L_2(Y,g)] |_{g - g^*_2} = \frac{d}{d g} E[(Y- g)^2]|_{g = g^*_2} = 2 E[Y- g^*_2]
$$

thus, $g_2^* = E(Y)$

Replacing $L_2(y,g)$ with $L_1(y,g) = |y - g|$ yields a different optimal predictor, specifically the population median:

$$Q_{0.5} (Y) = \arg \min_{g} E[L_1(Y, g)]$$A broader class of loss function characterizes all quantiles over $\tau \in (0,1)$

Given $\tau$. the **check function** or **tick function** is

$$
\rho_\tau(u) \equiv u(\tau - \mathbb{1} \{ u < 0\})
$$ with $\tau = 0.5$ actually $\rho_{0.5} (u) = \frac{|u|}{2}$ not $|u|$

However, scaling by a constant does not affect minimization:

$$
\begin{aligned}
Q_{0.5} (Y) &= \arg \min_{g}E[L_1 (Y,g)] \\
&= \arg \min_g (1/2) E[L_1 (Y,g)] \\
&= \arg \min_g E[\rho_{0.5} (Y-g)] 
\end{aligned}
$$

More generally,

$$
Q_\tau(Y) = \arg \min_g E[\rho_\tau (Y-g)]
$$

that is the $\tau$-quantile of $Y$ is the optimal unconditional predictor of $Y$ under loss function $L(y,g) = \rho_\tau(y-g)$

<br>

When $\tau = 0.95$ penalize under-prediction ($g < y$) more heavily

Intuitively, it makes sense that $Q_{0.95} (Y)$ is a better predictor than $Q_{0.5}(Y)$ given the $\tau = 0.95$ loss function $\rho_{0.95}(.)$

**Sample minimization**

Quadratic loss

$$
\hat{\theta}_1 = \arg \min_{t \in \mathbb{R}} \frac{1}{n} \sum_{i = 1}^n (Y_i -t)^2
$$

Absolute loss

$$
\hat{\theta}_2 = \arg \min_{t \in \mathbb{R}} \frac{1}{n} \sum_{i = 1}^n |Y_i -t|
$$

As with the mean, there are 2 approaches to estimating quantiles

1.  We could "plug in" the estimated CDF into a CDF-based definition. With iid data the empirical CDF is

$$
\hat{F}_Y(y) = \frac{1}{n} \sum_{i=1}^n \mathbb{1} \{Y_i \le y\}, \forall y \in R
$$

(i.e., the sample proportion of $Y_i$ below the point of evaluation $y$)

This is the CDF for a discrete distribution with probability $\frac{1}{n}$ on each observed $Y_i$ value (if values are unique)

The population mean is

$$
E(Y) = \int_\mathbb{R} y dF_Y(y)
$$

The plug-in principle or analogy principle suggests "plugging in" $\hat{F}_Y(.)$ for $F_Y(.)$ to get the sample analog of $E(Y)$

$$
\hat{E}(Y) = \int_\mathbb{R} y d \hat{F}_Y (y) = \sum_{i=1}^n Y_i (1/n) = \bar{Y}_n
$$

for familiar sample mean.

For $Q_\tau(Y)$, we can replace $F_Y(.)$

$$
\hat{Q}_\tau (Y) = \inf \{y: \hat{F}_Y(y) \ge \tau\}
$$

called the sample $\tau$-quantile

The second quantile estimation approach relates to prediction: solve the sample version of the population minimization problem. For the mean,

$$
E(Y) = \arg \min_g E[(Y - g)^2]
$$

in the population.

Replacing the population expectation $E[.]$ with the sample expectation $\hat{E}[.]$ (i.e., sample average)

$$
\hat{E} (Y) = \arg \min_g \hat{E}[(Y -g)^2] = \arg\min_g \frac{1}{n} \sum_{i=1}^n (Y_i - g)^2
$$

which is the familiar "least squares" approach, minimizing the sum of squared residuals.

For quantiles, replacing $E[.]$ with $\hat{E}[.]$

$$
\hat{Q}_\tau (Y) = \arg\min_g \hat{E} [ \rho_\tau (Y -g)] = \arg \min_g \frac{1}{n} \sum_{i=1}^n \rho_\tau (Y_i - g)
$$<br>

## Censoring

Quantiles are useful when observations are censored (i.e., we do not always observe the true value)

More specifically, the observed value is a function of the true value, but this function is not injective (not one-to-one), so the true values can't be recovered exactly from the censored values

Example top-coding of earnings data

Approaches:

1.  impute values (i.e., guess the true values)
2.  ask economic question that don't rely on the very upper tail (e.g., questions often involve quantiles)

Consider income inequality: The qualitative idea of "inequality" can be quantified in many possible ways:

1.  Standard deviation, but it depends on the very upper tail that we can't observe
2.  The difference between the 0.9-quantile and the 0.1-quantile (the 0.9-0.1 interquantile range) does not require any knowledge about the top 10% of the distribution

**Identification**:

-   Assume we can learn about the joint distribution of observable variables (e..g, we can consistently estimate the population distribution)

-   In a time series setting, this may not make sense

-   We assume that we can learn the population distribution of observables, is that that sufficient to learn about the parameter of interest?

Let $\mathcal{F}$ be a set of possible joint distribution of observable variables.

Parameter $\theta \in \mathbb{R}$ is identified on $\mathcal{F}$ is $F$ uniquely determines $\theta$ for all $F \in \mathcal{F}$

Consider the following form of top-coding.

An individual's true earnings are $Y^*$

Constant $c$ is the top-coding threshold. The observed $Y$ is

$$
Y=
\begin{cases}
Y^* && \text{if } Y^* \le c \\
c && \text{if } Y^* > c
\end{cases}
$$

Since $P(Y=c) = P(Y^* \ge c)$, the distribution of $Y$may have a mass point at $c$ even if $Y^*$ is continuous

This means the observable CDF $F(.)$ may jump discontinuously at $c$ since $F(c) = 1$

More generally, the CDF of the observed $Y$ is

$$
F(y) = 
\begin{cases}
F^* (y) && \text{if } y < c \\
1 && \text{if } y \ge c
\end{cases}
$$

If $c$ greater than median, then $\mathcal{F}$ be restricted to ensure they always have the same median.

```{r, message=FALSE, warning=FALSE}
plot(0:1,0:1,type='l')
lines(c(0,0.8,2),c(0,0.8,1), col=2)
lines(c(0,0.7,0.7,100),c(0,0.7,1,1), col=3)
```

## Robustness and Efficiency

The median is well defined for any probbability distribution, while the mean is not (e.g., Cauchy distribution has median 0, but undefined mean).

the median can have better estimation efficiency (i.e., smaller standard error) in the case of outliers.

For example, $Y_i = i$ for $i = 1, \dots, 98$ and $Y_{99} = J$

As $J \to \infty$, the sample mean $\hat{E}(Y) = \bar{Y}_n \to \infty$, butt the sample median remains 50

Quantile regression is only robust to outliers in $Y$, not $X$, to get robust regression methods, least median of squares can be better candidate.

## Inference

with iid sampling,

$$
\sqrt{n} (\hat{Q}_\tau (Y) - Q_\tau (Y)) \to^d N(0, \sigma^2) \\
\sigma^2 = \frac{\tau(1-\tau)}{[f_Y(T_\tau(Y))]^2}
$$

where $f_Y(.)$ is the PDF of $Y$

Given $\hat{\sigma} \to^p \sigma$, the CI $\hat{Q}_\tau(Y) \pm 1.96 \hat{\sigma}/\sqrt{n}$ has coverage probability approaching 95% as $n \to \infty$

But for finite samples, $\hat{\sigma}$ isn't accuate. Hence, other apporaches that account for the estimation error in the nonparaametric $\hat{\sigma}$ should be eexplored (e..g, Bayesian bootstrap, or CIs based onn order statistics)

<br>

## Description

### Conditional Quantile Function

CEF is the "best" predictor (because it's for a specific loss function - squared loss function).

Hence, depending on the true loss function, CEF might not be the best predictor. (Per task or job, we have a "true" loss function in a set of loss functions)

## Causality

1.  Potential outcomes "Treatment" ATE

SUTVA = no inference

2.  Structural

$$
Y = X' \beta +U \\
E(U|Z) = 0
$$

Let $Y_1$ = individual's treated potential outcome

$Y_0$ = same person untreated potential outcome

The treatment effect for an individual $Y_1 - Y_0$

The treatment effect for individual $i$ is $Y_{1i} - Y_{0i}$

The average treatment effect (ATE) = $E(Y_1 - Y_0)$ (Because of the linear operation of the expectation $E(Y_1 - Y_0) = E(Y_1) - E(Y_0)$ (treatment effect on the mean)

Potential outcomes

| $Y_0$ | $Y_1$ | $Y_1 - Y_0$ | Prob |
|-------|-------|-------------|------|
| 0     | 1     | 1           | 0.25 |
| 1     | 2     | 1           | 0.25 |
| 2     | 4     | 2           | 0.25 |
| 3     | 0     | -3          | 0.25 |

Table 6.1 p. 79 (Distributional and nonparametric econometrics)

$$
E(Y_0) = (0 + 1+ 2 + 3)/4 = 6/4\\
E(Y_1 = (1 + 2 + 4 + 0) /4 = 7/4 \\
E(Y_1) - E(Y_0) = 7/4-6/4 = 1/4 \\
E(Y_1 - Y_0) = (1 + 1 + 2 + -3) /4 = 1/4 \\
Q_{0.4}(Y_1) - Q_{0.4}(Y_0) = 1 - 1 = 0 \\
Q_{0.4}(Y_1 - Y_0) = 1
$$

Because of the linear operation of the expectation we have $E(Y_1) - E(Y_0) = E(Y_1 - Y_0)$

$Q_\tau (Y_1 - Y_0) \neq Q_\tau (Y_1) - Q_\tau(Y_0)$ (nonlinear operation)

We want to learn: do scholarships increase earning?

Discussion 6.4

A. $\beta_0(u_2) > \beta_0(u_1)$

B. $\beta_1 (u_2) >< \beta_1 (u_1)$

C. $\beta_0 (u_2) + \beta_1 (u_2) > \beta_0(u_1) + \beta_1(u_1)$

Wage if $X_1, U = u_2$ \> Wage if $X=1, U = u_1$

<br>

$$
Y = \beta_0 (U) + \beta_1(U) X
$$

individual = $(Y_i, X_i,U_i)$

if monotonicity + $U \perp\!\!\!\perp X$ + norm $U \sim Unif(0,1)$ to have $U = 0.5 = \text{median}$

$Q_\tau(Y|X=x) = \beta_0(\tau)+ \beta_1 (\tau) X$

but we don't usually believe $U \perp X$ is true, now we try to weaken this assumption

We can technically think of the smoothness in terms of bias-variance trade off. However, it's better to think of

-   smoothness in `ivgrsee` can help with computation problem + statistically problem

$$
Y_1 SD_1 Y_2 
$$

means that $Y_1$ **first-order stochastic dominance** $Y_2$, which means that

$$
E[u(Y_1)] \ge E[u(Y_2)] \forall u \in \mathcal{U}
$$

where $\mathcal{U}$ is all possible utility function (weakly increasing)

equivalently $F_1(.) \le F_2(.)$ (cdf 1 less than cdf 2) (this is the easiest to deal with). Because with lower function, when we have a particular value of $\tau$, and we draw it on $F_1,F_2$, then $Q_2(\tau) < Q_1(\tau)$

equivalently, $Q_1(.) \ge Q_2(.)$ (quantile function 1 less than quantile function 2)
