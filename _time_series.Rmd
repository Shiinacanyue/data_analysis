# Time Series

Download data from [FRED](https://fred.stlouisfed.org/series/GDP)

```{r}
library(tidyverse)
library(tsibble)


gdp = read.csv(file.path(getwd(),"data","GDP.csv")) %>% 
    
    mutate(DATE = as.Date(DATE)) %>% 
    # transform the dataframe into tsibble format
    as_tsibble(index = DATE, regular = F) %>% # because every quarter not on the same date, regular is set to FALSE
    index_by(qtr = ~ yearquarter(.))
```

Alternatively, `fable` is another package that can handle time-series data

## Data Assumptions

### Sample Autocorrelation Function (ACF)

Check for autocorrelation

```{r}
acf(gdp$GDP, plot=TRUE, main="Autocorrelation")

# partial autocorrelation
pacf(gdp$GDP, plot=TRUE, main="Autocorrelation")
```

From the PACF we see that most of the auto-correlation can be attributed to the first lag

### Stationarity

When p

#### Augmented Dickey-Fuller (ADF) test

#### Kwiatkowski--Phillips--Schmidt--Shin (KPSS) test

## Autoregressive (AR) {#autoregressive-ar}

$$
Y_t = a + \sum_{i=1}^p \beta_i Y_{t-i} + \epsilon_t
$$

where

-   $p$ is the order of the auto regression

```{r}
#load data
# gdp = read.csv("https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv")


ar_gdp = ar.ols(gdp$GDP) # lag term based on min AIC
ar_gdp
```

## Autoregressive Conditional Heteroskedasticity (ARCH)

From the [Autoregressive (AR)](#autoregressive-ar) model, we relax the assumption of homoscedasticity in the variance of the error term, where it is now a function of the actual sizes of the previous time periods' errors.

This assumption is plausible when the error variance in a time series follows an [Autoregressive (AR)](#autoregressive-ar) model

The general form of an ARCH(q) process is

$$
y_t = \alpha_0 + \sum_{i=1}^q a_i y_{t-i} + \epsilon_t
$$

where

-   $\epsilon_t = \sigma_t z_t$

    -   $z_t$ = stochastic white noise process

    -   $\sigma_t$ = time-dependent standard deviation

$$
\sigma_t^2 = \alpha_0 + \sum_{i=1}^q \alpha_i \epsilon_{t-i}^2
$$

```{r}
library(fGarch)
garchFit( ~ garch(1, 0), data = gdp$GDP, trace = F)
```

## Moving Average (MA) {#moving-average-ma}

Used for processes with serial correlation

A moving average process MA(q) is

$$
y_t = \mu + \sum_{i =0}^q \beta_i \epsilon_{t-i}
$$

where $\epsilon_t \sim N(0, \sigma^2_\epsilon)$

In words, output is a linear combination of current and past values of a stochastic term ($\epsilon_t$)

Properties of MA(1) process

$$
y_t = \mu + \epsilon_t + \beta_1 \epsilon_{t-1}
$$

-   Constant mean: $E(y_t) = E(\mu + \epsilon_t + \beta_1 \epsilon_{t-1}) = \mu$

-   Constant variance $Var(y_t) = Var(\mu + \epsilon_t + \beta_1 \epsilon_{t-1}) = (1 +\beta_1^2) \times \sigma^2_\epsilon$

-   The covariance between $y_t$ and $y_{t-q}$ decreases as $q \to \infty$

```{r}
set.seed(1)
# simulate 
DT = arima.sim(n = 1000, model = list(ma = c(0.1, 0.3, 0.5)))

plot(DT, ylab = "Value")


# Autocorrelation Function
acf(DT, type = "covariance")


set.seed(123)
DT = arima.sim(n = 1000, model = list(ma = c(0.1, 0.3, 0.5)))

# estimate MA(3) model
arima(x = DT, order = c(0,0,3))


#We can also estimate a MA(7) model and see that the ma4, ma5, ma6, and ma7 are close to 0 and insignificant.
arima(x = DT, order = c(0,0,7))


library(fable)
library(tsibble)
library(dplyr)

# convert to tsibble
DT = DT %>%
  as_tsibble()


# Automatic estimation
DT %>%
  as_tsibble() %>%
  model(arima = ARIMA(value)) %>%
  report()


# Manual estimation
DT %>%
  as_tsibble() %>%
  model(arima = ARIMA(value ~ 0 + pdq(0, 0, 3))) %>%
  report()

```

## Autoregressive Moving Average (ARMA)

-   is the combination of [Autoregressive (AR)](#autoregressive-ar) and [Moving Average (MA)](#moving-average-ma)

An ARMA(p,q) has the form of

$$
y_t = c + \sum_{i=1}^p \beta_i y_{t-1} + \sum_{j = 1}^q \theta_j \epsilon_{t-j} + \epsilon_t
$$

Since ARMA models cannot be estimated with OLS due to the MA part, MLE typically is the first choice to estimate these models.

-   ARMA models can only handle univariate time series, for multiple time series, vector autoregressive is more appropriate

-   ARMA is only appropriate for stationary time series. For non-stationary time series, [Autoregressive Integrated Moving Average (ARIMA)](#autoregressive-integrated-moving-average-arima) is the go-to models

```{r}
library(tidyverse)
library(tsibble)


gdp = read.csv(file.path(getwd(),"data","GDP.csv")) %>% 
    
    mutate(DATE = as.Date(DATE)) %>% 
    # transform the dataframe into tsibble format
    as_tsibble(index = DATE, regular = F) %>% # because every quarter not on the same date, regular is set to FALSE
    index_by(qtr = ~ yearquarter(.)) %>% 
    
    # get the first difference of log gdp 
    mutate(lgdp = log(GDP)) %>% 
    mutate(ldiffgdp = difference(lgdp, lag = 1, differences = 1))

# Estiamte ARMA
arima(gdp$lgdp[2:292], order=c(3,0,1)) # exclude the first observation (due to differencing)


```

## Autoregressive Integrated Moving Average (ARIMA) {#autoregressive-integrated-moving-average-arima}

-   is the combination of [Autoregressive (AR)](#autoregressive-ar) and [Moving Average (MA)](#moving-average-ma) and unit roots

```{r}
# Estiamte ARIMA
arima(gdp$lgdp[2:292]) # exclude the first observation (due to differencing)

# to auto-select p, q, and d
library(forecast)
auto_mod <- auto.arima(gdp$lgdp[2:292])
```

## Generalized Autoregressive Conditional Heterskedasticity (GARCH)

-   Assume when variance error is serially auto-correlated (i.e., following an auto-regressive moving average process)

-   Usually used to predict the volatility of returns, especially for assets with clustered period of volatility

```{r, warning=FALSE}
library(fGarch)


garch_mod <- garchFit(GDP ~garch(1,1), data = gdp, trace = F) 
summary(garch_mod)
```

## Autoregressive Lag (ADL)

-   Adding other variables and lags of the variable of interest, we have the autoregressive model

ADL(p,q) model is an autoregressive distributed lag model with p lags of $Y_t$ and q lags of $X_t$

$$
Y_t = \beta_0 + \sum_{i = 1}^p \beta_i Y_{t-i} + \sum_{j = 1}^q \delta_j X_{t-q} + u_t
$$

In words, the time series $Y_t$ is a linear function of $p$ of its lagged values and $q$ lags of $X_t$ time series

Properties:

-   $E(u_t | Y_{t-1}, \dots, X_{t-1}, \dots) = 0$

Simulate ADL data (simulation from the [LOST book](https://lost-stats.github.io/Time_Series/Granger_Causality.html))

```{r}
# set seed
set.seed(1)

# Simulate error
n     <- 100 # Sample size
rho   <- 0.5 # Correlation between Y errors
coe   <- 1.2 # Coefficient of X in model Y
alpha <- 0.5 # Intercept of the model Y

# Function to create the error of Y
ARsim2 <- function(rho, first, serieslength, distribution) {
    if (distribution == "runif") {
        a <- runif(serieslength, min = 0, max = 1)
    }
    else {
        a <- rnorm(serieslength, 0, 1)
    }
    Y <- first
    for (i in (length(rho) + 1):serieslength) {
        Y[i] <- rho * Y[i - 1] + (sqrt(1 - (rho ^ 2))) * a[i]
    }
    return(Y)
}

# Error for Y model
error <- ARsim2(rho, c(0, 0), n, "rnorm")

# times series X (simulation)
X <- arima.sim(list(order = c(1, 0, 0), ar = c(0.2)), n)

# times series Y (simulation)
Y <- NULL
for (i in 2:n) {
    Y[i] <- alpha + (coe * X[i - 1]) + error[i]
}

data <- as.data.frame(cbind(1:n,X,as.ts(Y)))
colnames(data) <- c("time", "X","Y")
```

Plot data

```{r}
graphdata <- data[2:n, ] %>%
    pivot_longer(cols = -c(time),
                 names_to = "variable",
                 values_to = "value")

ggplot(graphdata, aes(x = time, y = value, group = variable)) +
    geom_line(aes(color = variable), size = 0.7) +
    scale_color_manual(values = c("#00AFBB", "#E7B800")) +
    theme_minimal() +
    labs(title = "Simulated ADL models") +
    theme(text = element_text(size = 15))
```

Stationarity

```{r}
library(tseries)

# Augmented Dickeyâ€“Fuller Test
adf.test(X, k=3)
adf.test(na.omit(Y), k=3)
```

If we have stationarity time series, we do not need to do any transformation (e.g., differencing or log). In this example, we reject the null hypothesis (i.e., non-stationary time series), and conclude that we do have stationary time series.

### Granger causality test

-   test whether the signal of the the first variable is a good predictor of the other variable.

    -   For example, the variable $X$ Granger-cause $Y$ when $Y$ can be predicted from the lag of $X$ and $Y$ than from the lag of $Y$ only [@pierce1979r].

-   For non-parametric Granger causality test, refer to [@candelon2016]

-   More extensions: [@diks2020]

Check [Stationarity] assumptions for the two time series (consider using log or differences if you reject this assumption)

Estimate these two models

$$
y_t = \alpha + \sum_{j = 1}^p \beta_j y_{t-j} + \sum_{j=1}^r \theta_j x_{t-j} + \epsilon_t \\
x_t = \alpha^* + \sum_{j=1}^p \beta_j^* y_{t-j} + \sum_{j=1}^r \theta_j^* x_{t-j} + \epsilon_t
$$

assuming $E(\epsilon_t|\mathcal{F}_{t-1})=0$ where $\mathcal{F}_{t-1}$ contains information of $x$ and $y$ up to time $t-1$

Considering the first case (whether lags of $X$ is predictive of $Y$)

F-test to determine significance

$$
H_0: \theta_j = 0 \\
H_1: \theta_j \neq 0
$$

In words,

-   H0: The lags of $x$ is not predictive of $y$ above the lags of $y$

-   H1: At least one lag of $x$ provides additional info

If one rejects the null hypothesis, we say $x_t$ helps predict $y_t$ (i.e., $X$ Granger-causes $Y$)

```{r}
library(lmtest)

# another dataset that can be used is 
# data("ChickEgg")

# Y ~ X
grangertest(Y ~ X, order = 2, data = data)

# X ~ Y
grangertest(X ~ Y, order = 2, data = data)
```

$X$ Granger-causes $Y$, but $Y$ does not Granger-cause $X$

Notes:

-   Model 1 is the unrestricted model with the Granger-causal terms

-   Model 2 is the restricted model without the Granger-causal terms

To choose the number of lags, we have to make tradeoff between bias and power.

-   Too few lags, the test will be biased due to lack of residual autocorrelation

-   To many lags, we might reject the null hypothesis due to spurious correlation

Conventionally, we can vary the number of lags and compare AIC and BIC for each lag length and pick the one with lowest AIC or BIC.

Due to convenience reason, `grangertest` in R use the number of lags for both $X$ and $Y$. However, there is no theoretical basis, and we should compare different combinations for different number of lags for $X$ and $Y$.

Since this test is model dependent, omitted variables bias is a big concern. Hence, one should regress on a matrix of $\mathbf{X}$ instead of using only pair-wise as in `grangertest`

Hence, `bruceR` [package](https://psychbruce.github.io/bruceR/) might be better

```{r}
library(bruceR)

# multivariate
library(vars)
data(Canada)
VARselect(Canada)

# choose 3 from VARselect
vm = VAR(Canada, p=3)

granger_causality(vm)

# bivariate (similar to grangertest)
# granger_test()
```

#### Non-linear Granger

Non-linear Granger causality test using artificial neural networks

Model 1: using the target time series (ts1) to model an artificial neural network

Model: 2: using the two time series (ts1, ts2) to model another artificial neural network

H0: The second time series does not predict the first time series

see [@hmamouche2020] for details

```{r}
library(NlinTS)

library (timeSeries) # to extract time series
library (NlinTS)
data = LPP2005REC
model = nlin_causality.test (data[, 1], data[, 2], 2, c(2), c(4), 50, 0.01, "sgd", 30, TRUE, 5)
model$summary()
```

We cannot reject the null hypothesis, which means that the second times series does not predict the first one.

#### Non-parametric Granger

Additionally, non-parametric wavelet Granger causality is suitable for multiple time horizons

See[@li2021] for an example in finance

## Unobserved Component Models

The UCM is proposed by [@harvey1990] to perform decomposition on a time series into various components:

-   Trend: natural tendency of a series to increase or decrease overtime. UCM can model trend either by

    -   a random walk assuming the trend remains roughly constant over the observed window

    -   being a locally linear trend assuming the trend either has an upward or downward slope

-   Seasonality: consistent seasonal parttern

-   Cycle: rise and fall that is not of fixed period

-   Regression effects due to predictor series

$$
Y_t = \mu_t + \gamma_t + \phi_t + \sum_{i = 1}^m \beta_i X_{it} + \epsilon_t
$$

where

-   $\mu_t$ = time varying mean (level/trend)

-   $\gamma_t$ = seasonality (periodic component)

-   $\phi_t$ = cycle

-   $\sum_{i=1}^m \beta_i X_{it}$ = effect of predictors (i.e., contribution of predictors with fixed or time-varying regression coefficients)

Example by [the package's authors](https://cran.r-project.org/web/packages/rucm/vignettes/rucm_vignettes.html)

```{r}
library(rucm)

# univariate model
modelNile <-
    ucm(
        formula = Nile ~ 0,
        data = Nile,
        level = TRUE
    )
modelNile

plot(Nile, ylab = "Flow of Nile")
lines(modelNile$s.level, col = "blue")
legend(
    "topright",
    legend = c("Observed flow", "S_level"),
    col = c("black", "blue"),
    lty = 1
)

# for prediction
predict(modelNile$model, n.ahead = 12)
```

## State Space Models

Readings

-   [@petris2011]
