1.  Causal effects + identification
2.  Randomization
3.  Testing
4.  Estimation

$$
E(Y^1 - Y^0) = ATE
$$

What is $E$ over?

1.  Sample: $\frac{1}{n} \sum_{i=1}^n (Y_i^1 - Y_i^0) = SATE$
2.  Population: $Y_i^a \sim^{iid} P^*_a$

Philosophy

1.  Target parameter
2.  Identification

Definition

Identification is

$\Psi = \Psi(p)$ is identified if $\Psi(P^*) \neq\Psi(Q^*) \to P \neq Q \forall P^*, Q^* \in \mathcal{P}$

Example:

We observe $(A,Y) \sim^{iid} P$

$$
A = 
\begin{cases}
1 \to Y = Y^1 \\
0 \to Y = Y^0 
\end{cases}
$$

$ATE = E(Y^1 - Y^0)$ is not identified

Randomization

1.  Bernoulli experiment: Assign $A \sim Ber(0.5)$

$$
(A,Y) \sim P
$$

Assume

1.  $A = a \to Y = Y^a$ (consistency - SUTVA)
2.  $A \perp Y^a$ for each $a$ (i.e., treatment decision is independent of the potential outcome) (randomization, no underlying confounders, exchangeability),
    1.  which is different from $A \perp Y$ (which is generally not true)

Then $E(Y^a) = E(Y |A =a)$

Proof

$$
\begin{aligned}
E(Y|A = a) &= E(Y^a|A =a) & \text{consistency}\\
&= E(Y^a) & \text{exchangeability}
\end{aligned}
$$

Define: an experiment is a study where treatment assignment is

1.  Probabilistic (random)
2.  Known

Completely randomized

Testing

Fishers' sharp null

$H_0: Y_i^1 = Y_i^0 \forall i$

(strong null because it has to be true for every single person)

We need

1.  Test statistic
2.  Its distribution under $H_0$

Difference in means

$$
\hat{\Psi} = \frac{1}{n} \sum_{A_i = 1} Y_i - \sum_{A_i = 0} Y_i \\
= \frac{P_n(AY)}{P_n(A)} - \frac{P_n((1-A)Y)}{P_n(1-A)} 
$$

Under $H_0: Y_i = Y_i^1 = Y_i^0$

$$
P_{H_0}(T_n \ge t) = P_{H_0} (T_n(A^n_i, y^n) \ge t) \\
= \sum_{a^n} (T_n(a^n, y^n) \ge t) P(A^n = a^n)
$$

Neyman approach

Sample

$$
\Psi_n = \frac{1}{n}\sum_{i=1}(y_i^1 - y_i^0)
$$

Population

$$
E(Y^1  - Y^0)
$$
