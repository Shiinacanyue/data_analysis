# Identification

$$
\Theta = g(\mathcal{F}(.))
$$

$\Theta$ is said to be identified if it **exist** and **unique** if $F \in \mathcal{F}$

-   $E(Y|X= x)$ is identified if $E(Y) < \infty$

-   Linear Projection Coefficient is identified: $L(Y|1, X)$ is identified as long as $E(X'X)$ is non-singular.

Most of the time: $\Theta$ = some function of observed moment random variables

`=` represents both existence and uniqueness

Examples:

Example 1: Linear model

$$
Y= \beta_0 + X \beta_1 + U
$$

Suppose identification assumption (1) is $E(u |X) = 0$, which means $E(Y|X) = \beta + X \beta_1$

then we can identify

$$
\beta_0 = E(Y|X =0) 
$$

and

$$
\begin{aligned}
Cov(Y,X) &= Cov( \beta_0, X) + Cov(X \beta_1, X) + Cov( u, X) \\
&= 0 + Var(X) \beta_1 + 0 \\
\beta_1&= \frac{Cov(Y,X)}{Var(X)} && \text{additional assumption } Var(X) \neq 0
\end{aligned}
$$

These are called necessaaray conditional for identifiaacation.

The assumption (2) of $Var(X) \neq 0$ is said that as long as X is not degenerate.

Example 2: Random Coefficient Model

$$
Y = U_0 + U_1 X
$$

Both $U_0, U_1$ are unobserved random variables

Identification assumptions

$$
\beta_0 = E(u_0) \\
\beta_1 = E(u_1)
$$

then the random coefficient model becomes

$$
Y = \beta_0 + U_0 - \beta + (\beta_1 + U_1 - \beta_1) X \\
Y = \beta+ \beta_1 X + U_0 - \beta_0 + (U_1 - \beta_1) X \\
$$

$$
Y = \beta_0 + \beta_1 X + V
$$

a\. $E(V |X) = 0$

$$
E(U_0 - \beta_0 +(U_1 -\beta_1) X |X) = 0
$$

1.  To have $E(U_0- \beta_0|X) = 0$, remember$\beta_0 = E(U_0))$

Then we need $E(U_0 |X) = E(U_0) = \beta_0$

2.  To have $E((U_1 - \beta_1) X|X) =0$

then $E(U_1 - \beta_1|X)X = 0$, which will result inn the exact condition as the one above

b\. $Var(X) \neq 0$

Alternatively,

$$
E(V) = 0 \\
Var(X) \neq 0 \\
Cov (X,V) =0 
$$

For the first condition I need

$$
E(V) = E(U_0  - \beta_0 + (U_1 - \beta_1) X) = 0 \\
= E(U_0)  - \beta_0 + E((U_1 - \beta_1)X) \\
= 0 + E((U_1 - \beta_1)X) \\
= 0 + E((U_1 - E(U_1))X) 
$$

we also need $Cov(U_1,X) = 0$ to have the second term to be 0

For the second condition I need

$$
Cov(X,V) = Cov(X,U_0) + Cov(X,(U_1 - \beta_1) X)
$$

we can safely/lightly assume that $Cov(X,U_0) = 0$

while the second term $Cov(X, (U_1 - \beta_1) X)$ (what video again for second lecture)

Example 3: Conditionnal Average Treatment Effects

$$
Y = h(X_1, X_2, U)
$$

$X_1 \in \{0,1\}$

$TE = h(1, X_2, U) - h(0, X_2, U)$

CATE

$$
CATE(X_2) = E(TE |X_2 = x_2) = E(h(1,X_2, U) - h(0,X_2,U) |X_2 \\
= \int h(1, X_2, U) - h(0, X_2, U) f_{u|X_2}u|X_2)du
$$

where $E(h(1,X_2, U)) = E(Y|X_1 =1, X_2 = 2, U =u)$

<br>

## Level of uncorrelatedness

| Level     | Assumption       | Implication                     |
|-----------|------------------|---------------------------------|
| Weakest   | $Cov(X,U) =0$    | $E(XU) = E(X) E(U)$             |
|           | $E(U|X) = EE(U)$ | $E(g(X)U) = E(g(X)) E(U)$       |
| Strongest | $U\perp X$       | $E(g(X) h(U)) = E(g(X))E(h(U))$ |

# Conditional Expectation

Economic questions concern the Conditional Expectation Function (CEF)

$$
E(Y|W,C)
$$

where

-   $Y$ = outcome

-   $W$ - variable of interest

-   $C$ = control variables (observed or unobserved)

Challenges with modelling/estimating the conditional expectation function

-   Difficult to determine how to model conditional expectation function (structural and causal)

    -   Linear projection

-   Not all controls are observed

    -   Endogeneity solved by IV or control function method

-   $Y$ and/or $W$ may not be well-measured

    -   Measurement error

Why only interest in the CEF ? Why not the entire conditional distribution?

$$
F_{Y|W,C} (Y|W,C)
$$

-   it's a lot more harder with more assumptions (not a lot of internal validity)

<!-- -->

    -   assumptions can be motivated by structural models/optimization problems or by distribution of the outcome variables

-   CEF is the most common but only a single summary of the conditional relationship (conditional quantile, etc. )

Examples

$$
\ln(wage) = \beta_0 + \beta_1 eeduc + \beta_2 exp + \beta_3 married + U
$$

where $U$ is the error term that contains all unobserved factors that affect the wage offer

-   What if I say the above equation is a structural model?

    -   It will be based on some utility maximization.

    -   My concern: I have to make a lot of assumptions, restriction to derive my structural become this equation.

-   What if I say the above equation is a conditional expectation function, what concerns do I have?

    -   Other things going on under $U$ that $E(U |X) \neq 0$

-   What if I say the above equation is a linear projection, I don't have any concern (but will more like descriptive)

Population of Interest

When we specify a conditional expatiation, we should also be specific to the population fo interest

$$
EE(Y|W, C_1, i \in \mathcal{I}) = E(Y|W,C)
$$

where $i \in \mathcal{I} = C_2$

A poor population of interest is where in your population you have no variation.

<br>

Modeling a Conditional Expectation

$$
E(Y |X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2
$$

This model is

-   Linear in Variables: Function is la linear combination of $(1,X_1,X_2)$

    -   Violation: $\beta_0 + \beta_1 X_1 + \beta_2 X^2_2$

    -   Violation: $\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3X_1X_2$

-   Linear in Parameters: Function is linear combination of $(\beta_0, \beta_1, \beta)2$

    -   Violation: $\beta_0 + X_1^\gamma + \beta_2 X_2$

-   Parametric: finite number of unknown parameters (i.e., not an unknown function)

    -   Violation: $\beta_0 + \beta_1 X_1 + g(X_2)$

<br>

-   **Parametric**: finite number of unknown parameters (i.e., not an unknown function) where we assume there is a defined relationship between X and $Y$ that is known

-   **Nonparametric**: a defined "relationship" between $X$ and $Y$ that is of some interest, is weakly established as an unknown function (infinite dimension of an unknown parameter)

-   **Semi-parametric**

    -   Finite dimensional parameter of interest

    -   A Infinite dimensional nuisance parameter

Notes:

-   Plug-in estimator (not semi parametric but often confused): non-parametric first stage, parametric second stage

-   Modeling assumptions entirely determine what type of model it is

    -   Same model can be understood as all 3 types: $Y= \mathbf{X} \beta + U$

<br>

Interpreting a Conditional Expectation

If $Y$ and $\mathbf{X}$ were related in a deterministic fashion:

$$
Y = g(\mathbf{X})
$$

then we'd be interested in how $X_j$ changes $Y$

$$
\frac{\partial Y}{\partial X_j} = \frac{g(\mathbf{X})}{\partial X_j}
$$

But we're in a stochastic setting where $Y$ is not perfectly detemrined by $\mathbf{X}$ in that there's some unobserved factors affecting $Y$

Instead, we are interested in the **partial effect** of $X_j$: marginal change in the conditional expectation, $E(Y | \mathbf{X})$ in response to a marginal change in $X_j$ holding all other observed variables fixed $\frac{\partial E(Y|\mathbf{X})}{\partial X_j}$

<br>

## Deriving Continuous Partial Effects

Example

Ex: Deriving partial effects with respect to $X_1$ and $X_2$ (continuous):

$$
E(Y | X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X^2_2
$$

For partial effect with respect to $X_1$

$$
\frac{\partial E(Y|\mathbf{X})}{\partial X_1} = \beta_1 = \text{constant}
$$

when a coefficient is consonant, it's a linear effect (i.e., average changes will be the same for all individuals)

For partial effect with respect to $X_2$

$$
\frac{\partial E(Y| \mathbf{X})}{\partial X_2} = \beta_2 + 2 \beta_3 X_2 = \text{random variable}
$$

<br>

Example 2

$$
E(Y|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2
$$

Interpret $X_1$

$$
PE_{X_1} = \beta_1 + \beta_3 X_2 = \text{random variable}
$$

where PE = partial effect

Interpret $X_2$

$$
PE_{X_2} = \beta_2 + \beta_3 X_1 = \text{random variable}
$$

Example 3

$$
E(Y|X_1, X_2) = \exp(\beta_0 + \beta_1 \ln (X_1 ) + \beta_2 X_2)
$$

Interpret $X_1$

$$
PE_{X_1} = \frac{\beta_1}{X_1} \times \exp(.) = \frac{\beta_1}{X_1} \times E(Y |X_1, X_2)
$$

Interpret $X_2$

$$
PE_{X_2} = \beta_2 \exp(.) = \beta_2 \times E(Y|X_1,X_2) = \text{random variable}
$$

Deriving Discrete Partial (i.e., treatment) effects

We don't typically use the word partial for discrete variables. Hence, the word treatment.

Deriving treatment effects with respect to $X_1$ and $X_2$ (discrete)

$$
E(Y|X_1, X_2) = \beta_) = \beta_1 X_1 + \beta_2 X_2  + \beta_3 X_2^2
$$

where $X_1$ is binary and $X_2$ takes on discrete positive integers

1.  For partial effect with respect to $X_1$

$$
E(Y |X_1 = 1, X_2) - E(Y |X_1 =0, X_2) = \beta_1
$$

2.  For partial effect with respect to $X_2$

$$
\begin{aligned}
&E(Y|X_1 , X_2 = k + 1) - E(Y |X_1, X_2 = k) \\
&= \beta_2 + \beta_3 ((k+1)^2 - k^2) \\
&=\beta_2 + \beta_3 (2k+1) \\
&\approx \beta_2 + 2 \beta_3 X_2 & \text{from the continous case}
\end{aligned}
$$

<br>

Are Partial Effects Causal Effects?

-   When we're interested in the causal effect of $W$ on $Y$

$$
E(Y|W, \mathbf{C})
$$

-   We decide (as economists) what is needed in $\mathbf{C}$ for this conditional expectation to represent a causal relationship

-   Does $E(Y|W, \mathbf{C}) = E(Y|\mathbf{X})$?

    -   If no, then the partial effect $E(Y|X)$ is not a causal effect (but still a statistical relationship)

    -   If yes, we would interpret the partial effect as the causal effect

<br>

## Elasticities and Semi-elasticities

Sometimes we're interested in a function of the partial effect

-   Scalar multiple; $\frac{\partial E(Y |\mathbf{X})}{\partial X_j}\times c$

-   Derivative for rate of change: $\frac{\partial^2 E(Y|\mathbf{X})}{\partial X_j^2}$

-   Elasticity: $\frac{\partial E(Y|\mathbf{X})}{\partial X_j} \times \frac{X_j}{E(Y|\mathbf{X})}$

    -   if $E(Y| \mathbf{X}) >0$ and $X_j >0$ then the elasticity is the same as

$$
\frac{\partial \ln (E(Y|\mathbf{X}))}{\partial \ln (X_j)} = \frac{d \ln(E(Y|\mathbf{X})) / dX}{d \ln (X) /dx}
$$

Example: Elasticity for $E(Y|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X^2_2$ with respect to $X_1$

$$
\frac{\partial \ln (E(Y| \mathbf{X})}{\partial \ln (X_j)}= \frac{d E(Y|\mathbf{X})}{d X_j}\times \frac{X_j}{E(Y|X_j)}= \frac{\beta_1 X_1}{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_2^2}
$$

$$
\begin{aligned}
E(Y|\mathbf{X}) &= \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_2^2 \\
\ln(E(Y|\mathbf{X})) &= \ln(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_2^2 ) \\
\frac{\partial \ln(E(Y|\mathbf{X}))}{\partial \ln(X_1)} &= \ln(\beta_0 + \beta_1 \exp(\ln(X_1)) + \beta_2 X_2 + \beta_3 X_2^2 ) \\
&= \beta_1 \exp(\ln(X)) \frac{1}{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_2^2 } \\
&= \frac{\beta_1 X_1}{\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_2^2 }
\end{aligned}
$$

Examples:

$$
E(Y|X_1, X_2) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2
$$

$$
\frac{\partial E(Y|\mathbf{X})}{\partial X_j} \times \frac{X_j}{E(Y|\mathbf{X})} = (\beta_1 + \beta_3 X_2) \times \frac{X_1}{E(Y|\mathbf{X})}
$$

which is hard to interpret

Example:

$$
E(Y|X_1, X_2) = \exp(\beta_0 + \beta_1\ln(X_1) + \beta_2 X_2)
$$

$$
\ln(E(Y|\mathbf{X}) = \beta_0 + \beta_1 \ln(X_1) + \beta_2 X_2 \\
\frac{\partial \ln (E(Y|\mathbf{X}))}{\partial \ln (X_j)} = \beta_1
$$

<br>

How do we compare between the true definition of elasticity and the one commonly used in models?

The true model

$$
\frac{\partial \ln(E(Y |\mathbf{X}))}{\partial \ln(X_j)} 
$$

The log-log model

$$
\frac{\partial E(\ln(Y) | \mathbf{X})}{\partial \ln (X_j)} 
$$

-   $E(\ln(Y) |\mathbf{X})$ is useful to model because it's relatively easy to transform the outcome variable compared to transforming the entire conditional expectation function

-   Jensen's inequality ($E(g(X)) \neq g(E(X))$: natural log of an expectation is NOT the same as the expectation of a log, but we are interested in their derivatives (the log-log would be a good approximate of the true model)

$$
E(\ln(X)) < \ln (E(X))
$$

Suppose

$$
\ln(Y) = \beta_0 + \beta_1 \ln(X_1) + \beta_2 X_2 + U
$$

equivalently,

$$
\begin{aligned}
Y &= \exp(\beta_0 + \beta_1 \ln(X_1) + \beta_2 X_2 + U) \\
&= \exp(\beta_0 + \beta_1 \ln(X_1) + \beta_2 X_2) \exp(U) \\
E(Y|X) &= E(\exp(\beta_0 + \beta_1 \ln(X_1) + \beta_2 X_2) \exp(U)|X) \\
&= \exp(\beta_0 + \beta_1 \ln(X_1) + \beta_2 X_2) E(\exp(U)|X)
\end{aligned}
$$

if $U$ is independent of $X_1, X_2$ then

-   log-log model: $E(\ln(Y) | \mathbf{X}) = \beta_0 + \beta_ 1 \ln(X_1) + \beta_2 X_2$

-   true model: $E(Y | \mathbf{X}) = \delta \exp(\beta_0 + \beta_1 \ln(X_1) + \beta_2 X_2)$ where $\delta = E(\exp(U))$ (by independence)

-   $\ln(E(Y| \mathbf{X})) = \ln(\delta) + \beta_0 + \beta_1 \ln(X_1) + \beta_2 X_2$

Then we can show

$$
\frac{\partial \ln(E(Y |\mathbf{X}))}{\partial \ln(X_j)}  = \frac{\partial E(\ln(Y) | \mathbf{X})}{\partial \ln (X_j)} 
$$

-   Generally, we take log transformation of the outcome/independent variables to model elasticities directly

-   The interpretation as elasticities are approximate

-   For the following equation,

$$
\ln(Y) = \beta_0 + \beta_1 \ln(X_1) + \beta_2 X_2 + U
$$

$100 \times \beta_2$ is the semielasticity, or is percent in $Y$ as a response to a unit change in $X_2$

<br>

Linear Projection

-   Most of the examples have imposed some sort of linearity assumption within the CEF that may not be true

-   Instead we will sue Linear Projections to obtain estimable equations that relax the linearity assumption on the CEF

<br>

## Linear Projection

Define $\mathbf{X} = (X_1, \dots, X_k)$ as a $1 \times k$ vector where the $k \times k$ variance-covariance matrix of $\mathbf{X}$ is a positive definite

Then the the Linear Projection **exists** and is **unique**

$$
L(Y |1, \mathbf{X}) = \gamma_0 + \gamma_1 X_1 + \dots + \gamma_k X_k
$$

where by definition the **Linear Projection Coefficients** are

-   $\mathbf{\gamma} = [Var(\mathbf{X}]^{-1} Cov(\mathbf{X},Y)$

-   $\gamma_0 = E(y) - E(\mathbf{X} \gamma)$

identified (and estimable) by population distribution

**Equivalently**,

$$
Y = \gamma_0 + \gamma_1 X_1 + \dots + \gamma_k X_k + U
$$

with the following definitional properties:

-   $E(U) = 0$

-   $Cov(X_j, U) = 0 \forall j = 1, \dots, k$

which does not imply $U$ is independent or even mean independent of $X$

### Properties of the Linear Projection

-   The linear projection coefficient is the **minimum mean squares linear predictor** because it solves

$$
\min_{b_0, \mathbf{b}} E[(Y - b_0 - \mathbf{Xb})^2]
$$

-   The conditional expectation function is **the minimum mean square predictor** because it solves

$$
\min_{g(.)} E[(Y - g(\mathbf{X}))^2]
$$

-   If the conditional expectation is linear in $\mathbf{X}$, then the linear projection is the linear projection is the conditional expectation

-   Even if the conditional expectation is not linear, the linear projection is the **best mean square linear approximation** to the conditional expectation function

Proof:

1.  the best mean suqare lienar apporixmaiton to the conditonal expectation fucniton solves the following:

$$
\min_{b_0 , \mathbf{b}} E[(E(Y |\mathbf{X}) - b_0 - \mathbf{Xb})^2]
$$

2.  We know the linear projection coefficient is the minimum mean squares linear predictor and therefore solves

$$
\min_{b_0, \mathbf{b}}E[(Y - b_0 - \mathbf{Xb})^2]
$$

3.  For any $b_0, \mathbf{b}$

$$
\begin{aligned}
E[(Y - b_0 - \mathbf{Xb})^2] & = E[(Y - E(Y | \mathbf{X}) + E(Y |\mathbf{X}) - b_0 - \mathbf{Xb})^2] \\
&= E[(Y - E(Y |\mathbf{X}))^2] + E[(E(Y | \mathbf{X}) - b_0 - \mathbf{Xb})^2] \\
&+ 2E[(Y - E(Y | \mathbf{X}))(E(Y|\mathbf{X}) - b_0 - \mathbf{Xb}))] \\
\end{aligned}
$$

where the last term, by law of iterated expectations (LIE)

$$
\begin{aligned}
&2E[(Y - E(Y | \mathbf{X}))(E(Y |\mathbf{X}) - b_0 - \mathbf{Xb})] \\
&= 2E[E(( Y - E(Y |\mathbf{X}))(E(Y |\mathbf{X})- b_0 - \mathbf{Xb}) | \mathbf{X})] && \text{LIE}\\
&= 2E[(E(Y|\mathbf{X}) - E(Y |\mathbf{X}))(E(Y |\mathbf{X}) - b_0 - \mathbf{Xb})] \\
&=0
\end{aligned}
$$

4.  Because $E[(Y -E(Y |\mathbf{X}))^2]$ does not depend on $b_0, \mathbf{b}$,

$$
\arg \min_{b_0, \mathbf{b}} E[(Y - b_0 -  \mathbf{Xb})^2] = \arg \min_{b_0, \mathbf{b}} E[(E(Y|\mathbf{X}- b_0 - \mathbf{Xb})^2]
$$

Hence, **the linear projection coefficient is the best mean square linear approximation to the conditional expectation function**.

<br>

# Linear Models

## Identification Assumptions

Assume that the population model is linear in its parameters

$$
Y = \beta_0 + \beta_1 X_1 + \dots, + \beta_k X_k + U
$$

-   $Y, X_1, \dots, X_k$ are observable random scalars

-   $U$ is a random disturbance

-   $\beta_0, \beta_1, \dots, \beta_k$ are parameters that we would like to estimate

Notation

Let the random $1 \times k$ vector $\mathbf{X} = (X_1, \dots, X_k)$ where $X_1 = 1$

The population model is

$$
Y = \mathbf{X \beta} + U
$$

### Assumption 1: Orthogonality

$$
E(\mathbf{X}' U) =0
$$

Estimating the Asymptotic Variance

-   A consistent (and unbiased) estimator of $\mathbf{A} = E(\mathbf{x}_i' \mathbf{x}_i)$

$$
\hat{\mathbf{A}} = n^{-1} \sum_{i=1}^n \mathbf{x}_i' \mathbf{x}_i = \mathbf{X'X}/n
$$

If we could observe $u_i$ a consistent (and unbiased) $\mathbf{B} = E(u^2_i \mathbf{x}_i' \mathbf{x}_i)$

$$
\tilde{\mathbf{B}} = n^{-1} \sum_{i=1}^n u^2_i \mathbf{x}_i' \mathbf{x}_i
$$

-   the asymptotic variance is of the "sandwich form" (does not assume homoskedasticity)

$$
Avar(\hat{\mathbf{\beta}}) = \mathbf{A^{-1} B A^{-1}} /n
$$

-   the estimator is also of sandwich form

$$
\begin{aligned}
\hat{A}
\end{aligned}
$$

# Endogeneity in the Linear Model

If $X_j$ is endogenous if OLS.1 fails,

$$
Cov(X_j, U) \neq 0
$$

-   $\beta_j$ is as an economic concept cannot be satisfied by the linear projection coefficient (which always exists)

-   Similar to the idea that correlation (linear projection coefficient) should not always be interpreted as causation (what we would like $\beta_j$ to represent)

Three sources of endogeneity:

1.  **Omitted variables**: we would like to include a variable as a control by because of data availability

2.  **Measurement Error**

    1.  Would like to evaluate the outcome $Y^*$, but can only observe an imperfect measure $Y$

    2.  Would like to understand the effect of $X^*_k$ but we can only observe an imperfect measure $X_k$

3.  Simultaneity:

    1.  Can be addressed with IV/GMM approach to endogeneity

    2.  relevant for general equilibrium models

        1.  A supply model depends on price which also depends on quantity demanded and a demand model depends on price which also depends on quantity supplied

Ignoring the Omitted Variable

A simplified Structural/Causal Model

$$
Y = \beta_0 + X \beta_1 + Q \gamma + V
$$

-   we assume that this model is a CEF $E(V|X, Q) = 0$

-   We are interested in $\beta_1$ which represents the average effect on $Y$ for a change in $X$ holding $Q$ constant.

-   is the (mean zero) omitted variable

$$
Y = \beta_0 + X \beta_1 + U \\
= \beta_0 + X \beta_1 + Q\gamma + V
$$

Che kc for OLS1 $E(U) = 0, Cov(X, U) =0$ (we worry about the second part)

$$
Cov(X,U) = Cov(X, Q\gamma + V) \\
= Cov(X,Q)\gamma + Cov(X,V)\\
= Cov(X, Q)\gamma
$$

Which does not equal 0 if both the following are true:

-   $X$ is correlated with $Q$

-   $\gamma \neq 0$ (it means $Q$ affects $Y$)

Lienarly project $Q$ onto hte observable covariate,

$$
Q = \delta_0 + X \delta_1 + R
$$

by definition

-   $E(R) = 0$

-   $Cov(X,R) = 0$

-   $\delta_1 = Cov(X,Q)/Var(X)$

Plug into the Structural /Causal model

$$
Y = (\beta_0 + \delta_0 \gamma) + X(\beta_1 + \delta_1 \gamma) + R \gamma + V
$$

where

-   $E(R \gamma + V) = 0$

-   $Cov(X, R \gamma + V) = 0$

OLS1 and OLS2 hold for the following equation

$$
Y = (\beta_0 + \delta_0 \gamma) + X(\beta_1 + \delta_1 \gamma) + (R\gamma + V)
$$

therefore, the OLS estimator is consistent for (the linear projection coefficient)

$$
plim (\hat{\beta}_1) = \beta_1 = \delta_1 \gamma = \beta_1 + \frac{Cov(X,Q)}{Var(X)} \gamma
$$

but is not consistent for what we establishment as the parameter of interest $\beta_1$ (this is the omitted variable inconsistency (bias))

The inconsistency is

$$
plim(\hat{\beta}_1) - \beta_1 = \frac{Cov(X,Q)}{Var(X)} \gamma
$$

The direction depends on the direction of the two components

1.  $Cov(X,Q)$ the direction of the correlation between the observed explanatory variable and the omitted variable
2.  $\gamma$ the effect the omitted variable has on the outcome after controlling for the effect of the observed explanatory variable.

# Instrumental Variable

Consider the structural/causal mdoel with endogeneity

$$
Y = \beta_0 + \beta_1 X + U
$$

but $Cov(X,U) \neq 0$

The identification problem:

-   Variation in $X$ (exogenous) is going to correpodn to variation in $U$ (endogenous) both of which contribute to variation in $Y$

$$
Cov( X, Y) = Var(X) \beta_1 + Cov(X, U)
$$

-   $\beta_1$ is not equal to the linear projection coefficient which is identified

$$
\begin{aligned}
\beta_1 &= \frac{Cov(X,Y)}{Var(X)} -\frac{Cov(X,U)}{Var(X)} \\
&= \text{linear projection} + \text{bias inconsistency (under large sample)}
\end{aligned}
$$

bias can stem from endogeneity (e.g., including sample selection)

The solution to the identification problem is to use new variation through an instrument $Z$

The instrument must satisfy

1.  Relevance (can check) $cov(Z,X) \neq 0$
2.  Exogeneity (based on faith): $cov(Z,U) =0$

Quarter of birth used to a good instrument to find the causal effect of schooling on some economic outcomes (wages, health, etc.), but a study that shows evidence that genetic factors influencing education are not random [@rietveld2016]

Using $Z$ to solve the identification problem

Linearly project $X$ onto the new variation $X$

$$
X = \delta_0 + \theta_Z + R \\ 
= \delta_0 + \text{exogenous} + \text{endogenous}
$$

where $E(R) = 0$ and $Cov(Z, R) = 0$

Because of endogeneity $Cov( X, U) \neq 0$

$$
\begin{aligned}
Cov(X,U) &= Cov(\theta Z + R, U) \\
&= Cov(\theta Z, U) + Cov(R,U) \\
&= 0 + Cov(R,U) \neq0
\end{aligned}
$$

<br>

## IV Estimation of a General Equation

Return to the more general Linear model

$$
Y = \beta_1 + \beta_2 X_2 + ... + \beta_k X_k + U \\
Y = \mathbf{X \beta} + \mathbf{U}
$$

where we will assume there is only one endogenous variable

$$
Cov(X_k, U) \neq 0 \\
Cov( X_j, U) = 0 , j = 2, \dots, k -1
$$

**Note**: there is no such thing as an OLS model or IV model, OLS and IV are estimation strategies for the same linear model

Suppose we have an instrument $Z_k$ for $X_k$ then we create an insturment vector

$$
\mathbf{Z} = (1, \dots, X_{k-1} , Z_k) \\
\mathbf{X} = (1, \dots, X_{k-1}, X_k)
$$

where both $\mathbf{Z,X}$ are $1 \times k$ vectors

Assume

-   Exogeneity: $E(\mathbf{Z}' U) =0$

-   Rank condition (instead of Relevance): $rank (E(Z'X)) = k$ (which means $E(\mathbf{Z}'X)$ is invertible)

### Understanding the Exogeneity Condition

The exogeneity condition can be rewritten in terms of Moment Condition

$$
\begin{aligned}
E(\mathbf{Z}'U) &= 0 \\
E(\mathbf{Z}'(Y - \mathbf{X} \beta)) &=0
\end{aligned}
$$

A moment condition is when you can write the expectation of some function of observable variables and unknown parameters equal to 0

Moment conditions are the basis of Generalize Method of Moments (GMM) estimation

### Understanding the Rank Condition

-   is the matrix version of the relevance condition combined with no multicollienarity

-   Assuming that there is no perfect multicollienarity in $1, X_2, \dots, X_{k-1}$, the rank condition holds iff the linear projection coefficient $\theta$ from

$$
L(X_k | 1, X_2, \dots, X_{k-1}, Z_k) = \delta_1 + \delta_2 X_2 + \dots+ \delta_{k-1}X_{k-1} + \theta Z_k
$$

is not 0

-   After partialling out the effects of $X_2, \dots, X_{k-1}, Z_k$ must be correlated with $X_k$

-   This is a testable condition

Under exogeneity and the rank condition, we can use the instrument to identify $\beta$

$$
\begin{aligned}
Y &= \mathbf{X \beta} + \mathbf{U} \\
\mathbf{Z}' Y &= \mathbf{Z} \mathbf{X \beta}  + \mathbf{Z}' U \\
E(\mathbf{Z}' Y) &= E(\mathbf{Z}'\mathbf{X}) \mathbf{\beta} + E(\mathbf{Z}'U) \\
E(\mathbf{Z}'Y) &= E(\mathbf{Z}'\mathbf{X})\mathbf{\beta} \\
[E(\mathbf{Z}'\mathbf{X})]^{-1} E(\mathbf{Z}' Y) &= \beta
\end{aligned}
$$

$$
\begin{aligned}
\hat{\beta}_{IV} &= (\mathbf{Z}'\mathbf{X}) \mathbf{Z}' \mathbf{Y}\\
&= (\mathbf{Z}'\mathbf{X}) \mathbf{Z}'(\mathbf{X \beta} + U) \\
&= (\mathbf{Z}'\mathbf{X}) \mathbf{Z}'\mathbf{X\beta} + (\mathbf{Z}'\mathbf{X}) \mathbf{Z}' U \\
&= \beta + (\mathbf{Z}'\mathbf{X}) \mathbf{Z}' U
\end{aligned}
$$

For consistency

If my controls are endogenous, I potentially have inconsistency on every $\hat{\beta}_{IV}$
