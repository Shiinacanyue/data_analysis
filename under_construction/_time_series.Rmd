# Time Series

Download data from [FRED](https://fred.stlouisfed.org/series/GDP)

```{r}
library(tidyverse)
library(tsibble)


gdp = read.csv(file.path(getwd(),"data","GDP.csv")) %>% 
    
    mutate(DATE = as.Date(DATE)) %>% 
    # transform the dataframe into tsibble format
    as_tsibble(index = DATE, regular = F) %>% # because every quarter not on the same date, regular is set to FALSE
    index_by(qtr = ~ yearquarter(.))
```

Alternatively, `fable` is another package that can handle time-series data

## Data Assumptions

### Sample Autocorrelation Function (ACF)

Check for autocorrelation

```{r}
acf(gdp$GDP, plot=TRUE, main="Autocorrelation")

# partial autocorrelation
pacf(gdp$GDP, plot=TRUE, main="Autocorrelation")
```

From the PACF we see that most of the auto-correlation can be attributed to the first lag

### Stationarity

When p

#### Augmented Dickey-Fuller (ADF) test

#### Kwiatkowski--Phillips--Schmidt--Shin (KPSS) test

## Autoregressive (AR) {#autoregressive-ar}

$$
Y_t = a + \sum_{i=1}^p \beta_i Y_{t-i} + \epsilon_t
$$

where

-   $p$ is the order of the auto regression

```{r}
#load data
# gdp = read.csv("https://github.com/LOST-STATS/lost-stats.github.io/raw/source/Time_Series/Data/GDPC1.csv")


ar_gdp = ar.ols(gdp$GDP) # lag term based on min AIC
ar_gdp
```

## Autoregressive Conditional Heteroskedasticity (ARCH)

From the [Autoregressive (AR)](#autoregressive-ar) model, we relax the assumption of homoscedasticity in the variance of the error term, where it is now a function of the actual sizes of the previous time periods' errors.

This assumption is plausible when the error variance in a time series follows an [Autoregressive (AR)](#autoregressive-ar) model

The general form of an ARCH(q) process is

$$
y_t = \alpha_0 + \sum_{i=1}^q a_i y_{t-i} + \epsilon_t
$$

where

-   $\epsilon_t = \sigma_t z_t$

    -   $z_t$ = stochastic white noise process

    -   $\sigma_t$ = time-dependent standard deviation

$$
\sigma_t^2 = \alpha_0 + \sum_{i=1}^q \alpha_i \epsilon_{t-i}^2
$$

```{r}
library(fGarch)
garchFit( ~ garch(1, 0), data = gdp$GDP, trace = F)
```

## Moving Average (MA) {#moving-average-ma}

Used for processes with serial correlation

A moving average process MA(q) is

$$
y_t = \mu + \sum_{i =0}^q \beta_i \epsilon_{t-i}
$$

where $\epsilon_t \sim N(0, \sigma^2_\epsilon)$

In words, output is a linear combination of current and past values of a stochastic term ($\epsilon_t$)

Properties of MA(1) process

$$
y_t = \mu + \epsilon_t + \beta_1 \epsilon_{t-1}
$$

-   Constant mean: $E(y_t) = E(\mu + \epsilon_t + \beta_1 \epsilon_{t-1}) = \mu$

-   Constant variance $Var(y_t) = Var(\mu + \epsilon_t + \beta_1 \epsilon_{t-1}) = (1 +\beta_1^2) \times \sigma^2_\epsilon$

-   The covariance between $y_t$ and $y_{t-q}$ decreases as $q \to \infty$

```{r}
set.seed(1)
# simulate 
DT = arima.sim(n = 1000, model = list(ma = c(0.1, 0.3, 0.5)))

plot(DT, ylab = "Value")


# Autocorrelation Function
acf(DT, type = "covariance")


set.seed(123)
DT = arima.sim(n = 1000, model = list(ma = c(0.1, 0.3, 0.5)))

# estimate MA(3) model
arima(x = DT, order = c(0,0,3))


#We can also estimate a MA(7) model and see that the ma4, ma5, ma6, and ma7 are close to 0 and insignificant.
arima(x = DT, order = c(0,0,7))


library(fable)
library(tsibble)
library(dplyr)

# convert to tsibble
DT = DT %>%
  as_tsibble()


# Automatic estimation
DT %>%
  as_tsibble() %>%
  model(arima = ARIMA(value)) %>%
  report()


# Manual estimation
DT %>%
  as_tsibble() %>%
  model(arima = ARIMA(value ~ 0 + pdq(0, 0, 3))) %>%
  report()

```

## Autoregressive Moving Average (ARMA) {#autoregressive-moving-average-arma}

-   is the combination of [Autoregressive (AR)](#autoregressive-ar) and [Moving Average (MA)](#moving-average-ma)

An ARMA(p,q) has the form of

$$
y_t = c + \sum_{i=1}^p \beta_i y_{t-1} + \sum_{j = 1}^q \theta_j \epsilon_{t-j} + \epsilon_t
$$

Since ARMA models cannot be estimated with OLS due to the MA part, MLE typically is the first choice to estimate these models.

-   ARMA models can only handle univariate time series, for multiple time series, vector autoregressive is more appropriate

-   ARMA is only appropriate for stationary time series. For non-stationary time series, [Autoregressive Integrated Moving Average (ARIMA)](#autoregressive-integrated-moving-average-arima) is the go-to models

```{r}
library(tidyverse)
library(tsibble)


gdp = read.csv(file.path(getwd(),"data","GDP.csv")) %>% 
    
    mutate(DATE = as.Date(DATE)) %>% 
    # transform the dataframe into tsibble format
    as_tsibble(index = DATE, regular = F) %>% # because every quarter not on the same date, regular is set to FALSE
    index_by(qtr = ~ yearquarter(.)) %>% 
    
    # get the first difference of log gdp 
    mutate(lgdp = log(GDP)) %>% 
    mutate(ldiffgdp = difference(lgdp, lag = 1, differences = 1))

# Estiamte ARMA
arima(gdp$lgdp[2:292], order=c(3,0,1)) # exclude the first observation (due to differencing)


```

## Autoregressive Integrated Moving Average (ARIMA) {#autoregressive-integrated-moving-average-arima}

-   is the combination of [Autoregressive (AR)](#autoregressive-ar) and [Moving Average (MA)](#moving-average-ma) and unit roots

```{r}
# Estiamte ARIMA
arima(gdp$lgdp[2:292]) # exclude the first observation (due to differencing)

# to auto-select p, q, and d
library(forecast)
auto_mod <- auto.arima(gdp$lgdp[2:292])
```

## Generalized Autoregressive Conditional Heteroskedasticity (GARCH)

-   Assume when variance error is serially auto-correlated (i.e., following an auto-regressive moving average process)

-   Usually used to predict the volatility of returns, especially for assets with clustered period of volatility

```{r, warning=FALSE}
library(fGarch)


garch_mod <- garchFit(GDP ~garch(1,1), data = gdp, trace = F) 
summary(garch_mod)
```

## Autoregressive Lag (ADL)

-   Adding other variables and lags of the variable of interest, we have the autoregressive model

ADL(p,q) model is an autoregressive distributed lag model with p lags of $Y_t$ and q lags of $X_t$

$$
Y_t = \beta_0 + \sum_{i = 1}^p \beta_i Y_{t-i} + \sum_{j = 1}^q \delta_j X_{t-q} + u_t
$$

In words, the time series $Y_t$ is a linear function of $p$ of its lagged values and $q$ lags of $X_t$ time series

Properties:

-   $E(u_t | Y_{t-1}, \dots, X_{t-1}, \dots) = 0$

Simulate ADL data (simulation from the [LOST book](https://lost-stats.github.io/Time_Series/Granger_Causality.html))

```{r}
# set seed
set.seed(1)

# Simulate error
n     <- 100 # Sample size
rho   <- 0.5 # Correlation between Y errors
coe   <- 1.2 # Coefficient of X in model Y
alpha <- 0.5 # Intercept of the model Y

# Function to create the error of Y
ARsim2 <- function(rho, first, serieslength, distribution) {
    if (distribution == "runif") {
        a <- runif(serieslength, min = 0, max = 1)
    }
    else {
        a <- rnorm(serieslength, 0, 1)
    }
    Y <- first
    for (i in (length(rho) + 1):serieslength) {
        Y[i] <- rho * Y[i - 1] + (sqrt(1 - (rho ^ 2))) * a[i]
    }
    return(Y)
}

# Error for Y model
error <- ARsim2(rho, c(0, 0), n, "rnorm")

# times series X (simulation)
X <- arima.sim(list(order = c(1, 0, 0), ar = c(0.2)), n)

# times series Y (simulation)
Y <- NULL
for (i in 2:n) {
    Y[i] <- alpha + (coe * X[i - 1]) + error[i]
}

data <- as.data.frame(cbind(1:n,X,as.ts(Y)))
colnames(data) <- c("time", "X","Y")
```

Plot data

```{r}
graphdata <- data[2:n, ] %>%
    pivot_longer(cols = -c(time),
                 names_to = "variable",
                 values_to = "value")

ggplot(graphdata, aes(x = time, y = value, group = variable)) +
    geom_line(aes(color = variable), size = 0.7) +
    scale_color_manual(values = c("#00AFBB", "#E7B800")) +
    theme_minimal() +
    labs(title = "Simulated ADL models") +
    theme(text = element_text(size = 15))
```

Stationarity

```{r}
library(tseries)

# Augmented Dickeyâ€“Fuller Test
adf.test(X, k=3)
adf.test(na.omit(Y), k=3)
```

If we have stationarity time series, we do not need to do any transformation (e.g., differencing or log). In this example, we reject the null hypothesis (i.e., non-stationary time series), and conclude that we do have stationary time series.

### Granger causality test

-   test whether the signal of the the first variable is a good predictor of the other variable.

    -   For example, the variable $X$ Granger-cause $Y$ when $Y$ can be predicted from the lag of $X$ and $Y$ than from the lag of $Y$ only [@pierce1979r].

-   For non-parametric Granger causality test, refer to [@candelon2016]

-   More extensions: [@diks2020]

Check [Stationarity] assumptions for the two time series (consider using log or differences if you reject this assumption)

Estimate these two models

$$
y_t = \alpha + \sum_{j = 1}^p \beta_j y_{t-j} + \sum_{j=1}^r \theta_j x_{t-j} + \epsilon_t \\
x_t = \alpha^* + \sum_{j=1}^p \beta_j^* y_{t-j} + \sum_{j=1}^r \theta_j^* x_{t-j} + \epsilon_t
$$

assuming $E(\epsilon_t|\mathcal{F}_{t-1})=0$ where $\mathcal{F}_{t-1}$ contains information of $x$ and $y$ up to time $t-1$

Considering the first case (whether lags of $X$ is predictive of $Y$)

F-test to determine significance

$$
H_0: \theta_j = 0 \\
H_1: \theta_j \neq 0
$$

In words,

-   H0: The lags of $x$ is not predictive of $y$ above the lags of $y$

-   H1: At least one lag of $x$ provides additional info

If one rejects the null hypothesis, we say $x_t$ helps predict $y_t$ (i.e., $X$ Granger-causes $Y$)

```{r}
library(lmtest)

# another dataset that can be used is 
# data("ChickEgg")

# Y ~ X
grangertest(Y ~ X, order = 2, data = data)

# X ~ Y
grangertest(X ~ Y, order = 2, data = data)
```

$X$ Granger-causes $Y$, but $Y$ does not Granger-cause $X$

Notes:

-   Model 1 is the unrestricted model with the Granger-causal terms

-   Model 2 is the restricted model without the Granger-causal terms

To choose the number of lags, we have to make tradeoff between bias and power.

-   Too few lags, the test will be biased due to lack of residual autocorrelation

-   To many lags, we might reject the null hypothesis due to spurious correlation

Conventionally, we can vary the number of lags and compare AIC and BIC for each lag length and pick the one with lowest AIC or BIC.

Due to convenience reason, `grangertest` in R use the number of lags for both $X$ and $Y$. However, there is no theoretical basis, and we should compare different combinations for different number of lags for $X$ and $Y$.

Since this test is model dependent, omitted variables bias is a big concern. Hence, one should regress on a matrix of $\mathbf{X}$ instead of using only pair-wise as in `grangertest`

Hence, `bruceR` [package](https://psychbruce.github.io/bruceR/) might be better

```{r}
library(bruceR)

# multivariate
library(vars)
data(Canada)
VARselect(Canada)

# choose 3 from VARselect
vm = VAR(Canada, p=3)

granger_causality(vm)

# bivariate (similar to grangertest)
# granger_test()
```

#### Non-linear Granger

Non-linear Granger causality test using artificial neural networks

Model 1: using the target time series (ts1) to model an artificial neural network

Model: 2: using the two time series (ts1, ts2) to model another artificial neural network

H0: The second time series does not predict the first time series

see [@hmamouche2020] for details

```{r}
library(NlinTS)

library (timeSeries) # to extract time series
library (NlinTS)
data = LPP2005REC
model = nlin_causality.test (data[, 1], data[, 2], 2, c(2), c(4), 50, 0.01, "sgd", 30, TRUE, 5)
model$summary()
```

We cannot reject the null hypothesis, which means that the second times series does not predict the first one.

#### Non-parametric Granger

Additionally, non-parametric wavelet Granger causality is suitable for multiple time horizons

See[@li2021] for an example in finance

## Distributed Lag Models

A general distributed lag model

$$
Y_t = \alpha + \sum_{s = 0}^\infty \beta_s X_{t-s} + \epsilon_t
$$

-   $\beta$ are lag weights

-   Short-run (Impact) Effect of $X$ is $\beta_0$

-   Cumulative effect at one lag: $\beta_0 + \beta_1$

-   Long-run Cumulative (Equilibrium) Effect of $X$ is $\beta = \sum_{s = 0}^\infty \beta_s < \infty$

Define the lag weights as $w_s = \frac{\beta_s}{\beta}, s = 0, 1, \dots$ where $\sum_{s = 0}^{\infty} w_s = 1$

The model becomes

$$
Y_t = \alpha + \beta \sum_{s = 0}^{\infty} w_s X_{t-s} + \epsilon_t
$$

Assume $w_s$'s have the same sign and $|w_s| <1$, then the period of adjustment to a new equilibrium is

-   Median Lag The smallest number of lags $m$ such that $\sum_{s = 0}^{m -1} w_s = 0.5$

-   Mean Lag weighted average of lags: $\sum_{s= 0}^{\infty} s w_s$

Usually the assumption that $s$ is non-negative is reasonable (i.e., $y_t$ does not depend on future $x$). But if consumers change their current behaviors in anticipation of future change (e.g., laws), this assumption might break down.

This model is similar to the [Autoregressive Moving Average (ARMA)](#autoregressive-moving-average-arma) process, where the lag polynomial is on the $X$ instead of the error process $\epsilon$

**Estimation**

-   Since an infinite number of $\beta$ cannot be estimated, we have to truncate the equation to finite lag length $q$

    -   The choice of $q$ is the difficult part

-   Alternatively, the lag distribution can gradually decay to 0.

For a finite time lag model, we have

$$
y_t = \alpha + \sum_{s=0}^q \beta_s x_{t-s} + u_t
$$

**Interpretation**

The marginal effect of each lag weight is

$$
\frac{\partial y_{t+s}}{\partial x_t} = \frac{\partial y_t}{x_{t-s}} = \beta_s
$$

The above equality depends on the assumption that correlation between $y$ and $x$ is stationary.

### Finite Distributed Lag

#### Unrestricted Distributed Lags

Advantage

-   can be estimated using GLS

Disadvantages

-   Inconsistent coefficient estimates (unreliable SE) when $x$ is stationary but highly auto-correlated

-   need a large sample (each time we want to estimate one more lag term, we lose 1 df for the term and 1 df because we reduce the sample by 1 period).

Hence, it's good when

-   lag weights decline to 0 quickly

-   regressor is not highly auto-correlated

-   sample is long (in time) relative to the length of the lag distribution.

#### Restricted Finite Distributed Lags

-   Imposing smoothness on the lag weights $\beta_s$ as a function of $s$

-   can decrease number of parameters to be estimated

-   Restrict the function so that each lag is smaller than its predecessor.

One possibility: Linearly declining lag weights

$$
\beta_s = \frac{q+1-s}{q+1} \beta_0
$$

where $s = 1, \dots, q$

Another possibility: Symmetric "tent" shape weights where the lag weight increases linearly to a peak at lag $m$ and decline to 0.

$$
\beta_s = \beta_m (1 - \frac{|m-s|}{m + 1})
$$

where $s = 0, \dots, 2m$

Common approach: polynomial distributed lag [@almon1965] where the lag coefficients are on a quadratic function

$$
\beta_s = \beta_0 + \beta_1 s + \beta_2 s^2
$$

where $s = 0,\dots, q$

To have more regularity, we can restrict $\beta_{q+1} = 0$ so that the quadratic lag distribution converges to 0.

We can also raise the function to any polynomial term that you want, but there will be less smoothness in the the lag distribution and more parameters to be estimated.

### Lagged Dependent Variables

#### Koyck Lag Models

-   Also known as Geometric Lag Models

-   Based on [@boschan1956]

$$
\begin{aligned}
y_t &= \delta + \phi_1 y_{t-1} + \theta_0 x_t + \epsilon_t\\
(1- \phi_1 L)y_t &= \delta + \theta_0 x_t + \epsilon_t \\
y_t &= \frac{\delta}{1- \phi_1 L} + \frac{\theta_0}{1- \phi_1 L} + \frac{1}{1- \phi_1 L} \epsilon_t
\end{aligned}
$$

An infinite distributed lag version of the Koyck function:

$$
y_t = \frac{\delta}{1-\phi_1} + \theta_0 \sum_{s=0}^{\infty} \phi_1^s x_{t-s} + \sum_{s=0}^\infty \phi_1^s \epsilon_{t-s}
$$

where

-   $|\phi_1 |< 1$

And we can regression the function considering

-   $\alpha = \frac{\delta}{1-\phi_1}$

-   $\beta_s = \theta_0 \phi_1^s$

The dynamic marginal effects of $x$ on $y$ are

$$
\frac{\partial y_{t+s}}{x_t} = \beta_s = \theta_0 \phi_1^s
$$

and the long-run cumulative effect is

$$
\theta_0 \sum_{s=0}^\infty \phi_1^s = \frac{\theta_0}{1 - \phi_1}
$$

Problems with this model

-   Different explanatory variables cannot have different exponential rate $\phi_1$ (e.g., the marginal effect of $z$ on $y$ would be $\lambda_0 \phi_1^s$ where $\lambda_0$ is the associated coefficient of $z$ in the first equation)

-   Since having the lagged dependent variable as a regressor cannot satisfy strict exogeneity assumption (even weak exogeneity is questionable), consistent estimation of the coefficients is questionable

    -   To fix this serially correlated error term, GLS should be used (with the Hildreth-Lu estimation)

#### Autoregressive Lags

The Koyck lag with more than one lag of the dependent variable can be written as the general autoregressive lag model [Autoregressive (AR)](#autoregressive-ar) (p):

$$
\phi(L)y_t = \delta + \theta_0 x_t + \epsilon_t
$$

where $\phi(L)$ is a p-order polynomial lag operator, and $|\phi_1|<1$ to guarantee the roots of $\phi(L)$ lie outside the unit circle)

#### Autoregressive Distributed Lags (ARDL)

With multiple lags of $x$, we have an even more general case of the Koyck lag function

$$
\phi(L)y_t = \delta + \theta(L) x_t + \epsilon_t
$$

where

-   $\phi(L)$ is a p-order polynomial lag operator

-   $|\phi_1|<1$ to guarantee the roots of $\phi(L)$ lie outside the unit circle)

-   $\theta(L)$ is an q-order polynomial.

ARDL is also known as the rational lag because it can be represented by the ratio of two finite lag polynomials, analogous to that rational numbers can be repeated as the ratio of two integers).

The difference between ARDL and [Autoregressive Moving Average (ARMA)](#autoregressive-moving-average-arma) is that it the lag structure is applied to $x$ instead of $\epsilon$

### Choosing the lag length

-   Statistical significance

    -   Add lag sequentially

    -   Remove lag sequentially

-   Information criteria; AIC, SBIC

-   Residual autocorrelation: add lags until residuals look like white noise, then test the residuals using Breusch-Godfrey LM test or Box-Ljung Q test.

## Unobserved Component Models

The UCM is proposed by [@harvey1990] to perform decomposition on a time series into various components:

-   Trend: natural tendency of a series to increase or decrease overtime. UCM can model trend either by

    -   a random walk assuming the trend remains roughly constant over the observed window

    -   being a locally linear trend assuming the trend either has an upward or downward slope

-   Seasonality: consistent seasonal pattern

-   Cycle: rise and fall that is not of fixed period

-   Regression effects due to predictor series

$$
Y_t = \mu_t + \gamma_t + \phi_t + \sum_{i = 1}^m \beta_i X_{it} + \epsilon_t
$$

where

-   $\mu_t$ = time varying mean (level/trend)

-   $\gamma_t$ = seasonality (periodic component)

-   $\phi_t$ = cycle

-   $\sum_{i=1}^m \beta_i X_{it}$ = effect of predictors (i.e., contribution of predictors with fixed or time-varying regression coefficients)

Example by [the package's authors](https://cran.r-project.org/web/packages/rucm/vignettes/rucm_vignettes.html)

```{r}
library(rucm)

# univariate model
modelNile <-
    ucm(
        formula = Nile ~ 0,
        data = Nile,
        level = TRUE
    )
modelNile

plot(Nile, ylab = "Flow of Nile")
lines(modelNile$s.level, col = "blue")
legend(
    "topright",
    legend = c("Observed flow", "S_level"),
    col = c("black", "blue"),
    lty = 1
)

# for prediction
predict(modelNile$model, n.ahead = 12)
```

## State Space Models

Readings

-   [@petris2011]
